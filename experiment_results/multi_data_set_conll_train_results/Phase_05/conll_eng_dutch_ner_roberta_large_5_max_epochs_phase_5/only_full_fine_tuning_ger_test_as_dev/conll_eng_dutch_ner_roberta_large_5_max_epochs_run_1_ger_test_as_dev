2023-06-26 23:09:36,955 ----------------------------------------------------------------------------------------------------
2023-06-26 23:09:36,958 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-26 23:09:36,958 ----------------------------------------------------------------------------------------------------
2023-06-26 23:09:36,959 Corpus: "MultiCorpus: 31080 train + 3160 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-26 23:09:36,959 ----------------------------------------------------------------------------------------------------
2023-06-26 23:09:36,959 Parameters:
2023-06-26 23:09:36,959  - learning_rate: "0.000005"
2023-06-26 23:09:36,959  - mini_batch_size: "4"
2023-06-26 23:09:36,959  - patience: "3"
2023-06-26 23:09:36,959  - anneal_factor: "0.5"
2023-06-26 23:09:36,959  - max_epochs: "5"
2023-06-26 23:09:36,959  - shuffle: "True"
2023-06-26 23:09:36,959  - train_with_dev: "False"
2023-06-26 23:09:36,959  - batch_growth_annealing: "False"
2023-06-26 23:09:36,959 ----------------------------------------------------------------------------------------------------
2023-06-26 23:09:36,959 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_5_max_epochs_run_1_ger_test_as_dev"
2023-06-26 23:09:36,959 ----------------------------------------------------------------------------------------------------
2023-06-26 23:09:36,959 Device: cuda:3
2023-06-26 23:09:36,959 ----------------------------------------------------------------------------------------------------
2023-06-26 23:09:36,959 Embeddings storage mode: none
2023-06-26 23:09:36,959 ----------------------------------------------------------------------------------------------------
2023-06-26 23:12:38,883 epoch 1 - iter 777/7770 - loss 1.68197009 - samples/sec: 17.09 - lr: 0.000001
2023-06-26 23:15:39,892 epoch 1 - iter 1554/7770 - loss 1.05771063 - samples/sec: 17.17 - lr: 0.000002
2023-06-26 23:18:33,659 epoch 1 - iter 2331/7770 - loss 0.84870425 - samples/sec: 17.89 - lr: 0.000003
2023-06-26 23:21:41,104 epoch 1 - iter 3108/7770 - loss 0.70104939 - samples/sec: 16.58 - lr: 0.000004
2023-06-26 23:24:37,022 epoch 1 - iter 3885/7770 - loss 0.60609215 - samples/sec: 17.67 - lr: 0.000005
2023-06-26 23:27:35,044 epoch 1 - iter 4662/7770 - loss 0.55248289 - samples/sec: 17.46 - lr: 0.000005
2023-06-26 23:30:25,971 epoch 1 - iter 5439/7770 - loss 0.51484412 - samples/sec: 18.19 - lr: 0.000005
2023-06-26 23:33:10,925 epoch 1 - iter 6216/7770 - loss 0.48032021 - samples/sec: 18.85 - lr: 0.000005
2023-06-26 23:36:01,933 epoch 1 - iter 6993/7770 - loss 0.45314789 - samples/sec: 18.18 - lr: 0.000005
2023-06-26 23:38:57,037 epoch 1 - iter 7770/7770 - loss 0.42752078 - samples/sec: 17.75 - lr: 0.000004
2023-06-26 23:38:57,038 ----------------------------------------------------------------------------------------------------
2023-06-26 23:38:57,038 EPOCH 1 done: loss 0.4275 - lr 0.000004
2023-06-26 23:39:52,011 Evaluating as a multi-label problem: False
2023-06-26 23:39:52,039 DEV : loss 0.26254498958587646 - f1-score (micro avg)  0.7393
2023-06-26 23:42:25,961 Evaluating as a multi-label problem: False
2023-06-26 23:42:26,016 TEST : loss 0.15720760822296143 - f1-score (micro avg)  0.8698
2023-06-26 23:42:26,164 BAD EPOCHS (no improvement): 4
2023-06-26 23:42:26,166 ----------------------------------------------------------------------------------------------------
2023-06-26 23:45:23,226 epoch 2 - iter 777/7770 - loss 0.22089019 - samples/sec: 17.56 - lr: 0.000004
2023-06-26 23:48:15,672 epoch 2 - iter 1554/7770 - loss 0.22257605 - samples/sec: 18.03 - lr: 0.000004
2023-06-26 23:51:23,362 epoch 2 - iter 2331/7770 - loss 0.22715719 - samples/sec: 16.56 - lr: 0.000004
2023-06-26 23:54:22,740 epoch 2 - iter 3108/7770 - loss 0.22679754 - samples/sec: 17.33 - lr: 0.000004
2023-06-26 23:57:25,481 epoch 2 - iter 3885/7770 - loss 0.22409347 - samples/sec: 17.01 - lr: 0.000004
2023-06-27 00:00:18,614 epoch 2 - iter 4662/7770 - loss 0.22313736 - samples/sec: 17.96 - lr: 0.000004
2023-06-27 00:03:04,366 epoch 2 - iter 5439/7770 - loss 0.22106776 - samples/sec: 18.76 - lr: 0.000004
2023-06-27 00:06:09,907 epoch 2 - iter 6216/7770 - loss 0.21882160 - samples/sec: 16.76 - lr: 0.000004
2023-06-27 00:09:03,508 epoch 2 - iter 6993/7770 - loss 0.21672028 - samples/sec: 17.91 - lr: 0.000003
2023-06-27 00:11:56,097 epoch 2 - iter 7770/7770 - loss 0.21535313 - samples/sec: 18.01 - lr: 0.000003
2023-06-27 00:11:56,099 ----------------------------------------------------------------------------------------------------
2023-06-27 00:11:56,099 EPOCH 2 done: loss 0.2154 - lr 0.000003
2023-06-27 00:12:50,657 Evaluating as a multi-label problem: False
2023-06-27 00:12:50,682 DEV : loss 0.23432596027851105 - f1-score (micro avg)  0.7627
2023-06-27 00:15:18,126 Evaluating as a multi-label problem: False
2023-06-27 00:15:18,193 TEST : loss 0.09379659593105316 - f1-score (micro avg)  0.9272
2023-06-27 00:15:18,359 BAD EPOCHS (no improvement): 4
2023-06-27 00:15:18,362 ----------------------------------------------------------------------------------------------------
2023-06-27 00:18:11,270 epoch 3 - iter 777/7770 - loss 0.18870137 - samples/sec: 17.98 - lr: 0.000003
2023-06-27 00:21:19,016 epoch 3 - iter 1554/7770 - loss 0.18820976 - samples/sec: 16.56 - lr: 0.000003
2023-06-27 00:24:09,306 epoch 3 - iter 2331/7770 - loss 0.18852732 - samples/sec: 18.26 - lr: 0.000003
2023-06-27 00:27:13,116 epoch 3 - iter 3108/7770 - loss 0.18919797 - samples/sec: 16.91 - lr: 0.000003
2023-06-27 00:30:05,649 epoch 3 - iter 3885/7770 - loss 0.18899396 - samples/sec: 18.02 - lr: 0.000003
2023-06-27 00:32:46,565 epoch 3 - iter 4662/7770 - loss 0.18692784 - samples/sec: 19.32 - lr: 0.000003
2023-06-27 00:35:51,765 epoch 3 - iter 5439/7770 - loss 0.18679698 - samples/sec: 16.79 - lr: 0.000003
2023-06-27 00:38:43,766 epoch 3 - iter 6216/7770 - loss 0.18563296 - samples/sec: 18.07 - lr: 0.000002
2023-06-27 00:41:41,050 epoch 3 - iter 6993/7770 - loss 0.18581325 - samples/sec: 17.54 - lr: 0.000002
2023-06-27 00:44:34,504 epoch 3 - iter 7770/7770 - loss 0.18559639 - samples/sec: 17.92 - lr: 0.000002
2023-06-27 00:44:34,506 ----------------------------------------------------------------------------------------------------
2023-06-27 00:44:34,506 EPOCH 3 done: loss 0.1856 - lr 0.000002
2023-06-27 00:45:35,086 Evaluating as a multi-label problem: False
2023-06-27 00:45:35,113 DEV : loss 0.29609549045562744 - f1-score (micro avg)  0.7495
2023-06-27 00:48:02,078 Evaluating as a multi-label problem: False
2023-06-27 00:48:02,133 TEST : loss 0.09876400977373123 - f1-score (micro avg)  0.9309
2023-06-27 00:48:02,282 BAD EPOCHS (no improvement): 4
2023-06-27 00:48:02,284 ----------------------------------------------------------------------------------------------------
2023-06-27 00:51:02,436 epoch 4 - iter 777/7770 - loss 0.18043947 - samples/sec: 17.26 - lr: 0.000002
2023-06-27 00:53:52,022 epoch 4 - iter 1554/7770 - loss 0.17814719 - samples/sec: 18.33 - lr: 0.000002
2023-06-27 00:56:55,445 epoch 4 - iter 2331/7770 - loss 0.17228192 - samples/sec: 16.95 - lr: 0.000002
2023-06-27 00:59:45,766 epoch 4 - iter 3108/7770 - loss 0.17125699 - samples/sec: 18.25 - lr: 0.000002
2023-06-27 01:02:40,501 epoch 4 - iter 3885/7770 - loss 0.17250538 - samples/sec: 17.79 - lr: 0.000002
2023-06-27 01:05:41,736 epoch 4 - iter 4662/7770 - loss 0.17398731 - samples/sec: 17.15 - lr: 0.000002
2023-06-27 01:08:31,440 epoch 4 - iter 5439/7770 - loss 0.17474602 - samples/sec: 18.32 - lr: 0.000001
2023-06-27 01:11:33,852 epoch 4 - iter 6216/7770 - loss 0.17435038 - samples/sec: 17.04 - lr: 0.000001
2023-06-27 01:14:26,207 epoch 4 - iter 6993/7770 - loss 0.17409014 - samples/sec: 18.04 - lr: 0.000001
2023-06-27 01:17:18,954 epoch 4 - iter 7770/7770 - loss 0.17420510 - samples/sec: 18.00 - lr: 0.000001
2023-06-27 01:17:18,957 ----------------------------------------------------------------------------------------------------
2023-06-27 01:17:18,957 EPOCH 4 done: loss 0.1742 - lr 0.000001
2023-06-27 01:18:23,125 Evaluating as a multi-label problem: False
2023-06-27 01:18:23,152 DEV : loss 0.29223835468292236 - f1-score (micro avg)  0.7592
2023-06-27 01:20:47,311 Evaluating as a multi-label problem: False
2023-06-27 01:20:47,367 TEST : loss 0.09410011023283005 - f1-score (micro avg)  0.9332
2023-06-27 01:20:47,511 BAD EPOCHS (no improvement): 4
2023-06-27 01:20:47,513 ----------------------------------------------------------------------------------------------------
2023-06-27 01:23:39,134 epoch 5 - iter 777/7770 - loss 0.16692645 - samples/sec: 18.11 - lr: 0.000001
2023-06-27 01:26:40,467 epoch 5 - iter 1554/7770 - loss 0.16457421 - samples/sec: 17.14 - lr: 0.000001
2023-06-27 01:29:29,903 epoch 5 - iter 2331/7770 - loss 0.16011669 - samples/sec: 18.35 - lr: 0.000001
2023-06-27 01:32:23,182 epoch 5 - iter 3108/7770 - loss 0.16089397 - samples/sec: 17.94 - lr: 0.000001
2023-06-27 01:35:20,435 epoch 5 - iter 3885/7770 - loss 0.16125416 - samples/sec: 17.54 - lr: 0.000001
2023-06-27 01:38:07,051 epoch 5 - iter 4662/7770 - loss 0.16117727 - samples/sec: 18.66 - lr: 0.000000
2023-06-27 01:41:07,571 epoch 5 - iter 5439/7770 - loss 0.16108487 - samples/sec: 17.22 - lr: 0.000000
2023-06-27 01:43:56,809 epoch 5 - iter 6216/7770 - loss 0.16079477 - samples/sec: 18.37 - lr: 0.000000
2023-06-27 01:46:54,307 epoch 5 - iter 6993/7770 - loss 0.16100486 - samples/sec: 17.51 - lr: 0.000000
2023-06-27 01:49:54,703 epoch 5 - iter 7770/7770 - loss 0.16136955 - samples/sec: 17.23 - lr: 0.000000
2023-06-27 01:49:54,705 ----------------------------------------------------------------------------------------------------
2023-06-27 01:49:54,705 EPOCH 5 done: loss 0.1614 - lr 0.000000
2023-06-27 01:50:48,427 Evaluating as a multi-label problem: False
2023-06-27 01:50:48,453 DEV : loss 0.30744555592536926 - f1-score (micro avg)  0.7541
2023-06-27 01:53:11,478 Evaluating as a multi-label problem: False
2023-06-27 01:53:11,533 TEST : loss 0.09729119390249252 - f1-score (micro avg)  0.9352
2023-06-27 01:53:11,665 BAD EPOCHS (no improvement): 4
2023-06-27 01:53:22,471 ----------------------------------------------------------------------------------------------------
2023-06-27 01:53:22,474 Testing using last state of model ...
2023-06-27 01:56:10,696 Evaluating as a multi-label problem: False
2023-06-27 01:56:10,752 0.9296	0.9409	0.9352	0.9077
2023-06-27 01:56:10,752 
Results:
- F-score (micro) 0.9352
- F-score (macro) 0.9317
- Accuracy 0.9077

By class:
              precision    recall  f1-score   support

         PER     0.9758    0.9794    0.9776      2715
         ORG     0.8976    0.9442    0.9203      2543
         LOC     0.9449    0.9345    0.9397      2442
        MISC     0.8889    0.8894    0.8891      1889

   micro avg     0.9296    0.9409    0.9352      9589
   macro avg     0.9268    0.9368    0.9317      9589
weighted avg     0.9301    0.9409    0.9353      9589

2023-06-27 01:56:10,752 ----------------------------------------------------------------------------------------------------
2023-06-27 01:56:10,752 ----------------------------------------------------------------------------------------------------
2023-06-27 01:57:51,319 Evaluating as a multi-label problem: False
2023-06-27 01:57:51,352 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-27 01:57:51,352 0.9324	0.934	0.9332	0.9139
2023-06-27 01:57:51,352 ----------------------------------------------------------------------------------------------------
2023-06-27 01:58:53,702 Evaluating as a multi-label problem: False
2023-06-27 01:58:53,737 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-27 01:58:53,737 0.9277	0.9455	0.9365	0.9034
