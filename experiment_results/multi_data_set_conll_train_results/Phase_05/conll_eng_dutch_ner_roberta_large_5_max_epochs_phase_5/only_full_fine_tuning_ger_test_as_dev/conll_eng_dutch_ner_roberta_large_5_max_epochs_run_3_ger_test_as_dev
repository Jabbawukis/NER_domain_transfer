2023-06-27 04:49:58,586 ----------------------------------------------------------------------------------------------------
2023-06-27 04:49:58,589 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-27 04:49:58,590 ----------------------------------------------------------------------------------------------------
2023-06-27 04:49:58,590 Corpus: "MultiCorpus: 31080 train + 3160 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-27 04:49:58,590 ----------------------------------------------------------------------------------------------------
2023-06-27 04:49:58,590 Parameters:
2023-06-27 04:49:58,590  - learning_rate: "0.000005"
2023-06-27 04:49:58,590  - mini_batch_size: "4"
2023-06-27 04:49:58,590  - patience: "3"
2023-06-27 04:49:58,590  - anneal_factor: "0.5"
2023-06-27 04:49:58,590  - max_epochs: "5"
2023-06-27 04:49:58,590  - shuffle: "True"
2023-06-27 04:49:58,590  - train_with_dev: "False"
2023-06-27 04:49:58,590  - batch_growth_annealing: "False"
2023-06-27 04:49:58,591 ----------------------------------------------------------------------------------------------------
2023-06-27 04:49:58,591 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_5_max_epochs_run_3_ger_test_as_dev"
2023-06-27 04:49:58,591 ----------------------------------------------------------------------------------------------------
2023-06-27 04:49:58,591 Device: cuda:3
2023-06-27 04:49:58,591 ----------------------------------------------------------------------------------------------------
2023-06-27 04:49:58,591 Embeddings storage mode: none
2023-06-27 04:49:58,591 ----------------------------------------------------------------------------------------------------
2023-06-27 04:53:05,028 epoch 1 - iter 777/7770 - loss 2.07420413 - samples/sec: 16.67 - lr: 0.000001
2023-06-27 04:56:09,907 epoch 1 - iter 1554/7770 - loss 1.24764294 - samples/sec: 16.81 - lr: 0.000002
2023-06-27 04:58:50,329 epoch 1 - iter 2331/7770 - loss 0.98003421 - samples/sec: 19.38 - lr: 0.000003
2023-06-27 05:01:46,435 epoch 1 - iter 3108/7770 - loss 0.79892795 - samples/sec: 17.65 - lr: 0.000004
2023-06-27 05:04:28,341 epoch 1 - iter 3885/7770 - loss 0.68565121 - samples/sec: 19.20 - lr: 0.000005
2023-06-27 05:07:20,811 epoch 1 - iter 4662/7770 - loss 0.61613333 - samples/sec: 18.02 - lr: 0.000005
2023-06-27 05:10:21,326 epoch 1 - iter 5439/7770 - loss 0.57128266 - samples/sec: 17.22 - lr: 0.000005
2023-06-27 05:13:09,201 epoch 1 - iter 6216/7770 - loss 0.52952367 - samples/sec: 18.52 - lr: 0.000005
2023-06-27 05:16:05,160 epoch 1 - iter 6993/7770 - loss 0.49684862 - samples/sec: 17.67 - lr: 0.000005
2023-06-27 05:18:57,746 epoch 1 - iter 7770/7770 - loss 0.46724577 - samples/sec: 18.01 - lr: 0.000004
2023-06-27 05:18:57,747 ----------------------------------------------------------------------------------------------------
2023-06-27 05:18:57,748 EPOCH 1 done: loss 0.4672 - lr 0.000004
2023-06-27 05:19:53,498 Evaluating as a multi-label problem: False
2023-06-27 05:19:53,527 DEV : loss 0.31363460421562195 - f1-score (micro avg)  0.7052
2023-06-27 05:22:48,252 Evaluating as a multi-label problem: False
2023-06-27 05:22:48,306 TEST : loss 0.14583025872707367 - f1-score (micro avg)  0.8703
2023-06-27 05:22:48,455 BAD EPOCHS (no improvement): 4
2023-06-27 05:22:48,457 ----------------------------------------------------------------------------------------------------
2023-06-27 05:25:56,167 epoch 2 - iter 777/7770 - loss 0.22889765 - samples/sec: 16.56 - lr: 0.000004
2023-06-27 05:28:57,197 epoch 2 - iter 1554/7770 - loss 0.22384724 - samples/sec: 17.17 - lr: 0.000004
2023-06-27 05:32:01,956 epoch 2 - iter 2331/7770 - loss 0.22305861 - samples/sec: 16.83 - lr: 0.000004
2023-06-27 05:34:58,658 epoch 2 - iter 3108/7770 - loss 0.22330590 - samples/sec: 17.59 - lr: 0.000004
2023-06-27 05:38:01,672 epoch 2 - iter 3885/7770 - loss 0.22176098 - samples/sec: 16.99 - lr: 0.000004
2023-06-27 05:41:02,244 epoch 2 - iter 4662/7770 - loss 0.22212197 - samples/sec: 17.22 - lr: 0.000004
2023-06-27 05:43:40,943 epoch 2 - iter 5439/7770 - loss 0.21995836 - samples/sec: 19.59 - lr: 0.000004
2023-06-27 05:46:30,237 epoch 2 - iter 6216/7770 - loss 0.21775615 - samples/sec: 18.36 - lr: 0.000004
2023-06-27 05:49:06,234 epoch 2 - iter 6993/7770 - loss 0.21712154 - samples/sec: 19.93 - lr: 0.000003
2023-06-27 05:51:56,722 epoch 2 - iter 7770/7770 - loss 0.21505496 - samples/sec: 18.23 - lr: 0.000003
2023-06-27 05:51:56,724 ----------------------------------------------------------------------------------------------------
2023-06-27 05:51:56,724 EPOCH 2 done: loss 0.2151 - lr 0.000003
2023-06-27 05:53:03,151 Evaluating as a multi-label problem: False
2023-06-27 05:53:03,177 DEV : loss 0.2855377793312073 - f1-score (micro avg)  0.7566
2023-06-27 05:55:52,631 Evaluating as a multi-label problem: False
2023-06-27 05:55:52,684 TEST : loss 0.10085310786962509 - f1-score (micro avg)  0.9215
2023-06-27 05:55:52,829 BAD EPOCHS (no improvement): 4
2023-06-27 05:55:52,831 ----------------------------------------------------------------------------------------------------
2023-06-27 05:58:32,937 epoch 3 - iter 777/7770 - loss 0.18594218 - samples/sec: 19.42 - lr: 0.000003
2023-06-27 06:01:37,328 epoch 3 - iter 1554/7770 - loss 0.18686142 - samples/sec: 16.86 - lr: 0.000003
2023-06-27 06:04:31,798 epoch 3 - iter 2331/7770 - loss 0.18395367 - samples/sec: 17.82 - lr: 0.000003
2023-06-27 06:07:28,327 epoch 3 - iter 3108/7770 - loss 0.18452680 - samples/sec: 17.61 - lr: 0.000003
2023-06-27 06:10:33,530 epoch 3 - iter 3885/7770 - loss 0.18589577 - samples/sec: 16.78 - lr: 0.000003
2023-06-27 06:13:26,353 epoch 3 - iter 4662/7770 - loss 0.18590545 - samples/sec: 17.99 - lr: 0.000003
2023-06-27 06:16:27,999 epoch 3 - iter 5439/7770 - loss 0.18561844 - samples/sec: 17.11 - lr: 0.000003
2023-06-27 06:19:20,370 epoch 3 - iter 6216/7770 - loss 0.18664737 - samples/sec: 18.03 - lr: 0.000002
2023-06-27 06:22:10,281 epoch 3 - iter 6993/7770 - loss 0.18686070 - samples/sec: 18.30 - lr: 0.000002
2023-06-27 06:25:15,193 epoch 3 - iter 7770/7770 - loss 0.18678716 - samples/sec: 16.81 - lr: 0.000002
2023-06-27 06:25:15,195 ----------------------------------------------------------------------------------------------------
2023-06-27 06:25:15,195 EPOCH 3 done: loss 0.1868 - lr 0.000002
2023-06-27 06:26:21,184 Evaluating as a multi-label problem: False
2023-06-27 06:26:21,224 DEV : loss 0.2454446405172348 - f1-score (micro avg)  0.7863
2023-06-27 06:29:06,727 Evaluating as a multi-label problem: False
2023-06-27 06:29:06,781 TEST : loss 0.09197796881198883 - f1-score (micro avg)  0.9314
2023-06-27 06:29:06,936 BAD EPOCHS (no improvement): 4
2023-06-27 06:29:06,938 ----------------------------------------------------------------------------------------------------
2023-06-27 06:31:57,799 epoch 4 - iter 777/7770 - loss 0.16011110 - samples/sec: 18.19 - lr: 0.000002
2023-06-27 06:34:50,584 epoch 4 - iter 1554/7770 - loss 0.16385437 - samples/sec: 17.99 - lr: 0.000002
2023-06-27 06:37:31,012 epoch 4 - iter 2331/7770 - loss 0.16993315 - samples/sec: 19.38 - lr: 0.000002
2023-06-27 06:40:40,021 epoch 4 - iter 3108/7770 - loss 0.17009427 - samples/sec: 16.45 - lr: 0.000002
2023-06-27 06:43:34,418 epoch 4 - iter 3885/7770 - loss 0.16936908 - samples/sec: 17.83 - lr: 0.000002
2023-06-27 06:46:37,287 epoch 4 - iter 4662/7770 - loss 0.16966259 - samples/sec: 17.00 - lr: 0.000002
2023-06-27 06:49:30,167 epoch 4 - iter 5439/7770 - loss 0.17010104 - samples/sec: 17.98 - lr: 0.000001
2023-06-27 06:52:22,508 epoch 4 - iter 6216/7770 - loss 0.16996061 - samples/sec: 18.04 - lr: 0.000001
2023-06-27 06:55:30,871 epoch 4 - iter 6993/7770 - loss 0.17012696 - samples/sec: 16.50 - lr: 0.000001
2023-06-27 06:58:18,711 epoch 4 - iter 7770/7770 - loss 0.17029530 - samples/sec: 18.52 - lr: 0.000001
2023-06-27 06:58:18,713 ----------------------------------------------------------------------------------------------------
2023-06-27 06:58:18,713 EPOCH 4 done: loss 0.1703 - lr 0.000001
2023-06-27 06:59:17,814 Evaluating as a multi-label problem: False
2023-06-27 06:59:17,839 DEV : loss 0.26252663135528564 - f1-score (micro avg)  0.7811
2023-06-27 07:02:05,319 Evaluating as a multi-label problem: False
2023-06-27 07:02:05,374 TEST : loss 0.0965665876865387 - f1-score (micro avg)  0.9322
2023-06-27 07:02:05,526 BAD EPOCHS (no improvement): 4
2023-06-27 07:02:05,529 ----------------------------------------------------------------------------------------------------
2023-06-27 07:04:58,184 epoch 5 - iter 777/7770 - loss 0.16376418 - samples/sec: 18.01 - lr: 0.000001
2023-06-27 07:07:51,102 epoch 5 - iter 1554/7770 - loss 0.16162851 - samples/sec: 17.98 - lr: 0.000001
2023-06-27 07:10:58,599 epoch 5 - iter 2331/7770 - loss 0.16501451 - samples/sec: 16.58 - lr: 0.000001
2023-06-27 07:13:52,820 epoch 5 - iter 3108/7770 - loss 0.16198357 - samples/sec: 17.84 - lr: 0.000001
2023-06-27 07:16:55,106 epoch 5 - iter 3885/7770 - loss 0.16230561 - samples/sec: 17.05 - lr: 0.000001
2023-06-27 07:19:47,848 epoch 5 - iter 4662/7770 - loss 0.16285984 - samples/sec: 18.00 - lr: 0.000000
2023-06-27 07:22:25,674 epoch 5 - iter 5439/7770 - loss 0.16292492 - samples/sec: 19.70 - lr: 0.000000
2023-06-27 07:25:14,710 epoch 5 - iter 6216/7770 - loss 0.16283315 - samples/sec: 18.39 - lr: 0.000000
2023-06-27 07:27:52,559 epoch 5 - iter 6993/7770 - loss 0.16333281 - samples/sec: 19.69 - lr: 0.000000
2023-06-27 07:30:40,404 epoch 5 - iter 7770/7770 - loss 0.16289212 - samples/sec: 18.52 - lr: 0.000000
2023-06-27 07:30:40,406 ----------------------------------------------------------------------------------------------------
2023-06-27 07:30:40,406 EPOCH 5 done: loss 0.1629 - lr 0.000000
2023-06-27 07:31:43,804 Evaluating as a multi-label problem: False
2023-06-27 07:31:43,830 DEV : loss 0.2869538962841034 - f1-score (micro avg)  0.7745
2023-06-27 07:34:21,564 Evaluating as a multi-label problem: False
2023-06-27 07:34:21,617 TEST : loss 0.09822642058134079 - f1-score (micro avg)  0.9341
2023-06-27 07:34:21,775 BAD EPOCHS (no improvement): 4
2023-06-27 07:34:34,401 ----------------------------------------------------------------------------------------------------
2023-06-27 07:34:34,407 Testing using last state of model ...
2023-06-27 07:37:13,437 Evaluating as a multi-label problem: False
2023-06-27 07:37:13,491 0.929	0.9393	0.9341	0.9063
2023-06-27 07:37:13,491 
Results:
- F-score (micro) 0.9341
- F-score (macro) 0.9308
- Accuracy 0.9063

By class:
              precision    recall  f1-score   support

         PER     0.9776    0.9797    0.9787      2715
         ORG     0.8952    0.9367    0.9154      2543
         LOC     0.9437    0.9328    0.9382      2442
        MISC     0.8884    0.8931    0.8907      1889

   micro avg     0.9290    0.9393    0.9341      9589
   macro avg     0.9262    0.9356    0.9308      9589
weighted avg     0.9295    0.9393    0.9343      9589

2023-06-27 07:37:13,491 ----------------------------------------------------------------------------------------------------
2023-06-27 07:37:13,491 ----------------------------------------------------------------------------------------------------
2023-06-27 07:38:58,610 Evaluating as a multi-label problem: False
2023-06-27 07:38:58,637 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-27 07:38:58,637 0.9307	0.9335	0.9321	0.9102
2023-06-27 07:38:58,637 ----------------------------------------------------------------------------------------------------
2023-06-27 07:40:00,032 Evaluating as a multi-label problem: False
2023-06-27 07:40:00,065 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-27 07:40:00,065 0.9279	0.9433	0.9356	0.9037
