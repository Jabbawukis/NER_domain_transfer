2023-06-27 07:07:36,461 ----------------------------------------------------------------------------------------------------
2023-06-27 07:07:36,464 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-27 07:07:36,465 ----------------------------------------------------------------------------------------------------
2023-06-27 07:07:36,465 Corpus: "Corpus: 16093 train + 3160 dev + 5314 test sentences"
2023-06-27 07:07:36,465 ----------------------------------------------------------------------------------------------------
2023-06-27 07:07:36,465 Parameters:
2023-06-27 07:07:36,465  - learning_rate: "0.000005"
2023-06-27 07:07:36,465  - mini_batch_size: "4"
2023-06-27 07:07:36,466  - patience: "3"
2023-06-27 07:07:36,466  - anneal_factor: "0.5"
2023-06-27 07:07:36,466  - max_epochs: "10"
2023-06-27 07:07:36,466  - shuffle: "True"
2023-06-27 07:07:36,466  - train_with_dev: "False"
2023-06-27 07:07:36,466  - batch_growth_annealing: "False"
2023-06-27 07:07:36,466 ----------------------------------------------------------------------------------------------------
2023-06-27 07:07:36,466 Model training base path: "resources/taggers/conll_dutch_ner_roberta_large_run_3_ger_test_as_dev"
2023-06-27 07:07:36,466 ----------------------------------------------------------------------------------------------------
2023-06-27 07:07:36,466 Device: cuda:2
2023-06-27 07:07:36,466 ----------------------------------------------------------------------------------------------------
2023-06-27 07:07:36,466 Embeddings storage mode: none
2023-06-27 07:07:36,466 ----------------------------------------------------------------------------------------------------
2023-06-27 07:09:41,665 epoch 1 - iter 402/4024 - loss 3.33122429 - samples/sec: 12.85 - lr: 0.000000
2023-06-27 07:11:42,532 epoch 1 - iter 804/4024 - loss 2.10849265 - samples/sec: 13.31 - lr: 0.000001
2023-06-27 07:13:49,143 epoch 1 - iter 1206/4024 - loss 1.54069148 - samples/sec: 12.70 - lr: 0.000001
2023-06-27 07:15:56,647 epoch 1 - iter 1608/4024 - loss 1.27101743 - samples/sec: 12.61 - lr: 0.000002
2023-06-27 07:18:04,119 epoch 1 - iter 2010/4024 - loss 1.08685554 - samples/sec: 12.62 - lr: 0.000002
2023-06-27 07:20:10,825 epoch 1 - iter 2412/4024 - loss 0.99830476 - samples/sec: 12.69 - lr: 0.000003
2023-06-27 07:22:20,905 epoch 1 - iter 2814/4024 - loss 0.89064432 - samples/sec: 12.36 - lr: 0.000003
2023-06-27 07:24:25,909 epoch 1 - iter 3216/4024 - loss 0.81185761 - samples/sec: 12.87 - lr: 0.000004
2023-06-27 07:26:38,232 epoch 1 - iter 3618/4024 - loss 0.74755355 - samples/sec: 12.15 - lr: 0.000004
2023-06-27 07:28:46,934 epoch 1 - iter 4020/4024 - loss 0.68950917 - samples/sec: 12.50 - lr: 0.000005
2023-06-27 07:28:48,161 ----------------------------------------------------------------------------------------------------
2023-06-27 07:28:48,161 EPOCH 1 done: loss 0.6893 - lr 0.000005
2023-06-27 07:30:00,214 Evaluating as a multi-label problem: False
2023-06-27 07:30:00,244 DEV : loss 0.20217663049697876 - f1-score (micro avg)  0.7056
2023-06-27 07:31:55,041 Evaluating as a multi-label problem: False
2023-06-27 07:31:55,071 TEST : loss 0.08334410190582275 - f1-score (micro avg)  0.8238
2023-06-27 07:31:55,159 BAD EPOCHS (no improvement): 4
2023-06-27 07:31:55,161 ----------------------------------------------------------------------------------------------------
2023-06-27 07:34:03,905 epoch 2 - iter 402/4024 - loss 0.23902253 - samples/sec: 12.49 - lr: 0.000005
2023-06-27 07:36:10,779 epoch 2 - iter 804/4024 - loss 0.23239209 - samples/sec: 12.68 - lr: 0.000005
2023-06-27 07:38:20,062 epoch 2 - iter 1206/4024 - loss 0.23346484 - samples/sec: 12.44 - lr: 0.000005
2023-06-27 07:40:29,815 epoch 2 - iter 1608/4024 - loss 0.23331709 - samples/sec: 12.39 - lr: 0.000005
2023-06-27 07:42:35,697 epoch 2 - iter 2010/4024 - loss 0.22936592 - samples/sec: 12.78 - lr: 0.000005
2023-06-27 07:44:38,706 epoch 2 - iter 2412/4024 - loss 0.22552423 - samples/sec: 13.07 - lr: 0.000005
2023-06-27 07:46:42,485 epoch 2 - iter 2814/4024 - loss 0.22328687 - samples/sec: 12.99 - lr: 0.000005
2023-06-27 07:48:49,654 epoch 2 - iter 3216/4024 - loss 0.22266765 - samples/sec: 12.65 - lr: 0.000005
2023-06-27 07:50:58,286 epoch 2 - iter 3618/4024 - loss 0.22185070 - samples/sec: 12.50 - lr: 0.000005
2023-06-27 07:53:07,973 epoch 2 - iter 4020/4024 - loss 0.22017385 - samples/sec: 12.40 - lr: 0.000004
2023-06-27 07:53:09,137 ----------------------------------------------------------------------------------------------------
2023-06-27 07:53:09,137 EPOCH 2 done: loss 0.2201 - lr 0.000004
2023-06-27 07:54:20,686 Evaluating as a multi-label problem: False
2023-06-27 07:54:20,714 DEV : loss 0.2742769122123718 - f1-score (micro avg)  0.7693
2023-06-27 07:56:18,627 Evaluating as a multi-label problem: False
2023-06-27 07:56:18,654 TEST : loss 0.06803122907876968 - f1-score (micro avg)  0.9142
2023-06-27 07:56:18,750 BAD EPOCHS (no improvement): 4
2023-06-27 07:56:18,753 ----------------------------------------------------------------------------------------------------
2023-06-27 07:58:25,015 epoch 3 - iter 402/4024 - loss 0.20094149 - samples/sec: 12.74 - lr: 0.000004
2023-06-27 08:00:28,594 epoch 3 - iter 804/4024 - loss 0.19891177 - samples/sec: 13.01 - lr: 0.000004
2023-06-27 08:02:36,875 epoch 3 - iter 1206/4024 - loss 0.19705654 - samples/sec: 12.54 - lr: 0.000004
2023-06-27 08:04:44,515 epoch 3 - iter 1608/4024 - loss 0.19736530 - samples/sec: 12.60 - lr: 0.000004
2023-06-27 08:06:52,790 epoch 3 - iter 2010/4024 - loss 0.19663582 - samples/sec: 12.54 - lr: 0.000004
2023-06-27 08:08:58,840 epoch 3 - iter 2412/4024 - loss 0.19433194 - samples/sec: 12.76 - lr: 0.000004
2023-06-27 08:11:08,180 epoch 3 - iter 2814/4024 - loss 0.19372898 - samples/sec: 12.43 - lr: 0.000004
2023-06-27 08:13:17,050 epoch 3 - iter 3216/4024 - loss 0.19303342 - samples/sec: 12.48 - lr: 0.000004
2023-06-27 08:15:23,714 epoch 3 - iter 3618/4024 - loss 0.19237377 - samples/sec: 12.70 - lr: 0.000004
2023-06-27 08:17:31,078 epoch 3 - iter 4020/4024 - loss 0.19090487 - samples/sec: 12.63 - lr: 0.000004
2023-06-27 08:17:32,303 ----------------------------------------------------------------------------------------------------
2023-06-27 08:17:32,304 EPOCH 3 done: loss 0.1909 - lr 0.000004
2023-06-27 08:18:47,030 Evaluating as a multi-label problem: False
2023-06-27 08:18:47,059 DEV : loss 0.33187222480773926 - f1-score (micro avg)  0.7571
2023-06-27 08:20:44,642 Evaluating as a multi-label problem: False
2023-06-27 08:20:44,668 TEST : loss 0.05603192746639252 - f1-score (micro avg)  0.9381
2023-06-27 08:20:44,763 BAD EPOCHS (no improvement): 4
2023-06-27 08:20:44,765 ----------------------------------------------------------------------------------------------------
2023-06-27 08:22:52,277 epoch 4 - iter 402/4024 - loss 0.17525316 - samples/sec: 12.61 - lr: 0.000004
2023-06-27 08:25:01,785 epoch 4 - iter 804/4024 - loss 0.17700740 - samples/sec: 12.42 - lr: 0.000004
2023-06-27 08:27:06,569 epoch 4 - iter 1206/4024 - loss 0.17701369 - samples/sec: 12.89 - lr: 0.000004
2023-06-27 08:29:07,932 epoch 4 - iter 1608/4024 - loss 0.17786682 - samples/sec: 13.25 - lr: 0.000004
2023-06-27 08:31:10,627 epoch 4 - iter 2010/4024 - loss 0.17994219 - samples/sec: 13.11 - lr: 0.000004
2023-06-27 08:33:16,481 epoch 4 - iter 2412/4024 - loss 0.17754662 - samples/sec: 12.78 - lr: 0.000004
2023-06-27 08:35:17,557 epoch 4 - iter 2814/4024 - loss 0.17872327 - samples/sec: 13.28 - lr: 0.000004
2023-06-27 08:37:21,305 epoch 4 - iter 3216/4024 - loss 0.17988789 - samples/sec: 13.00 - lr: 0.000003
2023-06-27 08:39:25,676 epoch 4 - iter 3618/4024 - loss 0.17922293 - samples/sec: 12.93 - lr: 0.000003
2023-06-27 08:41:26,431 epoch 4 - iter 4020/4024 - loss 0.17819407 - samples/sec: 13.32 - lr: 0.000003
2023-06-27 08:41:27,580 ----------------------------------------------------------------------------------------------------
2023-06-27 08:41:27,580 EPOCH 4 done: loss 0.1781 - lr 0.000003
2023-06-27 08:42:40,264 Evaluating as a multi-label problem: False
2023-06-27 08:42:40,290 DEV : loss 0.3744581937789917 - f1-score (micro avg)  0.7404
2023-06-27 08:44:40,585 Evaluating as a multi-label problem: False
2023-06-27 08:44:40,612 TEST : loss 0.05613360181450844 - f1-score (micro avg)  0.9413
2023-06-27 08:44:40,707 BAD EPOCHS (no improvement): 4
2023-06-27 08:44:40,709 ----------------------------------------------------------------------------------------------------
2023-06-27 08:46:48,106 epoch 5 - iter 402/4024 - loss 0.16792394 - samples/sec: 12.62 - lr: 0.000003
2023-06-27 08:48:53,343 epoch 5 - iter 804/4024 - loss 0.16631335 - samples/sec: 12.84 - lr: 0.000003
2023-06-27 08:51:00,695 epoch 5 - iter 1206/4024 - loss 0.16333235 - samples/sec: 12.63 - lr: 0.000003
2023-06-27 08:53:07,592 epoch 5 - iter 1608/4024 - loss 0.16416782 - samples/sec: 12.67 - lr: 0.000003
2023-06-27 08:55:09,512 epoch 5 - iter 2010/4024 - loss 0.16596660 - samples/sec: 13.19 - lr: 0.000003
2023-06-27 08:57:14,625 epoch 5 - iter 2412/4024 - loss 0.16701282 - samples/sec: 12.85 - lr: 0.000003
2023-06-27 08:59:21,545 epoch 5 - iter 2814/4024 - loss 0.16841821 - samples/sec: 12.67 - lr: 0.000003
2023-06-27 09:01:30,746 epoch 5 - iter 3216/4024 - loss 0.16781150 - samples/sec: 12.45 - lr: 0.000003
2023-06-27 09:03:40,692 epoch 5 - iter 3618/4024 - loss 0.16722777 - samples/sec: 12.38 - lr: 0.000003
2023-06-27 09:05:48,140 epoch 5 - iter 4020/4024 - loss 0.16665330 - samples/sec: 12.62 - lr: 0.000003
2023-06-27 09:05:49,467 ----------------------------------------------------------------------------------------------------
2023-06-27 09:05:49,467 EPOCH 5 done: loss 0.1667 - lr 0.000003
2023-06-27 09:07:00,399 Evaluating as a multi-label problem: False
2023-06-27 09:07:00,427 DEV : loss 0.35326719284057617 - f1-score (micro avg)  0.7532
2023-06-27 09:09:00,367 Evaluating as a multi-label problem: False
2023-06-27 09:09:00,394 TEST : loss 0.05539959296584129 - f1-score (micro avg)  0.9445
2023-06-27 09:09:00,484 BAD EPOCHS (no improvement): 4
2023-06-27 09:09:00,487 ----------------------------------------------------------------------------------------------------
2023-06-27 09:11:11,313 epoch 6 - iter 402/4024 - loss 0.15911409 - samples/sec: 12.29 - lr: 0.000003
2023-06-27 09:13:20,292 epoch 6 - iter 804/4024 - loss 0.16136040 - samples/sec: 12.47 - lr: 0.000003
2023-06-27 09:15:28,001 epoch 6 - iter 1206/4024 - loss 0.16197962 - samples/sec: 12.59 - lr: 0.000003
2023-06-27 09:17:33,394 epoch 6 - iter 1608/4024 - loss 0.16206731 - samples/sec: 12.83 - lr: 0.000003
2023-06-27 09:19:41,789 epoch 6 - iter 2010/4024 - loss 0.16153920 - samples/sec: 12.53 - lr: 0.000003
2023-06-27 09:21:49,375 epoch 6 - iter 2412/4024 - loss 0.16084782 - samples/sec: 12.61 - lr: 0.000002
2023-06-27 09:23:59,387 epoch 6 - iter 2814/4024 - loss 0.16127538 - samples/sec: 12.37 - lr: 0.000002
2023-06-27 09:26:05,972 epoch 6 - iter 3216/4024 - loss 0.16013276 - samples/sec: 12.70 - lr: 0.000002
2023-06-27 09:28:13,850 epoch 6 - iter 3618/4024 - loss 0.16040277 - samples/sec: 12.58 - lr: 0.000002
2023-06-27 09:30:19,526 epoch 6 - iter 4020/4024 - loss 0.15904564 - samples/sec: 12.80 - lr: 0.000002
2023-06-27 09:30:20,776 ----------------------------------------------------------------------------------------------------
2023-06-27 09:30:20,776 EPOCH 6 done: loss 0.1590 - lr 0.000002
2023-06-27 09:31:36,245 Evaluating as a multi-label problem: False
2023-06-27 09:31:36,273 DEV : loss 0.4613962471485138 - f1-score (micro avg)  0.7165
2023-06-27 09:33:39,563 Evaluating as a multi-label problem: False
2023-06-27 09:33:39,596 TEST : loss 0.055905796587467194 - f1-score (micro avg)  0.9434
2023-06-27 09:33:39,715 BAD EPOCHS (no improvement): 4
2023-06-27 09:33:39,718 ----------------------------------------------------------------------------------------------------
2023-06-27 09:35:46,782 epoch 7 - iter 402/4024 - loss 0.15642154 - samples/sec: 12.66 - lr: 0.000002
2023-06-27 09:37:52,899 epoch 7 - iter 804/4024 - loss 0.15391514 - samples/sec: 12.75 - lr: 0.000002
2023-06-27 09:40:03,004 epoch 7 - iter 1206/4024 - loss 0.15462709 - samples/sec: 12.36 - lr: 0.000002
2023-06-27 09:42:12,237 epoch 7 - iter 1608/4024 - loss 0.15437943 - samples/sec: 12.44 - lr: 0.000002
2023-06-27 09:44:18,916 epoch 7 - iter 2010/4024 - loss 0.15414600 - samples/sec: 12.70 - lr: 0.000002
2023-06-27 09:46:26,756 epoch 7 - iter 2412/4024 - loss 0.15165733 - samples/sec: 12.58 - lr: 0.000002
2023-06-27 09:48:29,971 epoch 7 - iter 2814/4024 - loss 0.15109824 - samples/sec: 13.05 - lr: 0.000002
2023-06-27 09:50:36,460 epoch 7 - iter 3216/4024 - loss 0.15071192 - samples/sec: 12.71 - lr: 0.000002
2023-06-27 09:52:42,856 epoch 7 - iter 3618/4024 - loss 0.15159685 - samples/sec: 12.72 - lr: 0.000002
2023-06-27 09:54:51,800 epoch 7 - iter 4020/4024 - loss 0.15243079 - samples/sec: 12.47 - lr: 0.000002
2023-06-27 09:54:52,939 ----------------------------------------------------------------------------------------------------
2023-06-27 09:54:52,939 EPOCH 7 done: loss 0.1524 - lr 0.000002
2023-06-27 09:56:06,188 Evaluating as a multi-label problem: False
2023-06-27 09:56:06,216 DEV : loss 0.3709466755390167 - f1-score (micro avg)  0.7693
2023-06-27 09:58:07,198 Evaluating as a multi-label problem: False
2023-06-27 09:58:07,226 TEST : loss 0.05745994299650192 - f1-score (micro avg)  0.9489
2023-06-27 09:58:07,316 BAD EPOCHS (no improvement): 4
2023-06-27 09:58:07,318 ----------------------------------------------------------------------------------------------------
2023-06-27 10:00:13,504 epoch 8 - iter 402/4024 - loss 0.14264614 - samples/sec: 12.74 - lr: 0.000002
2023-06-27 10:02:25,195 epoch 8 - iter 804/4024 - loss 0.14571366 - samples/sec: 12.21 - lr: 0.000002
2023-06-27 10:04:37,425 epoch 8 - iter 1206/4024 - loss 0.14649778 - samples/sec: 12.16 - lr: 0.000002
2023-06-27 10:06:46,773 epoch 8 - iter 1608/4024 - loss 0.14561669 - samples/sec: 12.43 - lr: 0.000001
2023-06-27 10:08:51,332 epoch 8 - iter 2010/4024 - loss 0.14581702 - samples/sec: 12.91 - lr: 0.000001
2023-06-27 10:10:57,831 epoch 8 - iter 2412/4024 - loss 0.14581430 - samples/sec: 12.71 - lr: 0.000001
2023-06-27 10:13:11,289 epoch 8 - iter 2814/4024 - loss 0.14755601 - samples/sec: 12.05 - lr: 0.000001
2023-06-27 10:15:17,255 epoch 8 - iter 3216/4024 - loss 0.14719533 - samples/sec: 12.77 - lr: 0.000001
2023-06-27 10:17:20,871 epoch 8 - iter 3618/4024 - loss 0.14747441 - samples/sec: 13.01 - lr: 0.000001
2023-06-27 10:19:28,162 epoch 8 - iter 4020/4024 - loss 0.14804333 - samples/sec: 12.63 - lr: 0.000001
2023-06-27 10:19:29,506 ----------------------------------------------------------------------------------------------------
2023-06-27 10:19:29,506 EPOCH 8 done: loss 0.1481 - lr 0.000001
2023-06-27 10:20:40,711 Evaluating as a multi-label problem: False
2023-06-27 10:20:40,737 DEV : loss 0.40579959750175476 - f1-score (micro avg)  0.7476
2023-06-27 10:22:44,056 Evaluating as a multi-label problem: False
2023-06-27 10:22:44,083 TEST : loss 0.05527907982468605 - f1-score (micro avg)  0.9442
2023-06-27 10:22:44,173 BAD EPOCHS (no improvement): 4
2023-06-27 10:22:44,176 ----------------------------------------------------------------------------------------------------
2023-06-27 10:24:50,563 epoch 9 - iter 402/4024 - loss 0.15452252 - samples/sec: 12.72 - lr: 0.000001
2023-06-27 10:26:54,929 epoch 9 - iter 804/4024 - loss 0.14946393 - samples/sec: 12.93 - lr: 0.000001
2023-06-27 10:29:01,570 epoch 9 - iter 1206/4024 - loss 0.15028698 - samples/sec: 12.70 - lr: 0.000001
2023-06-27 10:31:07,968 epoch 9 - iter 1608/4024 - loss 0.14855029 - samples/sec: 12.72 - lr: 0.000001
2023-06-27 10:33:15,724 epoch 9 - iter 2010/4024 - loss 0.14883760 - samples/sec: 12.59 - lr: 0.000001
2023-06-27 10:35:20,275 epoch 9 - iter 2412/4024 - loss 0.14714337 - samples/sec: 12.91 - lr: 0.000001
2023-06-27 10:37:27,871 epoch 9 - iter 2814/4024 - loss 0.14696595 - samples/sec: 12.60 - lr: 0.000001
2023-06-27 10:39:32,847 epoch 9 - iter 3216/4024 - loss 0.14609266 - samples/sec: 12.87 - lr: 0.000001
2023-06-27 10:41:39,474 epoch 9 - iter 3618/4024 - loss 0.14584903 - samples/sec: 12.70 - lr: 0.000001
2023-06-27 10:43:40,930 epoch 9 - iter 4020/4024 - loss 0.14582544 - samples/sec: 13.24 - lr: 0.000001
2023-06-27 10:43:42,248 ----------------------------------------------------------------------------------------------------
2023-06-27 10:43:42,248 EPOCH 9 done: loss 0.1458 - lr 0.000001
2023-06-27 10:44:57,836 Evaluating as a multi-label problem: False
2023-06-27 10:44:57,866 DEV : loss 0.39844873547554016 - f1-score (micro avg)  0.7542
2023-06-27 10:46:56,673 Evaluating as a multi-label problem: False
2023-06-27 10:46:56,701 TEST : loss 0.054832663387060165 - f1-score (micro avg)  0.9491
2023-06-27 10:46:56,796 BAD EPOCHS (no improvement): 4
2023-06-27 10:46:56,798 ----------------------------------------------------------------------------------------------------
2023-06-27 10:49:04,499 epoch 10 - iter 402/4024 - loss 0.14672481 - samples/sec: 12.59 - lr: 0.000001
2023-06-27 10:51:10,878 epoch 10 - iter 804/4024 - loss 0.14567512 - samples/sec: 12.73 - lr: 0.000000
2023-06-27 10:53:20,739 epoch 10 - iter 1206/4024 - loss 0.14351169 - samples/sec: 12.38 - lr: 0.000000
2023-06-27 10:55:27,999 epoch 10 - iter 1608/4024 - loss 0.14322382 - samples/sec: 12.64 - lr: 0.000000
2023-06-27 10:57:35,841 epoch 10 - iter 2010/4024 - loss 0.14337879 - samples/sec: 12.58 - lr: 0.000000
2023-06-27 10:59:42,451 epoch 10 - iter 2412/4024 - loss 0.14486864 - samples/sec: 12.70 - lr: 0.000000
2023-06-27 11:01:42,024 epoch 10 - iter 2814/4024 - loss 0.14349945 - samples/sec: 13.45 - lr: 0.000000
2023-06-27 11:03:47,484 epoch 10 - iter 3216/4024 - loss 0.14372420 - samples/sec: 12.82 - lr: 0.000000
2023-06-27 11:05:55,474 epoch 10 - iter 3618/4024 - loss 0.14318052 - samples/sec: 12.57 - lr: 0.000000
2023-06-27 11:08:02,117 epoch 10 - iter 4020/4024 - loss 0.14285505 - samples/sec: 12.70 - lr: 0.000000
2023-06-27 11:08:03,372 ----------------------------------------------------------------------------------------------------
2023-06-27 11:08:03,372 EPOCH 10 done: loss 0.1428 - lr 0.000000
2023-06-27 11:09:18,970 Evaluating as a multi-label problem: False
2023-06-27 11:09:18,998 DEV : loss 0.41604602336883545 - f1-score (micro avg)  0.747
2023-06-27 11:11:19,018 Evaluating as a multi-label problem: False
2023-06-27 11:11:19,043 TEST : loss 0.05471297726035118 - f1-score (micro avg)  0.9477
2023-06-27 11:11:19,135 BAD EPOCHS (no improvement): 4
2023-06-27 11:11:31,217 ----------------------------------------------------------------------------------------------------
2023-06-27 11:11:31,223 Testing using last state of model ...
2023-06-27 11:13:31,115 Evaluating as a multi-label problem: False
2023-06-27 11:13:31,141 0.9475	0.948	0.9477	0.9326
2023-06-27 11:13:31,141 
Results:
- F-score (micro) 0.9477
- F-score (macro) 0.9484
- Accuracy 0.9326

By class:
              precision    recall  f1-score   support

        MISC     0.9414    0.9208    0.9310      1187
         PER     0.9673    0.9699    0.9686      1098
         ORG     0.9216    0.9331    0.9273       882
         LOC     0.9581    0.9755    0.9667       774

   micro avg     0.9475    0.9480    0.9477      3941
   macro avg     0.9471    0.9498    0.9484      3941
weighted avg     0.9475    0.9480    0.9477      3941

2023-06-27 11:13:31,141 ----------------------------------------------------------------------------------------------------
