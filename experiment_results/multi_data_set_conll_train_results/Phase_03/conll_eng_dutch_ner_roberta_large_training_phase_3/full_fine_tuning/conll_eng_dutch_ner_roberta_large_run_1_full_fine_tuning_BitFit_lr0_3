2023-06-08 17:21:53,400 ----------------------------------------------------------------------------------------------------
2023-06-08 17:21:53,405 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-08 17:21:53,412 ----------------------------------------------------------------------------------------------------
2023-06-08 17:21:53,412 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-08 17:21:53,413 ----------------------------------------------------------------------------------------------------
2023-06-08 17:21:53,413 Parameters:
2023-06-08 17:21:53,413  - learning_rate: "0.001000"
2023-06-08 17:21:53,413  - mini_batch_size: "4"
2023-06-08 17:21:53,413  - patience: "3"
2023-06-08 17:21:53,413  - anneal_factor: "0.5"
2023-06-08 17:21:53,413  - max_epochs: "10"
2023-06-08 17:21:53,413  - shuffle: "True"
2023-06-08 17:21:53,413  - train_with_dev: "False"
2023-06-08 17:21:53,413  - batch_growth_annealing: "False"
2023-06-08 17:21:53,413 ----------------------------------------------------------------------------------------------------
2023-06-08 17:21:53,413 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_1_full_fine_tuning_BitFit_lr0_3"
2023-06-08 17:21:53,413 ----------------------------------------------------------------------------------------------------
2023-06-08 17:21:53,428 Device: cuda:2
2023-06-08 17:21:53,428 ----------------------------------------------------------------------------------------------------
2023-06-08 17:21:53,428 Embeddings storage mode: none
2023-06-08 17:21:53,428 ----------------------------------------------------------------------------------------------------
2023-06-08 17:24:25,797 epoch 1 - iter 777/7770 - loss 0.40866223 - samples/sec: 20.41 - lr: 0.000100
2023-06-08 17:26:58,016 epoch 1 - iter 1554/7770 - loss 0.31904317 - samples/sec: 20.43 - lr: 0.000200
2023-06-08 17:29:30,048 epoch 1 - iter 2331/7770 - loss 0.28946779 - samples/sec: 20.46 - lr: 0.000300
2023-06-08 17:31:59,304 epoch 1 - iter 3108/7770 - loss 0.26648055 - samples/sec: 20.83 - lr: 0.000400
2023-06-08 17:34:31,406 epoch 1 - iter 3885/7770 - loss 0.25506995 - samples/sec: 20.44 - lr: 0.000500
2023-06-08 17:36:59,141 epoch 1 - iter 4662/7770 - loss 0.25905196 - samples/sec: 21.05 - lr: 0.000600
2023-06-08 17:39:30,167 epoch 1 - iter 5439/7770 - loss 0.27350797 - samples/sec: 20.59 - lr: 0.000700
2023-06-08 17:42:00,560 epoch 1 - iter 6216/7770 - loss 0.27821059 - samples/sec: 20.68 - lr: 0.000800
2023-06-08 17:44:30,612 epoch 1 - iter 6993/7770 - loss 0.28971709 - samples/sec: 20.72 - lr: 0.000900
2023-06-08 17:46:58,070 epoch 1 - iter 7770/7770 - loss 0.28694863 - samples/sec: 21.09 - lr: 0.001000
2023-06-08 17:46:58,073 ----------------------------------------------------------------------------------------------------
2023-06-08 17:46:58,073 EPOCH 1 done: loss 0.2869 - lr 0.001000
2023-06-08 17:49:48,360 Evaluating as a multi-label problem: False
2023-06-08 17:49:48,474 DEV : loss 0.168614000082016 - f1-score (micro avg)  0.8363
2023-06-08 17:49:48,650 BAD EPOCHS (no improvement): 4
2023-06-08 17:49:48,661 ----------------------------------------------------------------------------------------------------
2023-06-08 17:52:26,618 epoch 2 - iter 777/7770 - loss 0.26540514 - samples/sec: 19.69 - lr: 0.000989
2023-06-08 17:54:57,995 epoch 2 - iter 1554/7770 - loss 0.26343428 - samples/sec: 20.54 - lr: 0.000978
2023-06-08 17:57:29,736 epoch 2 - iter 2331/7770 - loss 0.25732163 - samples/sec: 20.49 - lr: 0.000967
2023-06-08 18:00:00,519 epoch 2 - iter 3108/7770 - loss 0.26086294 - samples/sec: 20.62 - lr: 0.000956
2023-06-08 18:02:35,053 epoch 2 - iter 3885/7770 - loss 0.25872091 - samples/sec: 20.12 - lr: 0.000944
2023-06-08 18:05:06,762 epoch 2 - iter 4662/7770 - loss 0.25781035 - samples/sec: 20.50 - lr: 0.000933
2023-06-08 18:07:39,970 epoch 2 - iter 5439/7770 - loss 0.25618745 - samples/sec: 20.30 - lr: 0.000922
2023-06-08 18:10:10,771 epoch 2 - iter 6216/7770 - loss 0.25319857 - samples/sec: 20.62 - lr: 0.000911
2023-06-08 18:12:42,594 epoch 2 - iter 6993/7770 - loss 0.25257416 - samples/sec: 20.48 - lr: 0.000900
2023-06-08 18:15:15,055 epoch 2 - iter 7770/7770 - loss 0.25162109 - samples/sec: 20.40 - lr: 0.000889
2023-06-08 18:15:15,060 ----------------------------------------------------------------------------------------------------
2023-06-08 18:15:15,060 EPOCH 2 done: loss 0.2516 - lr 0.000889
2023-06-08 18:18:31,591 Evaluating as a multi-label problem: False
2023-06-08 18:18:31,707 DEV : loss 0.10693665593862534 - f1-score (micro avg)  0.8858
2023-06-08 18:18:31,915 BAD EPOCHS (no improvement): 4
2023-06-08 18:18:31,930 ----------------------------------------------------------------------------------------------------
2023-06-08 18:21:07,894 epoch 3 - iter 777/7770 - loss 0.25190887 - samples/sec: 19.94 - lr: 0.000878
2023-06-08 18:23:42,137 epoch 3 - iter 1554/7770 - loss 0.23986119 - samples/sec: 20.16 - lr: 0.000867
2023-06-08 18:26:20,917 epoch 3 - iter 2331/7770 - loss 0.23530224 - samples/sec: 19.59 - lr: 0.000856
2023-06-08 18:28:57,254 epoch 3 - iter 3108/7770 - loss 0.23362842 - samples/sec: 19.89 - lr: 0.000844
2023-06-08 18:31:30,191 epoch 3 - iter 3885/7770 - loss 0.23587366 - samples/sec: 20.33 - lr: 0.000833
2023-06-08 18:34:02,884 epoch 3 - iter 4662/7770 - loss 0.23525139 - samples/sec: 20.37 - lr: 0.000822
2023-06-08 18:36:35,524 epoch 3 - iter 5439/7770 - loss 0.23422905 - samples/sec: 20.37 - lr: 0.000811
2023-06-08 18:39:07,543 epoch 3 - iter 6216/7770 - loss 0.23134318 - samples/sec: 20.46 - lr: 0.000800
2023-06-08 18:41:38,179 epoch 3 - iter 6993/7770 - loss 0.22952926 - samples/sec: 20.64 - lr: 0.000789
2023-06-08 18:44:06,978 epoch 3 - iter 7770/7770 - loss 0.22998983 - samples/sec: 20.90 - lr: 0.000778
2023-06-08 18:44:06,982 ----------------------------------------------------------------------------------------------------
2023-06-08 18:44:06,982 EPOCH 3 done: loss 0.2300 - lr 0.000778
2023-06-08 18:47:12,457 Evaluating as a multi-label problem: False
2023-06-08 18:47:12,570 DEV : loss 0.15155155956745148 - f1-score (micro avg)  0.8978
2023-06-08 18:47:12,780 BAD EPOCHS (no improvement): 4
2023-06-08 18:47:12,793 ----------------------------------------------------------------------------------------------------
2023-06-08 18:49:47,007 epoch 4 - iter 777/7770 - loss 0.22264327 - samples/sec: 20.17 - lr: 0.000767
2023-06-08 18:52:21,178 epoch 4 - iter 1554/7770 - loss 0.22204207 - samples/sec: 20.17 - lr: 0.000756
2023-06-08 18:54:55,641 epoch 4 - iter 2331/7770 - loss 0.22173153 - samples/sec: 20.13 - lr: 0.000744
2023-06-08 18:57:28,261 epoch 4 - iter 3108/7770 - loss 0.22075853 - samples/sec: 20.38 - lr: 0.000733
2023-06-08 18:59:59,528 epoch 4 - iter 3885/7770 - loss 0.22300024 - samples/sec: 20.56 - lr: 0.000722
2023-06-08 19:02:28,439 epoch 4 - iter 4662/7770 - loss 0.22461805 - samples/sec: 20.88 - lr: 0.000711
2023-06-08 19:05:07,065 epoch 4 - iter 5439/7770 - loss 0.22501440 - samples/sec: 19.60 - lr: 0.000700
2023-06-08 19:07:38,454 epoch 4 - iter 6216/7770 - loss 0.22332004 - samples/sec: 20.54 - lr: 0.000689
2023-06-08 19:10:10,669 epoch 4 - iter 6993/7770 - loss 0.21787616 - samples/sec: 20.43 - lr: 0.000678
2023-06-08 19:12:42,045 epoch 4 - iter 7770/7770 - loss 0.21468648 - samples/sec: 20.54 - lr: 0.000667
2023-06-08 19:12:42,049 ----------------------------------------------------------------------------------------------------
2023-06-08 19:12:42,049 EPOCH 4 done: loss 0.2147 - lr 0.000667
2023-06-08 19:15:38,309 Evaluating as a multi-label problem: False
2023-06-08 19:15:38,413 DEV : loss 0.12338560074567795 - f1-score (micro avg)  0.9128
2023-06-08 19:15:38,595 BAD EPOCHS (no improvement): 4
2023-06-08 19:15:38,598 ----------------------------------------------------------------------------------------------------
2023-06-08 19:18:13,762 epoch 5 - iter 777/7770 - loss 0.20634570 - samples/sec: 20.04 - lr: 0.000656
2023-06-08 19:20:47,464 epoch 5 - iter 1554/7770 - loss 0.19373524 - samples/sec: 20.23 - lr: 0.000644
2023-06-08 19:23:17,114 epoch 5 - iter 2331/7770 - loss 0.19513555 - samples/sec: 20.78 - lr: 0.000633
2023-06-08 19:25:48,232 epoch 5 - iter 3108/7770 - loss 0.19672320 - samples/sec: 20.58 - lr: 0.000622
2023-06-08 19:28:35,301 epoch 5 - iter 3885/7770 - loss 0.19616978 - samples/sec: 18.61 - lr: 0.000611
2023-06-08 19:31:19,735 epoch 5 - iter 4662/7770 - loss 0.20011608 - samples/sec: 18.91 - lr: 0.000600
2023-06-08 19:33:49,961 epoch 5 - iter 5439/7770 - loss 0.19898672 - samples/sec: 20.70 - lr: 0.000589
2023-06-08 19:36:18,837 epoch 5 - iter 6216/7770 - loss 0.19852435 - samples/sec: 20.89 - lr: 0.000578
2023-06-08 19:38:52,478 epoch 5 - iter 6993/7770 - loss 0.19526959 - samples/sec: 20.24 - lr: 0.000567
2023-06-08 19:41:22,757 epoch 5 - iter 7770/7770 - loss 0.19497870 - samples/sec: 20.69 - lr: 0.000556
2023-06-08 19:41:22,761 ----------------------------------------------------------------------------------------------------
2023-06-08 19:41:22,761 EPOCH 5 done: loss 0.1950 - lr 0.000556
2023-06-08 19:44:34,032 Evaluating as a multi-label problem: False
2023-06-08 19:44:34,143 DEV : loss 0.12728537619113922 - f1-score (micro avg)  0.9132
2023-06-08 19:44:34,357 BAD EPOCHS (no improvement): 4
2023-06-08 19:44:34,369 ----------------------------------------------------------------------------------------------------
2023-06-08 19:47:11,145 epoch 6 - iter 777/7770 - loss 0.18629111 - samples/sec: 19.84 - lr: 0.000544
2023-06-08 19:49:45,651 epoch 6 - iter 1554/7770 - loss 0.18209287 - samples/sec: 20.13 - lr: 0.000533
2023-06-08 19:52:23,053 epoch 6 - iter 2331/7770 - loss 0.18630288 - samples/sec: 19.76 - lr: 0.000522
2023-06-08 19:54:55,738 epoch 6 - iter 3108/7770 - loss 0.18537405 - samples/sec: 20.37 - lr: 0.000511
2023-06-08 19:57:29,634 epoch 6 - iter 3885/7770 - loss 0.18658455 - samples/sec: 20.21 - lr: 0.000500
2023-06-08 20:00:03,020 epoch 6 - iter 4662/7770 - loss 0.18507321 - samples/sec: 20.27 - lr: 0.000489
2023-06-08 20:02:33,995 epoch 6 - iter 5439/7770 - loss 0.18392512 - samples/sec: 20.60 - lr: 0.000478
2023-06-08 20:05:06,367 epoch 6 - iter 6216/7770 - loss 0.18336570 - samples/sec: 20.41 - lr: 0.000467
2023-06-08 20:07:37,198 epoch 6 - iter 6993/7770 - loss 0.18345789 - samples/sec: 20.62 - lr: 0.000456
2023-06-08 20:10:07,283 epoch 6 - iter 7770/7770 - loss 0.18277979 - samples/sec: 20.72 - lr: 0.000445
2023-06-08 20:10:07,287 ----------------------------------------------------------------------------------------------------
2023-06-08 20:10:07,287 EPOCH 6 done: loss 0.1828 - lr 0.000445
2023-06-08 20:13:09,587 Evaluating as a multi-label problem: False
2023-06-08 20:13:09,694 DEV : loss 0.12147761136293411 - f1-score (micro avg)  0.9151
2023-06-08 20:13:09,957 BAD EPOCHS (no improvement): 4
2023-06-08 20:13:09,960 ----------------------------------------------------------------------------------------------------
2023-06-08 20:15:44,417 epoch 7 - iter 777/7770 - loss 0.18472057 - samples/sec: 20.13 - lr: 0.000433
2023-06-08 20:18:17,605 epoch 7 - iter 1554/7770 - loss 0.18296961 - samples/sec: 20.30 - lr: 0.000422
2023-06-08 20:20:48,740 epoch 7 - iter 2331/7770 - loss 0.17864287 - samples/sec: 20.58 - lr: 0.000411
2023-06-08 20:23:21,062 epoch 7 - iter 3108/7770 - loss 0.17582097 - samples/sec: 20.42 - lr: 0.000400
2023-06-08 20:25:51,759 epoch 7 - iter 3885/7770 - loss 0.17355079 - samples/sec: 20.64 - lr: 0.000389
2023-06-08 20:28:20,493 epoch 7 - iter 4662/7770 - loss 0.17270001 - samples/sec: 20.91 - lr: 0.000378
2023-06-08 20:30:59,090 epoch 7 - iter 5439/7770 - loss 0.17015341 - samples/sec: 19.61 - lr: 0.000367
2023-06-08 20:33:30,119 epoch 7 - iter 6216/7770 - loss 0.16862091 - samples/sec: 20.59 - lr: 0.000356
2023-06-08 20:36:01,354 epoch 7 - iter 6993/7770 - loss 0.16843515 - samples/sec: 20.56 - lr: 0.000345
2023-06-08 20:38:31,646 epoch 7 - iter 7770/7770 - loss 0.16691026 - samples/sec: 20.69 - lr: 0.000333
2023-06-08 20:38:31,649 ----------------------------------------------------------------------------------------------------
2023-06-08 20:38:31,650 EPOCH 7 done: loss 0.1669 - lr 0.000333
2023-06-08 20:41:22,677 Evaluating as a multi-label problem: False
2023-06-08 20:41:22,792 DEV : loss 0.10363174974918365 - f1-score (micro avg)  0.93
2023-06-08 20:41:22,988 BAD EPOCHS (no improvement): 4
2023-06-08 20:41:22,991 ----------------------------------------------------------------------------------------------------
2023-06-08 20:43:56,253 epoch 8 - iter 777/7770 - loss 0.16749744 - samples/sec: 20.29 - lr: 0.000322
2023-06-08 20:46:27,750 epoch 8 - iter 1554/7770 - loss 0.15297794 - samples/sec: 20.53 - lr: 0.000311
2023-06-08 20:48:59,433 epoch 8 - iter 2331/7770 - loss 0.15123932 - samples/sec: 20.50 - lr: 0.000300
2023-06-08 20:51:30,559 epoch 8 - iter 3108/7770 - loss 0.15293028 - samples/sec: 20.58 - lr: 0.000289
2023-06-08 20:54:01,262 epoch 8 - iter 3885/7770 - loss 0.15109777 - samples/sec: 20.64 - lr: 0.000278
2023-06-08 20:56:30,746 epoch 8 - iter 4662/7770 - loss 0.15275994 - samples/sec: 20.80 - lr: 0.000267
2023-06-08 20:59:02,760 epoch 8 - iter 5439/7770 - loss 0.15197540 - samples/sec: 20.46 - lr: 0.000256
2023-06-08 21:01:30,630 epoch 8 - iter 6216/7770 - loss 0.15070188 - samples/sec: 21.03 - lr: 0.000245
2023-06-08 21:04:00,474 epoch 8 - iter 6993/7770 - loss 0.14921173 - samples/sec: 20.75 - lr: 0.000233
2023-06-08 21:06:31,231 epoch 8 - iter 7770/7770 - loss 0.14742210 - samples/sec: 20.63 - lr: 0.000222
2023-06-08 21:06:31,235 ----------------------------------------------------------------------------------------------------
2023-06-08 21:06:31,235 EPOCH 8 done: loss 0.1474 - lr 0.000222
2023-06-08 21:09:39,492 Evaluating as a multi-label problem: False
2023-06-08 21:09:39,605 DEV : loss 0.09453260898590088 - f1-score (micro avg)  0.934
2023-06-08 21:09:39,833 BAD EPOCHS (no improvement): 4
2023-06-08 21:09:39,846 ----------------------------------------------------------------------------------------------------
2023-06-08 21:12:15,767 epoch 9 - iter 777/7770 - loss 0.13770428 - samples/sec: 19.95 - lr: 0.000211
2023-06-08 21:14:48,145 epoch 9 - iter 1554/7770 - loss 0.14179312 - samples/sec: 20.41 - lr: 0.000200
2023-06-08 21:17:27,649 epoch 9 - iter 2331/7770 - loss 0.14192320 - samples/sec: 19.50 - lr: 0.000189
2023-06-08 21:19:57,863 epoch 9 - iter 3108/7770 - loss 0.14422178 - samples/sec: 20.70 - lr: 0.000178
2023-06-08 21:22:31,628 epoch 9 - iter 3885/7770 - loss 0.14407254 - samples/sec: 20.22 - lr: 0.000167
2023-06-08 21:25:03,473 epoch 9 - iter 4662/7770 - loss 0.14265985 - samples/sec: 20.48 - lr: 0.000156
2023-06-08 21:27:34,213 epoch 9 - iter 5439/7770 - loss 0.14117599 - samples/sec: 20.63 - lr: 0.000145
2023-06-08 21:30:04,943 epoch 9 - iter 6216/7770 - loss 0.14339195 - samples/sec: 20.63 - lr: 0.000133
2023-06-08 21:32:36,351 epoch 9 - iter 6993/7770 - loss 0.14229031 - samples/sec: 20.54 - lr: 0.000122
2023-06-08 21:35:08,465 epoch 9 - iter 7770/7770 - loss 0.14065780 - samples/sec: 20.44 - lr: 0.000111
2023-06-08 21:35:08,468 ----------------------------------------------------------------------------------------------------
2023-06-08 21:35:08,468 EPOCH 9 done: loss 0.1407 - lr 0.000111
2023-06-08 21:38:13,797 Evaluating as a multi-label problem: False
2023-06-08 21:38:13,894 DEV : loss 0.10087399929761887 - f1-score (micro avg)  0.935
2023-06-08 21:38:14,109 BAD EPOCHS (no improvement): 4
2023-06-08 21:38:14,112 ----------------------------------------------------------------------------------------------------
2023-06-08 21:40:50,810 epoch 10 - iter 777/7770 - loss 0.13119287 - samples/sec: 19.85 - lr: 0.000100
2023-06-08 21:43:26,112 epoch 10 - iter 1554/7770 - loss 0.13256930 - samples/sec: 20.02 - lr: 0.000089
2023-06-08 21:45:59,283 epoch 10 - iter 2331/7770 - loss 0.13210556 - samples/sec: 20.30 - lr: 0.000078
2023-06-08 21:48:29,225 epoch 10 - iter 3108/7770 - loss 0.12984563 - samples/sec: 20.74 - lr: 0.000067
2023-06-08 21:50:57,371 epoch 10 - iter 3885/7770 - loss 0.12938899 - samples/sec: 20.99 - lr: 0.000056
2023-06-08 21:53:26,109 epoch 10 - iter 4662/7770 - loss 0.12803925 - samples/sec: 20.91 - lr: 0.000045
2023-06-08 21:56:04,658 epoch 10 - iter 5439/7770 - loss 0.12758181 - samples/sec: 19.61 - lr: 0.000033
2023-06-08 21:58:35,236 epoch 10 - iter 6216/7770 - loss 0.12899500 - samples/sec: 20.65 - lr: 0.000022
2023-06-08 22:01:05,572 epoch 10 - iter 6993/7770 - loss 0.12939417 - samples/sec: 20.69 - lr: 0.000011
2023-06-08 22:03:36,837 epoch 10 - iter 7770/7770 - loss 0.12976607 - samples/sec: 20.56 - lr: 0.000000
2023-06-08 22:03:36,841 ----------------------------------------------------------------------------------------------------
2023-06-08 22:03:36,841 EPOCH 10 done: loss 0.1298 - lr 0.000000
2023-06-08 22:06:31,583 Evaluating as a multi-label problem: False
2023-06-08 22:06:31,688 DEV : loss 0.09106076508760452 - f1-score (micro avg)  0.9403
2023-06-08 22:06:31,940 BAD EPOCHS (no improvement): 4
2023-06-08 22:06:43,661 ----------------------------------------------------------------------------------------------------
2023-06-08 22:06:43,665 Testing using last state of model ...
2023-06-08 22:10:47,216 Evaluating as a multi-label problem: False
2023-06-08 22:10:47,338 0.9136	0.929	0.9212	0.887
2023-06-08 22:10:47,339 
Results:
- F-score (micro) 0.9212
- F-score (macro) 0.9165
- Accuracy 0.887

By class:
              precision    recall  f1-score   support

         PER     0.9779    0.9761    0.9770      2715
         ORG     0.8787    0.9371    0.9069      2543
         LOC     0.9178    0.9238    0.9208      2442
        MISC     0.8658    0.8571    0.8614      1889

   micro avg     0.9136    0.9290    0.9212      9589
   macro avg     0.9100    0.9235    0.9165      9589
weighted avg     0.9142    0.9290    0.9213      9589

2023-06-08 22:10:47,339 ----------------------------------------------------------------------------------------------------
2023-06-08 22:10:47,339 ----------------------------------------------------------------------------------------------------
2023-06-08 22:13:23,807 Evaluating as a multi-label problem: False
2023-06-08 22:13:23,864 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-08 22:13:23,865 0.9193	0.9188	0.919	0.8956
2023-06-08 22:13:23,865 ----------------------------------------------------------------------------------------------------
2023-06-08 22:15:09,312 Evaluating as a multi-label problem: False
2023-06-08 22:15:09,386 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-08 22:15:09,387 0.9098	0.9361	0.9228	0.8812
