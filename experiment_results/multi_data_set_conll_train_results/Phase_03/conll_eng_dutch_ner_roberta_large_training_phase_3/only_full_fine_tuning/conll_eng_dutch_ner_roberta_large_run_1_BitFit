2023-06-02 22:11:56,747 ----------------------------------------------------------------------------------------------------
2023-06-02 22:11:56,751 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-02 22:11:56,754 ----------------------------------------------------------------------------------------------------
2023-06-02 22:11:56,755 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-02 22:11:56,756 ----------------------------------------------------------------------------------------------------
2023-06-02 22:11:56,756 Parameters:
2023-06-02 22:11:56,756  - learning_rate: "0.001000"
2023-06-02 22:11:56,757  - mini_batch_size: "4"
2023-06-02 22:11:56,757  - patience: "3"
2023-06-02 22:11:56,757  - anneal_factor: "0.5"
2023-06-02 22:11:56,757  - max_epochs: "10"
2023-06-02 22:11:56,758  - shuffle: "True"
2023-06-02 22:11:56,758  - train_with_dev: "False"
2023-06-02 22:11:56,758  - batch_growth_annealing: "False"
2023-06-02 22:11:56,758 ----------------------------------------------------------------------------------------------------
2023-06-02 22:11:56,758 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_1_BitFit"
2023-06-02 22:11:56,759 ----------------------------------------------------------------------------------------------------
2023-06-02 22:11:56,759 Device: cuda:1
2023-06-02 22:11:56,759 ----------------------------------------------------------------------------------------------------
2023-06-02 22:11:56,761 Embeddings storage mode: none
2023-06-02 22:11:56,761 ----------------------------------------------------------------------------------------------------
2023-06-02 22:14:23,374 epoch 1 - iter 777/7770 - loss 1.59448795 - samples/sec: 21.21 - lr: 0.000100
2023-06-02 22:16:45,956 epoch 1 - iter 1554/7770 - loss 1.04284789 - samples/sec: 21.81 - lr: 0.000200
2023-06-02 22:19:16,332 epoch 1 - iter 2331/7770 - loss 0.87173384 - samples/sec: 20.68 - lr: 0.000300
2023-06-02 22:21:45,318 epoch 1 - iter 3108/7770 - loss 0.74669270 - samples/sec: 20.87 - lr: 0.000400
2023-06-02 22:24:28,074 epoch 1 - iter 3885/7770 - loss 0.65917140 - samples/sec: 19.11 - lr: 0.000500
2023-06-02 22:26:51,844 epoch 1 - iter 4662/7770 - loss 0.61719485 - samples/sec: 21.63 - lr: 0.000600
2023-06-02 22:29:07,277 epoch 1 - iter 5439/7770 - loss 0.59299948 - samples/sec: 22.96 - lr: 0.000700
2023-06-02 22:31:25,887 epoch 1 - iter 6216/7770 - loss 0.56083866 - samples/sec: 22.43 - lr: 0.000800
2023-06-02 22:33:48,449 epoch 1 - iter 6993/7770 - loss 0.53496170 - samples/sec: 21.81 - lr: 0.000900
2023-06-02 22:36:28,213 epoch 1 - iter 7770/7770 - loss 0.50468481 - samples/sec: 19.46 - lr: 0.001000
2023-06-02 22:36:28,216 ----------------------------------------------------------------------------------------------------
2023-06-02 22:36:28,216 EPOCH 1 done: loss 0.5047 - lr 0.001000
2023-06-02 22:39:25,533 Evaluating as a multi-label problem: False
2023-06-02 22:39:25,628 DEV : loss 0.1737029105424881 - f1-score (micro avg)  0.7948
2023-06-02 22:39:25,829 BAD EPOCHS (no improvement): 4
2023-06-02 22:39:25,831 ----------------------------------------------------------------------------------------------------
2023-06-02 22:41:55,343 epoch 2 - iter 777/7770 - loss 0.26464475 - samples/sec: 20.80 - lr: 0.000989
2023-06-02 22:44:19,575 epoch 2 - iter 1554/7770 - loss 0.24947594 - samples/sec: 21.56 - lr: 0.000978
2023-06-02 22:46:49,332 epoch 2 - iter 2331/7770 - loss 0.23994587 - samples/sec: 20.77 - lr: 0.000967
2023-06-02 22:49:15,751 epoch 2 - iter 3108/7770 - loss 0.23417121 - samples/sec: 21.24 - lr: 0.000956
2023-06-02 22:51:41,437 epoch 2 - iter 3885/7770 - loss 0.22901486 - samples/sec: 21.35 - lr: 0.000944
2023-06-02 22:54:10,291 epoch 2 - iter 4662/7770 - loss 0.22581039 - samples/sec: 20.89 - lr: 0.000933
2023-06-02 22:56:37,876 epoch 2 - iter 5439/7770 - loss 0.22232505 - samples/sec: 21.07 - lr: 0.000922
2023-06-02 22:59:04,287 epoch 2 - iter 6216/7770 - loss 0.21994176 - samples/sec: 21.24 - lr: 0.000911
2023-06-02 23:01:31,297 epoch 2 - iter 6993/7770 - loss 0.21758495 - samples/sec: 21.15 - lr: 0.000900
2023-06-02 23:03:57,229 epoch 2 - iter 7770/7770 - loss 0.21508664 - samples/sec: 21.31 - lr: 0.000889
2023-06-02 23:03:57,233 ----------------------------------------------------------------------------------------------------
2023-06-02 23:03:57,233 EPOCH 2 done: loss 0.2151 - lr 0.000889
2023-06-02 23:06:45,633 Evaluating as a multi-label problem: False
2023-06-02 23:06:45,732 DEV : loss 0.12767019867897034 - f1-score (micro avg)  0.868
2023-06-02 23:06:45,928 BAD EPOCHS (no improvement): 4
2023-06-02 23:06:45,933 ----------------------------------------------------------------------------------------------------
2023-06-02 23:09:15,233 epoch 3 - iter 777/7770 - loss 0.19520638 - samples/sec: 20.83 - lr: 0.000878
2023-06-02 23:11:39,449 epoch 3 - iter 1554/7770 - loss 0.18736597 - samples/sec: 21.56 - lr: 0.000867
2023-06-02 23:14:07,428 epoch 3 - iter 2331/7770 - loss 0.18679192 - samples/sec: 21.02 - lr: 0.000856
2023-06-02 23:16:29,105 epoch 3 - iter 3108/7770 - loss 0.18428617 - samples/sec: 21.95 - lr: 0.000844
2023-06-02 23:18:55,831 epoch 3 - iter 3885/7770 - loss 0.18440191 - samples/sec: 21.20 - lr: 0.000833
2023-06-02 23:21:25,931 epoch 3 - iter 4662/7770 - loss 0.18275217 - samples/sec: 20.72 - lr: 0.000822
2023-06-02 23:23:54,079 epoch 3 - iter 5439/7770 - loss 0.18173071 - samples/sec: 20.99 - lr: 0.000811
2023-06-02 23:26:13,857 epoch 3 - iter 6216/7770 - loss 0.18070474 - samples/sec: 22.25 - lr: 0.000800
2023-06-02 23:28:31,381 epoch 3 - iter 6993/7770 - loss 0.17988868 - samples/sec: 22.61 - lr: 0.000789
2023-06-02 23:30:59,231 epoch 3 - iter 7770/7770 - loss 0.17913880 - samples/sec: 21.03 - lr: 0.000778
2023-06-02 23:30:59,235 ----------------------------------------------------------------------------------------------------
2023-06-02 23:30:59,235 EPOCH 3 done: loss 0.1791 - lr 0.000778
2023-06-02 23:33:44,884 Evaluating as a multi-label problem: False
2023-06-02 23:33:44,991 DEV : loss 0.11602690815925598 - f1-score (micro avg)  0.8781
2023-06-02 23:33:45,180 BAD EPOCHS (no improvement): 4
2023-06-02 23:33:45,183 ----------------------------------------------------------------------------------------------------
2023-06-02 23:36:07,421 epoch 4 - iter 777/7770 - loss 0.16528738 - samples/sec: 21.86 - lr: 0.000767
2023-06-02 23:38:34,525 epoch 4 - iter 1554/7770 - loss 0.15969349 - samples/sec: 21.14 - lr: 0.000756
2023-06-02 23:40:59,969 epoch 4 - iter 2331/7770 - loss 0.16250123 - samples/sec: 21.38 - lr: 0.000744
2023-06-02 23:43:19,905 epoch 4 - iter 3108/7770 - loss 0.16567693 - samples/sec: 22.22 - lr: 0.000733
2023-06-02 23:45:49,745 epoch 4 - iter 3885/7770 - loss 0.16624698 - samples/sec: 20.75 - lr: 0.000722
2023-06-02 23:48:21,366 epoch 4 - iter 4662/7770 - loss 0.16562371 - samples/sec: 20.51 - lr: 0.000711
2023-06-02 23:50:50,538 epoch 4 - iter 5439/7770 - loss 0.16544435 - samples/sec: 20.85 - lr: 0.000700
2023-06-02 23:53:20,221 epoch 4 - iter 6216/7770 - loss 0.16669244 - samples/sec: 20.78 - lr: 0.000689
2023-06-02 23:55:39,136 epoch 4 - iter 6993/7770 - loss 0.16651627 - samples/sec: 22.39 - lr: 0.000678
2023-06-02 23:58:09,101 epoch 4 - iter 7770/7770 - loss 0.16591397 - samples/sec: 20.74 - lr: 0.000667
2023-06-02 23:58:09,104 ----------------------------------------------------------------------------------------------------
2023-06-02 23:58:09,104 EPOCH 4 done: loss 0.1659 - lr 0.000667
2023-06-03 00:00:48,672 Evaluating as a multi-label problem: False
2023-06-03 00:00:48,774 DEV : loss 0.10555841773748398 - f1-score (micro avg)  0.8953
2023-06-03 00:00:48,982 BAD EPOCHS (no improvement): 4
2023-06-03 00:00:48,985 ----------------------------------------------------------------------------------------------------
2023-06-03 00:03:18,526 epoch 5 - iter 777/7770 - loss 0.16337831 - samples/sec: 20.80 - lr: 0.000656
2023-06-03 00:05:44,838 epoch 5 - iter 1554/7770 - loss 0.16190567 - samples/sec: 21.25 - lr: 0.000644
2023-06-03 00:08:12,995 epoch 5 - iter 2331/7770 - loss 0.16318726 - samples/sec: 20.99 - lr: 0.000633
2023-06-03 00:10:38,577 epoch 5 - iter 3108/7770 - loss 0.16144043 - samples/sec: 21.36 - lr: 0.000622
2023-06-03 00:13:01,468 epoch 5 - iter 3885/7770 - loss 0.15847769 - samples/sec: 21.76 - lr: 0.000611
2023-06-03 00:15:29,840 epoch 5 - iter 4662/7770 - loss 0.15811818 - samples/sec: 20.96 - lr: 0.000600
2023-06-03 00:17:53,091 epoch 5 - iter 5439/7770 - loss 0.15537069 - samples/sec: 21.71 - lr: 0.000589
2023-06-03 00:20:18,990 epoch 5 - iter 6216/7770 - loss 0.15647413 - samples/sec: 21.32 - lr: 0.000578
2023-06-03 00:22:54,516 epoch 5 - iter 6993/7770 - loss 0.15583442 - samples/sec: 19.99 - lr: 0.000567
2023-06-03 00:25:20,999 epoch 5 - iter 7770/7770 - loss 0.15564674 - samples/sec: 21.23 - lr: 0.000556
2023-06-03 00:25:21,003 ----------------------------------------------------------------------------------------------------
2023-06-03 00:25:21,003 EPOCH 5 done: loss 0.1556 - lr 0.000556
2023-06-03 00:27:52,547 Evaluating as a multi-label problem: False
2023-06-03 00:27:52,649 DEV : loss 0.1027720645070076 - f1-score (micro avg)  0.9082
2023-06-03 00:27:52,863 BAD EPOCHS (no improvement): 4
2023-06-03 00:27:52,866 ----------------------------------------------------------------------------------------------------
2023-06-03 00:30:25,154 epoch 6 - iter 777/7770 - loss 0.15044520 - samples/sec: 20.42 - lr: 0.000544
2023-06-03 00:32:49,961 epoch 6 - iter 1554/7770 - loss 0.14606525 - samples/sec: 21.48 - lr: 0.000533
2023-06-03 00:35:14,196 epoch 6 - iter 2331/7770 - loss 0.14495212 - samples/sec: 21.56 - lr: 0.000522
2023-06-03 00:37:41,307 epoch 6 - iter 3108/7770 - loss 0.14411150 - samples/sec: 21.14 - lr: 0.000511
2023-06-03 00:40:07,017 epoch 6 - iter 3885/7770 - loss 0.14503683 - samples/sec: 21.34 - lr: 0.000500
2023-06-03 00:42:36,555 epoch 6 - iter 4662/7770 - loss 0.14531032 - samples/sec: 20.80 - lr: 0.000489
2023-06-03 00:44:58,202 epoch 6 - iter 5439/7770 - loss 0.14467837 - samples/sec: 21.95 - lr: 0.000478
2023-06-03 00:47:26,006 epoch 6 - iter 6216/7770 - loss 0.14358065 - samples/sec: 21.04 - lr: 0.000467
2023-06-03 00:49:55,357 epoch 6 - iter 6993/7770 - loss 0.14452114 - samples/sec: 20.82 - lr: 0.000456
2023-06-03 00:52:28,463 epoch 6 - iter 7770/7770 - loss 0.14385394 - samples/sec: 20.31 - lr: 0.000445
2023-06-03 00:52:28,467 ----------------------------------------------------------------------------------------------------
2023-06-03 00:52:28,467 EPOCH 6 done: loss 0.1439 - lr 0.000445
2023-06-03 00:55:11,172 Evaluating as a multi-label problem: False
2023-06-03 00:55:11,276 DEV : loss 0.10137832164764404 - f1-score (micro avg)  0.9078
2023-06-03 00:55:11,496 BAD EPOCHS (no improvement): 4
2023-06-03 00:55:11,499 ----------------------------------------------------------------------------------------------------
2023-06-03 00:57:44,711 epoch 7 - iter 777/7770 - loss 0.14438295 - samples/sec: 20.30 - lr: 0.000433
2023-06-03 01:00:20,757 epoch 7 - iter 1554/7770 - loss 0.14425719 - samples/sec: 19.93 - lr: 0.000422
2023-06-03 01:02:45,406 epoch 7 - iter 2331/7770 - loss 0.14291707 - samples/sec: 21.50 - lr: 0.000411
2023-06-03 01:05:11,974 epoch 7 - iter 3108/7770 - loss 0.14056373 - samples/sec: 21.22 - lr: 0.000400
2023-06-03 01:07:37,451 epoch 7 - iter 3885/7770 - loss 0.13841746 - samples/sec: 21.38 - lr: 0.000389
2023-06-03 01:10:04,190 epoch 7 - iter 4662/7770 - loss 0.13849075 - samples/sec: 21.19 - lr: 0.000378
2023-06-03 01:12:26,751 epoch 7 - iter 5439/7770 - loss 0.13802167 - samples/sec: 21.81 - lr: 0.000367
2023-06-03 01:14:53,716 epoch 7 - iter 6216/7770 - loss 0.13816400 - samples/sec: 21.16 - lr: 0.000356
2023-06-03 01:17:27,698 epoch 7 - iter 6993/7770 - loss 0.13769191 - samples/sec: 20.20 - lr: 0.000345
2023-06-03 01:19:53,317 epoch 7 - iter 7770/7770 - loss 0.13760777 - samples/sec: 21.36 - lr: 0.000333
2023-06-03 01:19:53,320 ----------------------------------------------------------------------------------------------------
2023-06-03 01:19:53,320 EPOCH 7 done: loss 0.1376 - lr 0.000333
2023-06-03 01:22:29,093 Evaluating as a multi-label problem: False
2023-06-03 01:22:29,196 DEV : loss 0.10683774203062057 - f1-score (micro avg)  0.9109
2023-06-03 01:22:29,406 BAD EPOCHS (no improvement): 4
2023-06-03 01:22:29,408 ----------------------------------------------------------------------------------------------------
2023-06-03 01:24:55,222 epoch 8 - iter 777/7770 - loss 0.11834871 - samples/sec: 21.33 - lr: 0.000322
2023-06-03 01:27:21,830 epoch 8 - iter 1554/7770 - loss 0.12421998 - samples/sec: 21.21 - lr: 0.000311
2023-06-03 01:29:47,090 epoch 8 - iter 2331/7770 - loss 0.12672450 - samples/sec: 21.41 - lr: 0.000300
2023-06-03 01:32:11,199 epoch 8 - iter 3108/7770 - loss 0.13007404 - samples/sec: 21.58 - lr: 0.000289
2023-06-03 01:34:35,947 epoch 8 - iter 3885/7770 - loss 0.13006708 - samples/sec: 21.48 - lr: 0.000278
2023-06-03 01:37:01,648 epoch 8 - iter 4662/7770 - loss 0.12948231 - samples/sec: 21.34 - lr: 0.000267
2023-06-03 01:39:29,111 epoch 8 - iter 5439/7770 - loss 0.12845488 - samples/sec: 21.09 - lr: 0.000256
2023-06-03 01:41:52,423 epoch 8 - iter 6216/7770 - loss 0.12807632 - samples/sec: 21.70 - lr: 0.000245
2023-06-03 01:44:18,414 epoch 8 - iter 6993/7770 - loss 0.12784285 - samples/sec: 21.30 - lr: 0.000233
2023-06-03 01:46:44,259 epoch 8 - iter 7770/7770 - loss 0.12719976 - samples/sec: 21.32 - lr: 0.000222
2023-06-03 01:46:44,263 ----------------------------------------------------------------------------------------------------
2023-06-03 01:46:44,263 EPOCH 8 done: loss 0.1272 - lr 0.000222
2023-06-03 01:49:35,160 Evaluating as a multi-label problem: False
2023-06-03 01:49:35,259 DEV : loss 0.09442587196826935 - f1-score (micro avg)  0.9222
2023-06-03 01:49:35,476 BAD EPOCHS (no improvement): 4
2023-06-03 01:49:35,480 ----------------------------------------------------------------------------------------------------
2023-06-03 01:52:02,500 epoch 9 - iter 777/7770 - loss 0.12641810 - samples/sec: 21.15 - lr: 0.000211
2023-06-03 01:54:28,666 epoch 9 - iter 1554/7770 - loss 0.11953887 - samples/sec: 21.28 - lr: 0.000200
2023-06-03 01:56:57,045 epoch 9 - iter 2331/7770 - loss 0.12023761 - samples/sec: 20.96 - lr: 0.000189
2023-06-03 01:59:27,005 epoch 9 - iter 3108/7770 - loss 0.11884724 - samples/sec: 20.74 - lr: 0.000178
2023-06-03 02:01:49,804 epoch 9 - iter 3885/7770 - loss 0.11914917 - samples/sec: 21.78 - lr: 0.000167
2023-06-03 02:04:12,796 epoch 9 - iter 4662/7770 - loss 0.11947128 - samples/sec: 21.75 - lr: 0.000156
2023-06-03 02:06:37,058 epoch 9 - iter 5439/7770 - loss 0.11962311 - samples/sec: 21.56 - lr: 0.000145
2023-06-03 02:09:00,615 epoch 9 - iter 6216/7770 - loss 0.11928706 - samples/sec: 21.66 - lr: 0.000133
2023-06-03 02:11:24,522 epoch 9 - iter 6993/7770 - loss 0.11869231 - samples/sec: 21.61 - lr: 0.000122
2023-06-03 02:13:48,649 epoch 9 - iter 7770/7770 - loss 0.11775517 - samples/sec: 21.58 - lr: 0.000111
2023-06-03 02:13:48,652 ----------------------------------------------------------------------------------------------------
2023-06-03 02:13:48,653 EPOCH 9 done: loss 0.1178 - lr 0.000111
2023-06-03 02:16:35,957 Evaluating as a multi-label problem: False
2023-06-03 02:16:36,059 DEV : loss 0.09200572222471237 - f1-score (micro avg)  0.9333
2023-06-03 02:16:36,271 BAD EPOCHS (no improvement): 4
2023-06-03 02:16:36,273 ----------------------------------------------------------------------------------------------------
2023-06-03 02:19:02,024 epoch 10 - iter 777/7770 - loss 0.11860467 - samples/sec: 21.34 - lr: 0.000100
2023-06-03 02:21:26,402 epoch 10 - iter 1554/7770 - loss 0.11217408 - samples/sec: 21.54 - lr: 0.000089
2023-06-03 02:23:49,337 epoch 10 - iter 2331/7770 - loss 0.11412535 - samples/sec: 21.76 - lr: 0.000078
2023-06-03 02:26:12,606 epoch 10 - iter 3108/7770 - loss 0.11347681 - samples/sec: 21.71 - lr: 0.000067
2023-06-03 02:28:44,439 epoch 10 - iter 3885/7770 - loss 0.11377707 - samples/sec: 20.48 - lr: 0.000056
2023-06-03 02:31:03,802 epoch 10 - iter 4662/7770 - loss 0.11291858 - samples/sec: 22.31 - lr: 0.000045
2023-06-03 02:33:30,209 epoch 10 - iter 5439/7770 - loss 0.11420102 - samples/sec: 21.24 - lr: 0.000033
2023-06-03 02:35:58,449 epoch 10 - iter 6216/7770 - loss 0.11433269 - samples/sec: 20.98 - lr: 0.000022
2023-06-03 02:38:21,225 epoch 10 - iter 6993/7770 - loss 0.11342641 - samples/sec: 21.78 - lr: 0.000011
2023-06-03 02:40:41,215 epoch 10 - iter 7770/7770 - loss 0.11332970 - samples/sec: 22.21 - lr: 0.000000
2023-06-03 02:40:41,219 ----------------------------------------------------------------------------------------------------
2023-06-03 02:40:41,220 EPOCH 10 done: loss 0.1133 - lr 0.000000
2023-06-03 02:43:33,850 Evaluating as a multi-label problem: False
2023-06-03 02:43:33,952 DEV : loss 0.09212752431631088 - f1-score (micro avg)  0.9312
2023-06-03 02:43:34,168 BAD EPOCHS (no improvement): 4
2023-06-03 02:43:46,816 ----------------------------------------------------------------------------------------------------
2023-06-03 02:43:46,822 Testing using last state of model ...
2023-06-03 02:48:13,988 Evaluating as a multi-label problem: False
2023-06-03 02:48:14,095 0.9047	0.9071	0.9059	0.8638
2023-06-03 02:48:14,096 
Results:
- F-score (micro) 0.9059
- F-score (macro) 0.9009
- Accuracy 0.8638

By class:
              precision    recall  f1-score   support

         PER     0.9731    0.9606    0.9668      2715
         ORG     0.8599    0.9127    0.8855      2543
         LOC     0.9356    0.8927    0.9137      2442
        MISC     0.8341    0.8412    0.8376      1889

   micro avg     0.9047    0.9071    0.9059      9589
   macro avg     0.9007    0.9018    0.9009      9589
weighted avg     0.9062    0.9071    0.9063      9589

2023-06-03 02:48:14,096 ----------------------------------------------------------------------------------------------------
2023-06-03 02:48:14,096 ----------------------------------------------------------------------------------------------------
2023-06-03 02:50:58,290 Evaluating as a multi-label problem: False
2023-06-03 02:50:58,339 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-03 02:50:58,340 0.9044	0.8957	0.9001	0.8695
2023-06-03 02:50:58,340 ----------------------------------------------------------------------------------------------------
2023-06-03 02:52:48,881 Evaluating as a multi-label problem: False
2023-06-03 02:52:48,948 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-03 02:52:48,948 0.9049	0.915	0.9099	0.8599
