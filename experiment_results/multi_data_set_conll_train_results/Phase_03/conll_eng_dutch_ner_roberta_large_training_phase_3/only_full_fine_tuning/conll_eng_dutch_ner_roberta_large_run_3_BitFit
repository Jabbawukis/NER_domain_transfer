2023-06-03 07:34:22,785 ----------------------------------------------------------------------------------------------------
2023-06-03 07:34:22,790 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-03 07:34:22,791 ----------------------------------------------------------------------------------------------------
2023-06-03 07:34:22,792 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-03 07:34:22,792 ----------------------------------------------------------------------------------------------------
2023-06-03 07:34:22,792 Parameters:
2023-06-03 07:34:22,792  - learning_rate: "0.001000"
2023-06-03 07:34:22,792  - mini_batch_size: "4"
2023-06-03 07:34:22,792  - patience: "3"
2023-06-03 07:34:22,792  - anneal_factor: "0.5"
2023-06-03 07:34:22,792  - max_epochs: "10"
2023-06-03 07:34:22,792  - shuffle: "True"
2023-06-03 07:34:22,792  - train_with_dev: "False"
2023-06-03 07:34:22,792  - batch_growth_annealing: "False"
2023-06-03 07:34:22,792 ----------------------------------------------------------------------------------------------------
2023-06-03 07:34:22,792 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_3_BitFit"
2023-06-03 07:34:22,792 ----------------------------------------------------------------------------------------------------
2023-06-03 07:34:22,792 Device: cuda:1
2023-06-03 07:34:22,792 ----------------------------------------------------------------------------------------------------
2023-06-03 07:34:22,793 Embeddings storage mode: none
2023-06-03 07:34:22,793 ----------------------------------------------------------------------------------------------------
2023-06-03 07:36:50,053 epoch 1 - iter 777/7770 - loss 1.44623786 - samples/sec: 21.12 - lr: 0.000100
2023-06-03 07:39:11,585 epoch 1 - iter 1554/7770 - loss 0.89405075 - samples/sec: 21.97 - lr: 0.000200
2023-06-03 07:41:36,334 epoch 1 - iter 2331/7770 - loss 0.70851857 - samples/sec: 21.48 - lr: 0.000300
2023-06-03 07:44:04,956 epoch 1 - iter 3108/7770 - loss 0.58702642 - samples/sec: 20.92 - lr: 0.000400
2023-06-03 07:46:36,521 epoch 1 - iter 3885/7770 - loss 0.51046448 - samples/sec: 20.52 - lr: 0.000500
2023-06-03 07:48:48,783 epoch 1 - iter 4662/7770 - loss 0.46403502 - samples/sec: 23.51 - lr: 0.000600
2023-06-03 07:51:01,398 epoch 1 - iter 5439/7770 - loss 0.43506139 - samples/sec: 23.45 - lr: 0.000700
2023-06-03 07:53:14,011 epoch 1 - iter 6216/7770 - loss 0.40514563 - samples/sec: 23.45 - lr: 0.000800
2023-06-03 07:55:34,428 epoch 1 - iter 6993/7770 - loss 0.38465900 - samples/sec: 22.15 - lr: 0.000900
2023-06-03 07:57:58,450 epoch 1 - iter 7770/7770 - loss 0.36189001 - samples/sec: 21.59 - lr: 0.001000
2023-06-03 07:57:58,452 ----------------------------------------------------------------------------------------------------
2023-06-03 07:57:58,452 EPOCH 1 done: loss 0.3619 - lr 0.001000
2023-06-03 08:00:37,276 Evaluating as a multi-label problem: False
2023-06-03 08:00:37,380 DEV : loss 0.1327626258134842 - f1-score (micro avg)  0.8617
2023-06-03 08:00:37,614 BAD EPOCHS (no improvement): 4
2023-06-03 08:00:37,617 ----------------------------------------------------------------------------------------------------
2023-06-03 08:03:02,544 epoch 2 - iter 777/7770 - loss 0.17764772 - samples/sec: 21.46 - lr: 0.000989
2023-06-03 08:05:34,227 epoch 2 - iter 1554/7770 - loss 0.16931079 - samples/sec: 20.50 - lr: 0.000978
2023-06-03 08:08:01,762 epoch 2 - iter 2331/7770 - loss 0.16614686 - samples/sec: 21.08 - lr: 0.000967
2023-06-03 08:10:21,783 epoch 2 - iter 3108/7770 - loss 0.16725615 - samples/sec: 22.21 - lr: 0.000956
2023-06-03 08:12:48,624 epoch 2 - iter 3885/7770 - loss 0.16513995 - samples/sec: 21.18 - lr: 0.000944
2023-06-03 08:15:09,707 epoch 2 - iter 4662/7770 - loss 0.16437051 - samples/sec: 22.04 - lr: 0.000933
2023-06-03 08:17:39,204 epoch 2 - iter 5439/7770 - loss 0.16338877 - samples/sec: 20.80 - lr: 0.000922
2023-06-03 08:20:08,442 epoch 2 - iter 6216/7770 - loss 0.16356160 - samples/sec: 20.84 - lr: 0.000911
2023-06-03 08:22:34,252 epoch 2 - iter 6993/7770 - loss 0.16381258 - samples/sec: 21.33 - lr: 0.000900
2023-06-03 08:25:00,849 epoch 2 - iter 7770/7770 - loss 0.16357479 - samples/sec: 21.21 - lr: 0.000889
2023-06-03 08:25:00,852 ----------------------------------------------------------------------------------------------------
2023-06-03 08:25:00,853 EPOCH 2 done: loss 0.1636 - lr 0.000889
2023-06-03 08:27:43,335 Evaluating as a multi-label problem: False
2023-06-03 08:27:43,443 DEV : loss 0.12165475636720657 - f1-score (micro avg)  0.8986
2023-06-03 08:27:43,645 BAD EPOCHS (no improvement): 4
2023-06-03 08:27:43,648 ----------------------------------------------------------------------------------------------------
2023-06-03 08:30:21,249 epoch 3 - iter 777/7770 - loss 0.15882936 - samples/sec: 19.73 - lr: 0.000878
2023-06-03 08:32:49,719 epoch 3 - iter 1554/7770 - loss 0.15798509 - samples/sec: 20.95 - lr: 0.000867
2023-06-03 08:35:10,754 epoch 3 - iter 2331/7770 - loss 0.15707480 - samples/sec: 22.05 - lr: 0.000856
2023-06-03 08:37:40,118 epoch 3 - iter 3108/7770 - loss 0.16009447 - samples/sec: 20.82 - lr: 0.000844
2023-06-03 08:40:11,049 epoch 3 - iter 3885/7770 - loss 0.15767433 - samples/sec: 20.60 - lr: 0.000833
2023-06-03 08:42:34,275 epoch 3 - iter 4662/7770 - loss 0.15643998 - samples/sec: 21.71 - lr: 0.000822
2023-06-03 08:44:57,560 epoch 3 - iter 5439/7770 - loss 0.15592890 - samples/sec: 21.70 - lr: 0.000811
2023-06-03 08:47:18,771 epoch 3 - iter 6216/7770 - loss 0.15575564 - samples/sec: 22.02 - lr: 0.000800
2023-06-03 08:49:46,976 epoch 3 - iter 6993/7770 - loss 0.15610394 - samples/sec: 20.98 - lr: 0.000789
2023-06-03 08:52:13,642 epoch 3 - iter 7770/7770 - loss 0.15592740 - samples/sec: 21.20 - lr: 0.000778
2023-06-03 08:52:13,646 ----------------------------------------------------------------------------------------------------
2023-06-03 08:52:13,646 EPOCH 3 done: loss 0.1559 - lr 0.000778
2023-06-03 08:54:52,855 Evaluating as a multi-label problem: False
2023-06-03 08:54:52,954 DEV : loss 0.12331711500883102 - f1-score (micro avg)  0.9016
2023-06-03 08:54:53,215 BAD EPOCHS (no improvement): 4
2023-06-03 08:54:53,218 ----------------------------------------------------------------------------------------------------
2023-06-03 08:57:23,043 epoch 4 - iter 777/7770 - loss 0.14521839 - samples/sec: 20.76 - lr: 0.000767
2023-06-03 08:59:48,931 epoch 4 - iter 1554/7770 - loss 0.14806760 - samples/sec: 21.32 - lr: 0.000756
2023-06-03 09:02:23,321 epoch 4 - iter 2331/7770 - loss 0.14668176 - samples/sec: 20.14 - lr: 0.000744
2023-06-03 09:04:54,986 epoch 4 - iter 3108/7770 - loss 0.14819734 - samples/sec: 20.50 - lr: 0.000733
2023-06-03 09:07:23,651 epoch 4 - iter 3885/7770 - loss 0.14698635 - samples/sec: 20.92 - lr: 0.000722
2023-06-03 09:09:46,161 epoch 4 - iter 4662/7770 - loss 0.14863222 - samples/sec: 21.82 - lr: 0.000711
2023-06-03 09:12:12,830 epoch 4 - iter 5439/7770 - loss 0.15085072 - samples/sec: 21.20 - lr: 0.000700
2023-06-03 09:14:36,073 epoch 4 - iter 6216/7770 - loss 0.15074519 - samples/sec: 21.71 - lr: 0.000689
2023-06-03 09:16:57,465 epoch 4 - iter 6993/7770 - loss 0.15148497 - samples/sec: 21.99 - lr: 0.000678
2023-06-03 09:19:24,051 epoch 4 - iter 7770/7770 - loss 0.15079300 - samples/sec: 21.21 - lr: 0.000667
2023-06-03 09:19:24,055 ----------------------------------------------------------------------------------------------------
2023-06-03 09:19:24,055 EPOCH 4 done: loss 0.1508 - lr 0.000667
2023-06-03 09:22:15,822 Evaluating as a multi-label problem: False
2023-06-03 09:22:15,930 DEV : loss 0.12153099477291107 - f1-score (micro avg)  0.9094
2023-06-03 09:22:16,175 BAD EPOCHS (no improvement): 4
2023-06-03 09:22:16,182 ----------------------------------------------------------------------------------------------------
2023-06-03 09:24:40,054 epoch 5 - iter 777/7770 - loss 0.14929209 - samples/sec: 21.62 - lr: 0.000656
2023-06-03 09:27:00,637 epoch 5 - iter 1554/7770 - loss 0.14556004 - samples/sec: 22.12 - lr: 0.000644
2023-06-03 09:29:23,196 epoch 5 - iter 2331/7770 - loss 0.14639669 - samples/sec: 21.81 - lr: 0.000633
2023-06-03 09:31:44,675 epoch 5 - iter 3108/7770 - loss 0.14763440 - samples/sec: 21.98 - lr: 0.000622
2023-06-03 09:34:07,703 epoch 5 - iter 3885/7770 - loss 0.14509046 - samples/sec: 21.74 - lr: 0.000611
2023-06-03 09:36:31,308 epoch 5 - iter 4662/7770 - loss 0.14344244 - samples/sec: 21.66 - lr: 0.000600
2023-06-03 09:39:00,302 epoch 5 - iter 5439/7770 - loss 0.14256348 - samples/sec: 20.87 - lr: 0.000589
2023-06-03 09:41:30,484 epoch 5 - iter 6216/7770 - loss 0.14375392 - samples/sec: 20.71 - lr: 0.000578
2023-06-03 09:44:03,468 epoch 5 - iter 6993/7770 - loss 0.14469183 - samples/sec: 20.33 - lr: 0.000567
2023-06-03 09:46:26,404 epoch 5 - iter 7770/7770 - loss 0.14434178 - samples/sec: 21.76 - lr: 0.000556
2023-06-03 09:46:26,408 ----------------------------------------------------------------------------------------------------
2023-06-03 09:46:26,408 EPOCH 5 done: loss 0.1443 - lr 0.000556
2023-06-03 09:49:19,887 Evaluating as a multi-label problem: False
2023-06-03 09:49:19,990 DEV : loss 0.11084271967411041 - f1-score (micro avg)  0.9067
2023-06-03 09:49:20,195 BAD EPOCHS (no improvement): 4
2023-06-03 09:49:20,198 ----------------------------------------------------------------------------------------------------
2023-06-03 09:51:43,910 epoch 6 - iter 777/7770 - loss 0.14985201 - samples/sec: 21.64 - lr: 0.000544
2023-06-03 09:54:10,106 epoch 6 - iter 1554/7770 - loss 0.13925044 - samples/sec: 21.27 - lr: 0.000533
2023-06-03 09:56:33,595 epoch 6 - iter 2331/7770 - loss 0.13599666 - samples/sec: 21.67 - lr: 0.000522
2023-06-03 09:58:56,290 epoch 6 - iter 3108/7770 - loss 0.13712726 - samples/sec: 21.79 - lr: 0.000511
2023-06-03 10:01:22,313 epoch 6 - iter 3885/7770 - loss 0.13946303 - samples/sec: 21.30 - lr: 0.000500
2023-06-03 10:03:45,751 epoch 6 - iter 4662/7770 - loss 0.13886702 - samples/sec: 21.68 - lr: 0.000489
2023-06-03 10:06:10,315 epoch 6 - iter 5439/7770 - loss 0.13828430 - samples/sec: 21.51 - lr: 0.000478
2023-06-03 10:08:27,041 epoch 6 - iter 6216/7770 - loss 0.13638284 - samples/sec: 22.74 - lr: 0.000467
2023-06-03 10:10:51,130 epoch 6 - iter 6993/7770 - loss 0.13636459 - samples/sec: 21.58 - lr: 0.000456
2023-06-03 10:13:11,614 epoch 6 - iter 7770/7770 - loss 0.13627267 - samples/sec: 22.14 - lr: 0.000445
2023-06-03 10:13:11,616 ----------------------------------------------------------------------------------------------------
2023-06-03 10:13:11,616 EPOCH 6 done: loss 0.1363 - lr 0.000445
2023-06-03 10:16:10,616 Evaluating as a multi-label problem: False
2023-06-03 10:16:10,721 DEV : loss 0.09839588403701782 - f1-score (micro avg)  0.9235
2023-06-03 10:16:10,977 BAD EPOCHS (no improvement): 4
2023-06-03 10:16:10,980 ----------------------------------------------------------------------------------------------------
2023-06-03 10:18:37,517 epoch 7 - iter 777/7770 - loss 0.12705462 - samples/sec: 21.22 - lr: 0.000433
2023-06-03 10:21:02,229 epoch 7 - iter 1554/7770 - loss 0.13169074 - samples/sec: 21.49 - lr: 0.000422
2023-06-03 10:23:36,553 epoch 7 - iter 2331/7770 - loss 0.13369615 - samples/sec: 20.15 - lr: 0.000411
2023-06-03 10:26:05,096 epoch 7 - iter 3108/7770 - loss 0.13059838 - samples/sec: 20.93 - lr: 0.000400
2023-06-03 10:28:29,270 epoch 7 - iter 3885/7770 - loss 0.13202757 - samples/sec: 21.57 - lr: 0.000389
2023-06-03 10:30:53,899 epoch 7 - iter 4662/7770 - loss 0.12849095 - samples/sec: 21.50 - lr: 0.000378
2023-06-03 10:33:10,934 epoch 7 - iter 5439/7770 - loss 0.12864795 - samples/sec: 22.69 - lr: 0.000367
2023-06-03 10:35:31,591 epoch 7 - iter 6216/7770 - loss 0.13020650 - samples/sec: 22.11 - lr: 0.000356
2023-06-03 10:37:47,369 epoch 7 - iter 6993/7770 - loss 0.12996485 - samples/sec: 22.90 - lr: 0.000345
2023-06-03 10:40:01,871 epoch 7 - iter 7770/7770 - loss 0.12930180 - samples/sec: 23.12 - lr: 0.000333
2023-06-03 10:40:01,874 ----------------------------------------------------------------------------------------------------
2023-06-03 10:40:01,875 EPOCH 7 done: loss 0.1293 - lr 0.000333
2023-06-03 10:42:45,249 Evaluating as a multi-label problem: False
2023-06-03 10:42:45,351 DEV : loss 0.11285041272640228 - f1-score (micro avg)  0.9217
2023-06-03 10:42:45,643 BAD EPOCHS (no improvement): 4
2023-06-03 10:42:45,646 ----------------------------------------------------------------------------------------------------
2023-06-03 10:45:24,732 epoch 8 - iter 777/7770 - loss 0.13784324 - samples/sec: 19.55 - lr: 0.000322
2023-06-03 10:47:52,382 epoch 8 - iter 1554/7770 - loss 0.13133449 - samples/sec: 21.06 - lr: 0.000311
2023-06-03 10:50:21,147 epoch 8 - iter 2331/7770 - loss 0.13064335 - samples/sec: 20.90 - lr: 0.000300
2023-06-03 10:52:39,478 epoch 8 - iter 3108/7770 - loss 0.12792643 - samples/sec: 22.48 - lr: 0.000289
2023-06-03 10:55:05,801 epoch 8 - iter 3885/7770 - loss 0.12488496 - samples/sec: 21.25 - lr: 0.000278
2023-06-03 10:57:29,971 epoch 8 - iter 4662/7770 - loss 0.12400596 - samples/sec: 21.57 - lr: 0.000267
2023-06-03 10:59:57,416 epoch 8 - iter 5439/7770 - loss 0.12367601 - samples/sec: 21.09 - lr: 0.000256
2023-06-03 11:02:22,254 epoch 8 - iter 6216/7770 - loss 0.12193298 - samples/sec: 21.47 - lr: 0.000245
2023-06-03 11:04:43,688 epoch 8 - iter 6993/7770 - loss 0.12140181 - samples/sec: 21.99 - lr: 0.000233
2023-06-03 11:07:09,533 epoch 8 - iter 7770/7770 - loss 0.12154089 - samples/sec: 21.32 - lr: 0.000222
2023-06-03 11:07:09,537 ----------------------------------------------------------------------------------------------------
2023-06-03 11:07:09,537 EPOCH 8 done: loss 0.1215 - lr 0.000222
2023-06-03 11:10:07,295 Evaluating as a multi-label problem: False
2023-06-03 11:10:07,392 DEV : loss 0.09286589920520782 - f1-score (micro avg)  0.9341
2023-06-03 11:10:07,667 BAD EPOCHS (no improvement): 4
2023-06-03 11:10:07,670 ----------------------------------------------------------------------------------------------------
2023-06-03 11:12:36,425 epoch 9 - iter 777/7770 - loss 0.11797741 - samples/sec: 20.91 - lr: 0.000211
2023-06-03 11:15:00,606 epoch 9 - iter 1554/7770 - loss 0.11720619 - samples/sec: 21.57 - lr: 0.000200
2023-06-03 11:17:24,725 epoch 9 - iter 2331/7770 - loss 0.11442407 - samples/sec: 21.58 - lr: 0.000189
2023-06-03 11:19:49,850 epoch 9 - iter 3108/7770 - loss 0.11555951 - samples/sec: 21.43 - lr: 0.000178
2023-06-03 11:22:10,534 epoch 9 - iter 3885/7770 - loss 0.11731024 - samples/sec: 22.10 - lr: 0.000167
2023-06-03 11:24:35,943 epoch 9 - iter 4662/7770 - loss 0.11656842 - samples/sec: 21.39 - lr: 0.000156
2023-06-03 11:26:50,251 epoch 9 - iter 5439/7770 - loss 0.11650117 - samples/sec: 23.15 - lr: 0.000145
2023-06-03 11:29:12,185 epoch 9 - iter 6216/7770 - loss 0.11580002 - samples/sec: 21.91 - lr: 0.000133
2023-06-03 11:31:31,468 epoch 9 - iter 6993/7770 - loss 0.11463501 - samples/sec: 22.33 - lr: 0.000122
2023-06-03 11:33:44,236 epoch 9 - iter 7770/7770 - loss 0.11398852 - samples/sec: 23.42 - lr: 0.000111
2023-06-03 11:33:44,240 ----------------------------------------------------------------------------------------------------
2023-06-03 11:33:44,240 EPOCH 9 done: loss 0.1140 - lr 0.000111
2023-06-03 11:36:57,833 Evaluating as a multi-label problem: False
2023-06-03 11:36:57,930 DEV : loss 0.09818769991397858 - f1-score (micro avg)  0.9336
2023-06-03 11:36:58,198 BAD EPOCHS (no improvement): 4
2023-06-03 11:36:58,201 ----------------------------------------------------------------------------------------------------
2023-06-03 11:39:25,958 epoch 10 - iter 777/7770 - loss 0.11073364 - samples/sec: 21.05 - lr: 0.000100
2023-06-03 11:41:50,381 epoch 10 - iter 1554/7770 - loss 0.10848589 - samples/sec: 21.53 - lr: 0.000089
2023-06-03 11:44:14,402 epoch 10 - iter 2331/7770 - loss 0.10920884 - samples/sec: 21.59 - lr: 0.000078
2023-06-03 11:46:42,133 epoch 10 - iter 3108/7770 - loss 0.10835528 - samples/sec: 21.05 - lr: 0.000067
2023-06-03 11:49:09,158 epoch 10 - iter 3885/7770 - loss 0.10928060 - samples/sec: 21.15 - lr: 0.000056
2023-06-03 11:51:32,509 epoch 10 - iter 4662/7770 - loss 0.11004782 - samples/sec: 21.69 - lr: 0.000045
2023-06-03 11:53:56,993 epoch 10 - iter 5439/7770 - loss 0.11006242 - samples/sec: 21.52 - lr: 0.000033
2023-06-03 11:56:21,308 epoch 10 - iter 6216/7770 - loss 0.10884840 - samples/sec: 21.55 - lr: 0.000022
2023-06-03 11:58:43,389 epoch 10 - iter 6993/7770 - loss 0.10843303 - samples/sec: 21.89 - lr: 0.000011
2023-06-03 12:01:05,954 epoch 10 - iter 7770/7770 - loss 0.10795638 - samples/sec: 21.81 - lr: 0.000000
2023-06-03 12:01:05,957 ----------------------------------------------------------------------------------------------------
2023-06-03 12:01:05,958 EPOCH 10 done: loss 0.1080 - lr 0.000000
2023-06-03 12:03:59,962 Evaluating as a multi-label problem: False
2023-06-03 12:04:00,068 DEV : loss 0.09486938267946243 - f1-score (micro avg)  0.9371
2023-06-03 12:04:00,295 BAD EPOCHS (no improvement): 4
2023-06-03 12:04:16,269 ----------------------------------------------------------------------------------------------------
2023-06-03 12:04:16,272 Testing using last state of model ...
2023-06-03 12:08:11,885 Evaluating as a multi-label problem: False
2023-06-03 12:08:11,996 0.9099	0.908	0.909	0.8676
2023-06-03 12:08:11,997 
Results:
- F-score (micro) 0.909
- F-score (macro) 0.9043
- Accuracy 0.8676

By class:
              precision    recall  f1-score   support

         PER     0.9717    0.9738    0.9728      2715
         ORG     0.8669    0.9064    0.8862      2543
         LOC     0.9309    0.8821    0.9058      2442
        MISC     0.8555    0.8491    0.8523      1889

   micro avg     0.9099    0.9080    0.9090      9589
   macro avg     0.9062    0.9029    0.9043      9589
weighted avg     0.9106    0.9080    0.9090      9589

2023-06-03 12:08:11,997 ----------------------------------------------------------------------------------------------------
2023-06-03 12:08:11,997 ----------------------------------------------------------------------------------------------------
2023-06-03 12:10:44,101 Evaluating as a multi-label problem: False
2023-06-03 12:10:44,155 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-03 12:10:44,155 0.9102	0.9051	0.9076	0.881
2023-06-03 12:10:44,155 ----------------------------------------------------------------------------------------------------
2023-06-03 12:12:17,872 Evaluating as a multi-label problem: False
2023-06-03 12:12:17,947 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-03 12:12:17,947 0.9097	0.9101	0.9099	0.8585
