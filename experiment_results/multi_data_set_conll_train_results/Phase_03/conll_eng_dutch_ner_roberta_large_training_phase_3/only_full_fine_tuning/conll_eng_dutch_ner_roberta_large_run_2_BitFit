2023-06-09 19:03:44,082 ----------------------------------------------------------------------------------------------------
2023-06-09 19:03:44,088 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-09 19:03:44,094 ----------------------------------------------------------------------------------------------------
2023-06-09 19:03:44,095 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-09 19:03:44,095 ----------------------------------------------------------------------------------------------------
2023-06-09 19:03:44,095 Parameters:
2023-06-09 19:03:44,095  - learning_rate: "0.001000"
2023-06-09 19:03:44,095  - mini_batch_size: "4"
2023-06-09 19:03:44,095  - patience: "3"
2023-06-09 19:03:44,095  - anneal_factor: "0.5"
2023-06-09 19:03:44,095  - max_epochs: "10"
2023-06-09 19:03:44,095  - shuffle: "True"
2023-06-09 19:03:44,095  - train_with_dev: "False"
2023-06-09 19:03:44,095  - batch_growth_annealing: "False"
2023-06-09 19:03:44,095 ----------------------------------------------------------------------------------------------------
2023-06-09 19:03:44,095 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_2_BitFit"
2023-06-09 19:03:44,095 ----------------------------------------------------------------------------------------------------
2023-06-09 19:03:44,095 Device: cuda:2
2023-06-09 19:03:44,095 ----------------------------------------------------------------------------------------------------
2023-06-09 19:03:44,095 Embeddings storage mode: none
2023-06-09 19:03:44,096 ----------------------------------------------------------------------------------------------------
2023-06-09 19:06:31,541 epoch 1 - iter 777/7770 - loss 2.04909250 - samples/sec: 18.57 - lr: 0.000100
2023-06-09 19:09:12,769 epoch 1 - iter 1554/7770 - loss 1.27281859 - samples/sec: 19.29 - lr: 0.000200
2023-06-09 19:11:51,171 epoch 1 - iter 2331/7770 - loss 1.03418343 - samples/sec: 19.63 - lr: 0.000300
2023-06-09 19:14:31,254 epoch 1 - iter 3108/7770 - loss 0.87235212 - samples/sec: 19.42 - lr: 0.000400
2023-06-09 19:17:11,585 epoch 1 - iter 3885/7770 - loss 0.75975659 - samples/sec: 19.39 - lr: 0.000500
2023-06-09 19:19:47,176 epoch 1 - iter 4662/7770 - loss 0.70374629 - samples/sec: 19.99 - lr: 0.000600
2023-06-09 19:22:14,750 epoch 1 - iter 5439/7770 - loss 0.66822888 - samples/sec: 21.07 - lr: 0.000700
2023-06-09 19:24:45,682 epoch 1 - iter 6216/7770 - loss 0.62735257 - samples/sec: 20.60 - lr: 0.000800
2023-06-09 19:27:18,841 epoch 1 - iter 6993/7770 - loss 0.59506149 - samples/sec: 20.30 - lr: 0.000900
2023-06-09 19:29:51,574 epoch 1 - iter 7770/7770 - loss 0.55928000 - samples/sec: 20.36 - lr: 0.001000
2023-06-09 19:29:51,577 ----------------------------------------------------------------------------------------------------
2023-06-09 19:29:51,577 EPOCH 1 done: loss 0.5593 - lr 0.001000
2023-06-09 19:33:19,500 Evaluating as a multi-label problem: False
2023-06-09 19:33:19,608 DEV : loss 0.1731666624546051 - f1-score (micro avg)  0.7919
2023-06-09 19:33:19,840 BAD EPOCHS (no improvement): 4
2023-06-09 19:33:19,855 ----------------------------------------------------------------------------------------------------
2023-06-09 19:36:05,154 epoch 2 - iter 777/7770 - loss 0.25900365 - samples/sec: 18.81 - lr: 0.000989
2023-06-09 19:38:46,601 epoch 2 - iter 1554/7770 - loss 0.25630279 - samples/sec: 19.26 - lr: 0.000978
2023-06-09 19:41:25,787 epoch 2 - iter 2331/7770 - loss 0.24737112 - samples/sec: 19.54 - lr: 0.000967
2023-06-09 19:44:03,933 epoch 2 - iter 3108/7770 - loss 0.23988411 - samples/sec: 19.66 - lr: 0.000956
2023-06-09 19:46:50,209 epoch 2 - iter 3885/7770 - loss 0.23511647 - samples/sec: 18.70 - lr: 0.000944
2023-06-09 19:49:30,490 epoch 2 - iter 4662/7770 - loss 0.23255235 - samples/sec: 19.40 - lr: 0.000933
2023-06-09 19:52:10,697 epoch 2 - iter 5439/7770 - loss 0.22581398 - samples/sec: 19.41 - lr: 0.000922
2023-06-09 19:54:50,672 epoch 2 - iter 6216/7770 - loss 0.22174366 - samples/sec: 19.44 - lr: 0.000911
2023-06-09 19:57:26,594 epoch 2 - iter 6993/7770 - loss 0.21887771 - samples/sec: 19.95 - lr: 0.000900
2023-06-09 20:00:03,486 epoch 2 - iter 7770/7770 - loss 0.21506252 - samples/sec: 19.82 - lr: 0.000889
2023-06-09 20:00:03,491 ----------------------------------------------------------------------------------------------------
2023-06-09 20:00:03,491 EPOCH 2 done: loss 0.2151 - lr 0.000889
2023-06-09 20:03:07,705 Evaluating as a multi-label problem: False
2023-06-09 20:03:07,809 DEV : loss 0.12941785156726837 - f1-score (micro avg)  0.8478
2023-06-09 20:03:08,046 BAD EPOCHS (no improvement): 4
2023-06-09 20:03:08,050 ----------------------------------------------------------------------------------------------------
2023-06-09 20:05:49,898 epoch 3 - iter 777/7770 - loss 0.19340580 - samples/sec: 19.22 - lr: 0.000878
2023-06-09 20:08:30,208 epoch 3 - iter 1554/7770 - loss 0.19221309 - samples/sec: 19.40 - lr: 0.000867
2023-06-09 20:11:09,151 epoch 3 - iter 2331/7770 - loss 0.19162983 - samples/sec: 19.57 - lr: 0.000856
2023-06-09 20:13:48,390 epoch 3 - iter 3108/7770 - loss 0.19129660 - samples/sec: 19.53 - lr: 0.000844
2023-06-09 20:16:27,619 epoch 3 - iter 3885/7770 - loss 0.18943987 - samples/sec: 19.53 - lr: 0.000833
2023-06-09 20:19:04,193 epoch 3 - iter 4662/7770 - loss 0.18508754 - samples/sec: 19.86 - lr: 0.000822
2023-06-09 20:21:41,362 epoch 3 - iter 5439/7770 - loss 0.18380589 - samples/sec: 19.79 - lr: 0.000811
2023-06-09 20:24:17,261 epoch 3 - iter 6216/7770 - loss 0.18243514 - samples/sec: 19.95 - lr: 0.000800
2023-06-09 20:27:02,616 epoch 3 - iter 6993/7770 - loss 0.18109584 - samples/sec: 18.81 - lr: 0.000789
2023-06-09 20:29:43,088 epoch 3 - iter 7770/7770 - loss 0.18025045 - samples/sec: 19.38 - lr: 0.000778
2023-06-09 20:29:43,092 ----------------------------------------------------------------------------------------------------
2023-06-09 20:29:43,093 EPOCH 3 done: loss 0.1803 - lr 0.000778
2023-06-09 20:32:47,263 Evaluating as a multi-label problem: False
2023-06-09 20:32:47,372 DEV : loss 0.1169494017958641 - f1-score (micro avg)  0.8749
2023-06-09 20:32:47,617 BAD EPOCHS (no improvement): 4
2023-06-09 20:32:47,620 ----------------------------------------------------------------------------------------------------
2023-06-09 20:35:34,731 epoch 4 - iter 777/7770 - loss 0.16353898 - samples/sec: 18.61 - lr: 0.000767
2023-06-09 20:38:15,219 epoch 4 - iter 1554/7770 - loss 0.16235762 - samples/sec: 19.38 - lr: 0.000756
2023-06-09 20:40:56,213 epoch 4 - iter 2331/7770 - loss 0.16210896 - samples/sec: 19.32 - lr: 0.000744
2023-06-09 20:43:35,890 epoch 4 - iter 3108/7770 - loss 0.16161830 - samples/sec: 19.48 - lr: 0.000733
2023-06-09 20:46:15,606 epoch 4 - iter 3885/7770 - loss 0.16256900 - samples/sec: 19.47 - lr: 0.000722
2023-06-09 20:48:52,403 epoch 4 - iter 4662/7770 - loss 0.16354415 - samples/sec: 19.83 - lr: 0.000711
2023-06-09 20:51:29,075 epoch 4 - iter 5439/7770 - loss 0.16451784 - samples/sec: 19.85 - lr: 0.000700
2023-06-09 20:54:06,910 epoch 4 - iter 6216/7770 - loss 0.16349386 - samples/sec: 19.70 - lr: 0.000689
2023-06-09 20:56:43,789 epoch 4 - iter 6993/7770 - loss 0.16526326 - samples/sec: 19.82 - lr: 0.000678
2023-06-09 20:59:21,702 epoch 4 - iter 7770/7770 - loss 0.16449219 - samples/sec: 19.69 - lr: 0.000667
2023-06-09 20:59:21,706 ----------------------------------------------------------------------------------------------------
2023-06-09 20:59:21,707 EPOCH 4 done: loss 0.1645 - lr 0.000667
2023-06-09 21:02:48,599 Evaluating as a multi-label problem: False
2023-06-09 21:02:48,712 DEV : loss 0.11832726001739502 - f1-score (micro avg)  0.8966
2023-06-09 21:02:48,943 BAD EPOCHS (no improvement): 4
2023-06-09 21:02:48,946 ----------------------------------------------------------------------------------------------------
2023-06-09 21:05:32,734 epoch 5 - iter 777/7770 - loss 0.16539568 - samples/sec: 18.99 - lr: 0.000656
2023-06-09 21:08:15,284 epoch 5 - iter 1554/7770 - loss 0.16586218 - samples/sec: 19.13 - lr: 0.000644
2023-06-09 21:10:54,877 epoch 5 - iter 2331/7770 - loss 0.15955825 - samples/sec: 19.49 - lr: 0.000633
2023-06-09 21:13:35,868 epoch 5 - iter 3108/7770 - loss 0.16102609 - samples/sec: 19.32 - lr: 0.000622
2023-06-09 21:16:23,514 epoch 5 - iter 3885/7770 - loss 0.15851664 - samples/sec: 18.55 - lr: 0.000611
2023-06-09 21:19:05,344 epoch 5 - iter 4662/7770 - loss 0.15836334 - samples/sec: 19.22 - lr: 0.000600
2023-06-09 21:21:44,230 epoch 5 - iter 5439/7770 - loss 0.15695800 - samples/sec: 19.57 - lr: 0.000589
2023-06-09 21:24:22,718 epoch 5 - iter 6216/7770 - loss 0.15600849 - samples/sec: 19.62 - lr: 0.000578
2023-06-09 21:26:59,863 epoch 5 - iter 6993/7770 - loss 0.15580226 - samples/sec: 19.79 - lr: 0.000567
2023-06-09 21:29:37,203 epoch 5 - iter 7770/7770 - loss 0.15557799 - samples/sec: 19.77 - lr: 0.000556
2023-06-09 21:29:37,207 ----------------------------------------------------------------------------------------------------
2023-06-09 21:29:37,207 EPOCH 5 done: loss 0.1556 - lr 0.000556
2023-06-09 21:32:38,134 Evaluating as a multi-label problem: False
2023-06-09 21:32:38,243 DEV : loss 0.12176160514354706 - f1-score (micro avg)  0.8963
2023-06-09 21:32:38,490 BAD EPOCHS (no improvement): 4
2023-06-09 21:32:38,504 ----------------------------------------------------------------------------------------------------
2023-06-09 21:35:22,546 epoch 6 - iter 777/7770 - loss 0.15068280 - samples/sec: 18.96 - lr: 0.000544
2023-06-09 21:38:00,363 epoch 6 - iter 1554/7770 - loss 0.15228373 - samples/sec: 19.71 - lr: 0.000533
2023-06-09 21:40:39,953 epoch 6 - iter 2331/7770 - loss 0.15281529 - samples/sec: 19.49 - lr: 0.000522
2023-06-09 21:43:18,290 epoch 6 - iter 3108/7770 - loss 0.15005576 - samples/sec: 19.64 - lr: 0.000511
2023-06-09 21:45:54,035 epoch 6 - iter 3885/7770 - loss 0.14770158 - samples/sec: 19.97 - lr: 0.000500
2023-06-09 21:48:32,139 epoch 6 - iter 4662/7770 - loss 0.14718203 - samples/sec: 19.67 - lr: 0.000489
2023-06-09 21:51:06,564 epoch 6 - iter 5439/7770 - loss 0.14671410 - samples/sec: 20.14 - lr: 0.000478
2023-06-09 21:53:42,474 epoch 6 - iter 6216/7770 - loss 0.14571508 - samples/sec: 19.95 - lr: 0.000467
2023-06-09 21:56:30,863 epoch 6 - iter 6993/7770 - loss 0.14460065 - samples/sec: 18.47 - lr: 0.000456
2023-06-09 21:59:12,339 epoch 6 - iter 7770/7770 - loss 0.14599904 - samples/sec: 19.26 - lr: 0.000445
2023-06-09 21:59:12,344 ----------------------------------------------------------------------------------------------------
2023-06-09 21:59:12,345 EPOCH 6 done: loss 0.1460 - lr 0.000445
2023-06-09 22:02:22,449 Evaluating as a multi-label problem: False
2023-06-09 22:02:22,560 DEV : loss 0.10700776427984238 - f1-score (micro avg)  0.904
2023-06-09 22:02:22,819 BAD EPOCHS (no improvement): 4
2023-06-09 22:02:22,825 ----------------------------------------------------------------------------------------------------
2023-06-09 22:05:10,820 epoch 7 - iter 777/7770 - loss 0.13107047 - samples/sec: 18.51 - lr: 0.000433
2023-06-09 22:07:52,174 epoch 7 - iter 1554/7770 - loss 0.14182616 - samples/sec: 19.27 - lr: 0.000422
2023-06-09 22:10:32,592 epoch 7 - iter 2331/7770 - loss 0.13912068 - samples/sec: 19.39 - lr: 0.000411
2023-06-09 22:13:09,093 epoch 7 - iter 3108/7770 - loss 0.13964240 - samples/sec: 19.87 - lr: 0.000400
2023-06-09 22:15:46,950 epoch 7 - iter 3885/7770 - loss 0.14084442 - samples/sec: 19.70 - lr: 0.000389
2023-06-09 22:18:24,036 epoch 7 - iter 4662/7770 - loss 0.14026125 - samples/sec: 19.80 - lr: 0.000378
2023-06-09 22:21:01,808 epoch 7 - iter 5439/7770 - loss 0.13999496 - samples/sec: 19.71 - lr: 0.000367
2023-06-09 22:23:37,876 epoch 7 - iter 6216/7770 - loss 0.13872110 - samples/sec: 19.93 - lr: 0.000356
2023-06-09 22:26:13,786 epoch 7 - iter 6993/7770 - loss 0.13881724 - samples/sec: 19.95 - lr: 0.000345
2023-06-09 22:28:49,976 epoch 7 - iter 7770/7770 - loss 0.13687564 - samples/sec: 19.91 - lr: 0.000333
2023-06-09 22:28:49,980 ----------------------------------------------------------------------------------------------------
2023-06-09 22:28:49,980 EPOCH 7 done: loss 0.1369 - lr 0.000333
2023-06-09 22:32:15,098 Evaluating as a multi-label problem: False
2023-06-09 22:32:15,205 DEV : loss 0.11368750035762787 - f1-score (micro avg)  0.9096
2023-06-09 22:32:15,451 BAD EPOCHS (no improvement): 4
2023-06-09 22:32:15,729 ----------------------------------------------------------------------------------------------------
2023-06-09 22:34:59,342 epoch 8 - iter 777/7770 - loss 0.12473817 - samples/sec: 19.01 - lr: 0.000322
2023-06-09 22:37:37,643 epoch 8 - iter 1554/7770 - loss 0.13079679 - samples/sec: 19.65 - lr: 0.000311
2023-06-09 22:40:17,639 epoch 8 - iter 2331/7770 - loss 0.13021764 - samples/sec: 19.44 - lr: 0.000300
2023-06-09 22:42:56,651 epoch 8 - iter 3108/7770 - loss 0.13057134 - samples/sec: 19.56 - lr: 0.000289
2023-06-09 22:45:44,174 epoch 8 - iter 3885/7770 - loss 0.13020757 - samples/sec: 18.56 - lr: 0.000278
2023-06-09 22:48:24,350 epoch 8 - iter 4662/7770 - loss 0.13127963 - samples/sec: 19.42 - lr: 0.000267
2023-06-09 22:51:04,598 epoch 8 - iter 5439/7770 - loss 0.13257959 - samples/sec: 19.41 - lr: 0.000256
2023-06-09 22:53:44,033 epoch 8 - iter 6216/7770 - loss 0.13201852 - samples/sec: 19.51 - lr: 0.000245
2023-06-09 22:56:23,702 epoch 8 - iter 6993/7770 - loss 0.13159987 - samples/sec: 19.48 - lr: 0.000233
2023-06-09 22:59:00,286 epoch 8 - iter 7770/7770 - loss 0.13066321 - samples/sec: 19.86 - lr: 0.000222
2023-06-09 22:59:00,290 ----------------------------------------------------------------------------------------------------
2023-06-09 22:59:00,290 EPOCH 8 done: loss 0.1307 - lr 0.000222
2023-06-09 23:02:08,168 Evaluating as a multi-label problem: False
2023-06-09 23:02:08,279 DEV : loss 0.10649736225605011 - f1-score (micro avg)  0.9185
2023-06-09 23:02:08,516 BAD EPOCHS (no improvement): 4
2023-06-09 23:02:08,519 ----------------------------------------------------------------------------------------------------
2023-06-09 23:04:48,609 epoch 9 - iter 777/7770 - loss 0.11573016 - samples/sec: 19.43 - lr: 0.000211
2023-06-09 23:07:26,868 epoch 9 - iter 1554/7770 - loss 0.11782419 - samples/sec: 19.65 - lr: 0.000200
2023-06-09 23:10:05,042 epoch 9 - iter 2331/7770 - loss 0.11691338 - samples/sec: 19.66 - lr: 0.000189
2023-06-09 23:12:40,710 epoch 9 - iter 3108/7770 - loss 0.11671411 - samples/sec: 19.98 - lr: 0.000178
2023-06-09 23:15:16,260 epoch 9 - iter 3885/7770 - loss 0.11821013 - samples/sec: 19.99 - lr: 0.000167
2023-06-09 23:17:52,312 epoch 9 - iter 4662/7770 - loss 0.11903426 - samples/sec: 19.93 - lr: 0.000156
2023-06-09 23:20:26,569 epoch 9 - iter 5439/7770 - loss 0.11876763 - samples/sec: 20.16 - lr: 0.000145
2023-06-09 23:23:12,146 epoch 9 - iter 6216/7770 - loss 0.11913058 - samples/sec: 18.78 - lr: 0.000133
2023-06-09 23:25:56,124 epoch 9 - iter 6993/7770 - loss 0.11881178 - samples/sec: 18.96 - lr: 0.000122
2023-06-09 23:28:38,302 epoch 9 - iter 7770/7770 - loss 0.11948850 - samples/sec: 19.18 - lr: 0.000111
2023-06-09 23:28:38,306 ----------------------------------------------------------------------------------------------------
2023-06-09 23:28:38,306 EPOCH 9 done: loss 0.1195 - lr 0.000111
2023-06-09 23:31:46,196 Evaluating as a multi-label problem: False
2023-06-09 23:31:46,290 DEV : loss 0.10208438336849213 - f1-score (micro avg)  0.9217
2023-06-09 23:31:46,499 BAD EPOCHS (no improvement): 4
2023-06-09 23:31:46,502 ----------------------------------------------------------------------------------------------------
2023-06-09 23:34:26,181 epoch 10 - iter 777/7770 - loss 0.11246730 - samples/sec: 19.48 - lr: 0.000100
2023-06-09 23:37:06,430 epoch 10 - iter 1554/7770 - loss 0.11823788 - samples/sec: 19.41 - lr: 0.000089
2023-06-09 23:39:43,258 epoch 10 - iter 2331/7770 - loss 0.11403797 - samples/sec: 19.83 - lr: 0.000078
2023-06-09 23:42:20,395 epoch 10 - iter 3108/7770 - loss 0.11538478 - samples/sec: 19.79 - lr: 0.000067
2023-06-09 23:44:57,146 epoch 10 - iter 3885/7770 - loss 0.11632935 - samples/sec: 19.84 - lr: 0.000056
2023-06-09 23:47:31,865 epoch 10 - iter 4662/7770 - loss 0.11720464 - samples/sec: 20.10 - lr: 0.000045
2023-06-09 23:50:07,206 epoch 10 - iter 5439/7770 - loss 0.11729650 - samples/sec: 20.02 - lr: 0.000033
2023-06-09 23:52:42,570 epoch 10 - iter 6216/7770 - loss 0.11605056 - samples/sec: 20.02 - lr: 0.000022
2023-06-09 23:55:17,976 epoch 10 - iter 6993/7770 - loss 0.11532761 - samples/sec: 20.01 - lr: 0.000011
2023-06-09 23:57:51,469 epoch 10 - iter 7770/7770 - loss 0.11491113 - samples/sec: 20.26 - lr: 0.000000
2023-06-09 23:57:51,474 ----------------------------------------------------------------------------------------------------
2023-06-09 23:57:51,474 EPOCH 10 done: loss 0.1149 - lr 0.000000
2023-06-10 00:01:22,248 Evaluating as a multi-label problem: False
2023-06-10 00:01:22,354 DEV : loss 0.10068431496620178 - f1-score (micro avg)  0.9273
2023-06-10 00:01:22,577 BAD EPOCHS (no improvement): 4
2023-06-10 00:01:44,420 ----------------------------------------------------------------------------------------------------
2023-06-10 00:01:44,425 Testing using last state of model ...
2023-06-10 00:06:29,241 Evaluating as a multi-label problem: False
2023-06-10 00:06:29,366 0.8991	0.8978	0.8985	0.8504
2023-06-10 00:06:29,367 
Results:
- F-score (micro) 0.8985
- F-score (macro) 0.893
- Accuracy 0.8504

By class:
              precision    recall  f1-score   support

         PER     0.9726    0.9683    0.9705      2715
         ORG     0.8578    0.8919    0.8745      2543
         LOC     0.9181    0.8771    0.8972      2442
        MISC     0.8285    0.8311    0.8298      1889

   micro avg     0.8991    0.8978    0.8985      9589
   macro avg     0.8943    0.8921    0.8930      9589
weighted avg     0.8999    0.8978    0.8986      9589

2023-06-10 00:06:29,367 ----------------------------------------------------------------------------------------------------
2023-06-10 00:06:29,367 ----------------------------------------------------------------------------------------------------
2023-06-10 00:09:25,279 Evaluating as a multi-label problem: False
2023-06-10 00:09:25,333 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-10 00:09:25,334 0.9014	0.8886	0.895	0.8607
2023-06-10 00:09:25,334 ----------------------------------------------------------------------------------------------------
2023-06-10 00:11:11,814 Evaluating as a multi-label problem: False
2023-06-10 00:11:11,890 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-10 00:11:11,890 0.8977	0.9042	0.9009	0.8436
