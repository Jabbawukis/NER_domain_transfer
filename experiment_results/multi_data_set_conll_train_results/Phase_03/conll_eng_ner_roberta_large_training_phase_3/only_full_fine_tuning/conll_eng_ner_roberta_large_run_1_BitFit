2023-06-02 22:08:52,878 ----------------------------------------------------------------------------------------------------
2023-06-02 22:08:52,883 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-02 22:08:52,884 ----------------------------------------------------------------------------------------------------
2023-06-02 22:08:52,885 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-02 22:08:52,887 ----------------------------------------------------------------------------------------------------
2023-06-02 22:08:52,887 Parameters:
2023-06-02 22:08:52,887  - learning_rate: "0.001000"
2023-06-02 22:08:52,887  - mini_batch_size: "4"
2023-06-02 22:08:52,887  - patience: "3"
2023-06-02 22:08:52,887  - anneal_factor: "0.5"
2023-06-02 22:08:52,887  - max_epochs: "10"
2023-06-02 22:08:52,887  - shuffle: "True"
2023-06-02 22:08:52,887  - train_with_dev: "False"
2023-06-02 22:08:52,889  - batch_growth_annealing: "False"
2023-06-02 22:08:52,890 ----------------------------------------------------------------------------------------------------
2023-06-02 22:08:52,890 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_1_BitFit"
2023-06-02 22:08:52,890 ----------------------------------------------------------------------------------------------------
2023-06-02 22:08:52,890 Device: cuda:0
2023-06-02 22:08:52,890 ----------------------------------------------------------------------------------------------------
2023-06-02 22:08:52,890 Embeddings storage mode: none
2023-06-02 22:08:52,890 ----------------------------------------------------------------------------------------------------
2023-06-02 22:09:58,983 epoch 1 - iter 374/3747 - loss 2.80992951 - samples/sec: 22.65 - lr: 0.000100
2023-06-02 22:11:04,007 epoch 1 - iter 748/3747 - loss 1.97462811 - samples/sec: 23.02 - lr: 0.000200
2023-06-02 22:12:10,559 epoch 1 - iter 1122/3747 - loss 1.62355325 - samples/sec: 22.49 - lr: 0.000299
2023-06-02 22:13:16,312 epoch 1 - iter 1496/3747 - loss 1.44302005 - samples/sec: 22.76 - lr: 0.000399
2023-06-02 22:14:23,028 epoch 1 - iter 1870/3747 - loss 1.27113761 - samples/sec: 22.43 - lr: 0.000499
2023-06-02 22:15:33,415 epoch 1 - iter 2244/3747 - loss 1.12577402 - samples/sec: 21.26 - lr: 0.000599
2023-06-02 22:16:42,431 epoch 1 - iter 2618/3747 - loss 1.01734363 - samples/sec: 21.69 - lr: 0.000699
2023-06-02 22:17:52,960 epoch 1 - iter 2992/3747 - loss 0.94673112 - samples/sec: 21.22 - lr: 0.000799
2023-06-02 22:18:56,998 epoch 1 - iter 3366/3747 - loss 0.87888899 - samples/sec: 23.37 - lr: 0.000898
2023-06-02 22:20:04,188 epoch 1 - iter 3740/3747 - loss 0.81223988 - samples/sec: 22.28 - lr: 0.000998
2023-06-02 22:20:05,215 ----------------------------------------------------------------------------------------------------
2023-06-02 22:20:05,215 EPOCH 1 done: loss 0.8117 - lr 0.000998
2023-06-02 22:21:19,322 Evaluating as a multi-label problem: False
2023-06-02 22:21:19,404 DEV : loss 0.20643024146556854 - f1-score (micro avg)  0.7844
2023-06-02 22:21:19,509 BAD EPOCHS (no improvement): 4
2023-06-02 22:21:19,512 ----------------------------------------------------------------------------------------------------
2023-06-02 22:22:30,203 epoch 2 - iter 374/3747 - loss 0.32723284 - samples/sec: 21.17 - lr: 0.000989
2023-06-02 22:23:43,528 epoch 2 - iter 748/3747 - loss 0.31168831 - samples/sec: 20.41 - lr: 0.000978
2023-06-02 22:24:47,347 epoch 2 - iter 1122/3747 - loss 0.30940069 - samples/sec: 23.45 - lr: 0.000967
2023-06-02 22:25:55,774 epoch 2 - iter 1496/3747 - loss 0.30011260 - samples/sec: 21.87 - lr: 0.000956
2023-06-02 22:27:02,547 epoch 2 - iter 1870/3747 - loss 0.29311654 - samples/sec: 22.42 - lr: 0.000945
2023-06-02 22:28:09,246 epoch 2 - iter 2244/3747 - loss 0.28855376 - samples/sec: 22.44 - lr: 0.000933
2023-06-02 22:29:20,560 epoch 2 - iter 2618/3747 - loss 0.28339414 - samples/sec: 20.99 - lr: 0.000922
2023-06-02 22:30:29,359 epoch 2 - iter 2992/3747 - loss 0.27522395 - samples/sec: 21.76 - lr: 0.000911
2023-06-02 22:31:41,152 epoch 2 - iter 3366/3747 - loss 0.26922270 - samples/sec: 20.85 - lr: 0.000900
2023-06-02 22:32:50,249 epoch 2 - iter 3740/3747 - loss 0.26597156 - samples/sec: 21.66 - lr: 0.000889
2023-06-02 22:32:51,556 ----------------------------------------------------------------------------------------------------
2023-06-02 22:32:51,556 EPOCH 2 done: loss 0.2658 - lr 0.000889
2023-06-02 22:34:15,159 Evaluating as a multi-label problem: False
2023-06-02 22:34:15,225 DEV : loss 0.11763599514961243 - f1-score (micro avg)  0.883
2023-06-02 22:34:15,329 BAD EPOCHS (no improvement): 4
2023-06-02 22:34:15,336 ----------------------------------------------------------------------------------------------------
2023-06-02 22:35:23,123 epoch 3 - iter 374/3747 - loss 0.20889211 - samples/sec: 22.08 - lr: 0.000878
2023-06-02 22:36:28,745 epoch 3 - iter 748/3747 - loss 0.20295983 - samples/sec: 22.81 - lr: 0.000867
2023-06-02 22:37:35,696 epoch 3 - iter 1122/3747 - loss 0.20151932 - samples/sec: 22.36 - lr: 0.000856
2023-06-02 22:38:43,085 epoch 3 - iter 1496/3747 - loss 0.20096770 - samples/sec: 22.21 - lr: 0.000845
2023-06-02 22:39:50,816 epoch 3 - iter 1870/3747 - loss 0.20150985 - samples/sec: 22.10 - lr: 0.000833
2023-06-02 22:40:59,798 epoch 3 - iter 2244/3747 - loss 0.20322319 - samples/sec: 21.70 - lr: 0.000822
2023-06-02 22:42:10,991 epoch 3 - iter 2618/3747 - loss 0.20093146 - samples/sec: 21.03 - lr: 0.000811
2023-06-02 22:43:19,218 epoch 3 - iter 2992/3747 - loss 0.19801052 - samples/sec: 21.94 - lr: 0.000800
2023-06-02 22:44:26,028 epoch 3 - iter 3366/3747 - loss 0.19523577 - samples/sec: 22.40 - lr: 0.000789
2023-06-02 22:45:30,614 epoch 3 - iter 3740/3747 - loss 0.19302199 - samples/sec: 23.18 - lr: 0.000778
2023-06-02 22:45:31,835 ----------------------------------------------------------------------------------------------------
2023-06-02 22:45:31,835 EPOCH 3 done: loss 0.1930 - lr 0.000778
2023-06-02 22:46:54,527 Evaluating as a multi-label problem: False
2023-06-02 22:46:54,573 DEV : loss 0.11126094311475754 - f1-score (micro avg)  0.9002
2023-06-02 22:46:54,648 BAD EPOCHS (no improvement): 4
2023-06-02 22:46:54,658 ----------------------------------------------------------------------------------------------------
2023-06-02 22:48:02,584 epoch 4 - iter 374/3747 - loss 0.18198788 - samples/sec: 22.04 - lr: 0.000767
2023-06-02 22:49:11,518 epoch 4 - iter 748/3747 - loss 0.18216106 - samples/sec: 21.71 - lr: 0.000756
2023-06-02 22:50:21,039 epoch 4 - iter 1122/3747 - loss 0.18274370 - samples/sec: 21.53 - lr: 0.000745
2023-06-02 22:51:30,667 epoch 4 - iter 1496/3747 - loss 0.17995309 - samples/sec: 21.50 - lr: 0.000733
2023-06-02 22:52:39,154 epoch 4 - iter 1870/3747 - loss 0.17994574 - samples/sec: 21.86 - lr: 0.000722
2023-06-02 22:53:47,616 epoch 4 - iter 2244/3747 - loss 0.18097362 - samples/sec: 21.86 - lr: 0.000711
2023-06-02 22:54:56,079 epoch 4 - iter 2618/3747 - loss 0.17822527 - samples/sec: 21.86 - lr: 0.000700
2023-06-02 22:56:05,283 epoch 4 - iter 2992/3747 - loss 0.17881392 - samples/sec: 21.63 - lr: 0.000689
2023-06-02 22:57:14,834 epoch 4 - iter 3366/3747 - loss 0.18038135 - samples/sec: 21.52 - lr: 0.000678
2023-06-02 22:58:20,334 epoch 4 - iter 3740/3747 - loss 0.17919476 - samples/sec: 22.85 - lr: 0.000667
2023-06-02 22:58:21,620 ----------------------------------------------------------------------------------------------------
2023-06-02 22:58:21,620 EPOCH 4 done: loss 0.1790 - lr 0.000667
2023-06-02 22:59:46,674 Evaluating as a multi-label problem: False
2023-06-02 22:59:46,746 DEV : loss 0.1070593073964119 - f1-score (micro avg)  0.913
2023-06-02 22:59:46,852 BAD EPOCHS (no improvement): 4
2023-06-02 22:59:46,855 ----------------------------------------------------------------------------------------------------
2023-06-02 23:01:01,714 epoch 5 - iter 374/3747 - loss 0.16882396 - samples/sec: 20.00 - lr: 0.000656
2023-06-02 23:02:12,386 epoch 5 - iter 748/3747 - loss 0.16021595 - samples/sec: 21.18 - lr: 0.000645
2023-06-02 23:03:20,453 epoch 5 - iter 1122/3747 - loss 0.16055685 - samples/sec: 21.99 - lr: 0.000633
2023-06-02 23:04:31,398 epoch 5 - iter 1496/3747 - loss 0.15999551 - samples/sec: 21.10 - lr: 0.000622
2023-06-02 23:05:44,362 epoch 5 - iter 1870/3747 - loss 0.16106054 - samples/sec: 20.51 - lr: 0.000611
2023-06-02 23:06:52,775 epoch 5 - iter 2244/3747 - loss 0.16205773 - samples/sec: 21.88 - lr: 0.000600
2023-06-02 23:07:59,612 epoch 5 - iter 2618/3747 - loss 0.16640948 - samples/sec: 22.40 - lr: 0.000589
2023-06-02 23:09:07,925 epoch 5 - iter 2992/3747 - loss 0.16654763 - samples/sec: 21.91 - lr: 0.000578
2023-06-02 23:10:16,230 epoch 5 - iter 3366/3747 - loss 0.16652719 - samples/sec: 21.91 - lr: 0.000567
2023-06-02 23:11:23,434 epoch 5 - iter 3740/3747 - loss 0.16553745 - samples/sec: 22.27 - lr: 0.000556
2023-06-02 23:11:24,692 ----------------------------------------------------------------------------------------------------
2023-06-02 23:11:24,693 EPOCH 5 done: loss 0.1654 - lr 0.000556
2023-06-02 23:12:49,120 Evaluating as a multi-label problem: False
2023-06-02 23:12:49,189 DEV : loss 0.11586443334817886 - f1-score (micro avg)  0.9196
2023-06-02 23:12:49,304 BAD EPOCHS (no improvement): 4
2023-06-02 23:12:49,307 ----------------------------------------------------------------------------------------------------
2023-06-02 23:13:55,675 epoch 6 - iter 374/3747 - loss 0.16360960 - samples/sec: 22.56 - lr: 0.000545
2023-06-02 23:14:59,799 epoch 6 - iter 748/3747 - loss 0.16427065 - samples/sec: 23.34 - lr: 0.000533
2023-06-02 23:16:07,093 epoch 6 - iter 1122/3747 - loss 0.15972205 - samples/sec: 22.24 - lr: 0.000522
2023-06-02 23:17:15,668 epoch 6 - iter 1496/3747 - loss 0.16017461 - samples/sec: 21.83 - lr: 0.000511
2023-06-02 23:18:22,847 epoch 6 - iter 1870/3747 - loss 0.15811747 - samples/sec: 22.28 - lr: 0.000500
2023-06-02 23:19:30,757 epoch 6 - iter 2244/3747 - loss 0.16037364 - samples/sec: 22.04 - lr: 0.000489
2023-06-02 23:20:39,619 epoch 6 - iter 2618/3747 - loss 0.15954433 - samples/sec: 21.74 - lr: 0.000478
2023-06-02 23:21:47,613 epoch 6 - iter 2992/3747 - loss 0.15894049 - samples/sec: 22.01 - lr: 0.000467
2023-06-02 23:23:01,252 epoch 6 - iter 3366/3747 - loss 0.15800145 - samples/sec: 20.33 - lr: 0.000456
2023-06-02 23:24:05,855 epoch 6 - iter 3740/3747 - loss 0.15723272 - samples/sec: 23.17 - lr: 0.000445
2023-06-02 23:24:07,120 ----------------------------------------------------------------------------------------------------
2023-06-02 23:24:07,120 EPOCH 6 done: loss 0.1571 - lr 0.000445
2023-06-02 23:25:28,431 Evaluating as a multi-label problem: False
2023-06-02 23:25:28,502 DEV : loss 0.09951959550380707 - f1-score (micro avg)  0.924
2023-06-02 23:25:28,605 BAD EPOCHS (no improvement): 4
2023-06-02 23:25:28,609 ----------------------------------------------------------------------------------------------------
2023-06-02 23:26:40,821 epoch 7 - iter 374/3747 - loss 0.14281989 - samples/sec: 20.73 - lr: 0.000433
2023-06-02 23:27:48,270 epoch 7 - iter 748/3747 - loss 0.14019540 - samples/sec: 22.19 - lr: 0.000422
2023-06-02 23:28:52,269 epoch 7 - iter 1122/3747 - loss 0.14055824 - samples/sec: 23.39 - lr: 0.000411
2023-06-02 23:29:55,953 epoch 7 - iter 1496/3747 - loss 0.14048482 - samples/sec: 23.50 - lr: 0.000400
2023-06-02 23:31:00,945 epoch 7 - iter 1870/3747 - loss 0.14223222 - samples/sec: 23.03 - lr: 0.000389
2023-06-02 23:32:03,741 epoch 7 - iter 2244/3747 - loss 0.14400577 - samples/sec: 23.84 - lr: 0.000378
2023-06-02 23:33:08,654 epoch 7 - iter 2618/3747 - loss 0.14608153 - samples/sec: 23.06 - lr: 0.000367
2023-06-02 23:34:16,230 epoch 7 - iter 2992/3747 - loss 0.14920579 - samples/sec: 22.15 - lr: 0.000356
2023-06-02 23:35:17,495 epoch 7 - iter 3366/3747 - loss 0.14986510 - samples/sec: 24.43 - lr: 0.000345
2023-06-02 23:36:22,988 epoch 7 - iter 3740/3747 - loss 0.14895364 - samples/sec: 22.85 - lr: 0.000334
2023-06-02 23:36:24,333 ----------------------------------------------------------------------------------------------------
2023-06-02 23:36:24,333 EPOCH 7 done: loss 0.1489 - lr 0.000334
2023-06-02 23:37:46,274 Evaluating as a multi-label problem: False
2023-06-02 23:37:46,342 DEV : loss 0.10671740025281906 - f1-score (micro avg)  0.9261
2023-06-02 23:37:46,461 BAD EPOCHS (no improvement): 4
2023-06-02 23:37:46,464 ----------------------------------------------------------------------------------------------------
2023-06-02 23:38:58,051 epoch 8 - iter 374/3747 - loss 0.14865787 - samples/sec: 20.91 - lr: 0.000322
2023-06-02 23:40:02,578 epoch 8 - iter 748/3747 - loss 0.14735914 - samples/sec: 23.20 - lr: 0.000311
2023-06-02 23:41:09,015 epoch 8 - iter 1122/3747 - loss 0.14753874 - samples/sec: 22.53 - lr: 0.000300
2023-06-02 23:42:19,485 epoch 8 - iter 1496/3747 - loss 0.14623603 - samples/sec: 21.24 - lr: 0.000289
2023-06-02 23:43:28,859 epoch 8 - iter 1870/3747 - loss 0.14232886 - samples/sec: 21.58 - lr: 0.000278
2023-06-02 23:44:38,654 epoch 8 - iter 2244/3747 - loss 0.14069604 - samples/sec: 21.45 - lr: 0.000267
2023-06-02 23:45:48,919 epoch 8 - iter 2618/3747 - loss 0.13936649 - samples/sec: 21.30 - lr: 0.000256
2023-06-02 23:46:57,214 epoch 8 - iter 2992/3747 - loss 0.14195801 - samples/sec: 21.92 - lr: 0.000245
2023-06-02 23:48:04,342 epoch 8 - iter 3366/3747 - loss 0.14113521 - samples/sec: 22.30 - lr: 0.000234
2023-06-02 23:49:12,162 epoch 8 - iter 3740/3747 - loss 0.14193995 - samples/sec: 22.07 - lr: 0.000223
2023-06-02 23:49:13,444 ----------------------------------------------------------------------------------------------------
2023-06-02 23:49:13,444 EPOCH 8 done: loss 0.1418 - lr 0.000223
2023-06-02 23:50:38,287 Evaluating as a multi-label problem: False
2023-06-02 23:50:38,360 DEV : loss 0.10988642275333405 - f1-score (micro avg)  0.9251
2023-06-02 23:50:38,477 BAD EPOCHS (no improvement): 4
2023-06-02 23:50:38,480 ----------------------------------------------------------------------------------------------------
2023-06-02 23:51:49,896 epoch 9 - iter 374/3747 - loss 0.13020762 - samples/sec: 20.96 - lr: 0.000211
2023-06-02 23:52:59,396 epoch 9 - iter 748/3747 - loss 0.13952154 - samples/sec: 21.54 - lr: 0.000200
2023-06-02 23:54:03,434 epoch 9 - iter 1122/3747 - loss 0.13896408 - samples/sec: 23.37 - lr: 0.000189
2023-06-02 23:55:10,865 epoch 9 - iter 1496/3747 - loss 0.14026680 - samples/sec: 22.20 - lr: 0.000178
2023-06-02 23:56:18,739 epoch 9 - iter 1870/3747 - loss 0.13926908 - samples/sec: 22.05 - lr: 0.000167
2023-06-02 23:57:28,815 epoch 9 - iter 2244/3747 - loss 0.13784393 - samples/sec: 21.36 - lr: 0.000156
2023-06-02 23:58:37,346 epoch 9 - iter 2618/3747 - loss 0.13649569 - samples/sec: 21.84 - lr: 0.000145
2023-06-02 23:59:47,898 epoch 9 - iter 2992/3747 - loss 0.13520005 - samples/sec: 21.22 - lr: 0.000134
2023-06-03 00:00:53,113 epoch 9 - iter 3366/3747 - loss 0.13543851 - samples/sec: 22.95 - lr: 0.000123
2023-06-03 00:02:03,477 epoch 9 - iter 3740/3747 - loss 0.13608206 - samples/sec: 21.27 - lr: 0.000111
2023-06-03 00:02:04,727 ----------------------------------------------------------------------------------------------------
2023-06-03 00:02:04,727 EPOCH 9 done: loss 0.1360 - lr 0.000111
2023-06-03 00:03:27,799 Evaluating as a multi-label problem: False
2023-06-03 00:03:27,869 DEV : loss 0.09809794276952744 - f1-score (micro avg)  0.9333
2023-06-03 00:03:27,990 BAD EPOCHS (no improvement): 4
2023-06-03 00:03:27,994 ----------------------------------------------------------------------------------------------------
2023-06-03 00:04:35,995 epoch 10 - iter 374/3747 - loss 0.13506615 - samples/sec: 22.01 - lr: 0.000100
2023-06-03 00:05:44,281 epoch 10 - iter 748/3747 - loss 0.13984306 - samples/sec: 21.92 - lr: 0.000089
2023-06-03 00:06:52,885 epoch 10 - iter 1122/3747 - loss 0.13471723 - samples/sec: 21.82 - lr: 0.000078
2023-06-03 00:08:01,798 epoch 10 - iter 1496/3747 - loss 0.13203620 - samples/sec: 21.72 - lr: 0.000067
2023-06-03 00:09:09,887 epoch 10 - iter 1870/3747 - loss 0.13097582 - samples/sec: 21.98 - lr: 0.000056
2023-06-03 00:10:16,335 epoch 10 - iter 2244/3747 - loss 0.13012090 - samples/sec: 22.53 - lr: 0.000045
2023-06-03 00:11:24,786 epoch 10 - iter 2618/3747 - loss 0.12849720 - samples/sec: 21.87 - lr: 0.000034
2023-06-03 00:12:34,546 epoch 10 - iter 2992/3747 - loss 0.12888109 - samples/sec: 21.46 - lr: 0.000023
2023-06-03 00:13:42,112 epoch 10 - iter 3366/3747 - loss 0.13065819 - samples/sec: 22.15 - lr: 0.000011
2023-06-03 00:14:54,014 epoch 10 - iter 3740/3747 - loss 0.13125814 - samples/sec: 20.82 - lr: 0.000000
2023-06-03 00:14:55,459 ----------------------------------------------------------------------------------------------------
2023-06-03 00:14:55,460 EPOCH 10 done: loss 0.1312 - lr 0.000000
2023-06-03 00:16:19,728 Evaluating as a multi-label problem: False
2023-06-03 00:16:19,797 DEV : loss 0.1001829132437706 - f1-score (micro avg)  0.9304
2023-06-03 00:16:19,915 BAD EPOCHS (no improvement): 4
2023-06-03 00:16:34,444 ----------------------------------------------------------------------------------------------------
2023-06-03 00:16:34,447 Testing using last state of model ...
2023-06-03 00:18:03,426 Evaluating as a multi-label problem: False
2023-06-03 00:18:03,493 0.8868	0.8948	0.8908	0.8281
2023-06-03 00:18:03,493 
Results:
- F-score (micro) 0.8908
- F-score (macro) 0.8711
- Accuracy 0.8281

By class:
              precision    recall  f1-score   support

         ORG     0.8571    0.8778    0.8673      1661
         LOC     0.9162    0.8717    0.8934      1668
         PER     0.9802    0.9814    0.9808      1617
        MISC     0.7008    0.7906    0.7430       702

   micro avg     0.8868    0.8948    0.8908      5648
   macro avg     0.8636    0.8804    0.8711      5648
weighted avg     0.8904    0.8948    0.8921      5648

2023-06-03 00:18:03,493 ----------------------------------------------------------------------------------------------------
