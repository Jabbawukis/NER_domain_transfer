2023-06-03 02:28:21,224 ----------------------------------------------------------------------------------------------------
2023-06-03 02:28:21,229 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-03 02:28:21,234 ----------------------------------------------------------------------------------------------------
2023-06-03 02:28:21,234 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-03 02:28:21,234 ----------------------------------------------------------------------------------------------------
2023-06-03 02:28:21,234 Parameters:
2023-06-03 02:28:21,234  - learning_rate: "0.001000"
2023-06-03 02:28:21,234  - mini_batch_size: "4"
2023-06-03 02:28:21,235  - patience: "3"
2023-06-03 02:28:21,235  - anneal_factor: "0.5"
2023-06-03 02:28:21,235  - max_epochs: "10"
2023-06-03 02:28:21,235  - shuffle: "True"
2023-06-03 02:28:21,235  - train_with_dev: "False"
2023-06-03 02:28:21,235  - batch_growth_annealing: "False"
2023-06-03 02:28:21,235 ----------------------------------------------------------------------------------------------------
2023-06-03 02:28:21,235 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_3_BitFit"
2023-06-03 02:28:21,235 ----------------------------------------------------------------------------------------------------
2023-06-03 02:28:21,235 Device: cuda:0
2023-06-03 02:28:21,235 ----------------------------------------------------------------------------------------------------
2023-06-03 02:28:21,235 Embeddings storage mode: none
2023-06-03 02:28:21,235 ----------------------------------------------------------------------------------------------------
2023-06-03 02:29:26,414 epoch 1 - iter 374/3747 - loss 1.92614476 - samples/sec: 22.96 - lr: 0.000100
2023-06-03 02:30:34,462 epoch 1 - iter 748/3747 - loss 1.27520961 - samples/sec: 22.00 - lr: 0.000200
2023-06-03 02:31:32,167 epoch 1 - iter 1122/3747 - loss 1.01428927 - samples/sec: 25.94 - lr: 0.000299
2023-06-03 02:32:38,072 epoch 1 - iter 1496/3747 - loss 0.87945348 - samples/sec: 22.71 - lr: 0.000399
2023-06-03 02:33:47,467 epoch 1 - iter 1870/3747 - loss 0.76243478 - samples/sec: 21.57 - lr: 0.000499
2023-06-03 02:34:53,675 epoch 1 - iter 2244/3747 - loss 0.66893632 - samples/sec: 22.61 - lr: 0.000599
2023-06-03 02:36:01,736 epoch 1 - iter 2618/3747 - loss 0.60241134 - samples/sec: 21.99 - lr: 0.000699
2023-06-03 02:37:13,345 epoch 1 - iter 2992/3747 - loss 0.55834056 - samples/sec: 20.90 - lr: 0.000799
2023-06-03 02:38:21,875 epoch 1 - iter 3366/3747 - loss 0.51897232 - samples/sec: 21.84 - lr: 0.000898
2023-06-03 02:39:35,423 epoch 1 - iter 3740/3747 - loss 0.48213860 - samples/sec: 20.35 - lr: 0.000998
2023-06-03 02:39:36,728 ----------------------------------------------------------------------------------------------------
2023-06-03 02:39:36,728 EPOCH 1 done: loss 0.4817 - lr 0.000998
2023-06-03 02:41:15,174 Evaluating as a multi-label problem: False
2023-06-03 02:41:15,255 DEV : loss 0.12531030178070068 - f1-score (micro avg)  0.8794
2023-06-03 02:41:15,376 BAD EPOCHS (no improvement): 4
2023-06-03 02:41:15,378 ----------------------------------------------------------------------------------------------------
2023-06-03 02:42:24,621 epoch 2 - iter 374/3747 - loss 0.21528192 - samples/sec: 21.62 - lr: 0.000989
2023-06-03 02:43:34,368 epoch 2 - iter 748/3747 - loss 0.21809683 - samples/sec: 21.46 - lr: 0.000978
2023-06-03 02:44:42,499 epoch 2 - iter 1122/3747 - loss 0.21170288 - samples/sec: 21.97 - lr: 0.000967
2023-06-03 02:45:51,202 epoch 2 - iter 1496/3747 - loss 0.20357598 - samples/sec: 21.79 - lr: 0.000956
2023-06-03 02:47:00,684 epoch 2 - iter 1870/3747 - loss 0.19809887 - samples/sec: 21.54 - lr: 0.000945
2023-06-03 02:48:09,923 epoch 2 - iter 2244/3747 - loss 0.19716201 - samples/sec: 21.62 - lr: 0.000933
2023-06-03 02:49:18,282 epoch 2 - iter 2618/3747 - loss 0.19522860 - samples/sec: 21.90 - lr: 0.000922
2023-06-03 02:50:26,678 epoch 2 - iter 2992/3747 - loss 0.19324746 - samples/sec: 21.88 - lr: 0.000911
2023-06-03 02:51:34,828 epoch 2 - iter 3366/3747 - loss 0.19276289 - samples/sec: 21.96 - lr: 0.000900
2023-06-03 02:52:49,661 epoch 2 - iter 3740/3747 - loss 0.19069629 - samples/sec: 20.00 - lr: 0.000889
2023-06-03 02:52:51,115 ----------------------------------------------------------------------------------------------------
2023-06-03 02:52:51,115 EPOCH 2 done: loss 0.1904 - lr 0.000889
2023-06-03 02:54:31,924 Evaluating as a multi-label problem: False
2023-06-03 02:54:31,975 DEV : loss 0.1060413047671318 - f1-score (micro avg)  0.924
2023-06-03 02:54:32,067 BAD EPOCHS (no improvement): 4
2023-06-03 02:54:32,070 ----------------------------------------------------------------------------------------------------
2023-06-03 02:55:47,094 epoch 3 - iter 374/3747 - loss 0.16021254 - samples/sec: 19.95 - lr: 0.000878
2023-06-03 02:56:55,060 epoch 3 - iter 748/3747 - loss 0.16507777 - samples/sec: 22.02 - lr: 0.000867
2023-06-03 02:58:05,002 epoch 3 - iter 1122/3747 - loss 0.16648915 - samples/sec: 21.40 - lr: 0.000856
2023-06-03 02:59:15,195 epoch 3 - iter 1496/3747 - loss 0.16707440 - samples/sec: 21.32 - lr: 0.000845
2023-06-03 03:00:28,154 epoch 3 - iter 1870/3747 - loss 0.16934852 - samples/sec: 20.52 - lr: 0.000833
2023-06-03 03:01:34,465 epoch 3 - iter 2244/3747 - loss 0.17148807 - samples/sec: 22.57 - lr: 0.000822
2023-06-03 03:02:42,802 epoch 3 - iter 2618/3747 - loss 0.16905125 - samples/sec: 21.90 - lr: 0.000811
2023-06-03 03:03:49,853 epoch 3 - iter 2992/3747 - loss 0.16864048 - samples/sec: 22.32 - lr: 0.000800
2023-06-03 03:04:55,621 epoch 3 - iter 3366/3747 - loss 0.16772395 - samples/sec: 22.76 - lr: 0.000789
2023-06-03 03:06:03,192 epoch 3 - iter 3740/3747 - loss 0.16490755 - samples/sec: 22.15 - lr: 0.000778
2023-06-03 03:06:04,522 ----------------------------------------------------------------------------------------------------
2023-06-03 03:06:04,522 EPOCH 3 done: loss 0.1648 - lr 0.000778
2023-06-03 03:07:24,246 Evaluating as a multi-label problem: False
2023-06-03 03:07:24,306 DEV : loss 0.11246398091316223 - f1-score (micro avg)  0.9186
2023-06-03 03:07:24,416 BAD EPOCHS (no improvement): 4
2023-06-03 03:07:24,419 ----------------------------------------------------------------------------------------------------
2023-06-03 03:08:37,471 epoch 4 - iter 374/3747 - loss 0.17360240 - samples/sec: 20.49 - lr: 0.000767
2023-06-03 03:09:45,243 epoch 4 - iter 748/3747 - loss 0.17430772 - samples/sec: 22.09 - lr: 0.000756
2023-06-03 03:10:54,031 epoch 4 - iter 1122/3747 - loss 0.17410677 - samples/sec: 21.76 - lr: 0.000745
2023-06-03 03:12:06,834 epoch 4 - iter 1496/3747 - loss 0.16792367 - samples/sec: 20.56 - lr: 0.000733
2023-06-03 03:13:17,123 epoch 4 - iter 1870/3747 - loss 0.16860996 - samples/sec: 21.29 - lr: 0.000722
2023-06-03 03:14:22,781 epoch 4 - iter 2244/3747 - loss 0.16833171 - samples/sec: 22.80 - lr: 0.000711
2023-06-03 03:15:30,826 epoch 4 - iter 2618/3747 - loss 0.16723135 - samples/sec: 22.00 - lr: 0.000700
2023-06-03 03:16:40,407 epoch 4 - iter 2992/3747 - loss 0.16625140 - samples/sec: 21.51 - lr: 0.000689
2023-06-03 03:17:44,687 epoch 4 - iter 3366/3747 - loss 0.16376798 - samples/sec: 23.29 - lr: 0.000678
2023-06-03 03:18:51,890 epoch 4 - iter 3740/3747 - loss 0.16165497 - samples/sec: 22.27 - lr: 0.000667
2023-06-03 03:18:53,087 ----------------------------------------------------------------------------------------------------
2023-06-03 03:18:53,087 EPOCH 4 done: loss 0.1616 - lr 0.000667
2023-06-03 03:20:17,753 Evaluating as a multi-label problem: False
2023-06-03 03:20:17,822 DEV : loss 0.09598939120769501 - f1-score (micro avg)  0.9274
2023-06-03 03:20:17,959 BAD EPOCHS (no improvement): 4
2023-06-03 03:20:17,962 ----------------------------------------------------------------------------------------------------
2023-06-03 03:21:26,356 epoch 5 - iter 374/3747 - loss 0.15390089 - samples/sec: 21.89 - lr: 0.000656
2023-06-03 03:22:34,980 epoch 5 - iter 748/3747 - loss 0.15831262 - samples/sec: 21.81 - lr: 0.000645
2023-06-03 03:23:45,184 epoch 5 - iter 1122/3747 - loss 0.16182776 - samples/sec: 21.32 - lr: 0.000633
2023-06-03 03:24:51,716 epoch 5 - iter 1496/3747 - loss 0.16003081 - samples/sec: 22.50 - lr: 0.000622
2023-06-03 03:25:58,283 epoch 5 - iter 1870/3747 - loss 0.15869262 - samples/sec: 22.49 - lr: 0.000611
2023-06-03 03:27:06,651 epoch 5 - iter 2244/3747 - loss 0.15834158 - samples/sec: 21.89 - lr: 0.000600
2023-06-03 03:28:12,068 epoch 5 - iter 2618/3747 - loss 0.15974394 - samples/sec: 22.88 - lr: 0.000589
2023-06-03 03:29:19,648 epoch 5 - iter 2992/3747 - loss 0.15699323 - samples/sec: 22.15 - lr: 0.000578
2023-06-03 03:30:28,428 epoch 5 - iter 3366/3747 - loss 0.15562771 - samples/sec: 21.76 - lr: 0.000567
2023-06-03 03:31:31,396 epoch 5 - iter 3740/3747 - loss 0.15521816 - samples/sec: 23.77 - lr: 0.000556
2023-06-03 03:31:32,516 ----------------------------------------------------------------------------------------------------
2023-06-03 03:31:32,516 EPOCH 5 done: loss 0.1551 - lr 0.000556
2023-06-03 03:32:58,282 Evaluating as a multi-label problem: False
2023-06-03 03:32:58,350 DEV : loss 0.1022176519036293 - f1-score (micro avg)  0.9292
2023-06-03 03:32:58,487 BAD EPOCHS (no improvement): 4
2023-06-03 03:32:58,490 ----------------------------------------------------------------------------------------------------
2023-06-03 03:34:11,327 epoch 6 - iter 374/3747 - loss 0.14914668 - samples/sec: 20.55 - lr: 0.000545
2023-06-03 03:35:18,042 epoch 6 - iter 748/3747 - loss 0.14899314 - samples/sec: 22.44 - lr: 0.000533
2023-06-03 03:36:32,427 epoch 6 - iter 1122/3747 - loss 0.14225886 - samples/sec: 20.12 - lr: 0.000522
2023-06-03 03:37:40,576 epoch 6 - iter 1496/3747 - loss 0.14253349 - samples/sec: 21.96 - lr: 0.000511
2023-06-03 03:38:48,542 epoch 6 - iter 1870/3747 - loss 0.14206330 - samples/sec: 22.02 - lr: 0.000500
2023-06-03 03:39:54,680 epoch 6 - iter 2244/3747 - loss 0.14279034 - samples/sec: 22.63 - lr: 0.000489
2023-06-03 03:40:57,398 epoch 6 - iter 2618/3747 - loss 0.14182433 - samples/sec: 23.87 - lr: 0.000478
2023-06-03 03:42:10,235 epoch 6 - iter 2992/3747 - loss 0.14281118 - samples/sec: 20.55 - lr: 0.000467
2023-06-03 03:43:24,130 epoch 6 - iter 3366/3747 - loss 0.14272302 - samples/sec: 20.26 - lr: 0.000456
2023-06-03 03:44:32,122 epoch 6 - iter 3740/3747 - loss 0.14192707 - samples/sec: 22.01 - lr: 0.000445
2023-06-03 03:44:33,231 ----------------------------------------------------------------------------------------------------
2023-06-03 03:44:33,231 EPOCH 6 done: loss 0.1419 - lr 0.000445
2023-06-03 03:45:54,144 Evaluating as a multi-label problem: False
2023-06-03 03:45:54,215 DEV : loss 0.11144684255123138 - f1-score (micro avg)  0.9297
2023-06-03 03:45:54,358 BAD EPOCHS (no improvement): 4
2023-06-03 03:45:54,361 ----------------------------------------------------------------------------------------------------
2023-06-03 03:47:04,947 epoch 7 - iter 374/3747 - loss 0.14053218 - samples/sec: 21.21 - lr: 0.000433
2023-06-03 03:48:15,606 epoch 7 - iter 748/3747 - loss 0.14007913 - samples/sec: 21.18 - lr: 0.000422
2023-06-03 03:49:32,273 epoch 7 - iter 1122/3747 - loss 0.14377998 - samples/sec: 19.52 - lr: 0.000411
2023-06-03 03:50:47,222 epoch 7 - iter 1496/3747 - loss 0.14197567 - samples/sec: 19.97 - lr: 0.000400
2023-06-03 03:52:01,727 epoch 7 - iter 1870/3747 - loss 0.13998245 - samples/sec: 20.09 - lr: 0.000389
2023-06-03 03:53:08,385 epoch 7 - iter 2244/3747 - loss 0.13679439 - samples/sec: 22.45 - lr: 0.000378
2023-06-03 03:54:21,935 epoch 7 - iter 2618/3747 - loss 0.13492453 - samples/sec: 20.35 - lr: 0.000367
2023-06-03 03:55:36,583 epoch 7 - iter 2992/3747 - loss 0.13849772 - samples/sec: 20.05 - lr: 0.000356
2023-06-03 03:56:46,213 epoch 7 - iter 3366/3747 - loss 0.13761865 - samples/sec: 21.50 - lr: 0.000345
2023-06-03 03:57:59,420 epoch 7 - iter 3740/3747 - loss 0.13765795 - samples/sec: 20.45 - lr: 0.000334
2023-06-03 03:58:00,787 ----------------------------------------------------------------------------------------------------
2023-06-03 03:58:00,788 EPOCH 7 done: loss 0.1376 - lr 0.000334
2023-06-03 03:59:32,742 Evaluating as a multi-label problem: False
2023-06-03 03:59:32,809 DEV : loss 0.10787805914878845 - f1-score (micro avg)  0.9265
2023-06-03 03:59:32,944 BAD EPOCHS (no improvement): 4
2023-06-03 03:59:32,947 ----------------------------------------------------------------------------------------------------
2023-06-03 04:00:42,389 epoch 8 - iter 374/3747 - loss 0.13159159 - samples/sec: 21.56 - lr: 0.000322
2023-06-03 04:01:49,391 epoch 8 - iter 748/3747 - loss 0.13656976 - samples/sec: 22.34 - lr: 0.000311
2023-06-03 04:02:53,757 epoch 8 - iter 1122/3747 - loss 0.14232940 - samples/sec: 23.25 - lr: 0.000300
2023-06-03 04:04:00,858 epoch 8 - iter 1496/3747 - loss 0.14223631 - samples/sec: 22.31 - lr: 0.000289
2023-06-03 04:05:08,900 epoch 8 - iter 1870/3747 - loss 0.14003217 - samples/sec: 22.00 - lr: 0.000278
2023-06-03 04:06:14,925 epoch 8 - iter 2244/3747 - loss 0.13960810 - samples/sec: 22.67 - lr: 0.000267
2023-06-03 04:07:15,284 epoch 8 - iter 2618/3747 - loss 0.13991099 - samples/sec: 24.80 - lr: 0.000256
2023-06-03 04:08:26,435 epoch 8 - iter 2992/3747 - loss 0.13683889 - samples/sec: 21.04 - lr: 0.000245
2023-06-03 04:09:34,581 epoch 8 - iter 3366/3747 - loss 0.13525957 - samples/sec: 21.97 - lr: 0.000234
2023-06-03 04:10:43,474 epoch 8 - iter 3740/3747 - loss 0.13332591 - samples/sec: 21.73 - lr: 0.000223
2023-06-03 04:10:44,730 ----------------------------------------------------------------------------------------------------
2023-06-03 04:10:44,730 EPOCH 8 done: loss 0.1332 - lr 0.000223
2023-06-03 04:12:10,185 Evaluating as a multi-label problem: False
2023-06-03 04:12:10,259 DEV : loss 0.09555424749851227 - f1-score (micro avg)  0.9367
2023-06-03 04:12:10,378 BAD EPOCHS (no improvement): 4
2023-06-03 04:12:10,381 ----------------------------------------------------------------------------------------------------
2023-06-03 04:13:18,436 epoch 9 - iter 374/3747 - loss 0.14159385 - samples/sec: 21.99 - lr: 0.000211
2023-06-03 04:14:25,709 epoch 9 - iter 748/3747 - loss 0.13869920 - samples/sec: 22.25 - lr: 0.000200
2023-06-03 04:15:39,091 epoch 9 - iter 1122/3747 - loss 0.13371097 - samples/sec: 20.40 - lr: 0.000189
2023-06-03 04:16:50,275 epoch 9 - iter 1496/3747 - loss 0.13230644 - samples/sec: 21.03 - lr: 0.000178
2023-06-03 04:17:58,614 epoch 9 - iter 1870/3747 - loss 0.13351888 - samples/sec: 21.90 - lr: 0.000167
2023-06-03 04:19:05,631 epoch 9 - iter 2244/3747 - loss 0.13126994 - samples/sec: 22.34 - lr: 0.000156
2023-06-03 04:20:13,171 epoch 9 - iter 2618/3747 - loss 0.13112259 - samples/sec: 22.16 - lr: 0.000145
2023-06-03 04:21:21,573 epoch 9 - iter 2992/3747 - loss 0.12735788 - samples/sec: 21.88 - lr: 0.000134
2023-06-03 04:22:28,078 epoch 9 - iter 3366/3747 - loss 0.12552873 - samples/sec: 22.51 - lr: 0.000123
2023-06-03 04:23:32,235 epoch 9 - iter 3740/3747 - loss 0.12572977 - samples/sec: 23.33 - lr: 0.000111
2023-06-03 04:23:33,722 ----------------------------------------------------------------------------------------------------
2023-06-03 04:23:33,722 EPOCH 9 done: loss 0.1260 - lr 0.000111
2023-06-03 04:24:59,708 Evaluating as a multi-label problem: False
2023-06-03 04:24:59,780 DEV : loss 0.09293089061975479 - f1-score (micro avg)  0.9424
2023-06-03 04:24:59,914 BAD EPOCHS (no improvement): 4
2023-06-03 04:24:59,917 ----------------------------------------------------------------------------------------------------
2023-06-03 04:26:11,015 epoch 10 - iter 374/3747 - loss 0.10592753 - samples/sec: 21.05 - lr: 0.000100
2023-06-03 04:27:16,793 epoch 10 - iter 748/3747 - loss 0.11645023 - samples/sec: 22.76 - lr: 0.000089
2023-06-03 04:28:15,276 epoch 10 - iter 1122/3747 - loss 0.11694404 - samples/sec: 25.59 - lr: 0.000078
2023-06-03 04:29:21,792 epoch 10 - iter 1496/3747 - loss 0.11715372 - samples/sec: 22.50 - lr: 0.000067
2023-06-03 04:30:29,469 epoch 10 - iter 1870/3747 - loss 0.11947380 - samples/sec: 22.12 - lr: 0.000056
2023-06-03 04:31:41,347 epoch 10 - iter 2244/3747 - loss 0.11971905 - samples/sec: 20.82 - lr: 0.000045
2023-06-03 04:32:54,077 epoch 10 - iter 2618/3747 - loss 0.11935132 - samples/sec: 20.58 - lr: 0.000034
2023-06-03 04:34:02,761 epoch 10 - iter 2992/3747 - loss 0.11998374 - samples/sec: 21.79 - lr: 0.000023
2023-06-03 04:35:08,556 epoch 10 - iter 3366/3747 - loss 0.12087230 - samples/sec: 22.75 - lr: 0.000011
2023-06-03 04:36:24,007 epoch 10 - iter 3740/3747 - loss 0.12145919 - samples/sec: 19.84 - lr: 0.000000
2023-06-03 04:36:25,513 ----------------------------------------------------------------------------------------------------
2023-06-03 04:36:25,513 EPOCH 10 done: loss 0.1216 - lr 0.000000
2023-06-03 04:38:02,038 Evaluating as a multi-label problem: False
2023-06-03 04:38:02,112 DEV : loss 0.09379176795482635 - f1-score (micro avg)  0.9421
2023-06-03 04:38:02,259 BAD EPOCHS (no improvement): 4
2023-06-03 04:38:19,268 ----------------------------------------------------------------------------------------------------
2023-06-03 04:38:19,271 Testing using last state of model ...
2023-06-03 04:39:46,106 Evaluating as a multi-label problem: False
2023-06-03 04:39:46,174 0.8887	0.9033	0.896	0.8364
2023-06-03 04:39:46,174 
Results:
- F-score (micro) 0.896
- F-score (macro) 0.8777
- Accuracy 0.8364

By class:
              precision    recall  f1-score   support

         LOC     0.8928    0.8987    0.8957      1668
         ORG     0.8754    0.8754    0.8754      1661
         PER     0.9778    0.9808    0.9793      1617
        MISC     0.7227    0.8020    0.7603       702

   micro avg     0.8887    0.9033    0.8960      5648
   macro avg     0.8672    0.8892    0.8777      5648
weighted avg     0.8909    0.9033    0.8968      5648

2023-06-03 04:39:46,174 ----------------------------------------------------------------------------------------------------
