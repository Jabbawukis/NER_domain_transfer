2023-06-09 14:06:00,068 ----------------------------------------------------------------------------------------------------
2023-06-09 14:06:00,073 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-09 14:06:00,077 ----------------------------------------------------------------------------------------------------
2023-06-09 14:06:00,077 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-09 14:06:00,077 ----------------------------------------------------------------------------------------------------
2023-06-09 14:06:00,077 Parameters:
2023-06-09 14:06:00,077  - learning_rate: "0.001000"
2023-06-09 14:06:00,077  - mini_batch_size: "4"
2023-06-09 14:06:00,077  - patience: "3"
2023-06-09 14:06:00,077  - anneal_factor: "0.5"
2023-06-09 14:06:00,077  - max_epochs: "10"
2023-06-09 14:06:00,077  - shuffle: "True"
2023-06-09 14:06:00,078  - train_with_dev: "False"
2023-06-09 14:06:00,078  - batch_growth_annealing: "False"
2023-06-09 14:06:00,078 ----------------------------------------------------------------------------------------------------
2023-06-09 14:06:00,078 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_2_BitFit"
2023-06-09 14:06:00,078 ----------------------------------------------------------------------------------------------------
2023-06-09 14:06:00,078 Device: cuda:3
2023-06-09 14:06:00,078 ----------------------------------------------------------------------------------------------------
2023-06-09 14:06:00,078 Embeddings storage mode: none
2023-06-09 14:06:00,078 ----------------------------------------------------------------------------------------------------
2023-06-09 14:07:17,243 epoch 1 - iter 374/3747 - loss 2.04693762 - samples/sec: 19.40 - lr: 0.000100
2023-06-09 14:08:30,794 epoch 1 - iter 748/3747 - loss 1.52310525 - samples/sec: 20.35 - lr: 0.000200
2023-06-09 14:09:42,792 epoch 1 - iter 1122/3747 - loss 1.28513728 - samples/sec: 20.79 - lr: 0.000299
2023-06-09 14:10:54,896 epoch 1 - iter 1496/3747 - loss 1.16763681 - samples/sec: 20.76 - lr: 0.000399
2023-06-09 14:12:07,824 epoch 1 - iter 1870/3747 - loss 1.05627853 - samples/sec: 20.52 - lr: 0.000499
2023-06-09 14:13:21,979 epoch 1 - iter 2244/3747 - loss 0.95926156 - samples/sec: 20.18 - lr: 0.000599
2023-06-09 14:14:37,020 epoch 1 - iter 2618/3747 - loss 0.88003934 - samples/sec: 19.95 - lr: 0.000699
2023-06-09 14:15:51,441 epoch 1 - iter 2992/3747 - loss 0.82603847 - samples/sec: 20.11 - lr: 0.000799
2023-06-09 14:17:05,051 epoch 1 - iter 3366/3747 - loss 0.77109574 - samples/sec: 20.33 - lr: 0.000898
2023-06-09 14:18:20,428 epoch 1 - iter 3740/3747 - loss 0.71678672 - samples/sec: 19.86 - lr: 0.000998
2023-06-09 14:18:21,549 ----------------------------------------------------------------------------------------------------
2023-06-09 14:18:21,549 EPOCH 1 done: loss 0.7163 - lr 0.000998
2023-06-09 14:19:57,357 Evaluating as a multi-label problem: False
2023-06-09 14:19:57,435 DEV : loss 0.19888752698898315 - f1-score (micro avg)  0.7862
2023-06-09 14:19:57,633 BAD EPOCHS (no improvement): 4
2023-06-09 14:19:57,636 ----------------------------------------------------------------------------------------------------
2023-06-09 14:21:13,876 epoch 2 - iter 374/3747 - loss 0.31513833 - samples/sec: 19.63 - lr: 0.000989
2023-06-09 14:22:28,289 epoch 2 - iter 748/3747 - loss 0.31128233 - samples/sec: 20.12 - lr: 0.000978
2023-06-09 14:23:43,286 epoch 2 - iter 1122/3747 - loss 0.30535061 - samples/sec: 19.96 - lr: 0.000967
2023-06-09 14:24:56,936 epoch 2 - iter 1496/3747 - loss 0.30461478 - samples/sec: 20.32 - lr: 0.000956
2023-06-09 14:26:10,573 epoch 2 - iter 1870/3747 - loss 0.29620189 - samples/sec: 20.33 - lr: 0.000945
2023-06-09 14:27:28,017 epoch 2 - iter 2244/3747 - loss 0.28787693 - samples/sec: 19.33 - lr: 0.000933
2023-06-09 14:28:42,962 epoch 2 - iter 2618/3747 - loss 0.28146084 - samples/sec: 19.97 - lr: 0.000922
2023-06-09 14:29:57,938 epoch 2 - iter 2992/3747 - loss 0.27611284 - samples/sec: 19.96 - lr: 0.000911
2023-06-09 14:31:13,742 epoch 2 - iter 3366/3747 - loss 0.27065769 - samples/sec: 19.75 - lr: 0.000900
2023-06-09 14:32:27,520 epoch 2 - iter 3740/3747 - loss 0.26551055 - samples/sec: 20.29 - lr: 0.000889
2023-06-09 14:32:28,835 ----------------------------------------------------------------------------------------------------
2023-06-09 14:32:28,835 EPOCH 2 done: loss 0.2654 - lr 0.000889
2023-06-09 14:33:59,720 Evaluating as a multi-label problem: False
2023-06-09 14:33:59,788 DEV : loss 0.12688048183918 - f1-score (micro avg)  0.8614
2023-06-09 14:33:59,925 BAD EPOCHS (no improvement): 4
2023-06-09 14:33:59,953 ----------------------------------------------------------------------------------------------------
2023-06-09 14:35:15,835 epoch 3 - iter 374/3747 - loss 0.22258669 - samples/sec: 19.73 - lr: 0.000878
2023-06-09 14:36:29,875 epoch 3 - iter 748/3747 - loss 0.21415494 - samples/sec: 20.22 - lr: 0.000867
2023-06-09 14:37:43,231 epoch 3 - iter 1122/3747 - loss 0.20615584 - samples/sec: 20.41 - lr: 0.000856
2023-06-09 14:38:56,929 epoch 3 - iter 1496/3747 - loss 0.20109244 - samples/sec: 20.31 - lr: 0.000845
2023-06-09 14:40:09,039 epoch 3 - iter 1870/3747 - loss 0.19938325 - samples/sec: 20.76 - lr: 0.000833
2023-06-09 14:41:22,504 epoch 3 - iter 2244/3747 - loss 0.19568557 - samples/sec: 20.37 - lr: 0.000822
2023-06-09 14:42:36,124 epoch 3 - iter 2618/3747 - loss 0.19466801 - samples/sec: 20.33 - lr: 0.000811
2023-06-09 14:43:48,918 epoch 3 - iter 2992/3747 - loss 0.19337315 - samples/sec: 20.56 - lr: 0.000800
2023-06-09 14:45:01,599 epoch 3 - iter 3366/3747 - loss 0.19210764 - samples/sec: 20.59 - lr: 0.000789
2023-06-09 14:46:15,575 epoch 3 - iter 3740/3747 - loss 0.19106567 - samples/sec: 20.23 - lr: 0.000778
2023-06-09 14:46:16,920 ----------------------------------------------------------------------------------------------------
2023-06-09 14:46:16,920 EPOCH 3 done: loss 0.1911 - lr 0.000778
2023-06-09 14:47:53,616 Evaluating as a multi-label problem: False
2023-06-09 14:47:53,692 DEV : loss 0.11074236780405045 - f1-score (micro avg)  0.8959
2023-06-09 14:47:53,824 BAD EPOCHS (no improvement): 4
2023-06-09 14:47:53,827 ----------------------------------------------------------------------------------------------------
2023-06-09 14:49:10,495 epoch 4 - iter 374/3747 - loss 0.18253228 - samples/sec: 19.52 - lr: 0.000767
2023-06-09 14:50:24,825 epoch 4 - iter 748/3747 - loss 0.17937799 - samples/sec: 20.14 - lr: 0.000756
2023-06-09 14:51:38,949 epoch 4 - iter 1122/3747 - loss 0.17210380 - samples/sec: 20.19 - lr: 0.000745
2023-06-09 14:52:52,926 epoch 4 - iter 1496/3747 - loss 0.17030287 - samples/sec: 20.23 - lr: 0.000733
2023-06-09 14:54:11,084 epoch 4 - iter 1870/3747 - loss 0.16739694 - samples/sec: 19.15 - lr: 0.000722
2023-06-09 14:55:25,179 epoch 4 - iter 2244/3747 - loss 0.16774610 - samples/sec: 20.20 - lr: 0.000711
2023-06-09 14:56:38,377 epoch 4 - iter 2618/3747 - loss 0.16651873 - samples/sec: 20.45 - lr: 0.000700
2023-06-09 14:57:51,473 epoch 4 - iter 2992/3747 - loss 0.16748887 - samples/sec: 20.48 - lr: 0.000689
2023-06-09 14:59:04,182 epoch 4 - iter 3366/3747 - loss 0.16823136 - samples/sec: 20.59 - lr: 0.000678
2023-06-09 15:00:16,595 epoch 4 - iter 3740/3747 - loss 0.16715611 - samples/sec: 20.67 - lr: 0.000667
2023-06-09 15:00:17,935 ----------------------------------------------------------------------------------------------------
2023-06-09 15:00:17,935 EPOCH 4 done: loss 0.1670 - lr 0.000667
2023-06-09 15:01:48,549 Evaluating as a multi-label problem: False
2023-06-09 15:01:48,625 DEV : loss 0.11212347447872162 - f1-score (micro avg)  0.9064
2023-06-09 15:01:48,868 BAD EPOCHS (no improvement): 4
2023-06-09 15:01:48,873 ----------------------------------------------------------------------------------------------------
2023-06-09 15:03:04,298 epoch 5 - iter 374/3747 - loss 0.16236642 - samples/sec: 19.85 - lr: 0.000656
2023-06-09 15:04:17,433 epoch 5 - iter 748/3747 - loss 0.15819839 - samples/sec: 20.47 - lr: 0.000645
2023-06-09 15:05:31,156 epoch 5 - iter 1122/3747 - loss 0.16229248 - samples/sec: 20.30 - lr: 0.000633
2023-06-09 15:06:43,233 epoch 5 - iter 1496/3747 - loss 0.16740149 - samples/sec: 20.77 - lr: 0.000622
2023-06-09 15:07:55,308 epoch 5 - iter 1870/3747 - loss 0.16439563 - samples/sec: 20.77 - lr: 0.000611
2023-06-09 15:09:07,887 epoch 5 - iter 2244/3747 - loss 0.16455533 - samples/sec: 20.62 - lr: 0.000600
2023-06-09 15:10:20,036 epoch 5 - iter 2618/3747 - loss 0.16470751 - samples/sec: 20.75 - lr: 0.000589
2023-06-09 15:11:33,118 epoch 5 - iter 2992/3747 - loss 0.16438433 - samples/sec: 20.48 - lr: 0.000578
2023-06-09 15:12:44,435 epoch 5 - iter 3366/3747 - loss 0.16185838 - samples/sec: 20.99 - lr: 0.000567
2023-06-09 15:13:56,316 epoch 5 - iter 3740/3747 - loss 0.16059433 - samples/sec: 20.82 - lr: 0.000556
2023-06-09 15:13:57,801 ----------------------------------------------------------------------------------------------------
2023-06-09 15:13:57,802 EPOCH 5 done: loss 0.1606 - lr 0.000556
2023-06-09 15:15:33,073 Evaluating as a multi-label problem: False
2023-06-09 15:15:33,166 DEV : loss 0.10394289344549179 - f1-score (micro avg)  0.918
2023-06-09 15:15:33,319 BAD EPOCHS (no improvement): 4
2023-06-09 15:15:33,325 ----------------------------------------------------------------------------------------------------
2023-06-09 15:16:46,800 epoch 6 - iter 374/3747 - loss 0.15801370 - samples/sec: 20.37 - lr: 0.000545
2023-06-09 15:17:59,486 epoch 6 - iter 748/3747 - loss 0.15571765 - samples/sec: 20.59 - lr: 0.000533
2023-06-09 15:19:15,204 epoch 6 - iter 1122/3747 - loss 0.14954821 - samples/sec: 19.77 - lr: 0.000522
2023-06-09 15:20:28,759 epoch 6 - iter 1496/3747 - loss 0.15244411 - samples/sec: 20.35 - lr: 0.000511
2023-06-09 15:21:41,404 epoch 6 - iter 1870/3747 - loss 0.15114165 - samples/sec: 20.61 - lr: 0.000500
2023-06-09 15:22:53,322 epoch 6 - iter 2244/3747 - loss 0.15245737 - samples/sec: 20.81 - lr: 0.000489
2023-06-09 15:24:05,789 epoch 6 - iter 2618/3747 - loss 0.15079057 - samples/sec: 20.66 - lr: 0.000478
2023-06-09 15:25:18,651 epoch 6 - iter 2992/3747 - loss 0.15043418 - samples/sec: 20.54 - lr: 0.000467
2023-06-09 15:26:29,587 epoch 6 - iter 3366/3747 - loss 0.14983652 - samples/sec: 21.10 - lr: 0.000456
2023-06-09 15:27:40,713 epoch 6 - iter 3740/3747 - loss 0.15176180 - samples/sec: 21.05 - lr: 0.000445
2023-06-09 15:27:41,925 ----------------------------------------------------------------------------------------------------
2023-06-09 15:27:41,925 EPOCH 6 done: loss 0.1519 - lr 0.000445
2023-06-09 15:29:12,575 Evaluating as a multi-label problem: False
2023-06-09 15:29:12,643 DEV : loss 0.08776772767305374 - f1-score (micro avg)  0.9247
2023-06-09 15:29:12,754 BAD EPOCHS (no improvement): 4
2023-06-09 15:29:12,768 ----------------------------------------------------------------------------------------------------
2023-06-09 15:30:26,672 epoch 7 - iter 374/3747 - loss 0.15348736 - samples/sec: 20.25 - lr: 0.000433
2023-06-09 15:31:39,566 epoch 7 - iter 748/3747 - loss 0.14798562 - samples/sec: 20.53 - lr: 0.000422
2023-06-09 15:32:51,726 epoch 7 - iter 1122/3747 - loss 0.14447162 - samples/sec: 20.74 - lr: 0.000411
2023-06-09 15:34:04,425 epoch 7 - iter 1496/3747 - loss 0.14653232 - samples/sec: 20.59 - lr: 0.000400
2023-06-09 15:35:16,866 epoch 7 - iter 1870/3747 - loss 0.14419951 - samples/sec: 20.66 - lr: 0.000389
2023-06-09 15:36:29,188 epoch 7 - iter 2244/3747 - loss 0.14231670 - samples/sec: 20.70 - lr: 0.000378
2023-06-09 15:37:41,082 epoch 7 - iter 2618/3747 - loss 0.14218801 - samples/sec: 20.82 - lr: 0.000367
2023-06-09 15:38:52,982 epoch 7 - iter 2992/3747 - loss 0.14206606 - samples/sec: 20.82 - lr: 0.000356
2023-06-09 15:40:09,831 epoch 7 - iter 3366/3747 - loss 0.14161623 - samples/sec: 19.48 - lr: 0.000345
2023-06-09 15:41:23,368 epoch 7 - iter 3740/3747 - loss 0.14350606 - samples/sec: 20.35 - lr: 0.000334
2023-06-09 15:41:24,762 ----------------------------------------------------------------------------------------------------
2023-06-09 15:41:24,762 EPOCH 7 done: loss 0.1435 - lr 0.000334
2023-06-09 15:42:53,690 Evaluating as a multi-label problem: False
2023-06-09 15:42:53,762 DEV : loss 0.08948193490505219 - f1-score (micro avg)  0.9323
2023-06-09 15:42:53,876 BAD EPOCHS (no improvement): 4
2023-06-09 15:42:54,059 ----------------------------------------------------------------------------------------------------
2023-06-09 15:44:10,162 epoch 8 - iter 374/3747 - loss 0.14047586 - samples/sec: 19.68 - lr: 0.000322
2023-06-09 15:45:24,278 epoch 8 - iter 748/3747 - loss 0.14094290 - samples/sec: 20.19 - lr: 0.000311
2023-06-09 15:46:37,495 epoch 8 - iter 1122/3747 - loss 0.13601500 - samples/sec: 20.44 - lr: 0.000300
2023-06-09 15:47:50,004 epoch 8 - iter 1496/3747 - loss 0.13621968 - samples/sec: 20.64 - lr: 0.000289
2023-06-09 15:49:02,404 epoch 8 - iter 1870/3747 - loss 0.13574952 - samples/sec: 20.67 - lr: 0.000278
2023-06-09 15:50:15,477 epoch 8 - iter 2244/3747 - loss 0.13517899 - samples/sec: 20.48 - lr: 0.000267
2023-06-09 15:51:30,490 epoch 8 - iter 2618/3747 - loss 0.13677728 - samples/sec: 19.96 - lr: 0.000256
2023-06-09 15:52:44,130 epoch 8 - iter 2992/3747 - loss 0.13539208 - samples/sec: 20.33 - lr: 0.000245
2023-06-09 15:53:56,929 epoch 8 - iter 3366/3747 - loss 0.13513539 - samples/sec: 20.56 - lr: 0.000234
2023-06-09 15:55:09,546 epoch 8 - iter 3740/3747 - loss 0.13463685 - samples/sec: 20.61 - lr: 0.000223
2023-06-09 15:55:10,792 ----------------------------------------------------------------------------------------------------
2023-06-09 15:55:10,792 EPOCH 8 done: loss 0.1346 - lr 0.000223
2023-06-09 15:56:45,614 Evaluating as a multi-label problem: False
2023-06-09 15:56:45,686 DEV : loss 0.0943816751241684 - f1-score (micro avg)  0.9351
2023-06-09 15:56:45,813 BAD EPOCHS (no improvement): 4
2023-06-09 15:56:45,817 ----------------------------------------------------------------------------------------------------
2023-06-09 15:57:59,448 epoch 9 - iter 374/3747 - loss 0.15038247 - samples/sec: 20.33 - lr: 0.000211
2023-06-09 15:59:12,879 epoch 9 - iter 748/3747 - loss 0.14511453 - samples/sec: 20.38 - lr: 0.000200
2023-06-09 16:00:26,300 epoch 9 - iter 1122/3747 - loss 0.13894326 - samples/sec: 20.39 - lr: 0.000189
2023-06-09 16:01:39,408 epoch 9 - iter 1496/3747 - loss 0.13327866 - samples/sec: 20.47 - lr: 0.000178
2023-06-09 16:02:52,633 epoch 9 - iter 1870/3747 - loss 0.13272358 - samples/sec: 20.44 - lr: 0.000167
2023-06-09 16:04:04,737 epoch 9 - iter 2244/3747 - loss 0.13364451 - samples/sec: 20.76 - lr: 0.000156
2023-06-09 16:05:20,606 epoch 9 - iter 2618/3747 - loss 0.13452214 - samples/sec: 19.73 - lr: 0.000145
2023-06-09 16:06:34,247 epoch 9 - iter 2992/3747 - loss 0.13469434 - samples/sec: 20.33 - lr: 0.000134
2023-06-09 16:07:45,795 epoch 9 - iter 3366/3747 - loss 0.13382221 - samples/sec: 20.92 - lr: 0.000123
2023-06-09 16:08:58,854 epoch 9 - iter 3740/3747 - loss 0.13360012 - samples/sec: 20.49 - lr: 0.000111
2023-06-09 16:09:00,155 ----------------------------------------------------------------------------------------------------
2023-06-09 16:09:00,155 EPOCH 9 done: loss 0.1336 - lr 0.000111
2023-06-09 16:10:30,713 Evaluating as a multi-label problem: False
2023-06-09 16:10:30,784 DEV : loss 0.09330910444259644 - f1-score (micro avg)  0.937
2023-06-09 16:10:30,893 BAD EPOCHS (no improvement): 4
2023-06-09 16:10:31,012 ----------------------------------------------------------------------------------------------------
2023-06-09 16:11:45,789 epoch 10 - iter 374/3747 - loss 0.12784298 - samples/sec: 20.02 - lr: 0.000100
2023-06-09 16:12:59,067 epoch 10 - iter 748/3747 - loss 0.12963106 - samples/sec: 20.43 - lr: 0.000089
2023-06-09 16:14:10,091 epoch 10 - iter 1122/3747 - loss 0.13312062 - samples/sec: 21.07 - lr: 0.000078
2023-06-09 16:15:23,647 epoch 10 - iter 1496/3747 - loss 0.12866358 - samples/sec: 20.35 - lr: 0.000067
2023-06-09 16:16:35,838 epoch 10 - iter 1870/3747 - loss 0.13031037 - samples/sec: 20.73 - lr: 0.000056
2023-06-09 16:17:48,490 epoch 10 - iter 2244/3747 - loss 0.12916133 - samples/sec: 20.60 - lr: 0.000045
2023-06-09 16:18:59,769 epoch 10 - iter 2618/3747 - loss 0.12771286 - samples/sec: 21.00 - lr: 0.000034
2023-06-09 16:20:11,768 epoch 10 - iter 2992/3747 - loss 0.12736193 - samples/sec: 20.79 - lr: 0.000023
2023-06-09 16:21:22,863 epoch 10 - iter 3366/3747 - loss 0.12688793 - samples/sec: 21.05 - lr: 0.000011
2023-06-09 16:22:33,983 epoch 10 - iter 3740/3747 - loss 0.12816251 - samples/sec: 21.05 - lr: 0.000000
2023-06-09 16:22:35,250 ----------------------------------------------------------------------------------------------------
2023-06-09 16:22:35,251 EPOCH 10 done: loss 0.1280 - lr 0.000000
2023-06-09 16:24:10,369 Evaluating as a multi-label problem: False
2023-06-09 16:24:10,443 DEV : loss 0.08779094368219376 - f1-score (micro avg)  0.9409
2023-06-09 16:24:10,580 BAD EPOCHS (no improvement): 4
2023-06-09 16:24:19,628 ----------------------------------------------------------------------------------------------------
2023-06-09 16:24:19,632 Testing using last state of model ...
2023-06-09 16:25:56,877 Evaluating as a multi-label problem: False
2023-06-09 16:25:56,953 0.89	0.9049	0.8974	0.8423
2023-06-09 16:25:56,953 
Results:
- F-score (micro) 0.8974
- F-score (macro) 0.8784
- Accuracy 0.8423

By class:
              precision    recall  f1-score   support

         ORG     0.8672    0.8886    0.8778      1661
         LOC     0.9066    0.8963    0.9014      1668
         PER     0.9826    0.9759    0.9792      1617
        MISC     0.7150    0.8006    0.7554       702

   micro avg     0.8900    0.9049    0.8974      5648
   macro avg     0.8679    0.8903    0.8784      5648
weighted avg     0.8930    0.9049    0.8986      5648

2023-06-09 16:25:56,953 ----------------------------------------------------------------------------------------------------
