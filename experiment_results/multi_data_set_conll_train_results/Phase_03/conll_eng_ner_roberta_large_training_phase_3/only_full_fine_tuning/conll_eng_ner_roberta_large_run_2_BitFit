2023-06-03 00:18:03,527 ----------------------------------------------------------------------------------------------------
2023-06-03 00:18:03,532 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-03 00:18:03,533 ----------------------------------------------------------------------------------------------------
2023-06-03 00:18:03,534 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-03 00:18:03,534 ----------------------------------------------------------------------------------------------------
2023-06-03 00:18:03,534 Parameters:
2023-06-03 00:18:03,534  - learning_rate: "0.001000"
2023-06-03 00:18:03,534  - mini_batch_size: "4"
2023-06-03 00:18:03,534  - patience: "3"
2023-06-03 00:18:03,534  - anneal_factor: "0.5"
2023-06-03 00:18:03,534  - max_epochs: "10"
2023-06-03 00:18:03,534  - shuffle: "True"
2023-06-03 00:18:03,534  - train_with_dev: "False"
2023-06-03 00:18:03,534  - batch_growth_annealing: "False"
2023-06-03 00:18:03,534 ----------------------------------------------------------------------------------------------------
2023-06-03 00:18:03,534 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_2_BitFit"
2023-06-03 00:18:03,534 ----------------------------------------------------------------------------------------------------
2023-06-03 00:18:03,534 Device: cuda:0
2023-06-03 00:18:03,535 ----------------------------------------------------------------------------------------------------
2023-06-03 00:18:03,535 Embeddings storage mode: none
2023-06-03 00:18:03,535 ----------------------------------------------------------------------------------------------------
2023-06-03 00:19:12,697 epoch 1 - iter 374/3747 - loss 1.90083424 - samples/sec: 21.64 - lr: 0.000100
2023-06-03 00:20:24,101 epoch 1 - iter 748/3747 - loss 1.26621614 - samples/sec: 20.96 - lr: 0.000200
2023-06-03 00:21:30,600 epoch 1 - iter 1122/3747 - loss 1.01449454 - samples/sec: 22.51 - lr: 0.000299
2023-06-03 00:22:40,171 epoch 1 - iter 1496/3747 - loss 0.87906613 - samples/sec: 21.51 - lr: 0.000399
2023-06-03 00:23:44,207 epoch 1 - iter 1870/3747 - loss 0.76320910 - samples/sec: 23.37 - lr: 0.000499
2023-06-03 00:24:51,817 epoch 1 - iter 2244/3747 - loss 0.67300389 - samples/sec: 22.14 - lr: 0.000599
2023-06-03 00:26:00,945 epoch 1 - iter 2618/3747 - loss 0.61002385 - samples/sec: 21.65 - lr: 0.000699
2023-06-03 00:27:10,029 epoch 1 - iter 2992/3747 - loss 0.56771497 - samples/sec: 21.67 - lr: 0.000799
2023-06-03 00:28:19,767 epoch 1 - iter 3366/3747 - loss 0.52949216 - samples/sec: 21.46 - lr: 0.000898
2023-06-03 00:29:34,735 epoch 1 - iter 3740/3747 - loss 0.49255373 - samples/sec: 19.96 - lr: 0.000998
2023-06-03 00:29:35,940 ----------------------------------------------------------------------------------------------------
2023-06-03 00:29:35,941 EPOCH 1 done: loss 0.4922 - lr 0.000998
2023-06-03 00:31:03,855 Evaluating as a multi-label problem: False
2023-06-03 00:31:03,919 DEV : loss 0.14063555002212524 - f1-score (micro avg)  0.8744
2023-06-03 00:31:04,025 BAD EPOCHS (no improvement): 4
2023-06-03 00:31:04,035 ----------------------------------------------------------------------------------------------------
2023-06-03 00:32:10,227 epoch 2 - iter 374/3747 - loss 0.24547845 - samples/sec: 22.61 - lr: 0.000989
2023-06-03 00:33:18,693 epoch 2 - iter 748/3747 - loss 0.23388403 - samples/sec: 21.86 - lr: 0.000978
2023-06-03 00:34:27,229 epoch 2 - iter 1122/3747 - loss 0.22316405 - samples/sec: 21.84 - lr: 0.000967
2023-06-03 00:35:35,324 epoch 2 - iter 1496/3747 - loss 0.21973296 - samples/sec: 21.98 - lr: 0.000956
2023-06-03 00:36:43,756 epoch 2 - iter 1870/3747 - loss 0.21789259 - samples/sec: 21.87 - lr: 0.000945
2023-06-03 00:37:51,479 epoch 2 - iter 2244/3747 - loss 0.21311146 - samples/sec: 22.10 - lr: 0.000933
2023-06-03 00:38:59,636 epoch 2 - iter 2618/3747 - loss 0.20988905 - samples/sec: 21.96 - lr: 0.000922
2023-06-03 00:40:13,479 epoch 2 - iter 2992/3747 - loss 0.20942288 - samples/sec: 20.27 - lr: 0.000911
2023-06-03 00:41:26,023 epoch 2 - iter 3366/3747 - loss 0.20616402 - samples/sec: 20.63 - lr: 0.000900
2023-06-03 00:42:40,618 epoch 2 - iter 3740/3747 - loss 0.20435903 - samples/sec: 20.07 - lr: 0.000889
2023-06-03 00:42:41,844 ----------------------------------------------------------------------------------------------------
2023-06-03 00:42:41,844 EPOCH 2 done: loss 0.2045 - lr 0.000889
2023-06-03 00:44:14,813 Evaluating as a multi-label problem: False
2023-06-03 00:44:14,881 DEV : loss 0.11744706332683563 - f1-score (micro avg)  0.8873
2023-06-03 00:44:14,993 BAD EPOCHS (no improvement): 4
2023-06-03 00:44:14,996 ----------------------------------------------------------------------------------------------------
2023-06-03 00:45:24,792 epoch 3 - iter 374/3747 - loss 0.16712626 - samples/sec: 21.44 - lr: 0.000878
2023-06-03 00:46:28,971 epoch 3 - iter 748/3747 - loss 0.17444039 - samples/sec: 23.32 - lr: 0.000867
2023-06-03 00:47:33,899 epoch 3 - iter 1122/3747 - loss 0.17531552 - samples/sec: 23.05 - lr: 0.000856
2023-06-03 00:48:48,436 epoch 3 - iter 1496/3747 - loss 0.17393735 - samples/sec: 20.08 - lr: 0.000845
2023-06-03 00:49:55,289 epoch 3 - iter 1870/3747 - loss 0.17461337 - samples/sec: 22.39 - lr: 0.000833
2023-06-03 00:51:01,859 epoch 3 - iter 2244/3747 - loss 0.17694038 - samples/sec: 22.48 - lr: 0.000822
2023-06-03 00:52:13,004 epoch 3 - iter 2618/3747 - loss 0.17485913 - samples/sec: 21.04 - lr: 0.000811
2023-06-03 00:53:20,351 epoch 3 - iter 2992/3747 - loss 0.17533170 - samples/sec: 22.23 - lr: 0.000800
2023-06-03 00:54:31,241 epoch 3 - iter 3366/3747 - loss 0.17493713 - samples/sec: 21.11 - lr: 0.000789
2023-06-03 00:55:38,078 epoch 3 - iter 3740/3747 - loss 0.17404077 - samples/sec: 22.39 - lr: 0.000778
2023-06-03 00:55:39,266 ----------------------------------------------------------------------------------------------------
2023-06-03 00:55:39,267 EPOCH 3 done: loss 0.1741 - lr 0.000778
2023-06-03 00:57:07,237 Evaluating as a multi-label problem: False
2023-06-03 00:57:07,307 DEV : loss 0.11101089417934418 - f1-score (micro avg)  0.9184
2023-06-03 00:57:07,424 BAD EPOCHS (no improvement): 4
2023-06-03 00:57:07,430 ----------------------------------------------------------------------------------------------------
2023-06-03 00:58:15,880 epoch 4 - iter 374/3747 - loss 0.15137028 - samples/sec: 21.87 - lr: 0.000767
2023-06-03 00:59:24,514 epoch 4 - iter 748/3747 - loss 0.15325202 - samples/sec: 21.81 - lr: 0.000756
2023-06-03 01:00:33,180 epoch 4 - iter 1122/3747 - loss 0.15667145 - samples/sec: 21.80 - lr: 0.000745
2023-06-03 01:01:41,458 epoch 4 - iter 1496/3747 - loss 0.15591046 - samples/sec: 21.92 - lr: 0.000733
2023-06-03 01:02:47,565 epoch 4 - iter 1870/3747 - loss 0.15473638 - samples/sec: 22.64 - lr: 0.000722
2023-06-03 01:04:00,753 epoch 4 - iter 2244/3747 - loss 0.15784387 - samples/sec: 20.45 - lr: 0.000711
2023-06-03 01:05:09,947 epoch 4 - iter 2618/3747 - loss 0.15788366 - samples/sec: 21.63 - lr: 0.000700
2023-06-03 01:06:16,805 epoch 4 - iter 2992/3747 - loss 0.15674704 - samples/sec: 22.39 - lr: 0.000689
2023-06-03 01:07:23,327 epoch 4 - iter 3366/3747 - loss 0.15907985 - samples/sec: 22.50 - lr: 0.000678
2023-06-03 01:08:32,109 epoch 4 - iter 3740/3747 - loss 0.16025402 - samples/sec: 21.76 - lr: 0.000667
2023-06-03 01:08:33,316 ----------------------------------------------------------------------------------------------------
2023-06-03 01:08:33,317 EPOCH 4 done: loss 0.1603 - lr 0.000667
2023-06-03 01:09:51,980 Evaluating as a multi-label problem: False
2023-06-03 01:09:52,046 DEV : loss 0.09955872595310211 - f1-score (micro avg)  0.919
2023-06-03 01:09:52,150 BAD EPOCHS (no improvement): 4
2023-06-03 01:09:52,211 ----------------------------------------------------------------------------------------------------
2023-06-03 01:11:00,838 epoch 5 - iter 374/3747 - loss 0.15707856 - samples/sec: 21.81 - lr: 0.000656
2023-06-03 01:12:06,508 epoch 5 - iter 748/3747 - loss 0.16719683 - samples/sec: 22.79 - lr: 0.000645
2023-06-03 01:13:10,838 epoch 5 - iter 1122/3747 - loss 0.16635480 - samples/sec: 23.27 - lr: 0.000633
2023-06-03 01:14:14,381 epoch 5 - iter 1496/3747 - loss 0.16348846 - samples/sec: 23.56 - lr: 0.000622
2023-06-03 01:15:21,425 epoch 5 - iter 1870/3747 - loss 0.16139809 - samples/sec: 22.33 - lr: 0.000611
2023-06-03 01:16:28,185 epoch 5 - iter 2244/3747 - loss 0.16083512 - samples/sec: 22.42 - lr: 0.000600
2023-06-03 01:17:35,582 epoch 5 - iter 2618/3747 - loss 0.16065719 - samples/sec: 22.21 - lr: 0.000589
2023-06-03 01:18:43,900 epoch 5 - iter 2992/3747 - loss 0.15818374 - samples/sec: 21.91 - lr: 0.000578
2023-06-03 01:19:52,944 epoch 5 - iter 3366/3747 - loss 0.15808826 - samples/sec: 21.68 - lr: 0.000567
2023-06-03 01:21:01,189 epoch 5 - iter 3740/3747 - loss 0.15630878 - samples/sec: 21.93 - lr: 0.000556
2023-06-03 01:21:02,508 ----------------------------------------------------------------------------------------------------
2023-06-03 01:21:02,508 EPOCH 5 done: loss 0.1564 - lr 0.000556
2023-06-03 01:22:27,774 Evaluating as a multi-label problem: False
2023-06-03 01:22:27,849 DEV : loss 0.10102427005767822 - f1-score (micro avg)  0.9197
2023-06-03 01:22:27,958 BAD EPOCHS (no improvement): 4
2023-06-03 01:22:27,960 ----------------------------------------------------------------------------------------------------
2023-06-03 01:23:36,056 epoch 6 - iter 374/3747 - loss 0.14636419 - samples/sec: 21.98 - lr: 0.000545
2023-06-03 01:24:43,724 epoch 6 - iter 748/3747 - loss 0.14397581 - samples/sec: 22.12 - lr: 0.000533
2023-06-03 01:25:53,092 epoch 6 - iter 1122/3747 - loss 0.14623175 - samples/sec: 21.58 - lr: 0.000522
2023-06-03 01:27:04,727 epoch 6 - iter 1496/3747 - loss 0.14695802 - samples/sec: 20.89 - lr: 0.000511
2023-06-03 01:28:14,560 epoch 6 - iter 1870/3747 - loss 0.14744580 - samples/sec: 21.43 - lr: 0.000500
2023-06-03 01:29:19,203 epoch 6 - iter 2244/3747 - loss 0.14749997 - samples/sec: 23.16 - lr: 0.000489
2023-06-03 01:30:28,333 epoch 6 - iter 2618/3747 - loss 0.14757579 - samples/sec: 21.65 - lr: 0.000478
2023-06-03 01:31:36,404 epoch 6 - iter 2992/3747 - loss 0.14680996 - samples/sec: 21.99 - lr: 0.000467
2023-06-03 01:32:44,879 epoch 6 - iter 3366/3747 - loss 0.14738184 - samples/sec: 21.86 - lr: 0.000456
2023-06-03 01:33:52,724 epoch 6 - iter 3740/3747 - loss 0.14740744 - samples/sec: 22.06 - lr: 0.000445
2023-06-03 01:33:53,836 ----------------------------------------------------------------------------------------------------
2023-06-03 01:33:53,836 EPOCH 6 done: loss 0.1474 - lr 0.000445
2023-06-03 01:35:19,787 Evaluating as a multi-label problem: False
2023-06-03 01:35:19,856 DEV : loss 0.09748769551515579 - f1-score (micro avg)  0.9336
2023-06-03 01:35:19,983 BAD EPOCHS (no improvement): 4
2023-06-03 01:35:19,986 ----------------------------------------------------------------------------------------------------
2023-06-03 01:36:29,213 epoch 7 - iter 374/3747 - loss 0.12720019 - samples/sec: 21.62 - lr: 0.000433
2023-06-03 01:37:37,969 epoch 7 - iter 748/3747 - loss 0.13651361 - samples/sec: 21.77 - lr: 0.000422
2023-06-03 01:38:46,739 epoch 7 - iter 1122/3747 - loss 0.14325899 - samples/sec: 21.77 - lr: 0.000411
2023-06-03 01:39:55,131 epoch 7 - iter 1496/3747 - loss 0.14244111 - samples/sec: 21.89 - lr: 0.000400
2023-06-03 01:41:03,672 epoch 7 - iter 1870/3747 - loss 0.14510913 - samples/sec: 21.84 - lr: 0.000389
2023-06-03 01:42:13,217 epoch 7 - iter 2244/3747 - loss 0.14330932 - samples/sec: 21.52 - lr: 0.000378
2023-06-03 01:43:19,318 epoch 7 - iter 2618/3747 - loss 0.14469161 - samples/sec: 22.64 - lr: 0.000367
2023-06-03 01:44:28,104 epoch 7 - iter 2992/3747 - loss 0.14385925 - samples/sec: 21.76 - lr: 0.000356
2023-06-03 01:45:35,409 epoch 7 - iter 3366/3747 - loss 0.14127395 - samples/sec: 22.24 - lr: 0.000345
2023-06-03 01:46:47,494 epoch 7 - iter 3740/3747 - loss 0.14029877 - samples/sec: 20.76 - lr: 0.000334
2023-06-03 01:46:48,649 ----------------------------------------------------------------------------------------------------
2023-06-03 01:46:48,649 EPOCH 7 done: loss 0.1403 - lr 0.000334
2023-06-03 01:48:22,166 Evaluating as a multi-label problem: False
2023-06-03 01:48:22,242 DEV : loss 0.10484253615140915 - f1-score (micro avg)  0.9316
2023-06-03 01:48:22,371 BAD EPOCHS (no improvement): 4
2023-06-03 01:48:22,374 ----------------------------------------------------------------------------------------------------
2023-06-03 01:49:35,739 epoch 8 - iter 374/3747 - loss 0.13489845 - samples/sec: 20.40 - lr: 0.000322
2023-06-03 01:50:39,005 epoch 8 - iter 748/3747 - loss 0.14362629 - samples/sec: 23.66 - lr: 0.000311
2023-06-03 01:51:46,950 epoch 8 - iter 1122/3747 - loss 0.14117039 - samples/sec: 22.03 - lr: 0.000300
2023-06-03 01:52:54,569 epoch 8 - iter 1496/3747 - loss 0.13708961 - samples/sec: 22.14 - lr: 0.000289
2023-06-03 01:54:05,465 epoch 8 - iter 1870/3747 - loss 0.13501147 - samples/sec: 21.11 - lr: 0.000278
2023-06-03 01:55:18,951 epoch 8 - iter 2244/3747 - loss 0.13218969 - samples/sec: 20.37 - lr: 0.000267
2023-06-03 01:56:27,718 epoch 8 - iter 2618/3747 - loss 0.13122619 - samples/sec: 21.77 - lr: 0.000256
2023-06-03 01:57:31,189 epoch 8 - iter 2992/3747 - loss 0.13210352 - samples/sec: 23.58 - lr: 0.000245
2023-06-03 01:58:39,466 epoch 8 - iter 3366/3747 - loss 0.13267851 - samples/sec: 21.92 - lr: 0.000234
2023-06-03 01:59:47,312 epoch 8 - iter 3740/3747 - loss 0.13288053 - samples/sec: 22.06 - lr: 0.000223
2023-06-03 01:59:48,675 ----------------------------------------------------------------------------------------------------
2023-06-03 01:59:48,675 EPOCH 8 done: loss 0.1327 - lr 0.000223
2023-06-03 02:01:17,926 Evaluating as a multi-label problem: False
2023-06-03 02:01:17,971 DEV : loss 0.09168968349695206 - f1-score (micro avg)  0.9335
2023-06-03 02:01:18,078 BAD EPOCHS (no improvement): 4
2023-06-03 02:01:18,081 ----------------------------------------------------------------------------------------------------
2023-06-03 02:02:26,537 epoch 9 - iter 374/3747 - loss 0.15098288 - samples/sec: 21.87 - lr: 0.000211
2023-06-03 02:03:36,770 epoch 9 - iter 748/3747 - loss 0.13880027 - samples/sec: 21.31 - lr: 0.000200
2023-06-03 02:04:45,630 epoch 9 - iter 1122/3747 - loss 0.13340204 - samples/sec: 21.74 - lr: 0.000189
2023-06-03 02:05:54,047 epoch 9 - iter 1496/3747 - loss 0.13238299 - samples/sec: 21.88 - lr: 0.000178
2023-06-03 02:06:59,747 epoch 9 - iter 1870/3747 - loss 0.12907278 - samples/sec: 22.78 - lr: 0.000167
2023-06-03 02:08:09,814 epoch 9 - iter 2244/3747 - loss 0.12812285 - samples/sec: 21.36 - lr: 0.000156
2023-06-03 02:09:21,710 epoch 9 - iter 2618/3747 - loss 0.12859053 - samples/sec: 20.82 - lr: 0.000145
2023-06-03 02:10:29,891 epoch 9 - iter 2992/3747 - loss 0.12763898 - samples/sec: 21.95 - lr: 0.000134
2023-06-03 02:11:36,272 epoch 9 - iter 3366/3747 - loss 0.12645428 - samples/sec: 22.55 - lr: 0.000123
2023-06-03 02:12:45,376 epoch 9 - iter 3740/3747 - loss 0.12667720 - samples/sec: 21.66 - lr: 0.000111
2023-06-03 02:12:46,453 ----------------------------------------------------------------------------------------------------
2023-06-03 02:12:46,453 EPOCH 9 done: loss 0.1265 - lr 0.000111
2023-06-03 02:14:17,521 Evaluating as a multi-label problem: False
2023-06-03 02:14:17,591 DEV : loss 0.08964627981185913 - f1-score (micro avg)  0.9389
2023-06-03 02:14:17,715 BAD EPOCHS (no improvement): 4
2023-06-03 02:14:17,826 ----------------------------------------------------------------------------------------------------
2023-06-03 02:15:21,176 epoch 10 - iter 374/3747 - loss 0.10559854 - samples/sec: 23.63 - lr: 0.000100
2023-06-03 02:16:22,032 epoch 10 - iter 748/3747 - loss 0.11707583 - samples/sec: 24.60 - lr: 0.000089
2023-06-03 02:17:29,327 epoch 10 - iter 1122/3747 - loss 0.12091152 - samples/sec: 22.24 - lr: 0.000078
2023-06-03 02:18:36,885 epoch 10 - iter 1496/3747 - loss 0.11904246 - samples/sec: 22.16 - lr: 0.000067
2023-06-03 02:19:44,885 epoch 10 - iter 1870/3747 - loss 0.12060882 - samples/sec: 22.01 - lr: 0.000056
2023-06-03 02:20:49,796 epoch 10 - iter 2244/3747 - loss 0.11938489 - samples/sec: 23.06 - lr: 0.000045
2023-06-03 02:21:57,176 epoch 10 - iter 2618/3747 - loss 0.12038139 - samples/sec: 22.21 - lr: 0.000034
2023-06-03 02:23:07,228 epoch 10 - iter 2992/3747 - loss 0.12006689 - samples/sec: 21.37 - lr: 0.000023
2023-06-03 02:24:14,612 epoch 10 - iter 3366/3747 - loss 0.12017929 - samples/sec: 22.21 - lr: 0.000011
2023-06-03 02:25:19,860 epoch 10 - iter 3740/3747 - loss 0.12046493 - samples/sec: 22.94 - lr: 0.000000
2023-06-03 02:25:21,456 ----------------------------------------------------------------------------------------------------
2023-06-03 02:25:21,456 EPOCH 10 done: loss 0.1205 - lr 0.000000
2023-06-03 02:26:50,468 Evaluating as a multi-label problem: False
2023-06-03 02:26:50,534 DEV : loss 0.0919959768652916 - f1-score (micro avg)  0.9397
2023-06-03 02:26:50,652 BAD EPOCHS (no improvement): 4
2023-06-03 02:27:04,282 ----------------------------------------------------------------------------------------------------
2023-06-03 02:27:04,285 Testing using last state of model ...
2023-06-03 02:28:21,094 Evaluating as a multi-label problem: False
2023-06-03 02:28:21,164 0.8862	0.9	0.893	0.8333
2023-06-03 02:28:21,164 
Results:
- F-score (micro) 0.893
- F-score (macro) 0.8744
- Accuracy 0.8333

By class:
              precision    recall  f1-score   support

         ORG     0.8702    0.8796    0.8749      1661
         LOC     0.8920    0.8861    0.8890      1668
         PER     0.9802    0.9796    0.9799      1617
        MISC     0.7143    0.7977    0.7537       702

   micro avg     0.8862    0.9000    0.8930      5648
   macro avg     0.8642    0.8857    0.8744      5648
weighted avg     0.8887    0.9000    0.8941      5648

2023-06-03 02:28:21,164 ----------------------------------------------------------------------------------------------------
