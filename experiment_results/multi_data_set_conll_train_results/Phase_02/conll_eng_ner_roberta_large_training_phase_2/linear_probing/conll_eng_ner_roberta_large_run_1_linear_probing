2023-05-29 17:23:52,028 ----------------------------------------------------------------------------------------------------
2023-05-29 17:23:52,032 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-29 17:23:52,036 ----------------------------------------------------------------------------------------------------
2023-05-29 17:23:52,036 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-05-29 17:23:52,038 ----------------------------------------------------------------------------------------------------
2023-05-29 17:23:52,038 Parameters:
2023-05-29 17:23:52,038  - learning_rate: "0.800000"
2023-05-29 17:23:52,038  - mini_batch_size: "32"
2023-05-29 17:23:52,038  - patience: "3"
2023-05-29 17:23:52,038  - anneal_factor: "0.5"
2023-05-29 17:23:52,038  - max_epochs: "10"
2023-05-29 17:23:52,038  - shuffle: "True"
2023-05-29 17:23:52,038  - train_with_dev: "False"
2023-05-29 17:23:52,038  - batch_growth_annealing: "False"
2023-05-29 17:23:52,040 ----------------------------------------------------------------------------------------------------
2023-05-29 17:23:52,040 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_1_linear_probing"
2023-05-29 17:23:52,040 ----------------------------------------------------------------------------------------------------
2023-05-29 17:23:52,040 Device: cuda:3
2023-05-29 17:23:52,040 ----------------------------------------------------------------------------------------------------
2023-05-29 17:23:52,040 Embeddings storage mode: none
2023-05-29 17:23:52,040 ----------------------------------------------------------------------------------------------------
2023-05-29 17:24:13,300 epoch 1 - iter 46/469 - loss 1.49059686 - samples/sec: 69.27 - lr: 0.078465
2023-05-29 17:24:36,068 epoch 1 - iter 92/469 - loss 1.77792525 - samples/sec: 64.67 - lr: 0.156930
2023-05-29 17:24:56,204 epoch 1 - iter 138/469 - loss 2.33481647 - samples/sec: 73.14 - lr: 0.235394
2023-05-29 17:25:17,512 epoch 1 - iter 184/469 - loss 3.32253699 - samples/sec: 69.11 - lr: 0.313859
2023-05-29 17:25:33,885 epoch 1 - iter 230/469 - loss 4.09552539 - samples/sec: 89.95 - lr: 0.392324
2023-05-29 17:25:55,486 epoch 1 - iter 276/469 - loss 4.65203931 - samples/sec: 68.18 - lr: 0.470789
2023-05-29 17:26:19,942 epoch 1 - iter 322/469 - loss 5.56072309 - samples/sec: 60.21 - lr: 0.549254
2023-05-29 17:26:41,785 epoch 1 - iter 368/469 - loss 6.46929513 - samples/sec: 67.41 - lr: 0.627719
2023-05-29 17:27:05,521 epoch 1 - iter 414/469 - loss 6.91847912 - samples/sec: 62.03 - lr: 0.706183
2023-05-29 17:27:27,507 epoch 1 - iter 460/469 - loss 7.35436216 - samples/sec: 66.97 - lr: 0.784648
2023-05-29 17:27:31,585 ----------------------------------------------------------------------------------------------------
2023-05-29 17:27:31,585 EPOCH 1 done: loss 7.4801 - lr 0.784648
2023-05-29 17:28:50,208 Evaluating as a multi-label problem: False
2023-05-29 17:28:50,281 DEV : loss 5.545548915863037 - f1-score (micro avg)  0.6506
2023-05-29 17:28:50,366 BAD EPOCHS (no improvement): 4
2023-05-29 17:28:50,369 ----------------------------------------------------------------------------------------------------
2023-05-29 17:29:13,160 epoch 2 - iter 46/469 - loss 13.12119630 - samples/sec: 64.62 - lr: 0.791288
2023-05-29 17:29:37,934 epoch 2 - iter 92/469 - loss 13.21672891 - samples/sec: 59.43 - lr: 0.782576
2023-05-29 17:30:00,052 epoch 2 - iter 138/469 - loss 13.40841795 - samples/sec: 66.57 - lr: 0.773864
2023-05-29 17:30:19,896 epoch 2 - iter 184/469 - loss 13.23784055 - samples/sec: 74.21 - lr: 0.765152
2023-05-29 17:30:45,263 epoch 2 - iter 230/469 - loss 13.20606150 - samples/sec: 58.05 - lr: 0.756439
2023-05-29 17:31:04,438 epoch 2 - iter 276/469 - loss 13.05923278 - samples/sec: 76.80 - lr: 0.747727
2023-05-29 17:31:23,869 epoch 2 - iter 322/469 - loss 12.75833403 - samples/sec: 75.78 - lr: 0.739015
2023-05-29 17:31:44,172 epoch 2 - iter 368/469 - loss 12.42274119 - samples/sec: 72.52 - lr: 0.730303
2023-05-29 17:32:06,099 epoch 2 - iter 414/469 - loss 12.19154097 - samples/sec: 67.15 - lr: 0.721591
2023-05-29 17:32:28,567 epoch 2 - iter 460/469 - loss 12.09026348 - samples/sec: 65.54 - lr: 0.712879
2023-05-29 17:32:34,983 ----------------------------------------------------------------------------------------------------
2023-05-29 17:32:34,983 EPOCH 2 done: loss 12.0975 - lr 0.712879
2023-05-29 17:33:52,620 Evaluating as a multi-label problem: False
2023-05-29 17:33:52,687 DEV : loss 4.503268241882324 - f1-score (micro avg)  0.6252
2023-05-29 17:33:52,773 BAD EPOCHS (no improvement): 4
2023-05-29 17:33:52,776 ----------------------------------------------------------------------------------------------------
2023-05-29 17:34:15,422 epoch 3 - iter 46/469 - loss 11.78817575 - samples/sec: 65.03 - lr: 0.702462
2023-05-29 17:34:39,663 epoch 3 - iter 92/469 - loss 11.70296505 - samples/sec: 60.74 - lr: 0.693750
2023-05-29 17:35:02,270 epoch 3 - iter 138/469 - loss 11.66608072 - samples/sec: 65.14 - lr: 0.685038
2023-05-29 17:35:24,442 epoch 3 - iter 184/469 - loss 11.50009790 - samples/sec: 66.41 - lr: 0.676326
2023-05-29 17:35:49,096 epoch 3 - iter 230/469 - loss 11.55255346 - samples/sec: 59.72 - lr: 0.667614
2023-05-29 17:36:11,250 epoch 3 - iter 276/469 - loss 11.50758105 - samples/sec: 66.46 - lr: 0.658902
2023-05-29 17:36:33,476 epoch 3 - iter 322/469 - loss 11.60652174 - samples/sec: 66.25 - lr: 0.650189
2023-05-29 17:36:58,655 epoch 3 - iter 368/469 - loss 11.49243396 - samples/sec: 58.48 - lr: 0.641477
2023-05-29 17:37:20,994 epoch 3 - iter 414/469 - loss 11.50440132 - samples/sec: 65.91 - lr: 0.632765
2023-05-29 17:37:43,699 epoch 3 - iter 460/469 - loss 11.45391289 - samples/sec: 64.86 - lr: 0.624053
2023-05-29 17:37:49,828 ----------------------------------------------------------------------------------------------------
2023-05-29 17:37:49,828 EPOCH 3 done: loss 11.4484 - lr 0.624053
2023-05-29 17:39:07,349 Evaluating as a multi-label problem: False
2023-05-29 17:39:07,415 DEV : loss 4.043145656585693 - f1-score (micro avg)  0.66
2023-05-29 17:39:07,501 BAD EPOCHS (no improvement): 4
2023-05-29 17:39:07,504 ----------------------------------------------------------------------------------------------------
2023-05-29 17:39:29,519 epoch 4 - iter 46/469 - loss 10.81751605 - samples/sec: 66.90 - lr: 0.613636
2023-05-29 17:39:54,262 epoch 4 - iter 92/469 - loss 10.14608607 - samples/sec: 59.51 - lr: 0.604924
2023-05-29 17:40:14,114 epoch 4 - iter 138/469 - loss 9.82882993 - samples/sec: 74.17 - lr: 0.596212
2023-05-29 17:40:31,913 epoch 4 - iter 184/469 - loss 10.00272915 - samples/sec: 82.73 - lr: 0.587500
2023-05-29 17:40:51,289 epoch 4 - iter 230/469 - loss 9.75936755 - samples/sec: 75.99 - lr: 0.578788
2023-05-29 17:41:08,583 epoch 4 - iter 276/469 - loss 9.65992020 - samples/sec: 85.14 - lr: 0.570076
2023-05-29 17:41:26,468 epoch 4 - iter 322/469 - loss 9.62067346 - samples/sec: 82.33 - lr: 0.561364
2023-05-29 17:41:46,251 epoch 4 - iter 368/469 - loss 9.59912193 - samples/sec: 74.43 - lr: 0.552652
2023-05-29 17:42:03,802 epoch 4 - iter 414/469 - loss 9.49527768 - samples/sec: 83.89 - lr: 0.543939
2023-05-29 17:42:23,616 epoch 4 - iter 460/469 - loss 9.39217725 - samples/sec: 74.31 - lr: 0.535227
2023-05-29 17:42:26,655 ----------------------------------------------------------------------------------------------------
2023-05-29 17:42:26,656 EPOCH 4 done: loss 9.3991 - lr 0.535227
2023-05-29 17:43:26,368 Evaluating as a multi-label problem: False
2023-05-29 17:43:26,411 DEV : loss 3.9869887828826904 - f1-score (micro avg)  0.6406
2023-05-29 17:43:26,464 BAD EPOCHS (no improvement): 4
2023-05-29 17:43:26,467 ----------------------------------------------------------------------------------------------------
2023-05-29 17:43:48,256 epoch 5 - iter 46/469 - loss 9.45551658 - samples/sec: 67.58 - lr: 0.524811
2023-05-29 17:44:12,849 epoch 5 - iter 92/469 - loss 8.72965212 - samples/sec: 59.87 - lr: 0.516098
2023-05-29 17:44:33,201 epoch 5 - iter 138/469 - loss 8.76523936 - samples/sec: 72.35 - lr: 0.507386
2023-05-29 17:44:50,342 epoch 5 - iter 184/469 - loss 8.83195278 - samples/sec: 85.90 - lr: 0.498674
2023-05-29 17:45:10,051 epoch 5 - iter 230/469 - loss 8.84171839 - samples/sec: 74.71 - lr: 0.489962
2023-05-29 17:45:27,978 epoch 5 - iter 276/469 - loss 8.94449837 - samples/sec: 82.14 - lr: 0.481250
2023-05-29 17:45:47,369 epoch 5 - iter 322/469 - loss 8.95731015 - samples/sec: 75.93 - lr: 0.472538
2023-05-29 17:46:04,910 epoch 5 - iter 368/469 - loss 8.95580436 - samples/sec: 83.94 - lr: 0.463826
2023-05-29 17:46:23,672 epoch 5 - iter 414/469 - loss 8.92230689 - samples/sec: 78.49 - lr: 0.455114
2023-05-29 17:46:48,255 epoch 5 - iter 460/469 - loss 8.74583743 - samples/sec: 59.89 - lr: 0.446402
2023-05-29 17:46:52,359 ----------------------------------------------------------------------------------------------------
2023-05-29 17:46:52,359 EPOCH 5 done: loss 8.7280 - lr 0.446402
2023-05-29 17:48:00,545 Evaluating as a multi-label problem: False
2023-05-29 17:48:00,621 DEV : loss 2.82920503616333 - f1-score (micro avg)  0.6869
2023-05-29 17:48:00,726 BAD EPOCHS (no improvement): 4
2023-05-29 17:48:00,728 ----------------------------------------------------------------------------------------------------
2023-05-29 17:48:22,711 epoch 6 - iter 46/469 - loss 8.06791480 - samples/sec: 67.00 - lr: 0.435985
2023-05-29 17:48:47,001 epoch 6 - iter 92/469 - loss 7.71887392 - samples/sec: 60.62 - lr: 0.427273
2023-05-29 17:49:09,499 epoch 6 - iter 138/469 - loss 7.73201613 - samples/sec: 65.45 - lr: 0.418561
2023-05-29 17:49:32,624 epoch 6 - iter 184/469 - loss 7.78533497 - samples/sec: 63.68 - lr: 0.409848
2023-05-29 17:49:57,024 epoch 6 - iter 230/469 - loss 7.65936320 - samples/sec: 60.35 - lr: 0.401136
2023-05-29 17:50:19,841 epoch 6 - iter 276/469 - loss 7.56760545 - samples/sec: 64.53 - lr: 0.392424
2023-05-29 17:50:44,941 epoch 6 - iter 322/469 - loss 7.52615148 - samples/sec: 58.66 - lr: 0.383712
2023-05-29 17:51:06,949 epoch 6 - iter 368/469 - loss 7.50603254 - samples/sec: 66.90 - lr: 0.375000
2023-05-29 17:51:29,199 epoch 6 - iter 414/469 - loss 7.37544446 - samples/sec: 66.18 - lr: 0.366288
2023-05-29 17:51:54,398 epoch 6 - iter 460/469 - loss 7.31168705 - samples/sec: 58.43 - lr: 0.357576
2023-05-29 17:51:58,277 ----------------------------------------------------------------------------------------------------
2023-05-29 17:51:58,277 EPOCH 6 done: loss 7.2986 - lr 0.357576
2023-05-29 17:53:04,626 Evaluating as a multi-label problem: False
2023-05-29 17:53:04,699 DEV : loss 2.5360946655273438 - f1-score (micro avg)  0.6514
2023-05-29 17:53:04,805 BAD EPOCHS (no improvement): 4
2023-05-29 17:53:04,807 ----------------------------------------------------------------------------------------------------
2023-05-29 17:53:27,022 epoch 7 - iter 46/469 - loss 6.46772626 - samples/sec: 66.30 - lr: 0.347159
2023-05-29 17:53:51,649 epoch 7 - iter 92/469 - loss 6.36456797 - samples/sec: 59.79 - lr: 0.338447
2023-05-29 17:54:14,578 epoch 7 - iter 138/469 - loss 6.30204828 - samples/sec: 64.22 - lr: 0.329735
2023-05-29 17:54:39,127 epoch 7 - iter 184/469 - loss 6.43864202 - samples/sec: 59.98 - lr: 0.321023
2023-05-29 17:55:01,066 epoch 7 - iter 230/469 - loss 6.37390435 - samples/sec: 67.12 - lr: 0.312311
2023-05-29 17:55:23,536 epoch 7 - iter 276/469 - loss 6.27605600 - samples/sec: 65.53 - lr: 0.303598
2023-05-29 17:55:48,079 epoch 7 - iter 322/469 - loss 6.17600383 - samples/sec: 59.99 - lr: 0.294886
2023-05-29 17:56:10,528 epoch 7 - iter 368/469 - loss 6.17236314 - samples/sec: 65.59 - lr: 0.286174
2023-05-29 17:56:33,088 epoch 7 - iter 414/469 - loss 6.06963064 - samples/sec: 65.27 - lr: 0.277462
2023-05-29 17:56:55,489 epoch 7 - iter 460/469 - loss 6.02101163 - samples/sec: 65.73 - lr: 0.268750
2023-05-29 17:56:59,931 ----------------------------------------------------------------------------------------------------
2023-05-29 17:56:59,932 EPOCH 7 done: loss 6.0314 - lr 0.268750
2023-05-29 17:58:14,039 Evaluating as a multi-label problem: False
2023-05-29 17:58:14,094 DEV : loss 1.791252613067627 - f1-score (micro avg)  0.6843
2023-05-29 17:58:14,156 BAD EPOCHS (no improvement): 4
2023-05-29 17:58:14,158 ----------------------------------------------------------------------------------------------------
2023-05-29 17:58:33,636 epoch 8 - iter 46/469 - loss 4.82624659 - samples/sec: 75.60 - lr: 0.258333
2023-05-29 17:58:51,334 epoch 8 - iter 92/469 - loss 5.04543378 - samples/sec: 83.20 - lr: 0.249621
2023-05-29 17:59:10,028 epoch 8 - iter 138/469 - loss 5.00551786 - samples/sec: 78.77 - lr: 0.240909
2023-05-29 17:59:34,453 epoch 8 - iter 184/469 - loss 5.00210391 - samples/sec: 60.28 - lr: 0.232197
2023-05-29 17:59:57,154 epoch 8 - iter 230/469 - loss 4.98576256 - samples/sec: 64.87 - lr: 0.223485
2023-05-29 18:00:20,326 epoch 8 - iter 276/469 - loss 4.99539444 - samples/sec: 63.54 - lr: 0.214773
2023-05-29 18:00:44,635 epoch 8 - iter 322/469 - loss 4.92772942 - samples/sec: 60.57 - lr: 0.206061
2023-05-29 18:01:06,841 epoch 8 - iter 368/469 - loss 4.91265575 - samples/sec: 66.32 - lr: 0.197348
2023-05-29 18:01:29,908 epoch 8 - iter 414/469 - loss 4.84650404 - samples/sec: 63.84 - lr: 0.188636
2023-05-29 18:01:54,731 epoch 8 - iter 460/469 - loss 4.75281179 - samples/sec: 59.31 - lr: 0.179924
2023-05-29 18:01:58,758 ----------------------------------------------------------------------------------------------------
2023-05-29 18:01:58,759 EPOCH 8 done: loss 4.7295 - lr 0.179924
2023-05-29 18:03:17,070 Evaluating as a multi-label problem: False
2023-05-29 18:03:17,136 DEV : loss 1.267967700958252 - f1-score (micro avg)  0.7577
2023-05-29 18:03:17,242 BAD EPOCHS (no improvement): 4
2023-05-29 18:03:17,245 ----------------------------------------------------------------------------------------------------
2023-05-29 18:03:42,161 epoch 9 - iter 46/469 - loss 3.51569837 - samples/sec: 59.10 - lr: 0.169508
2023-05-29 18:04:05,025 epoch 9 - iter 92/469 - loss 3.46467036 - samples/sec: 64.40 - lr: 0.160795
2023-05-29 18:04:28,530 epoch 9 - iter 138/469 - loss 3.55818100 - samples/sec: 62.65 - lr: 0.152083
2023-05-29 18:04:53,650 epoch 9 - iter 184/469 - loss 3.55196908 - samples/sec: 58.62 - lr: 0.143371
2023-05-29 18:05:15,602 epoch 9 - iter 230/469 - loss 3.51899160 - samples/sec: 67.08 - lr: 0.134659
2023-05-29 18:05:38,502 epoch 9 - iter 276/469 - loss 3.47208131 - samples/sec: 64.30 - lr: 0.125947
2023-05-29 18:06:02,904 epoch 9 - iter 322/469 - loss 3.43956755 - samples/sec: 60.34 - lr: 0.117235
2023-05-29 18:06:25,483 epoch 9 - iter 368/469 - loss 3.42688068 - samples/sec: 65.21 - lr: 0.108523
2023-05-29 18:06:50,884 epoch 9 - iter 414/469 - loss 3.38115549 - samples/sec: 57.97 - lr: 0.099811
2023-05-29 18:07:13,096 epoch 9 - iter 460/469 - loss 3.32958982 - samples/sec: 66.29 - lr: 0.091098
2023-05-29 18:07:16,281 ----------------------------------------------------------------------------------------------------
2023-05-29 18:07:16,281 EPOCH 9 done: loss 3.3198 - lr 0.091098
2023-05-29 18:08:26,127 Evaluating as a multi-label problem: False
2023-05-29 18:08:26,190 DEV : loss 0.9865373969078064 - f1-score (micro avg)  0.7487
2023-05-29 18:08:26,278 BAD EPOCHS (no improvement): 4
2023-05-29 18:08:26,280 ----------------------------------------------------------------------------------------------------
2023-05-29 18:08:50,685 epoch 10 - iter 46/469 - loss 2.59729599 - samples/sec: 60.34 - lr: 0.080682
2023-05-29 18:09:13,256 epoch 10 - iter 92/469 - loss 2.42364830 - samples/sec: 65.23 - lr: 0.071970
2023-05-29 18:09:35,795 epoch 10 - iter 138/469 - loss 2.39575178 - samples/sec: 65.33 - lr: 0.063258
2023-05-29 18:10:00,008 epoch 10 - iter 184/469 - loss 2.32305454 - samples/sec: 60.81 - lr: 0.054545
2023-05-29 18:10:22,827 epoch 10 - iter 230/469 - loss 2.28545406 - samples/sec: 64.52 - lr: 0.045833
2023-05-29 18:10:45,195 epoch 10 - iter 276/469 - loss 2.21589249 - samples/sec: 65.83 - lr: 0.037121
2023-05-29 18:11:10,425 epoch 10 - iter 322/469 - loss 2.15845759 - samples/sec: 58.36 - lr: 0.028409
2023-05-29 18:11:33,154 epoch 10 - iter 368/469 - loss 2.09296627 - samples/sec: 64.78 - lr: 0.019697
2023-05-29 18:11:58,340 epoch 10 - iter 414/469 - loss 2.02880084 - samples/sec: 58.46 - lr: 0.010985
2023-05-29 18:12:20,738 epoch 10 - iter 460/469 - loss 1.98299192 - samples/sec: 65.74 - lr: 0.002273
2023-05-29 18:12:24,770 ----------------------------------------------------------------------------------------------------
2023-05-29 18:12:24,771 EPOCH 10 done: loss 1.9779 - lr 0.002273
2023-05-29 18:13:42,856 Evaluating as a multi-label problem: False
2023-05-29 18:13:42,919 DEV : loss 0.5654338002204895 - f1-score (micro avg)  0.7913
2023-05-29 18:13:43,012 BAD EPOCHS (no improvement): 4
2023-05-29 18:13:57,340 ----------------------------------------------------------------------------------------------------
2023-05-29 18:13:57,344 Testing using last state of model ...
2023-05-29 18:15:18,316 Evaluating as a multi-label problem: False
2023-05-29 18:15:18,376 0.7763	0.7532	0.7646	0.6563
2023-05-29 18:15:18,376 
Results:
- F-score (micro) 0.7646
- F-score (macro) 0.7449
- Accuracy 0.6563

By class:
              precision    recall  f1-score   support

         LOC     0.7431    0.7890    0.7653      1668
         PER     0.8984    0.9190    0.9086      1617
         ORG     0.6958    0.6334    0.6631      1661
        MISC     0.7366    0.5698    0.6426       702

   micro avg     0.7763    0.7532    0.7646      5648
   macro avg     0.7685    0.7278    0.7449      5648
weighted avg     0.7728    0.7532    0.7610      5648

2023-05-29 18:15:18,376 ----------------------------------------------------------------------------------------------------
