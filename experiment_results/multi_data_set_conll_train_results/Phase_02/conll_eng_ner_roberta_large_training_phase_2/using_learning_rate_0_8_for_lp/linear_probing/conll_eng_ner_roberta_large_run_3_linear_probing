2023-05-30 00:30:17,230 ----------------------------------------------------------------------------------------------------
2023-05-30 00:30:17,235 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-30 00:30:17,238 ----------------------------------------------------------------------------------------------------
2023-05-30 00:30:17,239 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-05-30 00:30:17,239 ----------------------------------------------------------------------------------------------------
2023-05-30 00:30:17,239 Parameters:
2023-05-30 00:30:17,239  - learning_rate: "0.800000"
2023-05-30 00:30:17,239  - mini_batch_size: "32"
2023-05-30 00:30:17,239  - patience: "3"
2023-05-30 00:30:17,239  - anneal_factor: "0.5"
2023-05-30 00:30:17,239  - max_epochs: "10"
2023-05-30 00:30:17,239  - shuffle: "True"
2023-05-30 00:30:17,239  - train_with_dev: "False"
2023-05-30 00:30:17,239  - batch_growth_annealing: "False"
2023-05-30 00:30:17,239 ----------------------------------------------------------------------------------------------------
2023-05-30 00:30:17,239 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_3_linear_probing"
2023-05-30 00:30:17,239 ----------------------------------------------------------------------------------------------------
2023-05-30 00:30:17,240 Device: cuda:3
2023-05-30 00:30:17,240 ----------------------------------------------------------------------------------------------------
2023-05-30 00:30:17,240 Embeddings storage mode: none
2023-05-30 00:30:17,240 ----------------------------------------------------------------------------------------------------
2023-05-30 00:30:42,154 epoch 1 - iter 46/469 - loss 1.38285454 - samples/sec: 59.11 - lr: 0.078465
2023-05-30 00:31:04,360 epoch 1 - iter 92/469 - loss 1.57590387 - samples/sec: 66.31 - lr: 0.156930
2023-05-30 00:31:25,526 epoch 1 - iter 138/469 - loss 2.10508243 - samples/sec: 69.57 - lr: 0.235394
2023-05-30 00:31:49,427 epoch 1 - iter 184/469 - loss 3.02394199 - samples/sec: 61.61 - lr: 0.313859
2023-05-30 00:32:09,358 epoch 1 - iter 230/469 - loss 3.69441693 - samples/sec: 73.88 - lr: 0.392324
2023-05-30 00:32:31,580 epoch 1 - iter 276/469 - loss 4.39881405 - samples/sec: 66.26 - lr: 0.470789
2023-05-30 00:32:57,963 epoch 1 - iter 322/469 - loss 5.30582012 - samples/sec: 55.81 - lr: 0.549254
2023-05-30 00:33:20,411 epoch 1 - iter 368/469 - loss 6.30793592 - samples/sec: 65.60 - lr: 0.627719
2023-05-30 00:33:38,603 epoch 1 - iter 414/469 - loss 6.86637195 - samples/sec: 80.94 - lr: 0.706183
2023-05-30 00:34:03,518 epoch 1 - iter 460/469 - loss 7.35020115 - samples/sec: 59.10 - lr: 0.784648
2023-05-30 00:34:07,603 ----------------------------------------------------------------------------------------------------
2023-05-30 00:34:07,603 EPOCH 1 done: loss 7.4569 - lr 0.784648
2023-05-30 00:35:19,924 Evaluating as a multi-label problem: False
2023-05-30 00:35:19,995 DEV : loss 4.711602210998535 - f1-score (micro avg)  0.6389
2023-05-30 00:35:20,127 BAD EPOCHS (no improvement): 4
2023-05-30 00:35:20,131 ----------------------------------------------------------------------------------------------------
2023-05-30 00:35:43,451 epoch 2 - iter 46/469 - loss 12.43042201 - samples/sec: 63.15 - lr: 0.791288
2023-05-30 00:36:05,030 epoch 2 - iter 92/469 - loss 12.66953613 - samples/sec: 68.24 - lr: 0.782576
2023-05-30 00:36:23,854 epoch 2 - iter 138/469 - loss 12.39072050 - samples/sec: 78.22 - lr: 0.773864
2023-05-30 00:36:41,203 epoch 2 - iter 184/469 - loss 12.23103031 - samples/sec: 84.87 - lr: 0.765152
2023-05-30 00:37:01,442 epoch 2 - iter 230/469 - loss 12.13693706 - samples/sec: 72.75 - lr: 0.756439
2023-05-30 00:37:19,071 epoch 2 - iter 276/469 - loss 12.10432698 - samples/sec: 83.53 - lr: 0.747727
2023-05-30 00:37:37,746 epoch 2 - iter 322/469 - loss 12.26269186 - samples/sec: 78.85 - lr: 0.739015
2023-05-30 00:38:02,446 epoch 2 - iter 368/469 - loss 12.34184659 - samples/sec: 59.62 - lr: 0.730303
2023-05-30 00:38:25,687 epoch 2 - iter 414/469 - loss 12.38352039 - samples/sec: 63.36 - lr: 0.721591
2023-05-30 00:38:49,037 epoch 2 - iter 460/469 - loss 12.27608864 - samples/sec: 63.07 - lr: 0.712879
2023-05-30 00:38:53,320 ----------------------------------------------------------------------------------------------------
2023-05-30 00:38:53,320 EPOCH 2 done: loss 12.2849 - lr 0.712879
2023-05-30 00:40:05,427 Evaluating as a multi-label problem: False
2023-05-30 00:40:05,502 DEV : loss 5.760930061340332 - f1-score (micro avg)  0.6697
2023-05-30 00:40:05,652 BAD EPOCHS (no improvement): 4
2023-05-30 00:40:05,655 ----------------------------------------------------------------------------------------------------
2023-05-30 00:40:29,334 epoch 3 - iter 46/469 - loss 12.81639197 - samples/sec: 62.20 - lr: 0.702462
2023-05-30 00:40:46,815 epoch 3 - iter 92/469 - loss 12.28585184 - samples/sec: 84.23 - lr: 0.693750
2023-05-30 00:41:04,870 epoch 3 - iter 138/469 - loss 11.85942008 - samples/sec: 81.55 - lr: 0.685038
2023-05-30 00:41:25,114 epoch 3 - iter 184/469 - loss 11.56169909 - samples/sec: 72.73 - lr: 0.676326
2023-05-30 00:41:47,347 epoch 3 - iter 230/469 - loss 11.58015559 - samples/sec: 66.23 - lr: 0.667614
2023-05-30 00:42:10,429 epoch 3 - iter 276/469 - loss 11.38198311 - samples/sec: 63.80 - lr: 0.658902
2023-05-30 00:42:31,922 epoch 3 - iter 322/469 - loss 11.34250228 - samples/sec: 68.51 - lr: 0.650189
2023-05-30 00:42:49,689 epoch 3 - iter 368/469 - loss 11.35744435 - samples/sec: 82.88 - lr: 0.641477
2023-05-30 00:43:09,430 epoch 3 - iter 414/469 - loss 11.44957197 - samples/sec: 74.59 - lr: 0.632765
2023-05-30 00:43:30,253 epoch 3 - iter 460/469 - loss 11.43163369 - samples/sec: 70.71 - lr: 0.624053
2023-05-30 00:43:34,281 ----------------------------------------------------------------------------------------------------
2023-05-30 00:43:34,281 EPOCH 3 done: loss 11.4286 - lr 0.624053
2023-05-30 00:44:57,746 Evaluating as a multi-label problem: False
2023-05-30 00:44:57,817 DEV : loss 4.377532005310059 - f1-score (micro avg)  0.6704
2023-05-30 00:44:57,964 BAD EPOCHS (no improvement): 4
2023-05-30 00:44:57,967 ----------------------------------------------------------------------------------------------------
2023-05-30 00:45:20,851 epoch 4 - iter 46/469 - loss 10.93395693 - samples/sec: 64.36 - lr: 0.613636
2023-05-30 00:45:43,905 epoch 4 - iter 92/469 - loss 10.52502310 - samples/sec: 63.87 - lr: 0.604924
2023-05-30 00:46:10,323 epoch 4 - iter 138/469 - loss 10.24308934 - samples/sec: 55.74 - lr: 0.596212
2023-05-30 00:46:34,294 epoch 4 - iter 184/469 - loss 10.20473194 - samples/sec: 61.43 - lr: 0.587500
2023-05-30 00:46:56,441 epoch 4 - iter 230/469 - loss 10.11507339 - samples/sec: 66.49 - lr: 0.578788
2023-05-30 00:47:23,142 epoch 4 - iter 276/469 - loss 10.02656403 - samples/sec: 55.15 - lr: 0.570076
2023-05-30 00:47:46,259 epoch 4 - iter 322/469 - loss 9.94345817 - samples/sec: 63.70 - lr: 0.561364
2023-05-30 00:48:10,040 epoch 4 - iter 368/469 - loss 10.10508486 - samples/sec: 61.92 - lr: 0.552652
2023-05-30 00:48:36,858 epoch 4 - iter 414/469 - loss 9.98123696 - samples/sec: 54.91 - lr: 0.543939
2023-05-30 00:48:59,960 epoch 4 - iter 460/469 - loss 9.82103096 - samples/sec: 63.74 - lr: 0.535227
2023-05-30 00:49:03,966 ----------------------------------------------------------------------------------------------------
2023-05-30 00:49:03,966 EPOCH 4 done: loss 9.7952 - lr 0.535227
2023-05-30 00:50:26,353 Evaluating as a multi-label problem: False
2023-05-30 00:50:26,396 DEV : loss 2.7245092391967773 - f1-score (micro avg)  0.682
2023-05-30 00:50:26,496 BAD EPOCHS (no improvement): 4
2023-05-30 00:50:26,498 ----------------------------------------------------------------------------------------------------
2023-05-30 00:50:49,545 epoch 5 - iter 46/469 - loss 8.19494754 - samples/sec: 63.90 - lr: 0.524811
2023-05-30 00:51:13,527 epoch 5 - iter 92/469 - loss 8.57845807 - samples/sec: 61.41 - lr: 0.516098
2023-05-30 00:51:36,411 epoch 5 - iter 138/469 - loss 8.60471471 - samples/sec: 64.35 - lr: 0.507386
2023-05-30 00:52:02,607 epoch 5 - iter 184/469 - loss 8.56671203 - samples/sec: 56.21 - lr: 0.498674
2023-05-30 00:52:26,447 epoch 5 - iter 230/469 - loss 8.57236660 - samples/sec: 61.77 - lr: 0.489962
2023-05-30 00:52:49,238 epoch 5 - iter 276/469 - loss 8.61949893 - samples/sec: 64.61 - lr: 0.481250
2023-05-30 00:53:15,353 epoch 5 - iter 322/469 - loss 8.63037479 - samples/sec: 56.38 - lr: 0.472538
2023-05-30 00:53:38,356 epoch 5 - iter 368/469 - loss 8.56899626 - samples/sec: 64.02 - lr: 0.463826
2023-05-30 00:54:00,782 epoch 5 - iter 414/469 - loss 8.48338190 - samples/sec: 65.66 - lr: 0.455114
2023-05-30 00:54:26,840 epoch 5 - iter 460/469 - loss 8.41333623 - samples/sec: 56.51 - lr: 0.446402
2023-05-30 00:54:31,274 ----------------------------------------------------------------------------------------------------
2023-05-30 00:54:31,274 EPOCH 5 done: loss 8.3963 - lr 0.446402
2023-05-30 00:55:53,791 Evaluating as a multi-label problem: False
2023-05-30 00:55:53,863 DEV : loss 2.6113193035125732 - f1-score (micro avg)  0.7204
2023-05-30 00:55:54,002 BAD EPOCHS (no improvement): 4
2023-05-30 00:55:54,005 ----------------------------------------------------------------------------------------------------
2023-05-30 00:56:16,668 epoch 6 - iter 46/469 - loss 8.32896272 - samples/sec: 64.99 - lr: 0.435985
2023-05-30 00:56:37,899 epoch 6 - iter 92/469 - loss 7.95505253 - samples/sec: 69.36 - lr: 0.427273
2023-05-30 00:57:00,260 epoch 6 - iter 138/469 - loss 7.79852792 - samples/sec: 65.85 - lr: 0.418561
2023-05-30 00:57:23,204 epoch 6 - iter 184/469 - loss 7.71206175 - samples/sec: 64.18 - lr: 0.409848
2023-05-30 00:57:46,764 epoch 6 - iter 230/469 - loss 7.66839643 - samples/sec: 62.50 - lr: 0.401136
2023-05-30 00:58:13,504 epoch 6 - iter 276/469 - loss 7.54074630 - samples/sec: 55.07 - lr: 0.392424
2023-05-30 00:58:35,814 epoch 6 - iter 322/469 - loss 7.61867821 - samples/sec: 66.01 - lr: 0.383712
2023-05-30 00:58:55,412 epoch 6 - iter 368/469 - loss 7.58149046 - samples/sec: 75.14 - lr: 0.375000
2023-05-30 00:59:19,622 epoch 6 - iter 414/469 - loss 7.48035665 - samples/sec: 60.82 - lr: 0.366288
2023-05-30 00:59:43,160 epoch 6 - iter 460/469 - loss 7.42715695 - samples/sec: 62.56 - lr: 0.357576
2023-05-30 00:59:47,406 ----------------------------------------------------------------------------------------------------
2023-05-30 00:59:47,406 EPOCH 6 done: loss 7.4170 - lr 0.357576
2023-05-30 01:01:04,028 Evaluating as a multi-label problem: False
2023-05-30 01:01:04,072 DEV : loss 2.4713826179504395 - f1-score (micro avg)  0.6936
2023-05-30 01:01:04,194 BAD EPOCHS (no improvement): 4
2023-05-30 01:01:04,283 ----------------------------------------------------------------------------------------------------
2023-05-30 01:01:27,178 epoch 7 - iter 46/469 - loss 6.23229509 - samples/sec: 64.36 - lr: 0.347159
2023-05-30 01:01:49,254 epoch 7 - iter 92/469 - loss 6.67772229 - samples/sec: 66.70 - lr: 0.338447
2023-05-30 01:02:07,743 epoch 7 - iter 138/469 - loss 6.49042388 - samples/sec: 79.64 - lr: 0.329735
2023-05-30 01:02:26,293 epoch 7 - iter 184/469 - loss 6.46934434 - samples/sec: 79.38 - lr: 0.321023
2023-05-30 01:02:49,055 epoch 7 - iter 230/469 - loss 6.37277661 - samples/sec: 64.69 - lr: 0.312311
2023-05-30 01:03:10,702 epoch 7 - iter 276/469 - loss 6.37854142 - samples/sec: 68.03 - lr: 0.303598
2023-05-30 01:03:33,247 epoch 7 - iter 322/469 - loss 6.32609278 - samples/sec: 65.32 - lr: 0.294886
2023-05-30 01:03:57,983 epoch 7 - iter 368/469 - loss 6.23579727 - samples/sec: 59.53 - lr: 0.286174
2023-05-30 01:04:16,163 epoch 7 - iter 414/469 - loss 6.18350287 - samples/sec: 81.00 - lr: 0.277462
2023-05-30 01:04:35,044 epoch 7 - iter 460/469 - loss 6.05715502 - samples/sec: 77.99 - lr: 0.268750
2023-05-30 01:04:39,328 ----------------------------------------------------------------------------------------------------
2023-05-30 01:04:39,329 EPOCH 7 done: loss 6.0437 - lr 0.268750
2023-05-30 01:05:57,811 Evaluating as a multi-label problem: False
2023-05-30 01:05:57,883 DEV : loss 2.0093932151794434 - f1-score (micro avg)  0.7035
2023-05-30 01:05:58,028 BAD EPOCHS (no improvement): 4
2023-05-30 01:05:58,031 ----------------------------------------------------------------------------------------------------
2023-05-30 01:06:21,101 epoch 8 - iter 46/469 - loss 5.47338320 - samples/sec: 63.84 - lr: 0.258333
2023-05-30 01:06:44,627 epoch 8 - iter 92/469 - loss 5.45011621 - samples/sec: 62.59 - lr: 0.249621
2023-05-30 01:07:11,381 epoch 8 - iter 138/469 - loss 5.30005721 - samples/sec: 55.04 - lr: 0.240909
2023-05-30 01:07:35,366 epoch 8 - iter 184/469 - loss 5.20028404 - samples/sec: 61.40 - lr: 0.232197
2023-05-30 01:07:58,216 epoch 8 - iter 230/469 - loss 5.12231829 - samples/sec: 64.45 - lr: 0.223485
2023-05-30 01:08:23,563 epoch 8 - iter 276/469 - loss 5.03061054 - samples/sec: 58.09 - lr: 0.214773
2023-05-30 01:08:44,979 epoch 8 - iter 322/469 - loss 4.94959766 - samples/sec: 68.76 - lr: 0.206061
2023-05-30 01:09:03,628 epoch 8 - iter 368/469 - loss 4.90357171 - samples/sec: 78.96 - lr: 0.197348
2023-05-30 01:09:30,003 epoch 8 - iter 414/469 - loss 4.85102406 - samples/sec: 55.83 - lr: 0.188636
2023-05-30 01:09:53,924 epoch 8 - iter 460/469 - loss 4.82566548 - samples/sec: 61.56 - lr: 0.179924
2023-05-30 01:09:57,752 ----------------------------------------------------------------------------------------------------
2023-05-30 01:09:57,752 EPOCH 8 done: loss 4.8172 - lr 0.179924
2023-05-30 01:11:14,879 Evaluating as a multi-label problem: False
2023-05-30 01:11:14,947 DEV : loss 1.6073676347732544 - f1-score (micro avg)  0.7319
2023-05-30 01:11:15,089 BAD EPOCHS (no improvement): 4
2023-05-30 01:11:15,093 ----------------------------------------------------------------------------------------------------
2023-05-30 01:11:35,525 epoch 9 - iter 46/469 - loss 4.16545895 - samples/sec: 72.08 - lr: 0.169508
2023-05-30 01:12:00,299 epoch 9 - iter 92/469 - loss 4.10720172 - samples/sec: 59.44 - lr: 0.160795
2023-05-30 01:12:24,169 epoch 9 - iter 138/469 - loss 4.06848849 - samples/sec: 61.69 - lr: 0.152083
2023-05-30 01:12:46,619 epoch 9 - iter 184/469 - loss 3.95854187 - samples/sec: 65.59 - lr: 0.143371
2023-05-30 01:13:11,655 epoch 9 - iter 230/469 - loss 3.87396011 - samples/sec: 58.81 - lr: 0.134659
2023-05-30 01:13:35,467 epoch 9 - iter 276/469 - loss 3.72758853 - samples/sec: 61.84 - lr: 0.125947
2023-05-30 01:13:59,032 epoch 9 - iter 322/469 - loss 3.61003859 - samples/sec: 62.49 - lr: 0.117235
2023-05-30 01:14:25,480 epoch 9 - iter 368/469 - loss 3.53787483 - samples/sec: 55.67 - lr: 0.108523
2023-05-30 01:14:45,476 epoch 9 - iter 414/469 - loss 3.48381808 - samples/sec: 73.64 - lr: 0.099811
2023-05-30 01:15:06,991 epoch 9 - iter 460/469 - loss 3.38036136 - samples/sec: 68.44 - lr: 0.091098
2023-05-30 01:15:15,564 ----------------------------------------------------------------------------------------------------
2023-05-30 01:15:15,564 EPOCH 9 done: loss 3.3615 - lr 0.091098
2023-05-30 01:16:33,874 Evaluating as a multi-label problem: False
2023-05-30 01:16:33,942 DEV : loss 0.8395700454711914 - f1-score (micro avg)  0.7708
2023-05-30 01:16:34,059 BAD EPOCHS (no improvement): 4
2023-05-30 01:16:34,062 ----------------------------------------------------------------------------------------------------
2023-05-30 01:16:57,857 epoch 10 - iter 46/469 - loss 2.47481010 - samples/sec: 61.89 - lr: 0.080682
2023-05-30 01:17:21,033 epoch 10 - iter 92/469 - loss 2.39314349 - samples/sec: 63.54 - lr: 0.071970
2023-05-30 01:17:45,078 epoch 10 - iter 138/469 - loss 2.35761790 - samples/sec: 61.24 - lr: 0.063258
2023-05-30 01:18:03,234 epoch 10 - iter 184/469 - loss 2.30163065 - samples/sec: 81.10 - lr: 0.054545
2023-05-30 01:18:21,396 epoch 10 - iter 230/469 - loss 2.24392734 - samples/sec: 81.08 - lr: 0.045833
2023-05-30 01:18:43,018 epoch 10 - iter 276/469 - loss 2.19890738 - samples/sec: 68.10 - lr: 0.037121
2023-05-30 01:19:06,981 epoch 10 - iter 322/469 - loss 2.10580991 - samples/sec: 61.45 - lr: 0.028409
2023-05-30 01:19:31,237 epoch 10 - iter 368/469 - loss 2.05881153 - samples/sec: 60.71 - lr: 0.019697
2023-05-30 01:19:57,893 epoch 10 - iter 414/469 - loss 1.99387929 - samples/sec: 55.24 - lr: 0.010985
2023-05-30 01:20:21,443 epoch 10 - iter 460/469 - loss 1.94779850 - samples/sec: 62.53 - lr: 0.002273
2023-05-30 01:20:25,500 ----------------------------------------------------------------------------------------------------
2023-05-30 01:20:25,500 EPOCH 10 done: loss 1.9435 - lr 0.002273
2023-05-30 01:21:36,382 Evaluating as a multi-label problem: False
2023-05-30 01:21:36,462 DEV : loss 0.5206480622291565 - f1-score (micro avg)  0.805
2023-05-30 01:21:36,626 BAD EPOCHS (no improvement): 4
2023-05-30 01:21:48,568 ----------------------------------------------------------------------------------------------------
2023-05-30 01:21:48,572 Testing using last state of model ...
2023-05-30 01:22:52,456 Evaluating as a multi-label problem: False
2023-05-30 01:22:52,497 0.7874	0.7659	0.7765	0.6705
2023-05-30 01:22:52,497 
Results:
- F-score (micro) 0.7765
- F-score (macro) 0.7596
- Accuracy 0.6705

By class:
              precision    recall  f1-score   support

         LOC     0.7735    0.7884    0.7809      1668
         PER     0.9254    0.9357    0.9305      1617
         ORG     0.6875    0.6370    0.6613      1661
        MISC     0.7097    0.6268    0.6657       702

   micro avg     0.7874    0.7659    0.7765      5648
   macro avg     0.7740    0.7469    0.7596      5648
weighted avg     0.7838    0.7659    0.7742      5648

2023-05-30 01:22:52,497 ----------------------------------------------------------------------------------------------------
