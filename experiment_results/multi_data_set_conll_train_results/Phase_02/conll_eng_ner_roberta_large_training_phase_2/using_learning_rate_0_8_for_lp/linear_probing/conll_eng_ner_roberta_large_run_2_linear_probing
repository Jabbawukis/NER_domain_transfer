2023-05-29 21:03:10,485 ----------------------------------------------------------------------------------------------------
2023-05-29 21:03:10,490 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-29 21:03:10,492 ----------------------------------------------------------------------------------------------------
2023-05-29 21:03:10,492 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-05-29 21:03:10,492 ----------------------------------------------------------------------------------------------------
2023-05-29 21:03:10,492 Parameters:
2023-05-29 21:03:10,492  - learning_rate: "0.800000"
2023-05-29 21:03:10,492  - mini_batch_size: "32"
2023-05-29 21:03:10,492  - patience: "3"
2023-05-29 21:03:10,492  - anneal_factor: "0.5"
2023-05-29 21:03:10,492  - max_epochs: "10"
2023-05-29 21:03:10,493  - shuffle: "True"
2023-05-29 21:03:10,493  - train_with_dev: "False"
2023-05-29 21:03:10,493  - batch_growth_annealing: "False"
2023-05-29 21:03:10,493 ----------------------------------------------------------------------------------------------------
2023-05-29 21:03:10,493 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_2_linear_probing"
2023-05-29 21:03:10,493 ----------------------------------------------------------------------------------------------------
2023-05-29 21:03:10,493 Device: cuda:3
2023-05-29 21:03:10,493 ----------------------------------------------------------------------------------------------------
2023-05-29 21:03:10,493 Embeddings storage mode: none
2023-05-29 21:03:10,493 ----------------------------------------------------------------------------------------------------
2023-05-29 21:03:28,629 epoch 1 - iter 46/469 - loss 1.31582969 - samples/sec: 81.20 - lr: 0.078465
2023-05-29 21:03:54,173 epoch 1 - iter 92/469 - loss 1.59783016 - samples/sec: 57.65 - lr: 0.156930
2023-05-29 21:04:15,748 epoch 1 - iter 138/469 - loss 2.19134193 - samples/sec: 68.26 - lr: 0.235394
2023-05-29 21:04:34,772 epoch 1 - iter 184/469 - loss 3.10628475 - samples/sec: 77.41 - lr: 0.313859
2023-05-29 21:04:56,417 epoch 1 - iter 230/469 - loss 3.91346836 - samples/sec: 68.03 - lr: 0.392324
2023-05-29 21:05:18,817 epoch 1 - iter 276/469 - loss 4.77323836 - samples/sec: 65.74 - lr: 0.470789
2023-05-29 21:05:41,367 epoch 1 - iter 322/469 - loss 5.47263908 - samples/sec: 65.30 - lr: 0.549254
2023-05-29 21:06:06,649 epoch 1 - iter 368/469 - loss 6.28540797 - samples/sec: 58.24 - lr: 0.627719
2023-05-29 21:06:29,103 epoch 1 - iter 414/469 - loss 6.90002814 - samples/sec: 65.57 - lr: 0.706183
2023-05-29 21:06:51,349 epoch 1 - iter 460/469 - loss 7.35976797 - samples/sec: 66.19 - lr: 0.784648
2023-05-29 21:06:57,956 ----------------------------------------------------------------------------------------------------
2023-05-29 21:06:57,956 EPOCH 1 done: loss 7.4973 - lr 0.784648
2023-05-29 21:08:17,339 Evaluating as a multi-label problem: False
2023-05-29 21:08:17,412 DEV : loss 5.7524094581604 - f1-score (micro avg)  0.6029
2023-05-29 21:08:17,512 BAD EPOCHS (no improvement): 4
2023-05-29 21:08:17,515 ----------------------------------------------------------------------------------------------------
2023-05-29 21:08:40,395 epoch 2 - iter 46/469 - loss 14.63118991 - samples/sec: 64.37 - lr: 0.791288
2023-05-29 21:09:03,219 epoch 2 - iter 92/469 - loss 15.02588173 - samples/sec: 64.52 - lr: 0.782576
2023-05-29 21:09:28,594 epoch 2 - iter 138/469 - loss 14.26448629 - samples/sec: 58.03 - lr: 0.773864
2023-05-29 21:09:51,541 epoch 2 - iter 184/469 - loss 13.65840768 - samples/sec: 64.17 - lr: 0.765152
2023-05-29 21:10:14,300 epoch 2 - iter 230/469 - loss 13.54000694 - samples/sec: 64.70 - lr: 0.756439
2023-05-29 21:10:36,406 epoch 2 - iter 276/469 - loss 13.25579714 - samples/sec: 66.61 - lr: 0.747727
2023-05-29 21:10:54,171 epoch 2 - iter 322/469 - loss 13.09920573 - samples/sec: 82.88 - lr: 0.739015
2023-05-29 21:11:16,681 epoch 2 - iter 368/469 - loss 12.87501944 - samples/sec: 65.42 - lr: 0.730303
2023-05-29 21:11:43,049 epoch 2 - iter 414/469 - loss 12.79184205 - samples/sec: 55.84 - lr: 0.721591
2023-05-29 21:12:05,734 epoch 2 - iter 460/469 - loss 12.61856449 - samples/sec: 64.91 - lr: 0.712879
2023-05-29 21:12:09,816 ----------------------------------------------------------------------------------------------------
2023-05-29 21:12:09,816 EPOCH 2 done: loss 12.5935 - lr 0.712879
2023-05-29 21:13:31,348 Evaluating as a multi-label problem: False
2023-05-29 21:13:31,416 DEV : loss 3.9173848628997803 - f1-score (micro avg)  0.6826
2023-05-29 21:13:31,515 BAD EPOCHS (no improvement): 4
2023-05-29 21:13:31,518 ----------------------------------------------------------------------------------------------------
2023-05-29 21:13:54,282 epoch 3 - iter 46/469 - loss 12.47773266 - samples/sec: 64.69 - lr: 0.702462
2023-05-29 21:14:20,921 epoch 3 - iter 92/469 - loss 11.93380234 - samples/sec: 55.28 - lr: 0.693750
2023-05-29 21:14:44,987 epoch 3 - iter 138/469 - loss 11.53222067 - samples/sec: 61.19 - lr: 0.685038
2023-05-29 21:15:08,656 epoch 3 - iter 184/469 - loss 11.69261035 - samples/sec: 62.22 - lr: 0.676326
2023-05-29 21:15:35,749 epoch 3 - iter 230/469 - loss 11.90239487 - samples/sec: 54.35 - lr: 0.667614
2023-05-29 21:15:58,192 epoch 3 - iter 276/469 - loss 11.58503578 - samples/sec: 65.61 - lr: 0.658902
2023-05-29 21:16:19,813 epoch 3 - iter 322/469 - loss 11.40483444 - samples/sec: 68.11 - lr: 0.650189
2023-05-29 21:16:46,073 epoch 3 - iter 368/469 - loss 11.15930934 - samples/sec: 56.07 - lr: 0.641477
2023-05-29 21:17:06,787 epoch 3 - iter 414/469 - loss 11.05760903 - samples/sec: 71.09 - lr: 0.632765
2023-05-29 21:17:28,963 epoch 3 - iter 460/469 - loss 10.99811745 - samples/sec: 66.40 - lr: 0.624053
2023-05-29 21:17:32,850 ----------------------------------------------------------------------------------------------------
2023-05-29 21:17:32,850 EPOCH 3 done: loss 10.9658 - lr 0.624053
2023-05-29 21:18:46,380 Evaluating as a multi-label problem: False
2023-05-29 21:18:46,449 DEV : loss 4.277531147003174 - f1-score (micro avg)  0.7143
2023-05-29 21:18:46,582 BAD EPOCHS (no improvement): 4
2023-05-29 21:18:46,585 ----------------------------------------------------------------------------------------------------
2023-05-29 21:19:09,860 epoch 4 - iter 46/469 - loss 10.87038710 - samples/sec: 63.27 - lr: 0.613636
2023-05-29 21:19:29,032 epoch 4 - iter 92/469 - loss 10.67133236 - samples/sec: 76.81 - lr: 0.604924
2023-05-29 21:19:52,085 epoch 4 - iter 138/469 - loss 10.32382962 - samples/sec: 63.87 - lr: 0.596212
2023-05-29 21:20:13,861 epoch 4 - iter 184/469 - loss 10.15354436 - samples/sec: 67.62 - lr: 0.587500
2023-05-29 21:20:36,543 epoch 4 - iter 230/469 - loss 10.06729542 - samples/sec: 64.93 - lr: 0.578788
2023-05-29 21:21:02,913 epoch 4 - iter 276/469 - loss 10.00716653 - samples/sec: 55.84 - lr: 0.570076
2023-05-29 21:21:23,996 epoch 4 - iter 322/469 - loss 9.88473335 - samples/sec: 69.85 - lr: 0.561364
2023-05-29 21:21:42,487 epoch 4 - iter 368/469 - loss 9.77524560 - samples/sec: 79.64 - lr: 0.552652
2023-05-29 21:22:04,349 epoch 4 - iter 414/469 - loss 9.67160309 - samples/sec: 67.36 - lr: 0.543939
2023-05-29 21:22:29,033 epoch 4 - iter 460/469 - loss 9.63091489 - samples/sec: 59.65 - lr: 0.535227
2023-05-29 21:22:33,253 ----------------------------------------------------------------------------------------------------
2023-05-29 21:22:33,253 EPOCH 4 done: loss 9.6150 - lr 0.535227
2023-05-29 21:23:53,453 Evaluating as a multi-label problem: False
2023-05-29 21:23:53,520 DEV : loss 2.6252217292785645 - f1-score (micro avg)  0.7089
2023-05-29 21:23:53,657 BAD EPOCHS (no improvement): 4
2023-05-29 21:23:53,660 ----------------------------------------------------------------------------------------------------
2023-05-29 21:24:16,731 epoch 5 - iter 46/469 - loss 9.38492485 - samples/sec: 63.84 - lr: 0.524811
2023-05-29 21:24:42,524 epoch 5 - iter 92/469 - loss 8.97174239 - samples/sec: 57.09 - lr: 0.516098
2023-05-29 21:25:05,956 epoch 5 - iter 138/469 - loss 8.96116807 - samples/sec: 62.84 - lr: 0.507386
2023-05-29 21:25:29,122 epoch 5 - iter 184/469 - loss 8.87870485 - samples/sec: 63.57 - lr: 0.498674
2023-05-29 21:25:54,644 epoch 5 - iter 230/469 - loss 8.92182274 - samples/sec: 57.69 - lr: 0.489962
2023-05-29 21:26:18,344 epoch 5 - iter 276/469 - loss 9.01826887 - samples/sec: 62.13 - lr: 0.481250
2023-05-29 21:26:37,389 epoch 5 - iter 322/469 - loss 8.99564254 - samples/sec: 77.32 - lr: 0.472538
2023-05-29 21:27:02,855 epoch 5 - iter 368/469 - loss 8.90031637 - samples/sec: 57.82 - lr: 0.463826
2023-05-29 21:27:24,144 epoch 5 - iter 414/469 - loss 8.74993417 - samples/sec: 69.17 - lr: 0.455114
2023-05-29 21:27:42,600 epoch 5 - iter 460/469 - loss 8.65270319 - samples/sec: 79.79 - lr: 0.446402
2023-05-29 21:27:45,841 ----------------------------------------------------------------------------------------------------
2023-05-29 21:27:45,842 EPOCH 5 done: loss 8.6445 - lr 0.446402
2023-05-29 21:28:47,653 Evaluating as a multi-label problem: False
2023-05-29 21:28:47,726 DEV : loss 2.346428155899048 - f1-score (micro avg)  0.7084
2023-05-29 21:28:47,859 BAD EPOCHS (no improvement): 4
2023-05-29 21:28:47,862 ----------------------------------------------------------------------------------------------------
2023-05-29 21:29:13,360 epoch 6 - iter 46/469 - loss 7.24137243 - samples/sec: 57.76 - lr: 0.435985
2023-05-29 21:29:31,183 epoch 6 - iter 92/469 - loss 7.24260561 - samples/sec: 82.62 - lr: 0.427273
2023-05-29 21:29:53,280 epoch 6 - iter 138/469 - loss 7.17540610 - samples/sec: 66.64 - lr: 0.418561
2023-05-29 21:30:19,358 epoch 6 - iter 184/469 - loss 7.22251421 - samples/sec: 56.46 - lr: 0.409848
2023-05-29 21:30:40,380 epoch 6 - iter 230/469 - loss 7.08537852 - samples/sec: 70.05 - lr: 0.401136
2023-05-29 21:30:58,194 epoch 6 - iter 276/469 - loss 7.20323083 - samples/sec: 82.66 - lr: 0.392424
2023-05-29 21:31:21,073 epoch 6 - iter 322/469 - loss 7.31402391 - samples/sec: 64.36 - lr: 0.383712
2023-05-29 21:31:42,509 epoch 6 - iter 368/469 - loss 7.26352073 - samples/sec: 68.70 - lr: 0.375000
2023-05-29 21:32:05,155 epoch 6 - iter 414/469 - loss 7.21865822 - samples/sec: 65.03 - lr: 0.366288
2023-05-29 21:32:30,033 epoch 6 - iter 460/469 - loss 7.17321347 - samples/sec: 59.19 - lr: 0.357576
2023-05-29 21:32:33,362 ----------------------------------------------------------------------------------------------------
2023-05-29 21:32:33,362 EPOCH 6 done: loss 7.1401 - lr 0.357576
2023-05-29 21:33:51,278 Evaluating as a multi-label problem: False
2023-05-29 21:33:51,351 DEV : loss 2.428938627243042 - f1-score (micro avg)  0.6981
2023-05-29 21:33:51,481 BAD EPOCHS (no improvement): 4
2023-05-29 21:33:51,484 ----------------------------------------------------------------------------------------------------
2023-05-29 21:34:14,338 epoch 7 - iter 46/469 - loss 7.32406764 - samples/sec: 64.44 - lr: 0.347159
2023-05-29 21:34:39,680 epoch 7 - iter 92/469 - loss 6.96105852 - samples/sec: 58.10 - lr: 0.338447
2023-05-29 21:35:03,022 epoch 7 - iter 138/469 - loss 6.79469112 - samples/sec: 63.09 - lr: 0.329735
2023-05-29 21:35:21,892 epoch 7 - iter 184/469 - loss 6.65007737 - samples/sec: 78.03 - lr: 0.321023
2023-05-29 21:35:47,772 epoch 7 - iter 230/469 - loss 6.57275224 - samples/sec: 56.90 - lr: 0.312311
2023-05-29 21:36:11,354 epoch 7 - iter 276/469 - loss 6.38609048 - samples/sec: 62.45 - lr: 0.303598
2023-05-29 21:36:34,918 epoch 7 - iter 322/469 - loss 6.30554047 - samples/sec: 62.49 - lr: 0.294886
2023-05-29 21:36:56,557 epoch 7 - iter 368/469 - loss 6.29571138 - samples/sec: 68.05 - lr: 0.286174
2023-05-29 21:37:14,640 epoch 7 - iter 414/469 - loss 6.18690113 - samples/sec: 81.43 - lr: 0.277462
2023-05-29 21:37:32,127 epoch 7 - iter 460/469 - loss 6.09754211 - samples/sec: 84.20 - lr: 0.268750
2023-05-29 21:37:35,275 ----------------------------------------------------------------------------------------------------
2023-05-29 21:37:35,275 EPOCH 7 done: loss 6.0972 - lr 0.268750
2023-05-29 21:38:36,669 Evaluating as a multi-label problem: False
2023-05-29 21:38:36,713 DEV : loss 2.0878777503967285 - f1-score (micro avg)  0.7101
2023-05-29 21:38:36,800 BAD EPOCHS (no improvement): 4
2023-05-29 21:38:36,802 ----------------------------------------------------------------------------------------------------
2023-05-29 21:38:57,172 epoch 8 - iter 46/469 - loss 5.44550620 - samples/sec: 72.29 - lr: 0.258333
2023-05-29 21:39:15,129 epoch 8 - iter 92/469 - loss 5.83239530 - samples/sec: 82.00 - lr: 0.249621
2023-05-29 21:39:33,414 epoch 8 - iter 138/469 - loss 5.74077393 - samples/sec: 80.53 - lr: 0.240909
2023-05-29 21:39:56,220 epoch 8 - iter 184/469 - loss 5.61743126 - samples/sec: 64.56 - lr: 0.232197
2023-05-29 21:40:18,854 epoch 8 - iter 230/469 - loss 5.45274775 - samples/sec: 65.06 - lr: 0.223485
2023-05-29 21:40:41,817 epoch 8 - iter 276/469 - loss 5.37944646 - samples/sec: 64.13 - lr: 0.214773
2023-05-29 21:41:07,806 epoch 8 - iter 322/469 - loss 5.25326472 - samples/sec: 56.66 - lr: 0.206061
2023-05-29 21:41:30,686 epoch 8 - iter 368/469 - loss 5.16253133 - samples/sec: 64.36 - lr: 0.197348
2023-05-29 21:41:52,882 epoch 8 - iter 414/469 - loss 5.08328059 - samples/sec: 66.34 - lr: 0.188636
2023-05-29 21:42:18,543 epoch 8 - iter 460/469 - loss 4.97978401 - samples/sec: 57.38 - lr: 0.179924
2023-05-29 21:42:23,065 ----------------------------------------------------------------------------------------------------
2023-05-29 21:42:23,065 EPOCH 8 done: loss 4.9594 - lr 0.179924
2023-05-29 21:43:44,769 Evaluating as a multi-label problem: False
2023-05-29 21:43:44,838 DEV : loss 1.613186001777649 - f1-score (micro avg)  0.7216
2023-05-29 21:43:44,966 BAD EPOCHS (no improvement): 4
2023-05-29 21:43:44,969 ----------------------------------------------------------------------------------------------------
2023-05-29 21:44:06,763 epoch 9 - iter 46/469 - loss 4.24226284 - samples/sec: 67.58 - lr: 0.169508
2023-05-29 21:44:29,037 epoch 9 - iter 92/469 - loss 4.07385652 - samples/sec: 66.11 - lr: 0.160795
2023-05-29 21:44:52,112 epoch 9 - iter 138/469 - loss 3.99382564 - samples/sec: 63.81 - lr: 0.152083
2023-05-29 21:45:12,359 epoch 9 - iter 184/469 - loss 3.84443074 - samples/sec: 72.73 - lr: 0.143371
2023-05-29 21:45:31,686 epoch 9 - iter 230/469 - loss 3.77971998 - samples/sec: 76.20 - lr: 0.134659
2023-05-29 21:45:55,128 epoch 9 - iter 276/469 - loss 3.71453231 - samples/sec: 62.81 - lr: 0.125947
2023-05-29 21:46:14,411 epoch 9 - iter 322/469 - loss 3.66512766 - samples/sec: 76.37 - lr: 0.117235
2023-05-29 21:46:35,086 epoch 9 - iter 368/469 - loss 3.60797646 - samples/sec: 71.23 - lr: 0.108523
2023-05-29 21:47:01,269 epoch 9 - iter 414/469 - loss 3.52149238 - samples/sec: 56.24 - lr: 0.099811
2023-05-29 21:47:21,462 epoch 9 - iter 460/469 - loss 3.43513762 - samples/sec: 72.93 - lr: 0.091098
2023-05-29 21:47:24,852 ----------------------------------------------------------------------------------------------------
2023-05-29 21:47:24,852 EPOCH 9 done: loss 3.4229 - lr 0.091098
2023-05-29 21:48:39,447 Evaluating as a multi-label problem: False
2023-05-29 21:48:39,521 DEV : loss 1.0401453971862793 - f1-score (micro avg)  0.7514
2023-05-29 21:48:39,656 BAD EPOCHS (no improvement): 4
2023-05-29 21:48:39,659 ----------------------------------------------------------------------------------------------------
2023-05-29 21:49:05,586 epoch 10 - iter 46/469 - loss 2.73182748 - samples/sec: 56.80 - lr: 0.080682
2023-05-29 21:49:28,786 epoch 10 - iter 92/469 - loss 2.50627815 - samples/sec: 63.47 - lr: 0.071970
2023-05-29 21:49:49,183 epoch 10 - iter 138/469 - loss 2.44599149 - samples/sec: 72.20 - lr: 0.063258
2023-05-29 21:50:11,499 epoch 10 - iter 184/469 - loss 2.36964107 - samples/sec: 65.98 - lr: 0.054545
2023-05-29 21:50:33,685 epoch 10 - iter 230/469 - loss 2.28906857 - samples/sec: 66.37 - lr: 0.045833
2023-05-29 21:50:56,245 epoch 10 - iter 276/469 - loss 2.19902829 - samples/sec: 65.27 - lr: 0.037121
2023-05-29 21:51:22,783 epoch 10 - iter 322/469 - loss 2.15537919 - samples/sec: 55.48 - lr: 0.028409
2023-05-29 21:51:46,130 epoch 10 - iter 368/469 - loss 2.09396294 - samples/sec: 63.07 - lr: 0.019697
2023-05-29 21:52:07,525 epoch 10 - iter 414/469 - loss 2.04476867 - samples/sec: 68.83 - lr: 0.010985
2023-05-29 21:52:32,101 epoch 10 - iter 460/469 - loss 1.99486399 - samples/sec: 59.91 - lr: 0.002273
2023-05-29 21:52:36,592 ----------------------------------------------------------------------------------------------------
2023-05-29 21:52:36,593 EPOCH 10 done: loss 1.9868 - lr 0.002273
2023-05-29 21:53:58,869 Evaluating as a multi-label problem: False
2023-05-29 21:53:58,935 DEV : loss 0.5326583385467529 - f1-score (micro avg)  0.8085
2023-05-29 21:53:59,064 BAD EPOCHS (no improvement): 4
2023-05-29 21:54:11,254 ----------------------------------------------------------------------------------------------------
2023-05-29 21:54:11,257 Testing using last state of model ...
2023-05-29 21:55:25,939 Evaluating as a multi-label problem: False
2023-05-29 21:55:25,980 0.7905	0.7709	0.7806	0.6766
2023-05-29 21:55:25,980 
Results:
- F-score (micro) 0.7806
- F-score (macro) 0.7676
- Accuracy 0.6766

By class:
              precision    recall  f1-score   support

         LOC     0.7887    0.7698    0.7791      1668
         PER     0.9123    0.9326    0.9223      1617
         ORG     0.6868    0.6641    0.6752      1661
        MISC     0.7391    0.6538    0.6939       702

   micro avg     0.7905    0.7709    0.7806      5648
   macro avg     0.7817    0.7551    0.7676      5648
weighted avg     0.7880    0.7709    0.7790      5648

2023-05-29 21:55:25,980 ----------------------------------------------------------------------------------------------------
