2023-06-05 15:40:14,276 ----------------------------------------------------------------------------------------------------
2023-06-05 15:40:14,280 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-05 15:40:14,282 ----------------------------------------------------------------------------------------------------
2023-06-05 15:40:14,282 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-05 15:40:14,282 ----------------------------------------------------------------------------------------------------
2023-06-05 15:40:14,283 Parameters:
2023-06-05 15:40:14,283  - learning_rate: "0.800000"
2023-06-05 15:40:14,283  - mini_batch_size: "32"
2023-06-05 15:40:14,283  - patience: "3"
2023-06-05 15:40:14,283  - anneal_factor: "0.5"
2023-06-05 15:40:14,283  - max_epochs: "10"
2023-06-05 15:40:14,283  - shuffle: "True"
2023-06-05 15:40:14,283  - train_with_dev: "False"
2023-06-05 15:40:14,283  - batch_growth_annealing: "False"
2023-06-05 15:40:14,283 ----------------------------------------------------------------------------------------------------
2023-06-05 15:40:14,283 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_2_linear_probing"
2023-06-05 15:40:14,283 ----------------------------------------------------------------------------------------------------
2023-06-05 15:40:14,283 Device: cuda:0
2023-06-05 15:40:14,283 ----------------------------------------------------------------------------------------------------
2023-06-05 15:40:14,283 Embeddings storage mode: none
2023-06-05 15:40:14,283 ----------------------------------------------------------------------------------------------------
2023-06-05 15:40:39,510 epoch 1 - iter 46/469 - loss 1.49164178 - samples/sec: 58.37 - lr: 0.078465
2023-06-05 15:41:04,740 epoch 1 - iter 92/469 - loss 1.76975578 - samples/sec: 58.36 - lr: 0.156930
2023-06-05 15:41:32,152 epoch 1 - iter 138/469 - loss 2.22733286 - samples/sec: 53.71 - lr: 0.235394
2023-06-05 15:41:56,442 epoch 1 - iter 184/469 - loss 3.08870081 - samples/sec: 60.62 - lr: 0.313859
2023-06-05 15:42:20,645 epoch 1 - iter 230/469 - loss 3.62423991 - samples/sec: 60.84 - lr: 0.392324
2023-06-05 15:42:49,306 epoch 1 - iter 276/469 - loss 4.53555190 - samples/sec: 51.37 - lr: 0.470789
2023-06-05 15:43:16,151 epoch 1 - iter 322/469 - loss 5.46200141 - samples/sec: 54.85 - lr: 0.549254
2023-06-05 15:43:41,523 epoch 1 - iter 368/469 - loss 6.57583729 - samples/sec: 58.04 - lr: 0.627719
2023-06-05 15:44:09,679 epoch 1 - iter 414/469 - loss 7.02707437 - samples/sec: 52.29 - lr: 0.706183
2023-06-05 15:44:35,295 epoch 1 - iter 460/469 - loss 7.30294303 - samples/sec: 57.48 - lr: 0.784648
2023-06-05 15:44:39,965 ----------------------------------------------------------------------------------------------------
2023-06-05 15:44:39,965 EPOCH 1 done: loss 7.4124 - lr 0.784648
2023-06-05 15:46:08,852 Evaluating as a multi-label problem: False
2023-06-05 15:46:08,923 DEV : loss 5.465958595275879 - f1-score (micro avg)  0.7091
2023-06-05 15:46:09,040 BAD EPOCHS (no improvement): 4
2023-06-05 15:46:09,045 ----------------------------------------------------------------------------------------------------
2023-06-05 15:46:35,816 epoch 2 - iter 46/469 - loss 12.07911969 - samples/sec: 55.01 - lr: 0.791288
2023-06-05 15:46:59,872 epoch 2 - iter 92/469 - loss 12.37850062 - samples/sec: 61.21 - lr: 0.782576
2023-06-05 15:47:24,551 epoch 2 - iter 138/469 - loss 12.23613077 - samples/sec: 59.67 - lr: 0.773864
2023-06-05 15:47:54,955 epoch 2 - iter 184/469 - loss 12.10462130 - samples/sec: 48.43 - lr: 0.765152
2023-06-05 15:48:21,756 epoch 2 - iter 230/469 - loss 11.85102396 - samples/sec: 54.94 - lr: 0.756439
2023-06-05 15:48:48,045 epoch 2 - iter 276/469 - loss 11.97658541 - samples/sec: 56.01 - lr: 0.747727
2023-06-05 15:49:17,614 epoch 2 - iter 322/469 - loss 11.92587767 - samples/sec: 49.80 - lr: 0.739015
2023-06-05 15:49:46,110 epoch 2 - iter 368/469 - loss 11.94322001 - samples/sec: 51.67 - lr: 0.730303
2023-06-05 15:50:12,916 epoch 2 - iter 414/469 - loss 11.88996233 - samples/sec: 54.93 - lr: 0.721591
2023-06-05 15:50:42,461 epoch 2 - iter 460/469 - loss 11.85849756 - samples/sec: 49.83 - lr: 0.712879
2023-06-05 15:50:47,859 ----------------------------------------------------------------------------------------------------
2023-06-05 15:50:47,859 EPOCH 2 done: loss 11.8132 - lr 0.712879
2023-06-05 15:52:17,182 Evaluating as a multi-label problem: False
2023-06-05 15:52:17,249 DEV : loss 4.310565948486328 - f1-score (micro avg)  0.6915
2023-06-05 15:52:17,348 BAD EPOCHS (no improvement): 4
2023-06-05 15:52:17,352 ----------------------------------------------------------------------------------------------------
2023-06-05 15:52:42,445 epoch 3 - iter 46/469 - loss 11.67277790 - samples/sec: 58.68 - lr: 0.702462
2023-06-05 15:53:06,770 epoch 3 - iter 92/469 - loss 12.01078781 - samples/sec: 60.53 - lr: 0.693750
2023-06-05 15:53:36,387 epoch 3 - iter 138/469 - loss 11.69311999 - samples/sec: 49.72 - lr: 0.685038
2023-06-05 15:54:02,800 epoch 3 - iter 184/469 - loss 11.47504320 - samples/sec: 55.75 - lr: 0.676326
2023-06-05 15:54:29,915 epoch 3 - iter 230/469 - loss 11.46180899 - samples/sec: 54.30 - lr: 0.667614
2023-06-05 15:55:00,452 epoch 3 - iter 276/469 - loss 11.54989768 - samples/sec: 48.22 - lr: 0.658902
2023-06-05 15:55:29,275 epoch 3 - iter 322/469 - loss 11.53453353 - samples/sec: 51.08 - lr: 0.650189
2023-06-05 15:55:55,726 epoch 3 - iter 368/469 - loss 11.47387157 - samples/sec: 55.67 - lr: 0.641477
2023-06-05 15:56:25,223 epoch 3 - iter 414/469 - loss 11.30839472 - samples/sec: 49.91 - lr: 0.632765
2023-06-05 15:56:52,864 epoch 3 - iter 460/469 - loss 11.23503530 - samples/sec: 53.27 - lr: 0.624053
2023-06-05 15:56:57,886 ----------------------------------------------------------------------------------------------------
2023-06-05 15:56:57,886 EPOCH 3 done: loss 11.2031 - lr 0.624053
2023-06-05 15:58:27,148 Evaluating as a multi-label problem: False
2023-06-05 15:58:27,217 DEV : loss 3.703298568725586 - f1-score (micro avg)  0.6818
2023-06-05 15:58:27,322 BAD EPOCHS (no improvement): 4
2023-06-05 15:58:27,325 ----------------------------------------------------------------------------------------------------
2023-06-05 15:58:56,284 epoch 4 - iter 46/469 - loss 9.82716419 - samples/sec: 50.85 - lr: 0.613636
2023-06-05 15:59:23,361 epoch 4 - iter 92/469 - loss 10.58924207 - samples/sec: 54.38 - lr: 0.604924
2023-06-05 15:59:50,118 epoch 4 - iter 138/469 - loss 10.43351251 - samples/sec: 55.03 - lr: 0.596212
2023-06-05 16:00:15,233 epoch 4 - iter 184/469 - loss 10.33235991 - samples/sec: 58.63 - lr: 0.587500
2023-06-05 16:00:45,812 epoch 4 - iter 230/469 - loss 10.12121547 - samples/sec: 48.15 - lr: 0.578788
2023-06-05 16:01:12,085 epoch 4 - iter 276/469 - loss 9.99548801 - samples/sec: 56.04 - lr: 0.570076
2023-06-05 16:01:42,128 epoch 4 - iter 322/469 - loss 9.99815861 - samples/sec: 49.01 - lr: 0.561364
2023-06-05 16:02:08,614 epoch 4 - iter 368/469 - loss 9.96322721 - samples/sec: 55.59 - lr: 0.552652
2023-06-05 16:02:35,221 epoch 4 - iter 414/469 - loss 9.89721896 - samples/sec: 55.34 - lr: 0.543939
2023-06-05 16:03:05,739 epoch 4 - iter 460/469 - loss 9.74764204 - samples/sec: 48.25 - lr: 0.535227
2023-06-05 16:03:10,232 ----------------------------------------------------------------------------------------------------
2023-06-05 16:03:10,232 EPOCH 4 done: loss 9.7370 - lr 0.535227
2023-06-05 16:04:31,156 Evaluating as a multi-label problem: False
2023-06-05 16:04:31,234 DEV : loss 3.4033215045928955 - f1-score (micro avg)  0.6453
2023-06-05 16:04:31,343 BAD EPOCHS (no improvement): 4
2023-06-05 16:04:31,347 ----------------------------------------------------------------------------------------------------
2023-06-05 16:04:57,959 epoch 5 - iter 46/469 - loss 9.60292219 - samples/sec: 55.34 - lr: 0.524811
2023-06-05 16:05:24,935 epoch 5 - iter 92/469 - loss 8.89995857 - samples/sec: 54.58 - lr: 0.516098
2023-06-05 16:05:54,504 epoch 5 - iter 138/469 - loss 8.91222010 - samples/sec: 49.80 - lr: 0.507386
2023-06-05 16:06:21,258 epoch 5 - iter 184/469 - loss 8.66661165 - samples/sec: 55.04 - lr: 0.498674
2023-06-05 16:06:47,363 epoch 5 - iter 230/469 - loss 8.69081439 - samples/sec: 56.41 - lr: 0.489962
2023-06-05 16:07:14,400 epoch 5 - iter 276/469 - loss 8.69979473 - samples/sec: 54.46 - lr: 0.481250
2023-06-05 16:07:37,494 epoch 5 - iter 322/469 - loss 8.59576062 - samples/sec: 63.76 - lr: 0.472538
2023-06-05 16:08:00,506 epoch 5 - iter 368/469 - loss 8.49292842 - samples/sec: 63.98 - lr: 0.463826
2023-06-05 16:08:27,030 epoch 5 - iter 414/469 - loss 8.38059784 - samples/sec: 55.51 - lr: 0.455114
2023-06-05 16:08:52,624 epoch 5 - iter 460/469 - loss 8.20448272 - samples/sec: 57.53 - lr: 0.446402
2023-06-05 16:08:56,779 ----------------------------------------------------------------------------------------------------
2023-06-05 16:08:56,779 EPOCH 5 done: loss 8.1802 - lr 0.446402
2023-06-05 16:10:20,352 Evaluating as a multi-label problem: False
2023-06-05 16:10:20,408 DEV : loss 2.651977062225342 - f1-score (micro avg)  0.7192
2023-06-05 16:10:20,491 BAD EPOCHS (no improvement): 4
2023-06-05 16:10:20,519 ----------------------------------------------------------------------------------------------------
2023-06-05 16:10:46,580 epoch 6 - iter 46/469 - loss 7.34684875 - samples/sec: 56.51 - lr: 0.435985
2023-06-05 16:11:17,682 epoch 6 - iter 92/469 - loss 7.45723155 - samples/sec: 47.34 - lr: 0.427273
2023-06-05 16:11:43,574 epoch 6 - iter 138/469 - loss 7.34754434 - samples/sec: 56.87 - lr: 0.418561
2023-06-05 16:12:10,143 epoch 6 - iter 184/469 - loss 7.37937975 - samples/sec: 55.42 - lr: 0.409848
2023-06-05 16:12:41,240 epoch 6 - iter 230/469 - loss 7.45627606 - samples/sec: 47.35 - lr: 0.401136
2023-06-05 16:13:08,353 epoch 6 - iter 276/469 - loss 7.50094562 - samples/sec: 54.31 - lr: 0.392424
2023-06-05 16:13:35,142 epoch 6 - iter 322/469 - loss 7.47078258 - samples/sec: 54.96 - lr: 0.383712
2023-06-05 16:14:04,620 epoch 6 - iter 368/469 - loss 7.47200422 - samples/sec: 49.95 - lr: 0.375000
2023-06-05 16:14:31,713 epoch 6 - iter 414/469 - loss 7.39885999 - samples/sec: 54.36 - lr: 0.366288
2023-06-05 16:15:01,434 epoch 6 - iter 460/469 - loss 7.26996075 - samples/sec: 49.54 - lr: 0.357576
2023-06-05 16:15:06,647 ----------------------------------------------------------------------------------------------------
2023-06-05 16:15:06,647 EPOCH 6 done: loss 7.2365 - lr 0.357576
2023-06-05 16:16:33,926 Evaluating as a multi-label problem: False
2023-06-05 16:16:33,995 DEV : loss 2.5256059169769287 - f1-score (micro avg)  0.7062
2023-06-05 16:16:34,095 BAD EPOCHS (no improvement): 4
2023-06-05 16:16:34,098 ----------------------------------------------------------------------------------------------------
2023-06-05 16:17:00,599 epoch 7 - iter 46/469 - loss 6.59428127 - samples/sec: 55.57 - lr: 0.347159
2023-06-05 16:17:27,265 epoch 7 - iter 92/469 - loss 6.94407144 - samples/sec: 55.22 - lr: 0.338447
2023-06-05 16:17:56,698 epoch 7 - iter 138/469 - loss 6.68334326 - samples/sec: 50.03 - lr: 0.329735
2023-06-05 16:18:23,501 epoch 7 - iter 184/469 - loss 6.55073556 - samples/sec: 54.94 - lr: 0.321023
2023-06-05 16:18:50,034 epoch 7 - iter 230/469 - loss 6.43475259 - samples/sec: 55.50 - lr: 0.312311
2023-06-05 16:19:19,642 epoch 7 - iter 276/469 - loss 6.31555050 - samples/sec: 49.73 - lr: 0.303598
2023-06-05 16:19:44,254 epoch 7 - iter 322/469 - loss 6.20204154 - samples/sec: 59.83 - lr: 0.294886
2023-06-05 16:20:09,152 epoch 7 - iter 368/469 - loss 6.15861519 - samples/sec: 59.14 - lr: 0.286174
2023-06-05 16:20:39,663 epoch 7 - iter 414/469 - loss 6.07258785 - samples/sec: 48.26 - lr: 0.277462
2023-06-05 16:21:05,414 epoch 7 - iter 460/469 - loss 6.01283788 - samples/sec: 57.18 - lr: 0.268750
2023-06-05 16:21:10,310 ----------------------------------------------------------------------------------------------------
2023-06-05 16:21:10,310 EPOCH 7 done: loss 5.9882 - lr 0.268750
2023-06-05 16:22:38,931 Evaluating as a multi-label problem: False
2023-06-05 16:22:39,003 DEV : loss 2.1234679222106934 - f1-score (micro avg)  0.7264
2023-06-05 16:22:39,124 BAD EPOCHS (no improvement): 4
2023-06-05 16:22:39,126 ----------------------------------------------------------------------------------------------------
2023-06-05 16:23:08,400 epoch 8 - iter 46/469 - loss 5.54523911 - samples/sec: 50.31 - lr: 0.258333
2023-06-05 16:23:35,726 epoch 8 - iter 92/469 - loss 5.09859886 - samples/sec: 53.89 - lr: 0.249621
2023-06-05 16:24:03,423 epoch 8 - iter 138/469 - loss 5.17247341 - samples/sec: 53.16 - lr: 0.240909
2023-06-05 16:24:31,910 epoch 8 - iter 184/469 - loss 5.09011394 - samples/sec: 51.69 - lr: 0.232197
2023-06-05 16:24:58,127 epoch 8 - iter 230/469 - loss 5.12775435 - samples/sec: 56.16 - lr: 0.223485
2023-06-05 16:25:25,893 epoch 8 - iter 276/469 - loss 5.06380999 - samples/sec: 53.03 - lr: 0.214773
2023-06-05 16:25:56,330 epoch 8 - iter 322/469 - loss 4.96722555 - samples/sec: 48.38 - lr: 0.206061
2023-06-05 16:26:22,565 epoch 8 - iter 368/469 - loss 4.90706625 - samples/sec: 56.13 - lr: 0.197348
2023-06-05 16:26:49,515 epoch 8 - iter 414/469 - loss 4.81990829 - samples/sec: 54.64 - lr: 0.188636
2023-06-05 16:27:19,159 epoch 8 - iter 460/469 - loss 4.80137395 - samples/sec: 49.67 - lr: 0.179924
2023-06-05 16:27:24,009 ----------------------------------------------------------------------------------------------------
2023-06-05 16:27:24,009 EPOCH 8 done: loss 4.7936 - lr 0.179924
2023-06-05 16:28:52,058 Evaluating as a multi-label problem: False
2023-06-05 16:28:52,129 DEV : loss 1.8178515434265137 - f1-score (micro avg)  0.7274
2023-06-05 16:28:52,237 BAD EPOCHS (no improvement): 4
2023-06-05 16:28:52,244 ----------------------------------------------------------------------------------------------------
2023-06-05 16:29:18,794 epoch 9 - iter 46/469 - loss 4.02725873 - samples/sec: 55.47 - lr: 0.169508
2023-06-05 16:29:44,477 epoch 9 - iter 92/469 - loss 3.89829097 - samples/sec: 57.33 - lr: 0.160795
2023-06-05 16:30:10,941 epoch 9 - iter 138/469 - loss 3.81594872 - samples/sec: 55.63 - lr: 0.152083
2023-06-05 16:30:34,055 epoch 9 - iter 184/469 - loss 3.75154603 - samples/sec: 63.70 - lr: 0.143371
2023-06-05 16:30:58,429 epoch 9 - iter 230/469 - loss 3.72005346 - samples/sec: 60.41 - lr: 0.134659
2023-06-05 16:31:27,674 epoch 9 - iter 276/469 - loss 3.66267907 - samples/sec: 50.35 - lr: 0.125947
2023-06-05 16:31:53,582 epoch 9 - iter 322/469 - loss 3.63427172 - samples/sec: 56.83 - lr: 0.117235
2023-06-05 16:32:18,631 epoch 9 - iter 368/469 - loss 3.58884033 - samples/sec: 58.79 - lr: 0.108523
2023-06-05 16:32:49,217 epoch 9 - iter 414/469 - loss 3.52382645 - samples/sec: 48.14 - lr: 0.099811
2023-06-05 16:33:15,599 epoch 9 - iter 460/469 - loss 3.46486979 - samples/sec: 55.81 - lr: 0.091098
2023-06-05 16:33:20,766 ----------------------------------------------------------------------------------------------------
2023-06-05 16:33:20,766 EPOCH 9 done: loss 3.4527 - lr 0.091098
2023-06-05 16:34:49,473 Evaluating as a multi-label problem: False
2023-06-05 16:34:49,544 DEV : loss 1.0103751420974731 - f1-score (micro avg)  0.7706
2023-06-05 16:34:49,650 BAD EPOCHS (no improvement): 4
2023-06-05 16:34:49,656 ----------------------------------------------------------------------------------------------------
2023-06-05 16:35:16,470 epoch 10 - iter 46/469 - loss 2.81698011 - samples/sec: 54.92 - lr: 0.080682
2023-06-05 16:35:46,085 epoch 10 - iter 92/469 - loss 2.63104117 - samples/sec: 49.72 - lr: 0.071970
2023-06-05 16:36:12,264 epoch 10 - iter 138/469 - loss 2.56221558 - samples/sec: 56.25 - lr: 0.063258
2023-06-05 16:36:38,610 epoch 10 - iter 184/469 - loss 2.45059358 - samples/sec: 55.89 - lr: 0.054545
2023-06-05 16:37:09,053 epoch 10 - iter 230/469 - loss 2.35669719 - samples/sec: 48.36 - lr: 0.045833
2023-06-05 16:37:35,870 epoch 10 - iter 276/469 - loss 2.27907384 - samples/sec: 54.91 - lr: 0.037121
2023-06-05 16:38:02,502 epoch 10 - iter 322/469 - loss 2.22310659 - samples/sec: 55.29 - lr: 0.028409
2023-06-05 16:38:31,319 epoch 10 - iter 368/469 - loss 2.16143233 - samples/sec: 51.09 - lr: 0.019697
2023-06-05 16:38:56,352 epoch 10 - iter 414/469 - loss 2.09864414 - samples/sec: 58.82 - lr: 0.010985
2023-06-05 16:39:21,501 epoch 10 - iter 460/469 - loss 2.04092672 - samples/sec: 58.55 - lr: 0.002273
2023-06-05 16:39:28,855 ----------------------------------------------------------------------------------------------------
2023-06-05 16:39:28,855 EPOCH 10 done: loss 2.0340 - lr 0.002273
2023-06-05 16:40:57,617 Evaluating as a multi-label problem: False
2023-06-05 16:40:57,684 DEV : loss 0.5528352856636047 - f1-score (micro avg)  0.8054
2023-06-05 16:40:57,784 BAD EPOCHS (no improvement): 4
2023-06-05 16:41:12,304 ----------------------------------------------------------------------------------------------------
2023-06-05 16:41:12,307 Testing using last state of model ...
2023-06-05 16:42:42,800 Evaluating as a multi-label problem: False
2023-06-05 16:42:42,866 0.7891	0.7606	0.7746	0.6716
2023-06-05 16:42:42,866 
Results:
- F-score (micro) 0.7746
- F-score (macro) 0.7571
- Accuracy 0.6716

By class:
              precision    recall  f1-score   support

         LOC     0.7906    0.7674    0.7788      1668
         PER     0.9264    0.9412    0.9337      1617
         ORG     0.6745    0.6400    0.6568      1661
        MISC     0.7112    0.6140    0.6590       702

   micro avg     0.7891    0.7606    0.7746      5648
   macro avg     0.7757    0.7406    0.7571      5648
weighted avg     0.7855    0.7606    0.7724      5648

2023-06-05 16:42:42,866 ----------------------------------------------------------------------------------------------------
