2023-06-05 16:43:04,926 ----------------------------------------------------------------------------------------------------
2023-06-05 16:43:04,931 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-05 16:43:04,933 ----------------------------------------------------------------------------------------------------
2023-06-05 16:43:04,933 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-05 16:43:04,933 ----------------------------------------------------------------------------------------------------
2023-06-05 16:43:04,933 Parameters:
2023-06-05 16:43:04,933  - learning_rate: "0.000005"
2023-06-05 16:43:04,933  - mini_batch_size: "4"
2023-06-05 16:43:04,933  - patience: "3"
2023-06-05 16:43:04,933  - anneal_factor: "0.5"
2023-06-05 16:43:04,933  - max_epochs: "10"
2023-06-05 16:43:04,933  - shuffle: "True"
2023-06-05 16:43:04,934  - train_with_dev: "False"
2023-06-05 16:43:04,934  - batch_growth_annealing: "False"
2023-06-05 16:43:04,934 ----------------------------------------------------------------------------------------------------
2023-06-05 16:43:04,934 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_2_full_fine_tuning"
2023-06-05 16:43:04,934 ----------------------------------------------------------------------------------------------------
2023-06-05 16:43:04,934 Device: cuda:0
2023-06-05 16:43:04,934 ----------------------------------------------------------------------------------------------------
2023-06-05 16:43:04,934 Embeddings storage mode: none
2023-06-05 16:43:04,934 ----------------------------------------------------------------------------------------------------
2023-06-05 16:44:48,524 epoch 1 - iter 374/3747 - loss 1.56605325 - samples/sec: 14.45 - lr: 0.000000
2023-06-05 16:46:34,232 epoch 1 - iter 748/3747 - loss 1.27199945 - samples/sec: 14.16 - lr: 0.000001
2023-06-05 16:48:17,420 epoch 1 - iter 1122/3747 - loss 1.06962815 - samples/sec: 14.50 - lr: 0.000001
2023-06-05 16:50:00,399 epoch 1 - iter 1496/3747 - loss 0.93735845 - samples/sec: 14.53 - lr: 0.000002
2023-06-05 16:51:44,028 epoch 1 - iter 1870/3747 - loss 0.80922119 - samples/sec: 14.44 - lr: 0.000002
2023-06-05 16:53:30,304 epoch 1 - iter 2244/3747 - loss 0.71388048 - samples/sec: 14.08 - lr: 0.000003
2023-06-05 16:55:15,524 epoch 1 - iter 2618/3747 - loss 0.64880563 - samples/sec: 14.22 - lr: 0.000003
2023-06-05 16:57:00,313 epoch 1 - iter 2992/3747 - loss 0.60884570 - samples/sec: 14.28 - lr: 0.000004
2023-06-05 16:58:48,017 epoch 1 - iter 3366/3747 - loss 0.56994109 - samples/sec: 13.89 - lr: 0.000004
2023-06-05 17:00:32,533 epoch 1 - iter 3740/3747 - loss 0.53228126 - samples/sec: 14.32 - lr: 0.000005
2023-06-05 17:00:34,351 ----------------------------------------------------------------------------------------------------
2023-06-05 17:00:34,352 EPOCH 1 done: loss 0.5321 - lr 0.000005
2023-06-05 17:02:07,434 Evaluating as a multi-label problem: False
2023-06-05 17:02:07,509 DEV : loss 0.13975059986114502 - f1-score (micro avg)  0.9165
2023-06-05 17:02:07,635 BAD EPOCHS (no improvement): 4
2023-06-05 17:02:07,639 ----------------------------------------------------------------------------------------------------
2023-06-05 17:03:54,027 epoch 2 - iter 374/3747 - loss 0.26483110 - samples/sec: 14.07 - lr: 0.000005
2023-06-05 17:05:38,514 epoch 2 - iter 748/3747 - loss 0.27832617 - samples/sec: 14.32 - lr: 0.000005
2023-06-05 17:07:23,400 epoch 2 - iter 1122/3747 - loss 0.26708903 - samples/sec: 14.27 - lr: 0.000005
2023-06-05 17:09:12,803 epoch 2 - iter 1496/3747 - loss 0.25899350 - samples/sec: 13.68 - lr: 0.000005
2023-06-05 17:10:56,946 epoch 2 - iter 1870/3747 - loss 0.26073418 - samples/sec: 14.37 - lr: 0.000005
2023-06-05 17:12:42,548 epoch 2 - iter 2244/3747 - loss 0.26311561 - samples/sec: 14.17 - lr: 0.000005
2023-06-05 17:14:28,832 epoch 2 - iter 2618/3747 - loss 0.25599352 - samples/sec: 14.08 - lr: 0.000005
2023-06-05 17:16:12,674 epoch 2 - iter 2992/3747 - loss 0.25564243 - samples/sec: 14.41 - lr: 0.000005
2023-06-05 17:17:59,273 epoch 2 - iter 3366/3747 - loss 0.25215077 - samples/sec: 14.04 - lr: 0.000005
2023-06-05 17:19:43,057 epoch 2 - iter 3740/3747 - loss 0.25171147 - samples/sec: 14.42 - lr: 0.000004
2023-06-05 17:19:44,902 ----------------------------------------------------------------------------------------------------
2023-06-05 17:19:44,902 EPOCH 2 done: loss 0.2514 - lr 0.000004
2023-06-05 17:21:20,440 Evaluating as a multi-label problem: False
2023-06-05 17:21:20,508 DEV : loss 0.1301763355731964 - f1-score (micro avg)  0.9378
2023-06-05 17:21:20,647 BAD EPOCHS (no improvement): 4
2023-06-05 17:21:20,653 ----------------------------------------------------------------------------------------------------
2023-06-05 17:23:06,964 epoch 3 - iter 374/3747 - loss 0.20277787 - samples/sec: 14.08 - lr: 0.000004
2023-06-05 17:24:54,743 epoch 3 - iter 748/3747 - loss 0.19775738 - samples/sec: 13.89 - lr: 0.000004
2023-06-05 17:26:40,818 epoch 3 - iter 1122/3747 - loss 0.19975370 - samples/sec: 14.11 - lr: 0.000004
2023-06-05 17:28:26,792 epoch 3 - iter 1496/3747 - loss 0.19321072 - samples/sec: 14.12 - lr: 0.000004
2023-06-05 17:30:13,905 epoch 3 - iter 1870/3747 - loss 0.20569375 - samples/sec: 13.97 - lr: 0.000004
2023-06-05 17:32:00,636 epoch 3 - iter 2244/3747 - loss 0.20687098 - samples/sec: 14.02 - lr: 0.000004
2023-06-05 17:33:47,768 epoch 3 - iter 2618/3747 - loss 0.20146789 - samples/sec: 13.97 - lr: 0.000004
2023-06-05 17:35:33,343 epoch 3 - iter 2992/3747 - loss 0.19783874 - samples/sec: 14.18 - lr: 0.000004
2023-06-05 17:37:20,775 epoch 3 - iter 3366/3747 - loss 0.19472470 - samples/sec: 13.93 - lr: 0.000004
2023-06-05 17:39:11,812 epoch 3 - iter 3740/3747 - loss 0.19243956 - samples/sec: 13.48 - lr: 0.000004
2023-06-05 17:39:13,824 ----------------------------------------------------------------------------------------------------
2023-06-05 17:39:13,824 EPOCH 3 done: loss 0.1924 - lr 0.000004
2023-06-05 17:40:46,008 Evaluating as a multi-label problem: False
2023-06-05 17:40:46,076 DEV : loss 0.10017769783735275 - f1-score (micro avg)  0.9552
2023-06-05 17:40:46,200 BAD EPOCHS (no improvement): 4
2023-06-05 17:40:46,202 ----------------------------------------------------------------------------------------------------
2023-06-05 17:42:32,242 epoch 4 - iter 374/3747 - loss 0.16439358 - samples/sec: 14.11 - lr: 0.000004
2023-06-05 17:44:24,605 epoch 4 - iter 748/3747 - loss 0.16043434 - samples/sec: 13.32 - lr: 0.000004
2023-06-05 17:46:11,621 epoch 4 - iter 1122/3747 - loss 0.16588257 - samples/sec: 13.99 - lr: 0.000004
2023-06-05 17:47:56,223 epoch 4 - iter 1496/3747 - loss 0.15981308 - samples/sec: 14.31 - lr: 0.000004
2023-06-05 17:49:41,191 epoch 4 - iter 1870/3747 - loss 0.15345686 - samples/sec: 14.26 - lr: 0.000004
2023-06-05 17:51:26,047 epoch 4 - iter 2244/3747 - loss 0.15349497 - samples/sec: 14.27 - lr: 0.000004
2023-06-05 17:53:10,886 epoch 4 - iter 2618/3747 - loss 0.15444443 - samples/sec: 14.28 - lr: 0.000004
2023-06-05 17:54:55,533 epoch 4 - iter 2992/3747 - loss 0.14916163 - samples/sec: 14.30 - lr: 0.000003
2023-06-05 17:56:39,940 epoch 4 - iter 3366/3747 - loss 0.14864109 - samples/sec: 14.33 - lr: 0.000003
2023-06-05 17:58:24,899 epoch 4 - iter 3740/3747 - loss 0.14920384 - samples/sec: 14.26 - lr: 0.000003
2023-06-05 17:58:26,652 ----------------------------------------------------------------------------------------------------
2023-06-05 17:58:26,652 EPOCH 4 done: loss 0.1490 - lr 0.000003
2023-06-05 18:00:01,056 Evaluating as a multi-label problem: False
2023-06-05 18:00:01,130 DEV : loss 0.0923686996102333 - f1-score (micro avg)  0.9553
2023-06-05 18:00:01,275 BAD EPOCHS (no improvement): 4
2023-06-05 18:00:01,278 ----------------------------------------------------------------------------------------------------
2023-06-05 18:01:47,903 epoch 5 - iter 374/3747 - loss 0.11531150 - samples/sec: 14.04 - lr: 0.000003
2023-06-05 18:03:32,832 epoch 5 - iter 748/3747 - loss 0.11840356 - samples/sec: 14.26 - lr: 0.000003
2023-06-05 18:05:17,456 epoch 5 - iter 1122/3747 - loss 0.12543826 - samples/sec: 14.30 - lr: 0.000003
2023-06-05 18:07:03,472 epoch 5 - iter 1496/3747 - loss 0.12315430 - samples/sec: 14.12 - lr: 0.000003
2023-06-05 18:08:49,220 epoch 5 - iter 1870/3747 - loss 0.12661693 - samples/sec: 14.15 - lr: 0.000003
2023-06-05 18:10:33,919 epoch 5 - iter 2244/3747 - loss 0.13182485 - samples/sec: 14.29 - lr: 0.000003
2023-06-05 18:12:19,123 epoch 5 - iter 2618/3747 - loss 0.12977559 - samples/sec: 14.23 - lr: 0.000003
2023-06-05 18:14:09,774 epoch 5 - iter 2992/3747 - loss 0.13064975 - samples/sec: 13.53 - lr: 0.000003
2023-06-05 18:15:57,745 epoch 5 - iter 3366/3747 - loss 0.12750877 - samples/sec: 13.86 - lr: 0.000003
2023-06-05 18:17:43,134 epoch 5 - iter 3740/3747 - loss 0.12980336 - samples/sec: 14.20 - lr: 0.000003
2023-06-05 18:17:45,026 ----------------------------------------------------------------------------------------------------
2023-06-05 18:17:45,026 EPOCH 5 done: loss 0.1302 - lr 0.000003
2023-06-05 18:19:15,597 Evaluating as a multi-label problem: False
2023-06-05 18:19:15,669 DEV : loss 0.09538447111845016 - f1-score (micro avg)  0.9649
2023-06-05 18:19:15,777 BAD EPOCHS (no improvement): 4
2023-06-05 18:19:15,779 ----------------------------------------------------------------------------------------------------
2023-06-05 18:21:03,869 epoch 6 - iter 374/3747 - loss 0.09684822 - samples/sec: 13.85 - lr: 0.000003
2023-06-05 18:22:49,765 epoch 6 - iter 748/3747 - loss 0.10027881 - samples/sec: 14.13 - lr: 0.000003
2023-06-05 18:24:35,348 epoch 6 - iter 1122/3747 - loss 0.09962512 - samples/sec: 14.17 - lr: 0.000003
2023-06-05 18:26:20,478 epoch 6 - iter 1496/3747 - loss 0.10316808 - samples/sec: 14.24 - lr: 0.000003
2023-06-05 18:28:04,953 epoch 6 - iter 1870/3747 - loss 0.10137065 - samples/sec: 14.33 - lr: 0.000003
2023-06-05 18:29:48,987 epoch 6 - iter 2244/3747 - loss 0.10191420 - samples/sec: 14.39 - lr: 0.000002
2023-06-05 18:31:33,836 epoch 6 - iter 2618/3747 - loss 0.10107320 - samples/sec: 14.27 - lr: 0.000002
2023-06-05 18:33:18,071 epoch 6 - iter 2992/3747 - loss 0.10460395 - samples/sec: 14.36 - lr: 0.000002
2023-06-05 18:35:02,379 epoch 6 - iter 3366/3747 - loss 0.10375197 - samples/sec: 14.35 - lr: 0.000002
2023-06-05 18:36:47,544 epoch 6 - iter 3740/3747 - loss 0.10585260 - samples/sec: 14.23 - lr: 0.000002
2023-06-05 18:36:49,488 ----------------------------------------------------------------------------------------------------
2023-06-05 18:36:49,488 EPOCH 6 done: loss 0.1058 - lr 0.000002
2023-06-05 18:38:24,664 Evaluating as a multi-label problem: False
2023-06-05 18:38:24,731 DEV : loss 0.08746744692325592 - f1-score (micro avg)  0.9675
2023-06-05 18:38:24,862 BAD EPOCHS (no improvement): 4
2023-06-05 18:38:24,865 ----------------------------------------------------------------------------------------------------
2023-06-05 18:40:11,796 epoch 7 - iter 374/3747 - loss 0.08248727 - samples/sec: 14.00 - lr: 0.000002
2023-06-05 18:41:56,779 epoch 7 - iter 748/3747 - loss 0.09172687 - samples/sec: 14.26 - lr: 0.000002
2023-06-05 18:43:40,408 epoch 7 - iter 1122/3747 - loss 0.08872675 - samples/sec: 14.44 - lr: 0.000002
2023-06-05 18:45:23,913 epoch 7 - iter 1496/3747 - loss 0.08975253 - samples/sec: 14.46 - lr: 0.000002
2023-06-05 18:47:08,431 epoch 7 - iter 1870/3747 - loss 0.08817615 - samples/sec: 14.32 - lr: 0.000002
2023-06-05 18:48:57,318 epoch 7 - iter 2244/3747 - loss 0.08947862 - samples/sec: 13.74 - lr: 0.000002
2023-06-05 18:50:41,620 epoch 7 - iter 2618/3747 - loss 0.08755765 - samples/sec: 14.35 - lr: 0.000002
2023-06-05 18:52:27,689 epoch 7 - iter 2992/3747 - loss 0.09050745 - samples/sec: 14.11 - lr: 0.000002
2023-06-05 18:54:13,652 epoch 7 - iter 3366/3747 - loss 0.09114544 - samples/sec: 14.12 - lr: 0.000002
2023-06-05 18:56:01,154 epoch 7 - iter 3740/3747 - loss 0.09340334 - samples/sec: 13.92 - lr: 0.000002
2023-06-05 18:56:03,178 ----------------------------------------------------------------------------------------------------
2023-06-05 18:56:03,178 EPOCH 7 done: loss 0.0933 - lr 0.000002
2023-06-05 18:57:42,244 Evaluating as a multi-label problem: False
2023-06-05 18:57:42,310 DEV : loss 0.0959758460521698 - f1-score (micro avg)  0.9712
2023-06-05 18:57:42,425 BAD EPOCHS (no improvement): 4
2023-06-05 18:57:42,428 ----------------------------------------------------------------------------------------------------
2023-06-05 18:59:28,073 epoch 8 - iter 374/3747 - loss 0.10106758 - samples/sec: 14.17 - lr: 0.000002
2023-06-05 19:01:13,199 epoch 8 - iter 748/3747 - loss 0.09625798 - samples/sec: 14.24 - lr: 0.000002
2023-06-05 19:03:02,876 epoch 8 - iter 1122/3747 - loss 0.09234786 - samples/sec: 13.65 - lr: 0.000002
2023-06-05 19:04:48,421 epoch 8 - iter 1496/3747 - loss 0.09306334 - samples/sec: 14.18 - lr: 0.000001
2023-06-05 19:06:34,400 epoch 8 - iter 1870/3747 - loss 0.09012582 - samples/sec: 14.12 - lr: 0.000001
2023-06-05 19:08:19,962 epoch 8 - iter 2244/3747 - loss 0.08927010 - samples/sec: 14.18 - lr: 0.000001
2023-06-05 19:10:04,873 epoch 8 - iter 2618/3747 - loss 0.08539679 - samples/sec: 14.27 - lr: 0.000001
2023-06-05 19:11:50,485 epoch 8 - iter 2992/3747 - loss 0.08567314 - samples/sec: 14.17 - lr: 0.000001
2023-06-05 19:13:35,280 epoch 8 - iter 3366/3747 - loss 0.08443712 - samples/sec: 14.28 - lr: 0.000001
2023-06-05 19:15:21,724 epoch 8 - iter 3740/3747 - loss 0.08400902 - samples/sec: 14.06 - lr: 0.000001
2023-06-05 19:15:23,627 ----------------------------------------------------------------------------------------------------
2023-06-05 19:15:23,627 EPOCH 8 done: loss 0.0841 - lr 0.000001
2023-06-05 19:17:02,613 Evaluating as a multi-label problem: False
2023-06-05 19:17:02,682 DEV : loss 0.10392823815345764 - f1-score (micro avg)  0.9719
2023-06-05 19:17:02,825 BAD EPOCHS (no improvement): 4
2023-06-05 19:17:02,828 ----------------------------------------------------------------------------------------------------
2023-06-05 19:18:52,206 epoch 9 - iter 374/3747 - loss 0.07490896 - samples/sec: 13.68 - lr: 0.000001
2023-06-05 19:20:40,834 epoch 9 - iter 748/3747 - loss 0.07170033 - samples/sec: 13.78 - lr: 0.000001
2023-06-05 19:22:24,104 epoch 9 - iter 1122/3747 - loss 0.07795087 - samples/sec: 14.49 - lr: 0.000001
2023-06-05 19:24:12,908 epoch 9 - iter 1496/3747 - loss 0.07529775 - samples/sec: 13.75 - lr: 0.000001
2023-06-05 19:25:58,639 epoch 9 - iter 1870/3747 - loss 0.07875379 - samples/sec: 14.16 - lr: 0.000001
2023-06-05 19:27:44,974 epoch 9 - iter 2244/3747 - loss 0.07834907 - samples/sec: 14.07 - lr: 0.000001
2023-06-05 19:29:30,968 epoch 9 - iter 2618/3747 - loss 0.07861726 - samples/sec: 14.12 - lr: 0.000001
2023-06-05 19:31:15,393 epoch 9 - iter 2992/3747 - loss 0.07862489 - samples/sec: 14.33 - lr: 0.000001
2023-06-05 19:33:01,897 epoch 9 - iter 3366/3747 - loss 0.07721157 - samples/sec: 14.05 - lr: 0.000001
2023-06-05 19:34:46,688 epoch 9 - iter 3740/3747 - loss 0.07534128 - samples/sec: 14.28 - lr: 0.000001
2023-06-05 19:34:48,563 ----------------------------------------------------------------------------------------------------
2023-06-05 19:34:48,563 EPOCH 9 done: loss 0.0753 - lr 0.000001
2023-06-05 19:36:21,653 Evaluating as a multi-label problem: False
2023-06-05 19:36:21,718 DEV : loss 0.11130397021770477 - f1-score (micro avg)  0.9684
2023-06-05 19:36:21,856 BAD EPOCHS (no improvement): 4
2023-06-05 19:36:21,858 ----------------------------------------------------------------------------------------------------
2023-06-05 19:38:08,487 epoch 10 - iter 374/3747 - loss 0.07876880 - samples/sec: 14.04 - lr: 0.000001
2023-06-05 19:39:54,154 epoch 10 - iter 748/3747 - loss 0.07913268 - samples/sec: 14.16 - lr: 0.000000
2023-06-05 19:41:38,438 epoch 10 - iter 1122/3747 - loss 0.07529517 - samples/sec: 14.35 - lr: 0.000000
2023-06-05 19:43:22,609 epoch 10 - iter 1496/3747 - loss 0.07559077 - samples/sec: 14.37 - lr: 0.000000
2023-06-05 19:45:08,509 epoch 10 - iter 1870/3747 - loss 0.07444296 - samples/sec: 14.13 - lr: 0.000000
2023-06-05 19:46:53,183 epoch 10 - iter 2244/3747 - loss 0.07475923 - samples/sec: 14.30 - lr: 0.000000
2023-06-05 19:48:37,099 epoch 10 - iter 2618/3747 - loss 0.07490791 - samples/sec: 14.40 - lr: 0.000000
2023-06-05 19:50:25,600 epoch 10 - iter 2992/3747 - loss 0.07485022 - samples/sec: 13.79 - lr: 0.000000
2023-06-05 19:52:09,241 epoch 10 - iter 3366/3747 - loss 0.07318260 - samples/sec: 14.44 - lr: 0.000000
2023-06-05 19:53:59,741 epoch 10 - iter 3740/3747 - loss 0.07282612 - samples/sec: 13.54 - lr: 0.000000
2023-06-05 19:54:01,417 ----------------------------------------------------------------------------------------------------
2023-06-05 19:54:01,417 EPOCH 10 done: loss 0.0727 - lr 0.000000
2023-06-05 19:55:31,103 Evaluating as a multi-label problem: False
2023-06-05 19:55:31,171 DEV : loss 0.11146899312734604 - f1-score (micro avg)  0.9685
2023-06-05 19:55:31,306 BAD EPOCHS (no improvement): 4
2023-06-05 19:55:42,660 ----------------------------------------------------------------------------------------------------
2023-06-05 19:55:42,663 Testing using last state of model ...
2023-06-05 19:57:17,518 Evaluating as a multi-label problem: False
2023-06-05 19:57:17,590 0.9298	0.9444	0.937	0.9054
2023-06-05 19:57:17,590 
Results:
- F-score (micro) 0.937
- F-score (macro) 0.9231
- Accuracy 0.9054

By class:
              precision    recall  f1-score   support

         ORG     0.9166    0.9398    0.9281      1661
         LOC     0.9428    0.9478    0.9453      1668
         PER     0.9845    0.9814    0.9830      1617
        MISC     0.8121    0.8618    0.8362       702

   micro avg     0.9298    0.9444    0.9370      5648
   macro avg     0.9140    0.9327    0.9231      5648
weighted avg     0.9308    0.9444    0.9375      5648

2023-06-05 19:57:17,590 ----------------------------------------------------------------------------------------------------
