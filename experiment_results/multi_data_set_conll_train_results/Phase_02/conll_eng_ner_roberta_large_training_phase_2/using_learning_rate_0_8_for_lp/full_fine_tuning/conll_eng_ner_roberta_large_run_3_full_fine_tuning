2023-05-30 01:23:12,774 ----------------------------------------------------------------------------------------------------
2023-05-30 01:23:12,779 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-30 01:23:12,784 ----------------------------------------------------------------------------------------------------
2023-05-30 01:23:12,784 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-05-30 01:23:12,785 ----------------------------------------------------------------------------------------------------
2023-05-30 01:23:12,785 Parameters:
2023-05-30 01:23:12,785  - learning_rate: "0.000005"
2023-05-30 01:23:12,785  - mini_batch_size: "4"
2023-05-30 01:23:12,785  - patience: "3"
2023-05-30 01:23:12,785  - anneal_factor: "0.5"
2023-05-30 01:23:12,785  - max_epochs: "10"
2023-05-30 01:23:12,785  - shuffle: "True"
2023-05-30 01:23:12,785  - train_with_dev: "False"
2023-05-30 01:23:12,785  - batch_growth_annealing: "False"
2023-05-30 01:23:12,785 ----------------------------------------------------------------------------------------------------
2023-05-30 01:23:12,785 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_3_full_fine_tuning"
2023-05-30 01:23:12,785 ----------------------------------------------------------------------------------------------------
2023-05-30 01:23:12,786 Device: cuda:3
2023-05-30 01:23:12,786 ----------------------------------------------------------------------------------------------------
2023-05-30 01:23:12,786 Embeddings storage mode: none
2023-05-30 01:23:12,786 ----------------------------------------------------------------------------------------------------
2023-05-30 01:24:37,247 epoch 1 - iter 374/3747 - loss 1.43262716 - samples/sec: 17.72 - lr: 0.000000
2023-05-30 01:26:05,237 epoch 1 - iter 748/3747 - loss 1.18612546 - samples/sec: 17.01 - lr: 0.000001
2023-05-30 01:27:30,292 epoch 1 - iter 1122/3747 - loss 0.99118387 - samples/sec: 17.60 - lr: 0.000001
2023-05-30 01:28:56,542 epoch 1 - iter 1496/3747 - loss 0.86982887 - samples/sec: 17.35 - lr: 0.000002
2023-05-30 01:30:26,385 epoch 1 - iter 1870/3747 - loss 0.75764637 - samples/sec: 16.66 - lr: 0.000002
2023-05-30 01:31:53,380 epoch 1 - iter 2244/3747 - loss 0.67385487 - samples/sec: 17.20 - lr: 0.000003
2023-05-30 01:33:14,339 epoch 1 - iter 2618/3747 - loss 0.61644042 - samples/sec: 18.49 - lr: 0.000003
2023-05-30 01:34:40,438 epoch 1 - iter 2992/3747 - loss 0.57938113 - samples/sec: 17.38 - lr: 0.000004
2023-05-30 01:36:04,356 epoch 1 - iter 3366/3747 - loss 0.54600291 - samples/sec: 17.84 - lr: 0.000004
2023-05-30 01:37:25,068 epoch 1 - iter 3740/3747 - loss 0.50936000 - samples/sec: 18.54 - lr: 0.000005
2023-05-30 01:37:26,549 ----------------------------------------------------------------------------------------------------
2023-05-30 01:37:26,549 EPOCH 1 done: loss 0.5091 - lr 0.000005
2023-05-30 01:38:44,886 Evaluating as a multi-label problem: False
2023-05-30 01:38:44,933 DEV : loss 0.14314523339271545 - f1-score (micro avg)  0.9167
2023-05-30 01:38:45,037 BAD EPOCHS (no improvement): 4
2023-05-30 01:38:45,039 ----------------------------------------------------------------------------------------------------
2023-05-30 01:40:08,448 epoch 2 - iter 374/3747 - loss 0.23451746 - samples/sec: 17.95 - lr: 0.000005
2023-05-30 01:41:35,036 epoch 2 - iter 748/3747 - loss 0.23378956 - samples/sec: 17.29 - lr: 0.000005
2023-05-30 01:43:00,933 epoch 2 - iter 1122/3747 - loss 0.23266348 - samples/sec: 17.43 - lr: 0.000005
2023-05-30 01:44:26,963 epoch 2 - iter 1496/3747 - loss 0.23105291 - samples/sec: 17.40 - lr: 0.000005
2023-05-30 01:45:52,737 epoch 2 - iter 1870/3747 - loss 0.22723528 - samples/sec: 17.45 - lr: 0.000005
2023-05-30 01:47:19,452 epoch 2 - iter 2244/3747 - loss 0.22873904 - samples/sec: 17.26 - lr: 0.000005
2023-05-30 01:48:46,493 epoch 2 - iter 2618/3747 - loss 0.22164920 - samples/sec: 17.20 - lr: 0.000005
2023-05-30 01:50:11,510 epoch 2 - iter 2992/3747 - loss 0.22108423 - samples/sec: 17.61 - lr: 0.000005
2023-05-30 01:51:33,405 epoch 2 - iter 3366/3747 - loss 0.21979199 - samples/sec: 18.28 - lr: 0.000005
2023-05-30 01:52:56,919 epoch 2 - iter 3740/3747 - loss 0.21902867 - samples/sec: 17.92 - lr: 0.000004
2023-05-30 01:52:58,475 ----------------------------------------------------------------------------------------------------
2023-05-30 01:52:58,476 EPOCH 2 done: loss 0.2188 - lr 0.000004
2023-05-30 01:54:21,241 Evaluating as a multi-label problem: False
2023-05-30 01:54:21,314 DEV : loss 0.11211994290351868 - f1-score (micro avg)  0.9426
2023-05-30 01:54:21,467 BAD EPOCHS (no improvement): 4
2023-05-30 01:54:21,469 ----------------------------------------------------------------------------------------------------
2023-05-30 01:55:46,954 epoch 3 - iter 374/3747 - loss 0.24944414 - samples/sec: 17.51 - lr: 0.000004
2023-05-30 01:57:13,478 epoch 3 - iter 748/3747 - loss 0.22937520 - samples/sec: 17.30 - lr: 0.000004
2023-05-30 01:58:34,997 epoch 3 - iter 1122/3747 - loss 0.21528646 - samples/sec: 18.36 - lr: 0.000004
2023-05-30 01:59:56,546 epoch 3 - iter 1496/3747 - loss 0.20940686 - samples/sec: 18.35 - lr: 0.000004
2023-05-30 02:01:16,671 epoch 3 - iter 1870/3747 - loss 0.20357630 - samples/sec: 18.68 - lr: 0.000004
2023-05-30 02:02:42,220 epoch 3 - iter 2244/3747 - loss 0.19815186 - samples/sec: 17.50 - lr: 0.000004
2023-05-30 02:04:02,366 epoch 3 - iter 2618/3747 - loss 0.19197586 - samples/sec: 18.68 - lr: 0.000004
2023-05-30 02:05:17,703 epoch 3 - iter 2992/3747 - loss 0.18620375 - samples/sec: 19.87 - lr: 0.000004
2023-05-30 02:06:38,423 epoch 3 - iter 3366/3747 - loss 0.18545961 - samples/sec: 18.54 - lr: 0.000004
2023-05-30 02:08:01,018 epoch 3 - iter 3740/3747 - loss 0.18322404 - samples/sec: 18.12 - lr: 0.000004
2023-05-30 02:08:02,427 ----------------------------------------------------------------------------------------------------
2023-05-30 02:08:02,427 EPOCH 3 done: loss 0.1831 - lr 0.000004
2023-05-30 02:09:05,423 Evaluating as a multi-label problem: False
2023-05-30 02:09:05,488 DEV : loss 0.10053430497646332 - f1-score (micro avg)  0.9587
2023-05-30 02:09:05,651 BAD EPOCHS (no improvement): 4
2023-05-30 02:09:05,654 ----------------------------------------------------------------------------------------------------
2023-05-30 02:10:26,695 epoch 4 - iter 374/3747 - loss 0.17219008 - samples/sec: 18.47 - lr: 0.000004
2023-05-30 02:11:50,294 epoch 4 - iter 748/3747 - loss 0.16343310 - samples/sec: 17.90 - lr: 0.000004
2023-05-30 02:13:16,613 epoch 4 - iter 1122/3747 - loss 0.15817803 - samples/sec: 17.34 - lr: 0.000004
2023-05-30 02:14:43,147 epoch 4 - iter 1496/3747 - loss 0.15780607 - samples/sec: 17.30 - lr: 0.000004
2023-05-30 02:16:03,768 epoch 4 - iter 1870/3747 - loss 0.15271520 - samples/sec: 18.57 - lr: 0.000004
2023-05-30 02:17:24,947 epoch 4 - iter 2244/3747 - loss 0.14870648 - samples/sec: 18.44 - lr: 0.000004
2023-05-30 02:18:50,018 epoch 4 - iter 2618/3747 - loss 0.14540954 - samples/sec: 17.59 - lr: 0.000004
2023-05-30 02:20:09,950 epoch 4 - iter 2992/3747 - loss 0.14616116 - samples/sec: 18.73 - lr: 0.000003
2023-05-30 02:21:35,522 epoch 4 - iter 3366/3747 - loss 0.14427981 - samples/sec: 17.49 - lr: 0.000003
2023-05-30 02:22:52,757 epoch 4 - iter 3740/3747 - loss 0.14320106 - samples/sec: 19.38 - lr: 0.000003
2023-05-30 02:22:54,057 ----------------------------------------------------------------------------------------------------
2023-05-30 02:22:54,057 EPOCH 4 done: loss 0.1430 - lr 0.000003
2023-05-30 02:24:05,863 Evaluating as a multi-label problem: False
2023-05-30 02:24:05,935 DEV : loss 0.1042475476861 - f1-score (micro avg)  0.9613
2023-05-30 02:24:06,089 BAD EPOCHS (no improvement): 4
2023-05-30 02:24:06,092 ----------------------------------------------------------------------------------------------------
2023-05-30 02:25:31,370 epoch 5 - iter 374/3747 - loss 0.12545983 - samples/sec: 17.55 - lr: 0.000003
2023-05-30 02:27:00,376 epoch 5 - iter 748/3747 - loss 0.12256532 - samples/sec: 16.82 - lr: 0.000003
2023-05-30 02:28:25,363 epoch 5 - iter 1122/3747 - loss 0.12433955 - samples/sec: 17.61 - lr: 0.000003
2023-05-30 02:29:49,290 epoch 5 - iter 1496/3747 - loss 0.12830836 - samples/sec: 17.83 - lr: 0.000003
2023-05-30 02:31:13,580 epoch 5 - iter 1870/3747 - loss 0.12306510 - samples/sec: 17.76 - lr: 0.000003
2023-05-30 02:32:33,488 epoch 5 - iter 2244/3747 - loss 0.12153014 - samples/sec: 18.73 - lr: 0.000003
2023-05-30 02:33:56,829 epoch 5 - iter 2618/3747 - loss 0.12240373 - samples/sec: 17.96 - lr: 0.000003
2023-05-30 02:35:20,702 epoch 5 - iter 2992/3747 - loss 0.11963166 - samples/sec: 17.85 - lr: 0.000003
2023-05-30 02:36:46,352 epoch 5 - iter 3366/3747 - loss 0.11702458 - samples/sec: 17.48 - lr: 0.000003
2023-05-30 02:38:06,242 epoch 5 - iter 3740/3747 - loss 0.11461006 - samples/sec: 18.74 - lr: 0.000003
2023-05-30 02:38:07,846 ----------------------------------------------------------------------------------------------------
2023-05-30 02:38:07,846 EPOCH 5 done: loss 0.1146 - lr 0.000003
2023-05-30 02:39:21,178 Evaluating as a multi-label problem: False
2023-05-30 02:39:21,225 DEV : loss 0.0950525626540184 - f1-score (micro avg)  0.9672
2023-05-30 02:39:21,329 BAD EPOCHS (no improvement): 4
2023-05-30 02:39:21,331 ----------------------------------------------------------------------------------------------------
2023-05-30 02:40:39,168 epoch 6 - iter 374/3747 - loss 0.09746644 - samples/sec: 19.23 - lr: 0.000003
2023-05-30 02:42:02,076 epoch 6 - iter 748/3747 - loss 0.09297581 - samples/sec: 18.05 - lr: 0.000003
2023-05-30 02:43:26,803 epoch 6 - iter 1122/3747 - loss 0.09750766 - samples/sec: 17.67 - lr: 0.000003
2023-05-30 02:44:51,511 epoch 6 - iter 1496/3747 - loss 0.09602707 - samples/sec: 17.67 - lr: 0.000003
2023-05-30 02:46:14,700 epoch 6 - iter 1870/3747 - loss 0.10203806 - samples/sec: 17.99 - lr: 0.000003
2023-05-30 02:47:39,113 epoch 6 - iter 2244/3747 - loss 0.10155016 - samples/sec: 17.73 - lr: 0.000002
2023-05-30 02:48:58,684 epoch 6 - iter 2618/3747 - loss 0.09926572 - samples/sec: 18.81 - lr: 0.000002
2023-05-30 02:50:21,730 epoch 6 - iter 2992/3747 - loss 0.10063245 - samples/sec: 18.02 - lr: 0.000002
2023-05-30 02:51:47,278 epoch 6 - iter 3366/3747 - loss 0.10239810 - samples/sec: 17.50 - lr: 0.000002
2023-05-30 02:53:13,773 epoch 6 - iter 3740/3747 - loss 0.10074195 - samples/sec: 17.30 - lr: 0.000002
2023-05-30 02:53:15,406 ----------------------------------------------------------------------------------------------------
2023-05-30 02:53:15,407 EPOCH 6 done: loss 0.1007 - lr 0.000002
2023-05-30 02:54:36,509 Evaluating as a multi-label problem: False
2023-05-30 02:54:36,573 DEV : loss 0.09795873612165451 - f1-score (micro avg)  0.9672
2023-05-30 02:54:36,721 BAD EPOCHS (no improvement): 4
2023-05-30 02:54:36,724 ----------------------------------------------------------------------------------------------------
2023-05-30 02:56:01,082 epoch 7 - iter 374/3747 - loss 0.09233152 - samples/sec: 17.74 - lr: 0.000002
2023-05-30 02:57:24,515 epoch 7 - iter 748/3747 - loss 0.10356554 - samples/sec: 17.94 - lr: 0.000002
2023-05-30 02:58:49,158 epoch 7 - iter 1122/3747 - loss 0.09995523 - samples/sec: 17.68 - lr: 0.000002
2023-05-30 03:00:11,390 epoch 7 - iter 1496/3747 - loss 0.09430478 - samples/sec: 18.20 - lr: 0.000002
2023-05-30 03:01:36,154 epoch 7 - iter 1870/3747 - loss 0.09637048 - samples/sec: 17.66 - lr: 0.000002
2023-05-30 03:03:00,592 epoch 7 - iter 2244/3747 - loss 0.09657800 - samples/sec: 17.73 - lr: 0.000002
2023-05-30 03:04:24,262 epoch 7 - iter 2618/3747 - loss 0.09353903 - samples/sec: 17.89 - lr: 0.000002
2023-05-30 03:05:44,633 epoch 7 - iter 2992/3747 - loss 0.09381135 - samples/sec: 18.62 - lr: 0.000002
2023-05-30 03:07:08,876 epoch 7 - iter 3366/3747 - loss 0.09186346 - samples/sec: 17.77 - lr: 0.000002
2023-05-30 03:08:27,482 epoch 7 - iter 3740/3747 - loss 0.09163035 - samples/sec: 19.04 - lr: 0.000002
2023-05-30 03:08:28,932 ----------------------------------------------------------------------------------------------------
2023-05-30 03:08:28,933 EPOCH 7 done: loss 0.0916 - lr 0.000002
2023-05-30 03:09:38,085 Evaluating as a multi-label problem: False
2023-05-30 03:09:38,124 DEV : loss 0.08901438117027283 - f1-score (micro avg)  0.9684
2023-05-30 03:09:38,222 BAD EPOCHS (no improvement): 4
2023-05-30 03:09:38,225 ----------------------------------------------------------------------------------------------------
2023-05-30 03:10:53,349 epoch 8 - iter 374/3747 - loss 0.07011420 - samples/sec: 19.92 - lr: 0.000002
2023-05-30 03:12:15,732 epoch 8 - iter 748/3747 - loss 0.07947144 - samples/sec: 18.17 - lr: 0.000002
2023-05-30 03:13:34,432 epoch 8 - iter 1122/3747 - loss 0.08265699 - samples/sec: 19.02 - lr: 0.000002
2023-05-30 03:14:54,176 epoch 8 - iter 1496/3747 - loss 0.08223979 - samples/sec: 18.77 - lr: 0.000001
2023-05-30 03:16:16,182 epoch 8 - iter 1870/3747 - loss 0.07838376 - samples/sec: 18.25 - lr: 0.000001
2023-05-30 03:17:42,130 epoch 8 - iter 2244/3747 - loss 0.07848288 - samples/sec: 17.41 - lr: 0.000001
2023-05-30 03:19:05,363 epoch 8 - iter 2618/3747 - loss 0.07933646 - samples/sec: 17.98 - lr: 0.000001
2023-05-30 03:20:30,301 epoch 8 - iter 2992/3747 - loss 0.07949261 - samples/sec: 17.62 - lr: 0.000001
2023-05-30 03:21:45,303 epoch 8 - iter 3366/3747 - loss 0.07872597 - samples/sec: 19.96 - lr: 0.000001
2023-05-30 03:22:59,725 epoch 8 - iter 3740/3747 - loss 0.07817570 - samples/sec: 20.11 - lr: 0.000001
2023-05-30 03:23:01,252 ----------------------------------------------------------------------------------------------------
2023-05-30 03:23:01,252 EPOCH 8 done: loss 0.0781 - lr 0.000001
2023-05-30 03:24:10,575 Evaluating as a multi-label problem: False
2023-05-30 03:24:10,614 DEV : loss 0.10287363827228546 - f1-score (micro avg)  0.9699
2023-05-30 03:24:10,716 BAD EPOCHS (no improvement): 4
2023-05-30 03:24:10,719 ----------------------------------------------------------------------------------------------------
2023-05-30 03:25:35,226 epoch 9 - iter 374/3747 - loss 0.06533303 - samples/sec: 17.71 - lr: 0.000001
2023-05-30 03:26:55,455 epoch 9 - iter 748/3747 - loss 0.07276429 - samples/sec: 18.66 - lr: 0.000001
2023-05-30 03:28:11,308 epoch 9 - iter 1122/3747 - loss 0.07969250 - samples/sec: 19.73 - lr: 0.000001
2023-05-30 03:29:36,196 epoch 9 - iter 1496/3747 - loss 0.07832396 - samples/sec: 17.63 - lr: 0.000001
2023-05-30 03:30:59,357 epoch 9 - iter 1870/3747 - loss 0.07692683 - samples/sec: 18.00 - lr: 0.000001
2023-05-30 03:32:22,979 epoch 9 - iter 2244/3747 - loss 0.07583840 - samples/sec: 17.90 - lr: 0.000001
2023-05-30 03:33:45,951 epoch 9 - iter 2618/3747 - loss 0.07558423 - samples/sec: 18.04 - lr: 0.000001
2023-05-30 03:35:09,248 epoch 9 - iter 2992/3747 - loss 0.07596159 - samples/sec: 17.97 - lr: 0.000001
2023-05-30 03:36:32,472 epoch 9 - iter 3366/3747 - loss 0.07457371 - samples/sec: 17.98 - lr: 0.000001
2023-05-30 03:37:56,134 epoch 9 - iter 3740/3747 - loss 0.07448932 - samples/sec: 17.89 - lr: 0.000001
2023-05-30 03:37:57,604 ----------------------------------------------------------------------------------------------------
2023-05-30 03:37:57,605 EPOCH 9 done: loss 0.0746 - lr 0.000001
2023-05-30 03:39:21,343 Evaluating as a multi-label problem: False
2023-05-30 03:39:21,403 DEV : loss 0.1034218892455101 - f1-score (micro avg)  0.9706
2023-05-30 03:39:21,548 BAD EPOCHS (no improvement): 4
2023-05-30 03:39:21,552 ----------------------------------------------------------------------------------------------------
2023-05-30 03:40:47,826 epoch 10 - iter 374/3747 - loss 0.07227470 - samples/sec: 17.35 - lr: 0.000001
2023-05-30 03:42:08,237 epoch 10 - iter 748/3747 - loss 0.06883856 - samples/sec: 18.61 - lr: 0.000000
2023-05-30 03:43:27,655 epoch 10 - iter 1122/3747 - loss 0.07087755 - samples/sec: 18.85 - lr: 0.000000
2023-05-30 03:44:49,851 epoch 10 - iter 1496/3747 - loss 0.06997462 - samples/sec: 18.21 - lr: 0.000000
2023-05-30 03:46:09,871 epoch 10 - iter 1870/3747 - loss 0.06836850 - samples/sec: 18.70 - lr: 0.000000
2023-05-30 03:47:24,740 epoch 10 - iter 2244/3747 - loss 0.06602226 - samples/sec: 19.99 - lr: 0.000000
2023-05-30 03:48:45,565 epoch 10 - iter 2618/3747 - loss 0.06491841 - samples/sec: 18.52 - lr: 0.000000
2023-05-30 03:50:06,859 epoch 10 - iter 2992/3747 - loss 0.06666778 - samples/sec: 18.41 - lr: 0.000000
2023-05-30 03:51:23,110 epoch 10 - iter 3366/3747 - loss 0.06798477 - samples/sec: 19.63 - lr: 0.000000
2023-05-30 03:52:42,953 epoch 10 - iter 3740/3747 - loss 0.06902367 - samples/sec: 18.74 - lr: 0.000000
2023-05-30 03:52:44,420 ----------------------------------------------------------------------------------------------------
2023-05-30 03:52:44,420 EPOCH 10 done: loss 0.0689 - lr 0.000000
2023-05-30 03:54:03,841 Evaluating as a multi-label problem: False
2023-05-30 03:54:03,904 DEV : loss 0.11023669689893723 - f1-score (micro avg)  0.9707
2023-05-30 03:54:04,004 BAD EPOCHS (no improvement): 4
2023-05-30 03:54:16,701 ----------------------------------------------------------------------------------------------------
2023-05-30 03:54:16,704 Testing using last state of model ...
2023-05-30 03:55:33,830 Evaluating as a multi-label problem: False
2023-05-30 03:55:33,868 0.9269	0.9451	0.9359	0.9029
2023-05-30 03:55:33,868 
Results:
- F-score (micro) 0.9359
- F-score (macro) 0.9221
- Accuracy 0.9029

By class:
              precision    recall  f1-score   support

         ORG     0.9186    0.9374    0.9279      1661
         LOC     0.9410    0.9466    0.9438      1668
         PER     0.9851    0.9808    0.9830      1617
        MISC     0.7938    0.8775    0.8336       702

   micro avg     0.9269    0.9451    0.9359      5648
   macro avg     0.9096    0.9356    0.9221      5648
weighted avg     0.9287    0.9451    0.9366      5648

2023-05-30 03:55:33,868 ----------------------------------------------------------------------------------------------------
