2023-06-06 20:26:20,214 ----------------------------------------------------------------------------------------------------
2023-06-06 20:26:20,218 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-06 20:26:20,219 ----------------------------------------------------------------------------------------------------
2023-06-06 20:26:20,219 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-06 20:26:20,219 ----------------------------------------------------------------------------------------------------
2023-06-06 20:26:20,219 Parameters:
2023-06-06 20:26:20,220  - learning_rate: "0.300000"
2023-06-06 20:26:20,220  - mini_batch_size: "32"
2023-06-06 20:26:20,220  - patience: "3"
2023-06-06 20:26:20,220  - anneal_factor: "0.5"
2023-06-06 20:26:20,220  - max_epochs: "10"
2023-06-06 20:26:20,220  - shuffle: "True"
2023-06-06 20:26:20,220  - train_with_dev: "False"
2023-06-06 20:26:20,220  - batch_growth_annealing: "False"
2023-06-06 20:26:20,220 ----------------------------------------------------------------------------------------------------
2023-06-06 20:26:20,220 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_3_linear_probing"
2023-06-06 20:26:20,220 ----------------------------------------------------------------------------------------------------
2023-06-06 20:26:20,220 Device: cuda:1
2023-06-06 20:26:20,220 ----------------------------------------------------------------------------------------------------
2023-06-06 20:26:20,220 Embeddings storage mode: none
2023-06-06 20:26:20,220 ----------------------------------------------------------------------------------------------------
2023-06-06 20:26:41,577 epoch 1 - iter 46/469 - loss 1.44779799 - samples/sec: 68.95 - lr: 0.029424
2023-06-06 20:27:04,687 epoch 1 - iter 92/469 - loss 1.19727410 - samples/sec: 63.72 - lr: 0.058849
2023-06-06 20:27:28,925 epoch 1 - iter 138/469 - loss 1.23938759 - samples/sec: 60.75 - lr: 0.088273
2023-06-06 20:27:50,126 epoch 1 - iter 184/469 - loss 1.55521592 - samples/sec: 69.46 - lr: 0.117697
2023-06-06 20:28:12,187 epoch 1 - iter 230/469 - loss 1.78968184 - samples/sec: 66.74 - lr: 0.147122
2023-06-06 20:28:37,520 epoch 1 - iter 276/469 - loss 1.98390118 - samples/sec: 58.12 - lr: 0.176546
2023-06-06 20:28:59,250 epoch 1 - iter 322/469 - loss 2.18258412 - samples/sec: 67.76 - lr: 0.205970
2023-06-06 20:29:22,681 epoch 1 - iter 368/469 - loss 2.55244268 - samples/sec: 62.84 - lr: 0.235394
2023-06-06 20:29:47,721 epoch 1 - iter 414/469 - loss 2.77413398 - samples/sec: 58.80 - lr: 0.264819
2023-06-06 20:30:11,364 epoch 1 - iter 460/469 - loss 3.02850989 - samples/sec: 62.28 - lr: 0.294243
2023-06-06 20:30:15,605 ----------------------------------------------------------------------------------------------------
2023-06-06 20:30:15,605 EPOCH 1 done: loss 3.0718 - lr 0.294243
2023-06-06 20:31:37,886 Evaluating as a multi-label problem: False
2023-06-06 20:31:37,943 DEV : loss 2.3291640281677246 - f1-score (micro avg)  0.6862
2023-06-06 20:31:38,050 BAD EPOCHS (no improvement): 4
2023-06-06 20:31:38,053 ----------------------------------------------------------------------------------------------------
2023-06-06 20:32:01,258 epoch 2 - iter 46/469 - loss 5.74493674 - samples/sec: 63.46 - lr: 0.296733
2023-06-06 20:32:27,872 epoch 2 - iter 92/469 - loss 5.97070867 - samples/sec: 55.33 - lr: 0.293466
2023-06-06 20:32:49,798 epoch 2 - iter 138/469 - loss 6.07858541 - samples/sec: 67.16 - lr: 0.290199
2023-06-06 20:33:11,750 epoch 2 - iter 184/469 - loss 6.06294234 - samples/sec: 67.08 - lr: 0.286932
2023-06-06 20:33:38,309 epoch 2 - iter 230/469 - loss 6.08720991 - samples/sec: 55.44 - lr: 0.283665
2023-06-06 20:34:02,559 epoch 2 - iter 276/469 - loss 5.98820325 - samples/sec: 60.72 - lr: 0.280398
2023-06-06 20:34:26,492 epoch 2 - iter 322/469 - loss 5.97380696 - samples/sec: 61.53 - lr: 0.277131
2023-06-06 20:34:53,497 epoch 2 - iter 368/469 - loss 5.91795013 - samples/sec: 54.53 - lr: 0.273864
2023-06-06 20:35:17,800 epoch 2 - iter 414/469 - loss 5.88757214 - samples/sec: 60.59 - lr: 0.270597
2023-06-06 20:35:38,138 epoch 2 - iter 460/469 - loss 5.87437161 - samples/sec: 72.40 - lr: 0.267330
2023-06-06 20:35:43,931 ----------------------------------------------------------------------------------------------------
2023-06-06 20:35:43,931 EPOCH 2 done: loss 5.8723 - lr 0.267330
2023-06-06 20:37:06,255 Evaluating as a multi-label problem: False
2023-06-06 20:37:06,324 DEV : loss 1.9898535013198853 - f1-score (micro avg)  0.7315
2023-06-06 20:37:06,433 BAD EPOCHS (no improvement): 4
2023-06-06 20:37:06,436 ----------------------------------------------------------------------------------------------------
2023-06-06 20:37:29,921 epoch 3 - iter 46/469 - loss 4.79082235 - samples/sec: 62.71 - lr: 0.263423
2023-06-06 20:37:53,514 epoch 3 - iter 92/469 - loss 5.05672337 - samples/sec: 62.41 - lr: 0.260156
2023-06-06 20:38:16,554 epoch 3 - iter 138/469 - loss 5.12479965 - samples/sec: 63.91 - lr: 0.256889
2023-06-06 20:38:38,426 epoch 3 - iter 184/469 - loss 5.09771839 - samples/sec: 67.32 - lr: 0.253622
2023-06-06 20:39:02,406 epoch 3 - iter 230/469 - loss 5.11262512 - samples/sec: 61.41 - lr: 0.250355
2023-06-06 20:39:29,327 epoch 3 - iter 276/469 - loss 5.08926255 - samples/sec: 54.70 - lr: 0.247088
2023-06-06 20:39:52,356 epoch 3 - iter 322/469 - loss 5.06611436 - samples/sec: 63.94 - lr: 0.243821
2023-06-06 20:40:14,740 epoch 3 - iter 368/469 - loss 5.03987086 - samples/sec: 65.78 - lr: 0.240554
2023-06-06 20:40:41,474 epoch 3 - iter 414/469 - loss 5.03990218 - samples/sec: 55.08 - lr: 0.237287
2023-06-06 20:41:05,565 epoch 3 - iter 460/469 - loss 4.99783884 - samples/sec: 61.12 - lr: 0.234020
2023-06-06 20:41:10,139 ----------------------------------------------------------------------------------------------------
2023-06-06 20:41:10,140 EPOCH 3 done: loss 4.9978 - lr 0.234020
2023-06-06 20:42:35,424 Evaluating as a multi-label problem: False
2023-06-06 20:42:35,498 DEV : loss 1.5522565841674805 - f1-score (micro avg)  0.7363
2023-06-06 20:42:35,641 BAD EPOCHS (no improvement): 4
2023-06-06 20:42:35,643 ----------------------------------------------------------------------------------------------------
2023-06-06 20:42:59,558 epoch 4 - iter 46/469 - loss 4.98204769 - samples/sec: 61.58 - lr: 0.230114
2023-06-06 20:43:25,473 epoch 4 - iter 92/469 - loss 4.93159673 - samples/sec: 56.82 - lr: 0.226847
2023-06-06 20:43:49,764 epoch 4 - iter 138/469 - loss 4.70872101 - samples/sec: 60.62 - lr: 0.223580
2023-06-06 20:44:13,569 epoch 4 - iter 184/469 - loss 4.62470416 - samples/sec: 61.85 - lr: 0.220312
2023-06-06 20:44:40,660 epoch 4 - iter 230/469 - loss 4.55329639 - samples/sec: 54.35 - lr: 0.217045
2023-06-06 20:45:04,795 epoch 4 - iter 276/469 - loss 4.54731910 - samples/sec: 61.01 - lr: 0.213778
2023-06-06 20:45:27,414 epoch 4 - iter 322/469 - loss 4.55423223 - samples/sec: 65.10 - lr: 0.210511
2023-06-06 20:45:54,873 epoch 4 - iter 368/469 - loss 4.56064079 - samples/sec: 53.62 - lr: 0.207244
2023-06-06 20:46:17,430 epoch 4 - iter 414/469 - loss 4.53501877 - samples/sec: 65.28 - lr: 0.203977
2023-06-06 20:46:39,554 epoch 4 - iter 460/469 - loss 4.47302037 - samples/sec: 66.56 - lr: 0.200710
2023-06-06 20:46:44,047 ----------------------------------------------------------------------------------------------------
2023-06-06 20:46:44,048 EPOCH 4 done: loss 4.4650 - lr 0.200710
2023-06-06 20:48:08,431 Evaluating as a multi-label problem: False
2023-06-06 20:48:08,500 DEV : loss 1.5018619298934937 - f1-score (micro avg)  0.7568
2023-06-06 20:48:08,605 BAD EPOCHS (no improvement): 4
2023-06-06 20:48:08,607 ----------------------------------------------------------------------------------------------------
2023-06-06 20:48:34,662 epoch 5 - iter 46/469 - loss 4.43113993 - samples/sec: 56.52 - lr: 0.196804
2023-06-06 20:48:59,175 epoch 5 - iter 92/469 - loss 4.17106887 - samples/sec: 60.07 - lr: 0.193537
2023-06-06 20:49:25,854 epoch 5 - iter 138/469 - loss 4.12778509 - samples/sec: 55.19 - lr: 0.190270
2023-06-06 20:49:50,045 epoch 5 - iter 184/469 - loss 4.07351932 - samples/sec: 60.87 - lr: 0.187003
2023-06-06 20:50:14,895 epoch 5 - iter 230/469 - loss 3.98553238 - samples/sec: 59.26 - lr: 0.183736
2023-06-06 20:50:40,375 epoch 5 - iter 276/469 - loss 4.00186358 - samples/sec: 57.79 - lr: 0.180469
2023-06-06 20:51:05,143 epoch 5 - iter 322/469 - loss 3.97108604 - samples/sec: 59.45 - lr: 0.177202
2023-06-06 20:51:27,736 epoch 5 - iter 368/469 - loss 3.97594310 - samples/sec: 65.18 - lr: 0.173935
2023-06-06 20:51:53,447 epoch 5 - iter 414/469 - loss 3.97690598 - samples/sec: 57.27 - lr: 0.170668
2023-06-06 20:52:16,484 epoch 5 - iter 460/469 - loss 3.94017558 - samples/sec: 63.92 - lr: 0.167401
2023-06-06 20:52:20,524 ----------------------------------------------------------------------------------------------------
2023-06-06 20:52:20,525 EPOCH 5 done: loss 3.9349 - lr 0.167401
2023-06-06 20:53:44,961 Evaluating as a multi-label problem: False
2023-06-06 20:53:45,033 DEV : loss 1.3932946920394897 - f1-score (micro avg)  0.7412
2023-06-06 20:53:45,143 BAD EPOCHS (no improvement): 4
2023-06-06 20:53:45,146 ----------------------------------------------------------------------------------------------------
2023-06-06 20:54:07,802 epoch 6 - iter 46/469 - loss 3.47821312 - samples/sec: 65.00 - lr: 0.163494
2023-06-06 20:54:34,125 epoch 6 - iter 92/469 - loss 3.50649835 - samples/sec: 55.94 - lr: 0.160227
2023-06-06 20:54:56,962 epoch 6 - iter 138/469 - loss 3.63310731 - samples/sec: 64.48 - lr: 0.156960
2023-06-06 20:55:20,647 epoch 6 - iter 184/469 - loss 3.58744177 - samples/sec: 62.17 - lr: 0.153693
2023-06-06 20:55:45,882 epoch 6 - iter 230/469 - loss 3.58554695 - samples/sec: 58.35 - lr: 0.150426
2023-06-06 20:56:09,299 epoch 6 - iter 276/469 - loss 3.53841941 - samples/sec: 62.88 - lr: 0.147159
2023-06-06 20:56:33,604 epoch 6 - iter 322/469 - loss 3.49754118 - samples/sec: 60.59 - lr: 0.143892
2023-06-06 20:57:00,423 epoch 6 - iter 368/469 - loss 3.45467974 - samples/sec: 54.90 - lr: 0.140625
2023-06-06 20:57:23,887 epoch 6 - iter 414/469 - loss 3.42627127 - samples/sec: 62.76 - lr: 0.137358
2023-06-06 20:57:47,316 epoch 6 - iter 460/469 - loss 3.40370090 - samples/sec: 62.85 - lr: 0.134091
2023-06-06 20:57:51,933 ----------------------------------------------------------------------------------------------------
2023-06-06 20:57:51,933 EPOCH 6 done: loss 3.4066 - lr 0.134091
2023-06-06 20:59:16,985 Evaluating as a multi-label problem: False
2023-06-06 20:59:17,053 DEV : loss 1.2245146036148071 - f1-score (micro avg)  0.7425
2023-06-06 20:59:17,160 BAD EPOCHS (no improvement): 4
2023-06-06 20:59:17,164 ----------------------------------------------------------------------------------------------------
2023-06-06 20:59:43,129 epoch 7 - iter 46/469 - loss 3.16605373 - samples/sec: 56.72 - lr: 0.130185
2023-06-06 21:00:07,824 epoch 7 - iter 92/469 - loss 2.95286518 - samples/sec: 59.63 - lr: 0.126918
2023-06-06 21:00:33,995 epoch 7 - iter 138/469 - loss 2.92005011 - samples/sec: 56.26 - lr: 0.123651
2023-06-06 21:00:57,933 epoch 7 - iter 184/469 - loss 2.88891363 - samples/sec: 61.51 - lr: 0.120384
2023-06-06 21:01:21,428 epoch 7 - iter 230/469 - loss 2.88992314 - samples/sec: 62.67 - lr: 0.117116
2023-06-06 21:01:46,402 epoch 7 - iter 276/469 - loss 2.87783078 - samples/sec: 58.96 - lr: 0.113849
2023-06-06 21:02:10,715 epoch 7 - iter 322/469 - loss 2.90696502 - samples/sec: 60.56 - lr: 0.110582
2023-06-06 21:02:34,398 epoch 7 - iter 368/469 - loss 2.89648261 - samples/sec: 62.18 - lr: 0.107315
2023-06-06 21:03:00,980 epoch 7 - iter 414/469 - loss 2.87923495 - samples/sec: 55.39 - lr: 0.104048
2023-06-06 21:03:25,086 epoch 7 - iter 460/469 - loss 2.87608528 - samples/sec: 61.08 - lr: 0.100781
2023-06-06 21:03:29,021 ----------------------------------------------------------------------------------------------------
2023-06-06 21:03:29,021 EPOCH 7 done: loss 2.8777 - lr 0.100781
2023-06-06 21:04:52,711 Evaluating as a multi-label problem: False
2023-06-06 21:04:52,780 DEV : loss 0.84297776222229 - f1-score (micro avg)  0.7685
2023-06-06 21:04:52,888 BAD EPOCHS (no improvement): 4
2023-06-06 21:04:52,891 ----------------------------------------------------------------------------------------------------
2023-06-06 21:05:16,546 epoch 8 - iter 46/469 - loss 2.64323941 - samples/sec: 62.26 - lr: 0.096875
2023-06-06 21:05:43,497 epoch 8 - iter 92/469 - loss 2.52435562 - samples/sec: 54.63 - lr: 0.093608
2023-06-06 21:06:07,613 epoch 8 - iter 138/469 - loss 2.46359171 - samples/sec: 61.06 - lr: 0.090341
2023-06-06 21:06:31,850 epoch 8 - iter 184/469 - loss 2.42897477 - samples/sec: 60.76 - lr: 0.087074
2023-06-06 21:06:58,491 epoch 8 - iter 230/469 - loss 2.39261302 - samples/sec: 55.27 - lr: 0.083807
2023-06-06 21:07:21,141 epoch 8 - iter 276/469 - loss 2.34354906 - samples/sec: 65.01 - lr: 0.080540
2023-06-06 21:07:44,849 epoch 8 - iter 322/469 - loss 2.31184558 - samples/sec: 62.11 - lr: 0.077273
2023-06-06 21:08:12,420 epoch 8 - iter 368/469 - loss 2.25326556 - samples/sec: 53.41 - lr: 0.074006
2023-06-06 21:08:35,857 epoch 8 - iter 414/469 - loss 2.20440629 - samples/sec: 62.83 - lr: 0.070739
2023-06-06 21:08:59,705 epoch 8 - iter 460/469 - loss 2.17764430 - samples/sec: 61.75 - lr: 0.067472
2023-06-06 21:09:04,089 ----------------------------------------------------------------------------------------------------
2023-06-06 21:09:04,090 EPOCH 8 done: loss 2.1757 - lr 0.067472
2023-06-06 21:10:33,108 Evaluating as a multi-label problem: False
2023-06-06 21:10:33,187 DEV : loss 0.6844229698181152 - f1-score (micro avg)  0.7547
2023-06-06 21:10:33,301 BAD EPOCHS (no improvement): 4
2023-06-06 21:10:33,304 ----------------------------------------------------------------------------------------------------
2023-06-06 21:10:56,383 epoch 9 - iter 46/469 - loss 1.93452968 - samples/sec: 63.81 - lr: 0.063565
2023-06-06 21:11:19,571 epoch 9 - iter 92/469 - loss 1.93635446 - samples/sec: 63.50 - lr: 0.060298
2023-06-06 21:11:41,996 epoch 9 - iter 138/469 - loss 1.85676601 - samples/sec: 65.66 - lr: 0.057031
2023-06-06 21:12:08,479 epoch 9 - iter 184/469 - loss 1.83689012 - samples/sec: 55.60 - lr: 0.053764
2023-06-06 21:12:30,052 epoch 9 - iter 230/469 - loss 1.80866586 - samples/sec: 68.26 - lr: 0.050497
2023-06-06 21:12:52,444 epoch 9 - iter 276/469 - loss 1.75949694 - samples/sec: 65.76 - lr: 0.047230
2023-06-06 21:13:18,074 epoch 9 - iter 322/469 - loss 1.72454564 - samples/sec: 57.45 - lr: 0.043963
2023-06-06 21:13:42,124 epoch 9 - iter 368/469 - loss 1.69522383 - samples/sec: 61.23 - lr: 0.040696
2023-06-06 21:14:04,683 epoch 9 - iter 414/469 - loss 1.67500564 - samples/sec: 65.27 - lr: 0.037429
2023-06-06 21:14:31,169 epoch 9 - iter 460/469 - loss 1.63888085 - samples/sec: 55.59 - lr: 0.034162
2023-06-06 21:14:35,434 ----------------------------------------------------------------------------------------------------
2023-06-06 21:14:35,434 EPOCH 9 done: loss 1.6307 - lr 0.034162
2023-06-06 21:15:59,103 Evaluating as a multi-label problem: False
2023-06-06 21:15:59,169 DEV : loss 0.48227494955062866 - f1-score (micro avg)  0.7783
2023-06-06 21:15:59,275 BAD EPOCHS (no improvement): 4
2023-06-06 21:15:59,277 ----------------------------------------------------------------------------------------------------
2023-06-06 21:16:22,872 epoch 10 - iter 46/469 - loss 1.28620621 - samples/sec: 62.41 - lr: 0.030256
2023-06-06 21:16:48,641 epoch 10 - iter 92/469 - loss 1.25404563 - samples/sec: 57.14 - lr: 0.026989
2023-06-06 21:17:10,299 epoch 10 - iter 138/469 - loss 1.23041509 - samples/sec: 67.99 - lr: 0.023722
2023-06-06 21:17:34,153 epoch 10 - iter 184/469 - loss 1.20205023 - samples/sec: 61.73 - lr: 0.020455
2023-06-06 21:18:00,389 epoch 10 - iter 230/469 - loss 1.17968805 - samples/sec: 56.12 - lr: 0.017187
2023-06-06 21:18:25,439 epoch 10 - iter 276/469 - loss 1.14448924 - samples/sec: 58.78 - lr: 0.013920
2023-06-06 21:18:47,686 epoch 10 - iter 322/469 - loss 1.11923780 - samples/sec: 66.19 - lr: 0.010653
2023-06-06 21:19:13,354 epoch 10 - iter 368/469 - loss 1.09310692 - samples/sec: 57.36 - lr: 0.007386
2023-06-06 21:19:36,775 epoch 10 - iter 414/469 - loss 1.07208097 - samples/sec: 62.87 - lr: 0.004119
2023-06-06 21:19:57,987 epoch 10 - iter 460/469 - loss 1.05035167 - samples/sec: 69.42 - lr: 0.000852
2023-06-06 21:20:02,215 ----------------------------------------------------------------------------------------------------
2023-06-06 21:20:02,216 EPOCH 10 done: loss 1.0469 - lr 0.000852
2023-06-06 21:21:27,466 Evaluating as a multi-label problem: False
2023-06-06 21:21:27,532 DEV : loss 0.32719728350639343 - f1-score (micro avg)  0.8092
2023-06-06 21:21:27,667 BAD EPOCHS (no improvement): 4
2023-06-06 21:21:47,030 ----------------------------------------------------------------------------------------------------
2023-06-06 21:21:47,033 Testing using last state of model ...
2023-06-06 21:23:12,000 Evaluating as a multi-label problem: False
2023-06-06 21:23:12,066 0.8026	0.7682	0.7851	0.6819
2023-06-06 21:23:12,066 
Results:
- F-score (micro) 0.7851
- F-score (macro) 0.7696
- Accuracy 0.6819

By class:
              precision    recall  f1-score   support

         LOC     0.7888    0.7860    0.7874      1668
         PER     0.9326    0.9412    0.9369      1617
         ORG     0.7066    0.6321    0.6673      1661
        MISC     0.7284    0.6496    0.6867       702

   micro avg     0.8026    0.7682    0.7851      5648
   macro avg     0.7891    0.7522    0.7696      5648
weighted avg     0.7983    0.7682    0.7824      5648

2023-06-06 21:23:12,066 ----------------------------------------------------------------------------------------------------
