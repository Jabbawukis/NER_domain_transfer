2023-06-02 15:18:30,387 ----------------------------------------------------------------------------------------------------
2023-06-02 15:18:30,392 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-02 15:18:30,393 ----------------------------------------------------------------------------------------------------
2023-06-02 15:18:30,393 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-02 15:18:30,393 ----------------------------------------------------------------------------------------------------
2023-06-02 15:18:30,393 Parameters:
2023-06-02 15:18:30,393  - learning_rate: "0.300000"
2023-06-02 15:18:30,393  - mini_batch_size: "32"
2023-06-02 15:18:30,393  - patience: "3"
2023-06-02 15:18:30,393  - anneal_factor: "0.5"
2023-06-02 15:18:30,393  - max_epochs: "10"
2023-06-02 15:18:30,393  - shuffle: "True"
2023-06-02 15:18:30,393  - train_with_dev: "False"
2023-06-02 15:18:30,393  - batch_growth_annealing: "False"
2023-06-02 15:18:30,394 ----------------------------------------------------------------------------------------------------
2023-06-02 15:18:30,394 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_2_linear_probing"
2023-06-02 15:18:30,394 ----------------------------------------------------------------------------------------------------
2023-06-02 15:18:30,394 Device: cuda:0
2023-06-02 15:18:30,394 ----------------------------------------------------------------------------------------------------
2023-06-02 15:18:30,394 Embeddings storage mode: none
2023-06-02 15:18:30,394 ----------------------------------------------------------------------------------------------------
2023-06-02 15:19:00,588 epoch 1 - iter 46/469 - loss 1.38907423 - samples/sec: 48.77 - lr: 0.029424
2023-06-02 15:19:27,251 epoch 1 - iter 92/469 - loss 1.17150642 - samples/sec: 55.22 - lr: 0.058849
2023-06-02 15:19:53,000 epoch 1 - iter 138/469 - loss 1.23925767 - samples/sec: 57.18 - lr: 0.088273
2023-06-02 15:20:21,330 epoch 1 - iter 184/469 - loss 1.53796125 - samples/sec: 51.97 - lr: 0.117697
2023-06-02 15:20:49,765 epoch 1 - iter 230/469 - loss 1.77394122 - samples/sec: 51.78 - lr: 0.147122
2023-06-02 15:21:17,246 epoch 1 - iter 276/469 - loss 1.90958615 - samples/sec: 53.58 - lr: 0.176546
2023-06-02 15:21:48,441 epoch 1 - iter 322/469 - loss 2.16033532 - samples/sec: 47.20 - lr: 0.205970
2023-06-02 15:22:15,645 epoch 1 - iter 368/469 - loss 2.56658815 - samples/sec: 54.12 - lr: 0.235394
2023-06-02 15:22:44,075 epoch 1 - iter 414/469 - loss 2.74829647 - samples/sec: 51.79 - lr: 0.264819
2023-06-02 15:23:14,755 epoch 1 - iter 460/469 - loss 2.94062797 - samples/sec: 47.99 - lr: 0.294243
2023-06-02 15:23:19,728 ----------------------------------------------------------------------------------------------------
2023-06-02 15:23:19,728 EPOCH 1 done: loss 2.9967 - lr 0.294243
2023-06-02 15:24:51,851 Evaluating as a multi-label problem: False
2023-06-02 15:24:51,920 DEV : loss 2.563961982727051 - f1-score (micro avg)  0.6508
2023-06-02 15:24:52,031 BAD EPOCHS (no improvement): 4
2023-06-02 15:24:52,034 ----------------------------------------------------------------------------------------------------
2023-06-02 15:25:19,880 epoch 2 - iter 46/469 - loss 5.49536257 - samples/sec: 52.88 - lr: 0.296733
2023-06-02 15:25:47,674 epoch 2 - iter 92/469 - loss 5.40567679 - samples/sec: 52.98 - lr: 0.293466
2023-06-02 15:26:19,199 epoch 2 - iter 138/469 - loss 5.26256870 - samples/sec: 46.71 - lr: 0.290199
2023-06-02 15:26:48,918 epoch 2 - iter 184/469 - loss 5.39897937 - samples/sec: 49.54 - lr: 0.286932
2023-06-02 15:27:18,520 epoch 2 - iter 230/469 - loss 5.58274997 - samples/sec: 49.74 - lr: 0.283665
2023-06-02 15:27:49,385 epoch 2 - iter 276/469 - loss 5.58892806 - samples/sec: 47.70 - lr: 0.280398
2023-06-02 15:28:19,611 epoch 2 - iter 322/469 - loss 5.56195012 - samples/sec: 48.71 - lr: 0.277131
2023-06-02 15:28:52,083 epoch 2 - iter 368/469 - loss 5.56701347 - samples/sec: 45.34 - lr: 0.273864
2023-06-02 15:29:22,636 epoch 2 - iter 414/469 - loss 5.52469147 - samples/sec: 48.19 - lr: 0.270597
2023-06-02 15:29:53,060 epoch 2 - iter 460/469 - loss 5.51003435 - samples/sec: 48.40 - lr: 0.267330
2023-06-02 15:29:58,814 ----------------------------------------------------------------------------------------------------
2023-06-02 15:29:58,815 EPOCH 2 done: loss 5.5211 - lr 0.267330
2023-06-02 15:31:32,987 Evaluating as a multi-label problem: False
2023-06-02 15:31:33,036 DEV : loss 1.8982011079788208 - f1-score (micro avg)  0.7163
2023-06-02 15:31:33,113 BAD EPOCHS (no improvement): 4
2023-06-02 15:31:33,116 ----------------------------------------------------------------------------------------------------
2023-06-02 15:32:03,731 epoch 3 - iter 46/469 - loss 5.78495079 - samples/sec: 48.09 - lr: 0.263423
2023-06-02 15:32:34,695 epoch 3 - iter 92/469 - loss 5.37465958 - samples/sec: 47.55 - lr: 0.260156
2023-06-02 15:33:03,458 epoch 3 - iter 138/469 - loss 5.17969902 - samples/sec: 51.19 - lr: 0.256889
2023-06-02 15:33:36,084 epoch 3 - iter 184/469 - loss 5.14568345 - samples/sec: 45.13 - lr: 0.253622
2023-06-02 15:34:05,885 epoch 3 - iter 230/469 - loss 5.10704238 - samples/sec: 49.41 - lr: 0.250355
2023-06-02 15:34:35,439 epoch 3 - iter 276/469 - loss 5.08817664 - samples/sec: 49.82 - lr: 0.247088
2023-06-02 15:35:07,433 epoch 3 - iter 322/469 - loss 5.08471963 - samples/sec: 46.02 - lr: 0.243821
2023-06-02 15:35:35,023 epoch 3 - iter 368/469 - loss 5.09277682 - samples/sec: 53.37 - lr: 0.240554
2023-06-02 15:36:03,935 epoch 3 - iter 414/469 - loss 5.08082759 - samples/sec: 50.93 - lr: 0.237287
2023-06-02 15:36:37,094 epoch 3 - iter 460/469 - loss 5.05871350 - samples/sec: 44.40 - lr: 0.234020
2023-06-02 15:36:42,565 ----------------------------------------------------------------------------------------------------
2023-06-02 15:36:42,565 EPOCH 3 done: loss 5.0455 - lr 0.234020
2023-06-02 15:38:10,090 Evaluating as a multi-label problem: False
2023-06-02 15:38:10,162 DEV : loss 1.8672385215759277 - f1-score (micro avg)  0.7071
2023-06-02 15:38:10,262 BAD EPOCHS (no improvement): 4
2023-06-02 15:38:10,265 ----------------------------------------------------------------------------------------------------
2023-06-02 15:38:41,290 epoch 4 - iter 46/469 - loss 5.07962114 - samples/sec: 47.46 - lr: 0.230114
2023-06-02 15:39:13,711 epoch 4 - iter 92/469 - loss 4.86290000 - samples/sec: 45.42 - lr: 0.226847
2023-06-02 15:39:44,737 epoch 4 - iter 138/469 - loss 4.71462618 - samples/sec: 47.46 - lr: 0.223580
2023-06-02 15:40:13,843 epoch 4 - iter 184/469 - loss 4.66858431 - samples/sec: 50.59 - lr: 0.220312
2023-06-02 15:40:41,677 epoch 4 - iter 230/469 - loss 4.58614408 - samples/sec: 52.90 - lr: 0.217045
2023-06-02 15:41:14,814 epoch 4 - iter 276/469 - loss 4.57375897 - samples/sec: 44.43 - lr: 0.213778
2023-06-02 15:41:44,216 epoch 4 - iter 322/469 - loss 4.63332675 - samples/sec: 50.08 - lr: 0.210511
2023-06-02 15:42:15,357 epoch 4 - iter 368/469 - loss 4.61801774 - samples/sec: 47.28 - lr: 0.207244
2023-06-02 15:42:48,734 epoch 4 - iter 414/469 - loss 4.60423767 - samples/sec: 44.11 - lr: 0.203977
2023-06-02 15:43:18,984 epoch 4 - iter 460/469 - loss 4.57733207 - samples/sec: 48.68 - lr: 0.200710
2023-06-02 15:43:24,553 ----------------------------------------------------------------------------------------------------
2023-06-02 15:43:24,554 EPOCH 4 done: loss 4.5796 - lr 0.200710
2023-06-02 15:44:58,180 Evaluating as a multi-label problem: False
2023-06-02 15:44:58,256 DEV : loss 1.680154800415039 - f1-score (micro avg)  0.7122
2023-06-02 15:44:58,374 BAD EPOCHS (no improvement): 4
2023-06-02 15:44:58,377 ----------------------------------------------------------------------------------------------------
2023-06-02 15:45:31,059 epoch 5 - iter 46/469 - loss 4.31608295 - samples/sec: 45.06 - lr: 0.196804
2023-06-02 15:46:00,295 epoch 5 - iter 92/469 - loss 4.30452204 - samples/sec: 50.36 - lr: 0.193537
2023-06-02 15:46:29,324 epoch 5 - iter 138/469 - loss 4.26907333 - samples/sec: 50.72 - lr: 0.190270
2023-06-02 15:47:00,307 epoch 5 - iter 184/469 - loss 4.24007126 - samples/sec: 47.53 - lr: 0.187003
2023-06-02 15:47:29,778 epoch 5 - iter 230/469 - loss 4.20038259 - samples/sec: 49.96 - lr: 0.183736
2023-06-02 15:47:57,804 epoch 5 - iter 276/469 - loss 4.18236027 - samples/sec: 52.54 - lr: 0.180469
2023-06-02 15:48:30,791 epoch 5 - iter 322/469 - loss 4.12904126 - samples/sec: 44.64 - lr: 0.177202
2023-06-02 15:49:00,901 epoch 5 - iter 368/469 - loss 4.09215306 - samples/sec: 48.90 - lr: 0.173935
2023-06-02 15:49:30,959 epoch 5 - iter 414/469 - loss 4.07744107 - samples/sec: 48.99 - lr: 0.170668
2023-06-02 15:50:02,670 epoch 5 - iter 460/469 - loss 4.04598687 - samples/sec: 46.43 - lr: 0.167401
2023-06-02 15:50:08,156 ----------------------------------------------------------------------------------------------------
2023-06-02 15:50:08,157 EPOCH 5 done: loss 4.0305 - lr 0.167401
2023-06-02 15:51:39,343 Evaluating as a multi-label problem: False
2023-06-02 15:51:39,394 DEV : loss 1.2184265851974487 - f1-score (micro avg)  0.712
2023-06-02 15:51:39,469 BAD EPOCHS (no improvement): 4
2023-06-02 15:51:39,472 ----------------------------------------------------------------------------------------------------
2023-06-02 15:52:09,063 epoch 6 - iter 46/469 - loss 3.39894260 - samples/sec: 49.76 - lr: 0.163494
2023-06-02 15:52:42,894 epoch 6 - iter 92/469 - loss 3.27628142 - samples/sec: 43.52 - lr: 0.160227
2023-06-02 15:53:14,680 epoch 6 - iter 138/469 - loss 3.38432029 - samples/sec: 46.32 - lr: 0.156960
2023-06-02 15:53:44,648 epoch 6 - iter 184/469 - loss 3.36849267 - samples/sec: 49.13 - lr: 0.153693
2023-06-02 15:54:16,945 epoch 6 - iter 230/469 - loss 3.36470489 - samples/sec: 45.59 - lr: 0.150426
2023-06-02 15:54:46,451 epoch 6 - iter 276/469 - loss 3.34875380 - samples/sec: 49.90 - lr: 0.147159
2023-06-02 15:55:15,898 epoch 6 - iter 322/469 - loss 3.32306455 - samples/sec: 50.00 - lr: 0.143892
2023-06-02 15:55:45,722 epoch 6 - iter 368/469 - loss 3.30897542 - samples/sec: 49.37 - lr: 0.140625
2023-06-02 15:56:16,720 epoch 6 - iter 414/469 - loss 3.31999802 - samples/sec: 47.50 - lr: 0.137358
2023-06-02 15:56:45,802 epoch 6 - iter 460/469 - loss 3.30241218 - samples/sec: 50.63 - lr: 0.134091
2023-06-02 15:56:51,088 ----------------------------------------------------------------------------------------------------
2023-06-02 15:56:51,088 EPOCH 6 done: loss 3.3080 - lr 0.134091
2023-06-02 15:58:24,534 Evaluating as a multi-label problem: False
2023-06-02 15:58:24,606 DEV : loss 1.0595511198043823 - f1-score (micro avg)  0.7483
2023-06-02 15:58:24,712 BAD EPOCHS (no improvement): 4
2023-06-02 15:58:24,715 ----------------------------------------------------------------------------------------------------
2023-06-02 15:58:57,360 epoch 7 - iter 46/469 - loss 2.91544905 - samples/sec: 45.11 - lr: 0.130185
2023-06-02 15:59:27,169 epoch 7 - iter 92/469 - loss 2.88884349 - samples/sec: 49.40 - lr: 0.126918
2023-06-02 15:59:57,757 epoch 7 - iter 138/469 - loss 2.86651477 - samples/sec: 48.14 - lr: 0.123651
2023-06-02 16:00:30,625 epoch 7 - iter 184/469 - loss 2.91339152 - samples/sec: 44.80 - lr: 0.120384
2023-06-02 16:01:00,564 epoch 7 - iter 230/469 - loss 2.90690593 - samples/sec: 49.18 - lr: 0.117116
2023-06-02 16:01:29,939 epoch 7 - iter 276/469 - loss 2.88626517 - samples/sec: 50.13 - lr: 0.113849
2023-06-02 16:02:01,705 epoch 7 - iter 322/469 - loss 2.89119211 - samples/sec: 46.35 - lr: 0.110582
2023-06-02 16:02:31,192 epoch 7 - iter 368/469 - loss 2.87578149 - samples/sec: 49.94 - lr: 0.107315
2023-06-02 16:02:59,568 epoch 7 - iter 414/469 - loss 2.83540662 - samples/sec: 51.89 - lr: 0.104048
2023-06-02 16:03:31,496 epoch 7 - iter 460/469 - loss 2.79801538 - samples/sec: 46.12 - lr: 0.100781
2023-06-02 16:03:36,607 ----------------------------------------------------------------------------------------------------
2023-06-02 16:03:36,607 EPOCH 7 done: loss 2.7907 - lr 0.100781
2023-06-02 16:05:06,654 Evaluating as a multi-label problem: False
2023-06-02 16:05:06,718 DEV : loss 0.8865067958831787 - f1-score (micro avg)  0.7721
2023-06-02 16:05:06,808 BAD EPOCHS (no improvement): 4
2023-06-02 16:05:06,811 ----------------------------------------------------------------------------------------------------
2023-06-02 16:05:37,226 epoch 8 - iter 46/469 - loss 2.22206276 - samples/sec: 48.42 - lr: 0.096875
2023-06-02 16:06:07,528 epoch 8 - iter 92/469 - loss 2.31357723 - samples/sec: 48.59 - lr: 0.093608
2023-06-02 16:06:39,793 epoch 8 - iter 138/469 - loss 2.38314658 - samples/sec: 45.63 - lr: 0.090341
2023-06-02 16:07:10,231 epoch 8 - iter 184/469 - loss 2.37071535 - samples/sec: 48.37 - lr: 0.087074
2023-06-02 16:07:39,214 epoch 8 - iter 230/469 - loss 2.33166687 - samples/sec: 50.80 - lr: 0.083807
2023-06-02 16:08:10,843 epoch 8 - iter 276/469 - loss 2.29659896 - samples/sec: 46.55 - lr: 0.080540
2023-06-02 16:08:42,076 epoch 8 - iter 322/469 - loss 2.23149434 - samples/sec: 47.14 - lr: 0.077273
2023-06-02 16:09:12,205 epoch 8 - iter 368/469 - loss 2.19798425 - samples/sec: 48.87 - lr: 0.074006
2023-06-02 16:09:45,142 epoch 8 - iter 414/469 - loss 2.18539696 - samples/sec: 44.70 - lr: 0.070739
2023-06-02 16:10:15,820 epoch 8 - iter 460/469 - loss 2.16585427 - samples/sec: 48.00 - lr: 0.067472
2023-06-02 16:10:21,437 ----------------------------------------------------------------------------------------------------
2023-06-02 16:10:21,437 EPOCH 8 done: loss 2.1591 - lr 0.067472
2023-06-02 16:11:52,951 Evaluating as a multi-label problem: False
2023-06-02 16:11:53,021 DEV : loss 0.6325146555900574 - f1-score (micro avg)  0.7549
2023-06-02 16:11:53,134 BAD EPOCHS (no improvement): 4
2023-06-02 16:11:53,136 ----------------------------------------------------------------------------------------------------
2023-06-02 16:12:24,647 epoch 9 - iter 46/469 - loss 1.81925631 - samples/sec: 46.73 - lr: 0.063565
2023-06-02 16:12:54,456 epoch 9 - iter 92/469 - loss 1.78219879 - samples/sec: 49.40 - lr: 0.060298
2023-06-02 16:13:22,274 epoch 9 - iter 138/469 - loss 1.78847430 - samples/sec: 52.93 - lr: 0.057031
2023-06-02 16:13:54,491 epoch 9 - iter 184/469 - loss 1.76746709 - samples/sec: 45.70 - lr: 0.053764
2023-06-02 16:14:23,923 epoch 9 - iter 230/469 - loss 1.72117619 - samples/sec: 50.03 - lr: 0.050497
2023-06-02 16:14:54,540 epoch 9 - iter 276/469 - loss 1.69076228 - samples/sec: 48.09 - lr: 0.047230
2023-06-02 16:15:25,673 epoch 9 - iter 322/469 - loss 1.64954837 - samples/sec: 47.29 - lr: 0.043963
2023-06-02 16:15:54,577 epoch 9 - iter 368/469 - loss 1.61413895 - samples/sec: 50.94 - lr: 0.040696
2023-06-02 16:16:24,934 epoch 9 - iter 414/469 - loss 1.59123834 - samples/sec: 48.51 - lr: 0.037429
2023-06-02 16:16:57,247 epoch 9 - iter 460/469 - loss 1.57267961 - samples/sec: 45.57 - lr: 0.034162
2023-06-02 16:17:02,659 ----------------------------------------------------------------------------------------------------
2023-06-02 16:17:02,659 EPOCH 9 done: loss 1.5644 - lr 0.034162
2023-06-02 16:18:31,852 Evaluating as a multi-label problem: False
2023-06-02 16:18:31,921 DEV : loss 0.4844402074813843 - f1-score (micro avg)  0.7908
2023-06-02 16:18:32,019 BAD EPOCHS (no improvement): 4
2023-06-02 16:18:32,022 ----------------------------------------------------------------------------------------------------
2023-06-02 16:18:59,922 epoch 10 - iter 46/469 - loss 1.20012617 - samples/sec: 52.78 - lr: 0.030256
2023-06-02 16:19:30,226 epoch 10 - iter 92/469 - loss 1.21548625 - samples/sec: 48.59 - lr: 0.026989
2023-06-02 16:20:02,675 epoch 10 - iter 138/469 - loss 1.18336485 - samples/sec: 45.38 - lr: 0.023722
2023-06-02 16:20:31,549 epoch 10 - iter 184/469 - loss 1.17765637 - samples/sec: 51.00 - lr: 0.020455
2023-06-02 16:21:00,093 epoch 10 - iter 230/469 - loss 1.15781975 - samples/sec: 51.59 - lr: 0.017187
2023-06-02 16:21:29,466 epoch 10 - iter 276/469 - loss 1.12865275 - samples/sec: 50.13 - lr: 0.013920
2023-06-02 16:21:57,702 epoch 10 - iter 322/469 - loss 1.10129422 - samples/sec: 52.15 - lr: 0.010653
2023-06-02 16:22:28,437 epoch 10 - iter 368/469 - loss 1.08263890 - samples/sec: 47.91 - lr: 0.007386
2023-06-02 16:23:01,386 epoch 10 - iter 414/469 - loss 1.06032063 - samples/sec: 44.69 - lr: 0.004119
2023-06-02 16:23:31,336 epoch 10 - iter 460/469 - loss 1.03419858 - samples/sec: 49.17 - lr: 0.000852
2023-06-02 16:23:36,826 ----------------------------------------------------------------------------------------------------
2023-06-02 16:23:36,826 EPOCH 10 done: loss 1.0322 - lr 0.000852
2023-06-02 16:25:09,107 Evaluating as a multi-label problem: False
2023-06-02 16:25:09,176 DEV : loss 0.3229527771472931 - f1-score (micro avg)  0.8124
2023-06-02 16:25:09,282 BAD EPOCHS (no improvement): 4
2023-06-02 16:25:21,420 ----------------------------------------------------------------------------------------------------
2023-06-02 16:25:21,423 Testing using last state of model ...
2023-06-02 16:26:54,014 Evaluating as a multi-label problem: False
2023-06-02 16:26:54,076 0.8001	0.7783	0.7891	0.6862
2023-06-02 16:26:54,076 
Results:
- F-score (micro) 0.7891
- F-score (macro) 0.7703
- Accuracy 0.6862

By class:
              precision    recall  f1-score   support

         LOC     0.7977    0.7896    0.7936      1668
         ORG     0.7125    0.6863    0.6992      1661
         PER     0.9249    0.9289    0.9269      1617
        MISC     0.7060    0.6225    0.6616       702

   micro avg     0.8001    0.7783    0.7891      5648
   macro avg     0.7853    0.7568    0.7703      5648
weighted avg     0.7977    0.7783    0.7876      5648

2023-06-02 16:26:54,076 ----------------------------------------------------------------------------------------------------
