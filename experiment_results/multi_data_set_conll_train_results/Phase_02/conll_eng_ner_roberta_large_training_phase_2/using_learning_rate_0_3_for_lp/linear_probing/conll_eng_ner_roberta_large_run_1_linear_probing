2023-06-02 14:10:23,819 ----------------------------------------------------------------------------------------------------
2023-06-02 14:10:23,822 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-02 14:10:23,825 ----------------------------------------------------------------------------------------------------
2023-06-02 14:10:23,828 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-02 14:10:23,828 ----------------------------------------------------------------------------------------------------
2023-06-02 14:10:23,828 Parameters:
2023-06-02 14:10:23,828  - learning_rate: "0.300000"
2023-06-02 14:10:23,828  - mini_batch_size: "32"
2023-06-02 14:10:23,828  - patience: "3"
2023-06-02 14:10:23,828  - anneal_factor: "0.5"
2023-06-02 14:10:23,833  - max_epochs: "10"
2023-06-02 14:10:23,833  - shuffle: "True"
2023-06-02 14:10:23,835  - train_with_dev: "False"
2023-06-02 14:10:23,835  - batch_growth_annealing: "False"
2023-06-02 14:10:23,835 ----------------------------------------------------------------------------------------------------
2023-06-02 14:10:23,835 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_1_linear_probing"
2023-06-02 14:10:23,835 ----------------------------------------------------------------------------------------------------
2023-06-02 14:10:23,835 Device: cuda:0
2023-06-02 14:10:23,835 ----------------------------------------------------------------------------------------------------
2023-06-02 14:10:23,835 Embeddings storage mode: none
2023-06-02 14:10:23,835 ----------------------------------------------------------------------------------------------------
2023-06-02 14:10:55,006 epoch 1 - iter 46/469 - loss 1.33368892 - samples/sec: 47.24 - lr: 0.029424
2023-06-02 14:11:24,648 epoch 1 - iter 92/469 - loss 1.17282828 - samples/sec: 49.67 - lr: 0.058849
2023-06-02 14:11:50,162 epoch 1 - iter 138/469 - loss 1.18892146 - samples/sec: 57.71 - lr: 0.088273
2023-06-02 14:12:19,136 epoch 1 - iter 184/469 - loss 1.48068316 - samples/sec: 50.82 - lr: 0.117697
2023-06-02 14:12:44,130 epoch 1 - iter 230/469 - loss 1.66562978 - samples/sec: 58.91 - lr: 0.147122
2023-06-02 14:13:13,160 epoch 1 - iter 276/469 - loss 1.86135817 - samples/sec: 50.72 - lr: 0.176546
2023-06-02 14:13:39,460 epoch 1 - iter 322/469 - loss 2.21802472 - samples/sec: 55.98 - lr: 0.205970
2023-06-02 14:14:08,290 epoch 1 - iter 368/469 - loss 2.64340533 - samples/sec: 51.07 - lr: 0.235394
2023-06-02 14:14:39,574 epoch 1 - iter 414/469 - loss 2.92162362 - samples/sec: 47.06 - lr: 0.264819
2023-06-02 14:15:07,911 epoch 1 - iter 460/469 - loss 3.14033168 - samples/sec: 51.96 - lr: 0.294243
2023-06-02 14:15:13,328 ----------------------------------------------------------------------------------------------------
2023-06-02 14:15:13,328 EPOCH 1 done: loss 3.1789 - lr 0.294243
2023-06-02 14:16:46,164 Evaluating as a multi-label problem: False
2023-06-02 14:16:46,236 DEV : loss 2.4135360717773438 - f1-score (micro avg)  0.7103
2023-06-02 14:16:46,343 BAD EPOCHS (no improvement): 4
2023-06-02 14:16:46,346 ----------------------------------------------------------------------------------------------------
2023-06-02 14:17:15,386 epoch 2 - iter 46/469 - loss 6.10899835 - samples/sec: 50.71 - lr: 0.296733
2023-06-02 14:17:45,327 epoch 2 - iter 92/469 - loss 6.06190590 - samples/sec: 49.18 - lr: 0.293466
2023-06-02 14:18:16,053 epoch 2 - iter 138/469 - loss 6.11110211 - samples/sec: 47.92 - lr: 0.290199
2023-06-02 14:18:45,907 epoch 2 - iter 184/469 - loss 6.11454328 - samples/sec: 49.32 - lr: 0.286932
2023-06-02 14:19:17,228 epoch 2 - iter 230/469 - loss 6.09211533 - samples/sec: 47.01 - lr: 0.283665
2023-06-02 14:19:45,793 epoch 2 - iter 276/469 - loss 6.07599037 - samples/sec: 51.55 - lr: 0.280398
2023-06-02 14:20:15,884 epoch 2 - iter 322/469 - loss 6.02103700 - samples/sec: 48.93 - lr: 0.277131
2023-06-02 14:20:49,394 epoch 2 - iter 368/469 - loss 6.00686350 - samples/sec: 43.94 - lr: 0.273864
2023-06-02 14:21:18,867 epoch 2 - iter 414/469 - loss 5.94725701 - samples/sec: 49.96 - lr: 0.270597
2023-06-02 14:21:47,991 epoch 2 - iter 460/469 - loss 5.92774204 - samples/sec: 50.56 - lr: 0.267330
2023-06-02 14:21:55,258 ----------------------------------------------------------------------------------------------------
2023-06-02 14:21:55,259 EPOCH 2 done: loss 5.9317 - lr 0.267330
2023-06-02 14:23:27,556 Evaluating as a multi-label problem: False
2023-06-02 14:23:27,627 DEV : loss 2.1063942909240723 - f1-score (micro avg)  0.6796
2023-06-02 14:23:27,716 BAD EPOCHS (no improvement): 4
2023-06-02 14:23:27,719 ----------------------------------------------------------------------------------------------------
2023-06-02 14:23:56,205 epoch 3 - iter 46/469 - loss 5.36704660 - samples/sec: 51.70 - lr: 0.263423
2023-06-02 14:24:28,399 epoch 3 - iter 92/469 - loss 5.29112937 - samples/sec: 45.74 - lr: 0.260156
2023-06-02 14:24:58,516 epoch 3 - iter 138/469 - loss 5.23608572 - samples/sec: 48.89 - lr: 0.256889
2023-06-02 14:25:28,866 epoch 3 - iter 184/469 - loss 5.26016415 - samples/sec: 48.52 - lr: 0.253622
2023-06-02 14:26:01,286 epoch 3 - iter 230/469 - loss 5.22843927 - samples/sec: 45.42 - lr: 0.250355
2023-06-02 14:26:30,602 epoch 3 - iter 276/469 - loss 5.19891153 - samples/sec: 50.23 - lr: 0.247088
2023-06-02 14:26:58,677 epoch 3 - iter 322/469 - loss 5.19638292 - samples/sec: 52.42 - lr: 0.243821
2023-06-02 14:27:29,507 epoch 3 - iter 368/469 - loss 5.13141758 - samples/sec: 47.76 - lr: 0.240554
2023-06-02 14:27:59,847 epoch 3 - iter 414/469 - loss 5.06477625 - samples/sec: 48.53 - lr: 0.237287
2023-06-02 14:28:32,107 epoch 3 - iter 460/469 - loss 5.02538221 - samples/sec: 45.64 - lr: 0.234020
2023-06-02 14:28:37,359 ----------------------------------------------------------------------------------------------------
2023-06-02 14:28:37,360 EPOCH 3 done: loss 5.0241 - lr 0.234020
2023-06-02 14:30:06,320 Evaluating as a multi-label problem: False
2023-06-02 14:30:06,393 DEV : loss 1.4290482997894287 - f1-score (micro avg)  0.7371
2023-06-02 14:30:06,508 BAD EPOCHS (no improvement): 4
2023-06-02 14:30:06,511 ----------------------------------------------------------------------------------------------------
2023-06-02 14:30:35,341 epoch 4 - iter 46/469 - loss 4.26053172 - samples/sec: 51.08 - lr: 0.230114
2023-06-02 14:31:06,651 epoch 4 - iter 92/469 - loss 4.57529255 - samples/sec: 47.03 - lr: 0.226847
2023-06-02 14:31:35,026 epoch 4 - iter 138/469 - loss 4.62716996 - samples/sec: 51.89 - lr: 0.223580
2023-06-02 14:32:03,515 epoch 4 - iter 184/469 - loss 4.61261285 - samples/sec: 51.69 - lr: 0.220312
2023-06-02 14:32:34,789 epoch 4 - iter 230/469 - loss 4.56972625 - samples/sec: 47.08 - lr: 0.217045
2023-06-02 14:33:05,326 epoch 4 - iter 276/469 - loss 4.56435943 - samples/sec: 48.22 - lr: 0.213778
2023-06-02 14:33:35,610 epoch 4 - iter 322/469 - loss 4.53359994 - samples/sec: 48.62 - lr: 0.210511
2023-06-02 14:34:07,015 epoch 4 - iter 368/469 - loss 4.51020174 - samples/sec: 46.88 - lr: 0.207244
2023-06-02 14:34:36,744 epoch 4 - iter 414/469 - loss 4.46535974 - samples/sec: 49.53 - lr: 0.203977
2023-06-02 14:35:08,357 epoch 4 - iter 460/469 - loss 4.43129826 - samples/sec: 46.58 - lr: 0.200710
2023-06-02 14:35:13,931 ----------------------------------------------------------------------------------------------------
2023-06-02 14:35:13,932 EPOCH 4 done: loss 4.4358 - lr 0.200710
2023-06-02 14:36:43,332 Evaluating as a multi-label problem: False
2023-06-02 14:36:43,400 DEV : loss 1.544243574142456 - f1-score (micro avg)  0.7348
2023-06-02 14:36:43,498 BAD EPOCHS (no improvement): 4
2023-06-02 14:36:43,501 ----------------------------------------------------------------------------------------------------
2023-06-02 14:37:13,572 epoch 5 - iter 46/469 - loss 4.02894065 - samples/sec: 48.97 - lr: 0.196804
2023-06-02 14:37:45,881 epoch 5 - iter 92/469 - loss 3.92125783 - samples/sec: 45.57 - lr: 0.193537
2023-06-02 14:38:16,195 epoch 5 - iter 138/469 - loss 4.03891617 - samples/sec: 48.57 - lr: 0.190270
2023-06-02 14:38:46,559 epoch 5 - iter 184/469 - loss 4.09607944 - samples/sec: 48.49 - lr: 0.187003
2023-06-02 14:39:18,219 epoch 5 - iter 230/469 - loss 4.08194155 - samples/sec: 46.51 - lr: 0.183736
2023-06-02 14:39:47,284 epoch 5 - iter 276/469 - loss 4.06581496 - samples/sec: 50.66 - lr: 0.180469
2023-06-02 14:40:18,823 epoch 5 - iter 322/469 - loss 4.06742281 - samples/sec: 46.69 - lr: 0.177202
2023-06-02 14:40:47,850 epoch 5 - iter 368/469 - loss 4.02727033 - samples/sec: 50.73 - lr: 0.173935
2023-06-02 14:41:16,543 epoch 5 - iter 414/469 - loss 4.01042552 - samples/sec: 51.32 - lr: 0.170668
2023-06-02 14:41:48,128 epoch 5 - iter 460/469 - loss 3.97282366 - samples/sec: 46.62 - lr: 0.167401
2023-06-02 14:41:53,662 ----------------------------------------------------------------------------------------------------
2023-06-02 14:41:53,662 EPOCH 5 done: loss 3.9655 - lr 0.167401
2023-06-02 14:43:24,117 Evaluating as a multi-label problem: False
2023-06-02 14:43:24,186 DEV : loss 1.256630778312683 - f1-score (micro avg)  0.7232
2023-06-02 14:43:24,293 BAD EPOCHS (no improvement): 4
2023-06-02 14:43:24,296 ----------------------------------------------------------------------------------------------------
2023-06-02 14:43:54,203 epoch 6 - iter 46/469 - loss 3.48601445 - samples/sec: 49.24 - lr: 0.163494
2023-06-02 14:44:25,599 epoch 6 - iter 92/469 - loss 3.35322133 - samples/sec: 46.90 - lr: 0.160227
2023-06-02 14:44:54,361 epoch 6 - iter 138/469 - loss 3.56706877 - samples/sec: 51.19 - lr: 0.156960
2023-06-02 14:45:23,139 epoch 6 - iter 184/469 - loss 3.55249433 - samples/sec: 51.16 - lr: 0.153693
2023-06-02 14:45:54,177 epoch 6 - iter 230/469 - loss 3.51109195 - samples/sec: 47.44 - lr: 0.150426
2023-06-02 14:46:23,609 epoch 6 - iter 276/469 - loss 3.48322255 - samples/sec: 50.02 - lr: 0.147159
2023-06-02 14:46:53,396 epoch 6 - iter 322/469 - loss 3.47263848 - samples/sec: 49.43 - lr: 0.143892
2023-06-02 14:47:23,495 epoch 6 - iter 368/469 - loss 3.44151351 - samples/sec: 48.92 - lr: 0.140625
2023-06-02 14:47:53,178 epoch 6 - iter 414/469 - loss 3.41303447 - samples/sec: 49.60 - lr: 0.137358
2023-06-02 14:48:24,483 epoch 6 - iter 460/469 - loss 3.37571653 - samples/sec: 47.03 - lr: 0.134091
2023-06-02 14:48:29,727 ----------------------------------------------------------------------------------------------------
2023-06-02 14:48:29,727 EPOCH 6 done: loss 3.3652 - lr 0.134091
2023-06-02 14:50:02,915 Evaluating as a multi-label problem: False
2023-06-02 14:50:02,986 DEV : loss 1.0103031396865845 - f1-score (micro avg)  0.7441
2023-06-02 14:50:03,093 BAD EPOCHS (no improvement): 4
2023-06-02 14:50:03,095 ----------------------------------------------------------------------------------------------------
2023-06-02 14:50:33,317 epoch 7 - iter 46/469 - loss 2.85284709 - samples/sec: 48.73 - lr: 0.130185
2023-06-02 14:51:04,820 epoch 7 - iter 92/469 - loss 2.95543650 - samples/sec: 46.74 - lr: 0.126918
2023-06-02 14:51:32,932 epoch 7 - iter 138/469 - loss 2.92301860 - samples/sec: 52.38 - lr: 0.123651
2023-06-02 14:52:00,414 epoch 7 - iter 184/469 - loss 2.87208238 - samples/sec: 53.58 - lr: 0.120384
2023-06-02 14:52:31,680 epoch 7 - iter 230/469 - loss 2.88192145 - samples/sec: 47.09 - lr: 0.117116
2023-06-02 14:53:01,521 epoch 7 - iter 276/469 - loss 2.86532744 - samples/sec: 49.34 - lr: 0.113849
2023-06-02 14:53:31,458 epoch 7 - iter 322/469 - loss 2.85851928 - samples/sec: 49.18 - lr: 0.110582
2023-06-02 14:54:01,248 epoch 7 - iter 368/469 - loss 2.83495224 - samples/sec: 49.43 - lr: 0.107315
2023-06-02 14:54:29,802 epoch 7 - iter 414/469 - loss 2.81127651 - samples/sec: 51.57 - lr: 0.104048
2023-06-02 14:55:01,677 epoch 7 - iter 460/469 - loss 2.77878963 - samples/sec: 46.19 - lr: 0.100781
2023-06-02 14:55:07,076 ----------------------------------------------------------------------------------------------------
2023-06-02 14:55:07,076 EPOCH 7 done: loss 2.7759 - lr 0.100781
2023-06-02 14:56:38,492 Evaluating as a multi-label problem: False
2023-06-02 14:56:38,569 DEV : loss 0.7887231707572937 - f1-score (micro avg)  0.7508
2023-06-02 14:56:38,662 BAD EPOCHS (no improvement): 4
2023-06-02 14:56:38,665 ----------------------------------------------------------------------------------------------------
2023-06-02 14:57:08,649 epoch 8 - iter 46/469 - loss 2.28590654 - samples/sec: 49.11 - lr: 0.096875
2023-06-02 14:57:40,280 epoch 8 - iter 92/469 - loss 2.35259949 - samples/sec: 46.55 - lr: 0.093608
2023-06-02 14:58:10,098 epoch 8 - iter 138/469 - loss 2.39076008 - samples/sec: 49.38 - lr: 0.090341
2023-06-02 14:58:39,508 epoch 8 - iter 184/469 - loss 2.38553082 - samples/sec: 50.06 - lr: 0.087074
2023-06-02 14:59:10,154 epoch 8 - iter 230/469 - loss 2.36887177 - samples/sec: 48.04 - lr: 0.083807
2023-06-02 14:59:39,624 epoch 8 - iter 276/469 - loss 2.35221274 - samples/sec: 49.96 - lr: 0.080540
2023-06-02 15:00:11,842 epoch 8 - iter 322/469 - loss 2.30583592 - samples/sec: 45.70 - lr: 0.077273
2023-06-02 15:00:42,189 epoch 8 - iter 368/469 - loss 2.26984151 - samples/sec: 48.52 - lr: 0.074006
2023-06-02 15:01:12,242 epoch 8 - iter 414/469 - loss 2.23668982 - samples/sec: 48.99 - lr: 0.070739
2023-06-02 15:01:43,334 epoch 8 - iter 460/469 - loss 2.22196754 - samples/sec: 47.35 - lr: 0.067472
2023-06-02 15:01:48,741 ----------------------------------------------------------------------------------------------------
2023-06-02 15:01:48,741 EPOCH 8 done: loss 2.2180 - lr 0.067472
2023-06-02 15:03:21,894 Evaluating as a multi-label problem: False
2023-06-02 15:03:21,970 DEV : loss 0.7837596535682678 - f1-score (micro avg)  0.7171
2023-06-02 15:03:22,071 BAD EPOCHS (no improvement): 4
2023-06-02 15:03:22,075 ----------------------------------------------------------------------------------------------------
2023-06-02 15:03:50,914 epoch 9 - iter 46/469 - loss 1.79463165 - samples/sec: 51.06 - lr: 0.063565
2023-06-02 15:04:22,146 epoch 9 - iter 92/469 - loss 1.80869546 - samples/sec: 47.14 - lr: 0.060298
2023-06-02 15:04:51,534 epoch 9 - iter 138/469 - loss 1.76751765 - samples/sec: 50.11 - lr: 0.057031
2023-06-02 15:05:20,575 epoch 9 - iter 184/469 - loss 1.73237404 - samples/sec: 50.70 - lr: 0.053764
2023-06-02 15:05:53,018 epoch 9 - iter 230/469 - loss 1.71365530 - samples/sec: 45.38 - lr: 0.050497
2023-06-02 15:06:21,150 epoch 9 - iter 276/469 - loss 1.68686123 - samples/sec: 52.34 - lr: 0.047230
2023-06-02 15:06:51,045 epoch 9 - iter 322/469 - loss 1.67043505 - samples/sec: 49.25 - lr: 0.043963
2023-06-02 15:07:20,071 epoch 9 - iter 368/469 - loss 1.64008995 - samples/sec: 50.73 - lr: 0.040696
2023-06-02 15:07:49,819 epoch 9 - iter 414/469 - loss 1.62602347 - samples/sec: 49.50 - lr: 0.037429
2023-06-02 15:08:21,811 epoch 9 - iter 460/469 - loss 1.59111564 - samples/sec: 46.03 - lr: 0.034162
2023-06-02 15:08:26,894 ----------------------------------------------------------------------------------------------------
2023-06-02 15:08:26,894 EPOCH 9 done: loss 1.5865 - lr 0.034162
2023-06-02 15:09:59,612 Evaluating as a multi-label problem: False
2023-06-02 15:09:59,681 DEV : loss 0.49689802527427673 - f1-score (micro avg)  0.7844
2023-06-02 15:09:59,781 BAD EPOCHS (no improvement): 4
2023-06-02 15:09:59,783 ----------------------------------------------------------------------------------------------------
2023-06-02 15:10:29,405 epoch 10 - iter 46/469 - loss 1.32241255 - samples/sec: 49.72 - lr: 0.030256
2023-06-02 15:11:01,915 epoch 10 - iter 92/469 - loss 1.24136235 - samples/sec: 45.29 - lr: 0.026989
2023-06-02 15:11:30,878 epoch 10 - iter 138/469 - loss 1.22285668 - samples/sec: 50.84 - lr: 0.023722
2023-06-02 15:12:03,630 epoch 10 - iter 184/469 - loss 1.18286444 - samples/sec: 44.96 - lr: 0.020455
2023-06-02 15:12:32,066 epoch 10 - iter 230/469 - loss 1.16068674 - samples/sec: 51.78 - lr: 0.017187
2023-06-02 15:13:01,002 epoch 10 - iter 276/469 - loss 1.13487820 - samples/sec: 50.89 - lr: 0.013920
2023-06-02 15:13:33,164 epoch 10 - iter 322/469 - loss 1.11813707 - samples/sec: 45.78 - lr: 0.010653
2023-06-02 15:14:02,201 epoch 10 - iter 368/469 - loss 1.09756916 - samples/sec: 50.71 - lr: 0.007386
2023-06-02 15:14:30,948 epoch 10 - iter 414/469 - loss 1.06815302 - samples/sec: 51.22 - lr: 0.004119
2023-06-02 15:15:02,141 epoch 10 - iter 460/469 - loss 1.05385569 - samples/sec: 47.20 - lr: 0.000852
2023-06-02 15:15:07,822 ----------------------------------------------------------------------------------------------------
2023-06-02 15:15:07,823 EPOCH 10 done: loss 1.0504 - lr 0.000852
2023-06-02 15:16:41,165 Evaluating as a multi-label problem: False
2023-06-02 15:16:41,235 DEV : loss 0.31632259488105774 - f1-score (micro avg)  0.8123
2023-06-02 15:16:41,342 BAD EPOCHS (no improvement): 4
2023-06-02 15:16:54,607 ----------------------------------------------------------------------------------------------------
2023-06-02 15:16:54,610 Testing using last state of model ...
2023-06-02 15:18:30,295 Evaluating as a multi-label problem: False
2023-06-02 15:18:30,363 0.804	0.7801	0.7919	0.6935
2023-06-02 15:18:30,363 
Results:
- F-score (micro) 0.7919
- F-score (macro) 0.7726
- Accuracy 0.6935

By class:
              precision    recall  f1-score   support

         LOC     0.7892    0.7992    0.7942      1668
         PER     0.9276    0.9344    0.9310      1617
         ORG     0.7358    0.6707    0.7017      1661
        MISC     0.6914    0.6382    0.6637       702

   micro avg     0.8040    0.7801    0.7919      5648
   macro avg     0.7860    0.7606    0.7726      5648
weighted avg     0.8010    0.7801    0.7899      5648

2023-06-02 15:18:30,363 ----------------------------------------------------------------------------------------------------
