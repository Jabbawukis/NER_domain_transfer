2023-06-02 22:05:40,396 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:40,400 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-02 22:05:40,402 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:40,402 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-02 22:05:40,404 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:40,404 Parameters:
2023-06-02 22:05:40,404  - learning_rate: "0.000005"
2023-06-02 22:05:40,405  - mini_batch_size: "4"
2023-06-02 22:05:40,405  - patience: "3"
2023-06-02 22:05:40,405  - anneal_factor: "0.5"
2023-06-02 22:05:40,405  - max_epochs: "10"
2023-06-02 22:05:40,405  - shuffle: "True"
2023-06-02 22:05:40,405  - train_with_dev: "False"
2023-06-02 22:05:40,405  - batch_growth_annealing: "False"
2023-06-02 22:05:40,405 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:40,405 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_1_full_fine_tuning"
2023-06-02 22:05:40,405 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:40,405 Device: cuda:2
2023-06-02 22:05:40,405 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:40,405 Embeddings storage mode: none
2023-06-02 22:05:40,405 ----------------------------------------------------------------------------------------------------
2023-06-02 22:07:00,641 epoch 1 - iter 374/3747 - loss 0.88209529 - samples/sec: 18.65 - lr: 0.000000
2023-06-02 22:08:18,803 epoch 1 - iter 748/3747 - loss 0.76176044 - samples/sec: 19.15 - lr: 0.000001
2023-06-02 22:09:41,020 epoch 1 - iter 1122/3747 - loss 0.64858880 - samples/sec: 18.20 - lr: 0.000001
2023-06-02 22:11:02,867 epoch 1 - iter 1496/3747 - loss 0.58142715 - samples/sec: 18.29 - lr: 0.000002
2023-06-02 22:12:28,639 epoch 1 - iter 1870/3747 - loss 0.51555878 - samples/sec: 17.45 - lr: 0.000002
2023-06-02 22:13:56,840 epoch 1 - iter 2244/3747 - loss 0.46396229 - samples/sec: 16.97 - lr: 0.000003
2023-06-02 22:15:22,717 epoch 1 - iter 2618/3747 - loss 0.42810849 - samples/sec: 17.43 - lr: 0.000003
2023-06-02 22:16:47,280 epoch 1 - iter 2992/3747 - loss 0.41092546 - samples/sec: 17.70 - lr: 0.000004
2023-06-02 22:18:12,560 epoch 1 - iter 3366/3747 - loss 0.39497139 - samples/sec: 17.55 - lr: 0.000004
2023-06-02 22:19:38,847 epoch 1 - iter 3740/3747 - loss 0.37288060 - samples/sec: 17.35 - lr: 0.000005
2023-06-02 22:19:40,339 ----------------------------------------------------------------------------------------------------
2023-06-02 22:19:40,339 EPOCH 1 done: loss 0.3728 - lr 0.000005
2023-06-02 22:21:02,035 Evaluating as a multi-label problem: False
2023-06-02 22:21:02,105 DEV : loss 0.10974101722240448 - f1-score (micro avg)  0.9228
2023-06-02 22:21:02,213 BAD EPOCHS (no improvement): 4
2023-06-02 22:21:02,216 ----------------------------------------------------------------------------------------------------
2023-06-02 22:22:25,101 epoch 2 - iter 374/3747 - loss 0.19334429 - samples/sec: 18.06 - lr: 0.000005
2023-06-02 22:23:44,475 epoch 2 - iter 748/3747 - loss 0.21143353 - samples/sec: 18.86 - lr: 0.000005
2023-06-02 22:25:08,064 epoch 2 - iter 1122/3747 - loss 0.20198820 - samples/sec: 17.91 - lr: 0.000005
2023-06-02 22:26:31,963 epoch 2 - iter 1496/3747 - loss 0.20108787 - samples/sec: 17.84 - lr: 0.000005
2023-06-02 22:27:56,544 epoch 2 - iter 1870/3747 - loss 0.20471449 - samples/sec: 17.70 - lr: 0.000005
2023-06-02 22:29:24,152 epoch 2 - iter 2244/3747 - loss 0.20214109 - samples/sec: 17.08 - lr: 0.000005
2023-06-02 22:30:44,750 epoch 2 - iter 2618/3747 - loss 0.19787957 - samples/sec: 18.57 - lr: 0.000005
2023-06-02 22:32:03,009 epoch 2 - iter 2992/3747 - loss 0.19749768 - samples/sec: 19.13 - lr: 0.000005
2023-06-02 22:33:26,658 epoch 2 - iter 3366/3747 - loss 0.19391178 - samples/sec: 17.89 - lr: 0.000005
2023-06-02 22:34:48,908 epoch 2 - iter 3740/3747 - loss 0.19128161 - samples/sec: 18.20 - lr: 0.000004
2023-06-02 22:34:50,335 ----------------------------------------------------------------------------------------------------
2023-06-02 22:34:50,335 EPOCH 2 done: loss 0.1912 - lr 0.000004
2023-06-02 22:36:09,879 Evaluating as a multi-label problem: False
2023-06-02 22:36:09,948 DEV : loss 0.08764609694480896 - f1-score (micro avg)  0.9561
2023-06-02 22:36:10,036 BAD EPOCHS (no improvement): 4
2023-06-02 22:36:10,039 ----------------------------------------------------------------------------------------------------
2023-06-02 22:37:30,330 epoch 3 - iter 374/3747 - loss 0.15539940 - samples/sec: 18.64 - lr: 0.000004
2023-06-02 22:38:56,657 epoch 3 - iter 748/3747 - loss 0.15949755 - samples/sec: 17.34 - lr: 0.000004
2023-06-02 22:40:21,089 epoch 3 - iter 1122/3747 - loss 0.16310395 - samples/sec: 17.73 - lr: 0.000004
2023-06-02 22:41:44,706 epoch 3 - iter 1496/3747 - loss 0.15979307 - samples/sec: 17.90 - lr: 0.000004
2023-06-02 22:43:11,210 epoch 3 - iter 1870/3747 - loss 0.16220015 - samples/sec: 17.30 - lr: 0.000004
2023-06-02 22:44:32,547 epoch 3 - iter 2244/3747 - loss 0.15675279 - samples/sec: 18.40 - lr: 0.000004
2023-06-02 22:45:56,396 epoch 3 - iter 2618/3747 - loss 0.15415620 - samples/sec: 17.85 - lr: 0.000004
2023-06-02 22:47:18,898 epoch 3 - iter 2992/3747 - loss 0.15521849 - samples/sec: 18.14 - lr: 0.000004
2023-06-02 22:48:41,039 epoch 3 - iter 3366/3747 - loss 0.15710887 - samples/sec: 18.22 - lr: 0.000004
2023-06-02 22:50:04,108 epoch 3 - iter 3740/3747 - loss 0.15566339 - samples/sec: 18.02 - lr: 0.000004
2023-06-02 22:50:05,573 ----------------------------------------------------------------------------------------------------
2023-06-02 22:50:05,573 EPOCH 3 done: loss 0.1555 - lr 0.000004
2023-06-02 22:51:22,344 Evaluating as a multi-label problem: False
2023-06-02 22:51:22,409 DEV : loss 0.09677920490503311 - f1-score (micro avg)  0.959
2023-06-02 22:51:22,515 BAD EPOCHS (no improvement): 4
2023-06-02 22:51:22,518 ----------------------------------------------------------------------------------------------------
2023-06-02 22:52:47,318 epoch 4 - iter 374/3747 - loss 0.13056995 - samples/sec: 17.65 - lr: 0.000004
2023-06-02 22:54:05,606 epoch 4 - iter 748/3747 - loss 0.12848467 - samples/sec: 19.12 - lr: 0.000004
2023-06-02 22:55:26,542 epoch 4 - iter 1122/3747 - loss 0.12229932 - samples/sec: 18.49 - lr: 0.000004
2023-06-02 22:56:48,523 epoch 4 - iter 1496/3747 - loss 0.12471798 - samples/sec: 18.26 - lr: 0.000004
2023-06-02 22:58:06,415 epoch 4 - iter 1870/3747 - loss 0.13084338 - samples/sec: 19.22 - lr: 0.000004
2023-06-02 22:59:25,210 epoch 4 - iter 2244/3747 - loss 0.12692490 - samples/sec: 19.00 - lr: 0.000004
2023-06-02 23:00:48,258 epoch 4 - iter 2618/3747 - loss 0.12659532 - samples/sec: 18.02 - lr: 0.000004
2023-06-02 23:02:10,800 epoch 4 - iter 2992/3747 - loss 0.12434930 - samples/sec: 18.13 - lr: 0.000003
2023-06-02 23:03:31,226 epoch 4 - iter 3366/3747 - loss 0.12396443 - samples/sec: 18.61 - lr: 0.000003
2023-06-02 23:04:53,025 epoch 4 - iter 3740/3747 - loss 0.12177899 - samples/sec: 18.30 - lr: 0.000003
2023-06-02 23:04:54,653 ----------------------------------------------------------------------------------------------------
2023-06-02 23:04:54,653 EPOCH 4 done: loss 0.1218 - lr 0.000003
2023-06-02 23:06:17,958 Evaluating as a multi-label problem: False
2023-06-02 23:06:18,025 DEV : loss 0.08649207651615143 - f1-score (micro avg)  0.9571
2023-06-02 23:06:18,128 BAD EPOCHS (no improvement): 4
2023-06-02 23:06:18,132 ----------------------------------------------------------------------------------------------------
2023-06-02 23:07:37,451 epoch 5 - iter 374/3747 - loss 0.11168043 - samples/sec: 18.87 - lr: 0.000003
2023-06-02 23:08:57,275 epoch 5 - iter 748/3747 - loss 0.10858329 - samples/sec: 18.75 - lr: 0.000003
2023-06-02 23:10:19,938 epoch 5 - iter 1122/3747 - loss 0.10436878 - samples/sec: 18.11 - lr: 0.000003
2023-06-02 23:11:44,472 epoch 5 - iter 1496/3747 - loss 0.10562887 - samples/sec: 17.71 - lr: 0.000003
2023-06-02 23:13:04,201 epoch 5 - iter 1870/3747 - loss 0.10566323 - samples/sec: 18.77 - lr: 0.000003
2023-06-02 23:14:27,878 epoch 5 - iter 2244/3747 - loss 0.10646984 - samples/sec: 17.89 - lr: 0.000003
2023-06-02 23:15:50,477 epoch 5 - iter 2618/3747 - loss 0.10448117 - samples/sec: 18.12 - lr: 0.000003
2023-06-02 23:17:11,654 epoch 5 - iter 2992/3747 - loss 0.10465816 - samples/sec: 18.44 - lr: 0.000003
2023-06-02 23:18:35,558 epoch 5 - iter 3366/3747 - loss 0.10316994 - samples/sec: 17.84 - lr: 0.000003
2023-06-02 23:19:57,038 epoch 5 - iter 3740/3747 - loss 0.10285898 - samples/sec: 18.37 - lr: 0.000003
2023-06-02 23:19:58,460 ----------------------------------------------------------------------------------------------------
2023-06-02 23:19:58,460 EPOCH 5 done: loss 0.1027 - lr 0.000003
2023-06-02 23:21:07,843 Evaluating as a multi-label problem: False
2023-06-02 23:21:07,884 DEV : loss 0.08507949858903885 - f1-score (micro avg)  0.9665
2023-06-02 23:21:07,950 BAD EPOCHS (no improvement): 4
2023-06-02 23:21:07,954 ----------------------------------------------------------------------------------------------------
2023-06-02 23:22:31,820 epoch 6 - iter 374/3747 - loss 0.09978828 - samples/sec: 17.85 - lr: 0.000003
2023-06-02 23:23:56,977 epoch 6 - iter 748/3747 - loss 0.10522443 - samples/sec: 17.58 - lr: 0.000003
2023-06-02 23:25:21,973 epoch 6 - iter 1122/3747 - loss 0.10632068 - samples/sec: 17.61 - lr: 0.000003
2023-06-02 23:26:44,809 epoch 6 - iter 1496/3747 - loss 0.10111003 - samples/sec: 18.07 - lr: 0.000003
2023-06-02 23:28:07,846 epoch 6 - iter 1870/3747 - loss 0.10033371 - samples/sec: 18.03 - lr: 0.000003
2023-06-02 23:29:28,147 epoch 6 - iter 2244/3747 - loss 0.09729913 - samples/sec: 18.64 - lr: 0.000002
2023-06-02 23:30:54,361 epoch 6 - iter 2618/3747 - loss 0.09513197 - samples/sec: 17.36 - lr: 0.000002
2023-06-02 23:32:18,072 epoch 6 - iter 2992/3747 - loss 0.09616425 - samples/sec: 17.88 - lr: 0.000002
2023-06-02 23:33:37,555 epoch 6 - iter 3366/3747 - loss 0.09686763 - samples/sec: 18.83 - lr: 0.000002
2023-06-02 23:35:02,563 epoch 6 - iter 3740/3747 - loss 0.09542121 - samples/sec: 17.61 - lr: 0.000002
2023-06-02 23:35:04,144 ----------------------------------------------------------------------------------------------------
2023-06-02 23:35:04,144 EPOCH 6 done: loss 0.0954 - lr 0.000002
2023-06-02 23:36:25,650 Evaluating as a multi-label problem: False
2023-06-02 23:36:25,715 DEV : loss 0.08297324180603027 - f1-score (micro avg)  0.9703
2023-06-02 23:36:25,808 BAD EPOCHS (no improvement): 4
2023-06-02 23:36:25,811 ----------------------------------------------------------------------------------------------------
2023-06-02 23:37:48,099 epoch 7 - iter 374/3747 - loss 0.08476580 - samples/sec: 18.19 - lr: 0.000002
2023-06-02 23:39:12,337 epoch 7 - iter 748/3747 - loss 0.08182224 - samples/sec: 17.77 - lr: 0.000002
2023-06-02 23:40:33,942 epoch 7 - iter 1122/3747 - loss 0.08048671 - samples/sec: 18.34 - lr: 0.000002
2023-06-02 23:41:55,487 epoch 7 - iter 1496/3747 - loss 0.08038351 - samples/sec: 18.35 - lr: 0.000002
2023-06-02 23:43:18,010 epoch 7 - iter 1870/3747 - loss 0.08210296 - samples/sec: 18.14 - lr: 0.000002
2023-06-02 23:44:37,701 epoch 7 - iter 2244/3747 - loss 0.08477540 - samples/sec: 18.78 - lr: 0.000002
2023-06-02 23:46:04,279 epoch 7 - iter 2618/3747 - loss 0.08354931 - samples/sec: 17.29 - lr: 0.000002
2023-06-02 23:47:27,961 epoch 7 - iter 2992/3747 - loss 0.08252351 - samples/sec: 17.89 - lr: 0.000002
2023-06-02 23:48:47,956 epoch 7 - iter 3366/3747 - loss 0.08134161 - samples/sec: 18.71 - lr: 0.000002
2023-06-02 23:50:13,388 epoch 7 - iter 3740/3747 - loss 0.08166731 - samples/sec: 17.52 - lr: 0.000002
2023-06-02 23:50:15,006 ----------------------------------------------------------------------------------------------------
2023-06-02 23:50:15,007 EPOCH 7 done: loss 0.0820 - lr 0.000002
2023-06-02 23:51:38,100 Evaluating as a multi-label problem: False
2023-06-02 23:51:38,172 DEV : loss 0.08220367133617401 - f1-score (micro avg)  0.9693
2023-06-02 23:51:38,280 BAD EPOCHS (no improvement): 4
2023-06-02 23:51:38,282 ----------------------------------------------------------------------------------------------------
2023-06-02 23:53:05,498 epoch 8 - iter 374/3747 - loss 0.07660022 - samples/sec: 17.16 - lr: 0.000002
2023-06-02 23:54:26,695 epoch 8 - iter 748/3747 - loss 0.08333626 - samples/sec: 18.43 - lr: 0.000002
2023-06-02 23:55:48,397 epoch 8 - iter 1122/3747 - loss 0.07912025 - samples/sec: 18.32 - lr: 0.000002
2023-06-02 23:57:12,961 epoch 8 - iter 1496/3747 - loss 0.07748901 - samples/sec: 17.70 - lr: 0.000001
2023-06-02 23:58:33,554 epoch 8 - iter 1870/3747 - loss 0.07487306 - samples/sec: 18.57 - lr: 0.000001
2023-06-02 23:59:57,676 epoch 8 - iter 2244/3747 - loss 0.07315090 - samples/sec: 17.79 - lr: 0.000001
2023-06-03 00:01:18,721 epoch 8 - iter 2618/3747 - loss 0.07296014 - samples/sec: 18.47 - lr: 0.000001
2023-06-03 00:02:42,775 epoch 8 - iter 2992/3747 - loss 0.07345459 - samples/sec: 17.81 - lr: 0.000001
2023-06-03 00:04:04,840 epoch 8 - iter 3366/3747 - loss 0.07409916 - samples/sec: 18.24 - lr: 0.000001
2023-06-03 00:05:25,768 epoch 8 - iter 3740/3747 - loss 0.07442181 - samples/sec: 18.50 - lr: 0.000001
2023-06-03 00:05:27,365 ----------------------------------------------------------------------------------------------------
2023-06-03 00:05:27,366 EPOCH 8 done: loss 0.0743 - lr 0.000001
2023-06-03 00:06:47,684 Evaluating as a multi-label problem: False
2023-06-03 00:06:47,724 DEV : loss 0.0907180979847908 - f1-score (micro avg)  0.9707
2023-06-03 00:06:47,798 BAD EPOCHS (no improvement): 4
2023-06-03 00:06:47,800 ----------------------------------------------------------------------------------------------------
2023-06-03 00:08:10,727 epoch 9 - iter 374/3747 - loss 0.07237708 - samples/sec: 18.05 - lr: 0.000001
2023-06-03 00:09:27,799 epoch 9 - iter 748/3747 - loss 0.06711830 - samples/sec: 19.42 - lr: 0.000001
2023-06-03 00:10:52,036 epoch 9 - iter 1122/3747 - loss 0.06698082 - samples/sec: 17.77 - lr: 0.000001
2023-06-03 00:12:11,133 epoch 9 - iter 1496/3747 - loss 0.06343145 - samples/sec: 18.92 - lr: 0.000001
2023-06-03 00:13:31,948 epoch 9 - iter 1870/3747 - loss 0.06357628 - samples/sec: 18.52 - lr: 0.000001
2023-06-03 00:14:53,398 epoch 9 - iter 2244/3747 - loss 0.06376452 - samples/sec: 18.38 - lr: 0.000001
2023-06-03 00:16:14,595 epoch 9 - iter 2618/3747 - loss 0.06457385 - samples/sec: 18.43 - lr: 0.000001
2023-06-03 00:17:35,833 epoch 9 - iter 2992/3747 - loss 0.06568002 - samples/sec: 18.42 - lr: 0.000001
2023-06-03 00:19:01,302 epoch 9 - iter 3366/3747 - loss 0.06704146 - samples/sec: 17.51 - lr: 0.000001
2023-06-03 00:20:21,351 epoch 9 - iter 3740/3747 - loss 0.06677409 - samples/sec: 18.70 - lr: 0.000001
2023-06-03 00:20:22,847 ----------------------------------------------------------------------------------------------------
2023-06-03 00:20:22,847 EPOCH 9 done: loss 0.0667 - lr 0.000001
2023-06-03 00:21:42,365 Evaluating as a multi-label problem: False
2023-06-03 00:21:42,431 DEV : loss 0.08743598312139511 - f1-score (micro avg)  0.971
2023-06-03 00:21:42,540 BAD EPOCHS (no improvement): 4
2023-06-03 00:21:42,548 ----------------------------------------------------------------------------------------------------
2023-06-03 00:23:04,538 epoch 10 - iter 374/3747 - loss 0.06914568 - samples/sec: 18.26 - lr: 0.000001
2023-06-03 00:24:25,602 epoch 10 - iter 748/3747 - loss 0.06799106 - samples/sec: 18.46 - lr: 0.000000
2023-06-03 00:25:49,511 epoch 10 - iter 1122/3747 - loss 0.06883743 - samples/sec: 17.84 - lr: 0.000000
2023-06-03 00:27:14,154 epoch 10 - iter 1496/3747 - loss 0.06497558 - samples/sec: 17.68 - lr: 0.000000
2023-06-03 00:28:37,361 epoch 10 - iter 1870/3747 - loss 0.06415721 - samples/sec: 17.99 - lr: 0.000000
2023-06-03 00:29:56,571 epoch 10 - iter 2244/3747 - loss 0.06343325 - samples/sec: 18.90 - lr: 0.000000
2023-06-03 00:31:17,455 epoch 10 - iter 2618/3747 - loss 0.06251616 - samples/sec: 18.51 - lr: 0.000000
2023-06-03 00:32:38,636 epoch 10 - iter 2992/3747 - loss 0.06219772 - samples/sec: 18.44 - lr: 0.000000
2023-06-03 00:34:04,565 epoch 10 - iter 3366/3747 - loss 0.06157499 - samples/sec: 17.42 - lr: 0.000000
2023-06-03 00:35:26,815 epoch 10 - iter 3740/3747 - loss 0.06229413 - samples/sec: 18.20 - lr: 0.000000
2023-06-03 00:35:28,391 ----------------------------------------------------------------------------------------------------
2023-06-03 00:35:28,392 EPOCH 10 done: loss 0.0624 - lr 0.000000
2023-06-03 00:36:49,616 Evaluating as a multi-label problem: False
2023-06-03 00:36:49,680 DEV : loss 0.09270253032445908 - f1-score (micro avg)  0.9715
2023-06-03 00:36:49,778 BAD EPOCHS (no improvement): 4
2023-06-03 00:37:02,925 ----------------------------------------------------------------------------------------------------
2023-06-03 00:37:02,928 Testing using last state of model ...
2023-06-03 00:38:26,217 Evaluating as a multi-label problem: False
2023-06-03 00:38:26,278 0.9352	0.9478	0.9414	0.9136
2023-06-03 00:38:26,278 
Results:
- F-score (micro) 0.9414
- F-score (macro) 0.9283
- Accuracy 0.9136

By class:
              precision    recall  f1-score   support

         ORG     0.9238    0.9488    0.9361      1661
         LOC     0.9501    0.9472    0.9487      1668
         PER     0.9845    0.9802    0.9823      1617
        MISC     0.8215    0.8718    0.8459       702

   micro avg     0.9352    0.9478    0.9414      5648
   macro avg     0.9200    0.9370    0.9283      5648
weighted avg     0.9362    0.9478    0.9418      5648

2023-06-03 00:38:26,278 ----------------------------------------------------------------------------------------------------
