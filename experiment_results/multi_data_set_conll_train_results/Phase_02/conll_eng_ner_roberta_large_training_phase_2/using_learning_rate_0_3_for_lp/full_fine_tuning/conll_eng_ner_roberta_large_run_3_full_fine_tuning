2023-06-03 03:19:40,164 ----------------------------------------------------------------------------------------------------
2023-06-03 03:19:40,168 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-03 03:19:40,169 ----------------------------------------------------------------------------------------------------
2023-06-03 03:19:40,170 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-03 03:19:40,170 ----------------------------------------------------------------------------------------------------
2023-06-03 03:19:40,170 Parameters:
2023-06-03 03:19:40,170  - learning_rate: "0.000005"
2023-06-03 03:19:40,170  - mini_batch_size: "4"
2023-06-03 03:19:40,170  - patience: "3"
2023-06-03 03:19:40,170  - anneal_factor: "0.5"
2023-06-03 03:19:40,170  - max_epochs: "10"
2023-06-03 03:19:40,170  - shuffle: "True"
2023-06-03 03:19:40,170  - train_with_dev: "False"
2023-06-03 03:19:40,170  - batch_growth_annealing: "False"
2023-06-03 03:19:40,170 ----------------------------------------------------------------------------------------------------
2023-06-03 03:19:40,170 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_3_full_fine_tuning"
2023-06-03 03:19:40,170 ----------------------------------------------------------------------------------------------------
2023-06-03 03:19:40,170 Device: cuda:2
2023-06-03 03:19:40,170 ----------------------------------------------------------------------------------------------------
2023-06-03 03:19:40,170 Embeddings storage mode: none
2023-06-03 03:19:40,170 ----------------------------------------------------------------------------------------------------
2023-06-03 03:21:02,035 epoch 1 - iter 374/3747 - loss 0.86546158 - samples/sec: 18.28 - lr: 0.000000
2023-06-03 03:22:23,436 epoch 1 - iter 748/3747 - loss 0.74099671 - samples/sec: 18.39 - lr: 0.000001
2023-06-03 03:23:48,675 epoch 1 - iter 1122/3747 - loss 0.64236181 - samples/sec: 17.56 - lr: 0.000001
2023-06-03 03:25:14,959 epoch 1 - iter 1496/3747 - loss 0.57579744 - samples/sec: 17.35 - lr: 0.000002
2023-06-03 03:26:42,554 epoch 1 - iter 1870/3747 - loss 0.50934257 - samples/sec: 17.09 - lr: 0.000002
2023-06-03 03:28:09,896 epoch 1 - iter 2244/3747 - loss 0.45739810 - samples/sec: 17.13 - lr: 0.000003
2023-06-03 03:29:42,703 epoch 1 - iter 2618/3747 - loss 0.42339498 - samples/sec: 16.13 - lr: 0.000003
2023-06-03 03:31:11,522 epoch 1 - iter 2992/3747 - loss 0.40136950 - samples/sec: 16.85 - lr: 0.000004
2023-06-03 03:32:37,751 epoch 1 - iter 3366/3747 - loss 0.38444914 - samples/sec: 17.36 - lr: 0.000004
2023-06-03 03:34:05,132 epoch 1 - iter 3740/3747 - loss 0.36431725 - samples/sec: 17.13 - lr: 0.000005
2023-06-03 03:34:06,627 ----------------------------------------------------------------------------------------------------
2023-06-03 03:34:06,627 EPOCH 1 done: loss 0.3643 - lr 0.000005
2023-06-03 03:35:30,760 Evaluating as a multi-label problem: False
2023-06-03 03:35:30,830 DEV : loss 0.12223111838102341 - f1-score (micro avg)  0.919
2023-06-03 03:35:30,939 BAD EPOCHS (no improvement): 4
2023-06-03 03:35:30,942 ----------------------------------------------------------------------------------------------------
2023-06-03 03:37:00,142 epoch 2 - iter 374/3747 - loss 0.24092901 - samples/sec: 16.78 - lr: 0.000005
2023-06-03 03:38:29,096 epoch 2 - iter 748/3747 - loss 0.23625153 - samples/sec: 16.83 - lr: 0.000005
2023-06-03 03:39:57,584 epoch 2 - iter 1122/3747 - loss 0.22267594 - samples/sec: 16.91 - lr: 0.000005
2023-06-03 03:41:28,366 epoch 2 - iter 1496/3747 - loss 0.21659986 - samples/sec: 16.49 - lr: 0.000005
2023-06-03 03:42:58,577 epoch 2 - iter 1870/3747 - loss 0.20971324 - samples/sec: 16.59 - lr: 0.000005
2023-06-03 03:44:26,782 epoch 2 - iter 2244/3747 - loss 0.20317474 - samples/sec: 16.97 - lr: 0.000005
2023-06-03 03:45:49,815 epoch 2 - iter 2618/3747 - loss 0.19907536 - samples/sec: 18.02 - lr: 0.000005
2023-06-03 03:47:15,145 epoch 2 - iter 2992/3747 - loss 0.19301875 - samples/sec: 17.54 - lr: 0.000005
2023-06-03 03:48:48,841 epoch 2 - iter 3366/3747 - loss 0.19101384 - samples/sec: 15.97 - lr: 0.000005
2023-06-03 03:50:23,265 epoch 2 - iter 3740/3747 - loss 0.18669936 - samples/sec: 15.85 - lr: 0.000004
2023-06-03 03:50:24,850 ----------------------------------------------------------------------------------------------------
2023-06-03 03:50:24,850 EPOCH 2 done: loss 0.1865 - lr 0.000004
2023-06-03 03:51:50,382 Evaluating as a multi-label problem: False
2023-06-03 03:51:50,453 DEV : loss 0.0744890570640564 - f1-score (micro avg)  0.9545
2023-06-03 03:51:50,598 BAD EPOCHS (no improvement): 4
2023-06-03 03:51:50,602 ----------------------------------------------------------------------------------------------------
2023-06-03 03:53:18,597 epoch 3 - iter 374/3747 - loss 0.15870028 - samples/sec: 17.01 - lr: 0.000004
2023-06-03 03:54:46,332 epoch 3 - iter 748/3747 - loss 0.14893442 - samples/sec: 17.06 - lr: 0.000004
2023-06-03 03:56:16,268 epoch 3 - iter 1122/3747 - loss 0.15320789 - samples/sec: 16.64 - lr: 0.000004
2023-06-03 03:57:43,322 epoch 3 - iter 1496/3747 - loss 0.14815341 - samples/sec: 17.19 - lr: 0.000004
2023-06-03 03:59:15,286 epoch 3 - iter 1870/3747 - loss 0.14456315 - samples/sec: 16.27 - lr: 0.000004
2023-06-03 04:00:41,074 epoch 3 - iter 2244/3747 - loss 0.14294899 - samples/sec: 17.45 - lr: 0.000004
2023-06-03 04:02:09,655 epoch 3 - iter 2618/3747 - loss 0.14444174 - samples/sec: 16.90 - lr: 0.000004
2023-06-03 04:03:34,821 epoch 3 - iter 2992/3747 - loss 0.14330757 - samples/sec: 17.57 - lr: 0.000004
2023-06-03 04:05:03,986 epoch 3 - iter 3366/3747 - loss 0.14346213 - samples/sec: 16.79 - lr: 0.000004
2023-06-03 04:06:32,079 epoch 3 - iter 3740/3747 - loss 0.14570017 - samples/sec: 16.99 - lr: 0.000004
2023-06-03 04:06:33,809 ----------------------------------------------------------------------------------------------------
2023-06-03 04:06:33,809 EPOCH 3 done: loss 0.1456 - lr 0.000004
2023-06-03 04:08:09,873 Evaluating as a multi-label problem: False
2023-06-03 04:08:09,938 DEV : loss 0.08215619623661041 - f1-score (micro avg)  0.9539
2023-06-03 04:08:10,049 BAD EPOCHS (no improvement): 4
2023-06-03 04:08:10,056 ----------------------------------------------------------------------------------------------------
2023-06-03 04:09:41,039 epoch 4 - iter 374/3747 - loss 0.10920374 - samples/sec: 16.45 - lr: 0.000004
2023-06-03 04:11:11,206 epoch 4 - iter 748/3747 - loss 0.12198828 - samples/sec: 16.60 - lr: 0.000004
2023-06-03 04:12:39,529 epoch 4 - iter 1122/3747 - loss 0.12229423 - samples/sec: 16.95 - lr: 0.000004
2023-06-03 04:14:07,156 epoch 4 - iter 1496/3747 - loss 0.12635714 - samples/sec: 17.08 - lr: 0.000004
2023-06-03 04:15:36,312 epoch 4 - iter 1870/3747 - loss 0.12803622 - samples/sec: 16.79 - lr: 0.000004
2023-06-03 04:17:03,182 epoch 4 - iter 2244/3747 - loss 0.12690620 - samples/sec: 17.23 - lr: 0.000004
2023-06-03 04:18:37,550 epoch 4 - iter 2618/3747 - loss 0.12437691 - samples/sec: 15.86 - lr: 0.000004
2023-06-03 04:20:13,201 epoch 4 - iter 2992/3747 - loss 0.12261082 - samples/sec: 15.65 - lr: 0.000003
2023-06-03 04:21:41,597 epoch 4 - iter 3366/3747 - loss 0.12294285 - samples/sec: 16.93 - lr: 0.000003
2023-06-03 04:23:08,604 epoch 4 - iter 3740/3747 - loss 0.12199241 - samples/sec: 17.20 - lr: 0.000003
2023-06-03 04:23:10,192 ----------------------------------------------------------------------------------------------------
2023-06-03 04:23:10,192 EPOCH 4 done: loss 0.1219 - lr 0.000003
2023-06-03 04:24:36,166 Evaluating as a multi-label problem: False
2023-06-03 04:24:36,230 DEV : loss 0.09316971898078918 - f1-score (micro avg)  0.9644
2023-06-03 04:24:36,352 BAD EPOCHS (no improvement): 4
2023-06-03 04:24:36,355 ----------------------------------------------------------------------------------------------------
2023-06-03 04:26:03,794 epoch 5 - iter 374/3747 - loss 0.08319994 - samples/sec: 17.12 - lr: 0.000003
2023-06-03 04:27:31,496 epoch 5 - iter 748/3747 - loss 0.08775641 - samples/sec: 17.07 - lr: 0.000003
2023-06-03 04:28:58,703 epoch 5 - iter 1122/3747 - loss 0.08722657 - samples/sec: 17.16 - lr: 0.000003
2023-06-03 04:30:28,639 epoch 5 - iter 1496/3747 - loss 0.08938689 - samples/sec: 16.64 - lr: 0.000003
2023-06-03 04:31:56,992 epoch 5 - iter 1870/3747 - loss 0.09806670 - samples/sec: 16.94 - lr: 0.000003
2023-06-03 04:33:26,587 epoch 5 - iter 2244/3747 - loss 0.10364650 - samples/sec: 16.71 - lr: 0.000003
2023-06-03 04:34:55,023 epoch 5 - iter 2618/3747 - loss 0.10409690 - samples/sec: 16.92 - lr: 0.000003
2023-06-03 04:36:18,641 epoch 5 - iter 2992/3747 - loss 0.10230138 - samples/sec: 17.90 - lr: 0.000003
2023-06-03 04:37:40,239 epoch 5 - iter 3366/3747 - loss 0.10408233 - samples/sec: 18.34 - lr: 0.000003
2023-06-03 04:39:09,133 epoch 5 - iter 3740/3747 - loss 0.10348388 - samples/sec: 16.84 - lr: 0.000003
2023-06-03 04:39:10,744 ----------------------------------------------------------------------------------------------------
2023-06-03 04:39:10,744 EPOCH 5 done: loss 0.1039 - lr 0.000003
2023-06-03 04:40:24,361 Evaluating as a multi-label problem: False
2023-06-03 04:40:24,418 DEV : loss 0.08825872838497162 - f1-score (micro avg)  0.966
2023-06-03 04:40:24,512 BAD EPOCHS (no improvement): 4
2023-06-03 04:40:24,514 ----------------------------------------------------------------------------------------------------
2023-06-03 04:41:48,016 epoch 6 - iter 374/3747 - loss 0.09085280 - samples/sec: 17.92 - lr: 0.000003
2023-06-03 04:43:20,623 epoch 6 - iter 748/3747 - loss 0.09823071 - samples/sec: 16.16 - lr: 0.000003
2023-06-03 04:44:46,924 epoch 6 - iter 1122/3747 - loss 0.09479318 - samples/sec: 17.34 - lr: 0.000003
2023-06-03 04:46:12,948 epoch 6 - iter 1496/3747 - loss 0.09433275 - samples/sec: 17.40 - lr: 0.000003
2023-06-03 04:47:38,963 epoch 6 - iter 1870/3747 - loss 0.09525355 - samples/sec: 17.40 - lr: 0.000003
2023-06-03 04:49:05,050 epoch 6 - iter 2244/3747 - loss 0.09568441 - samples/sec: 17.39 - lr: 0.000002
2023-06-03 04:50:25,799 epoch 6 - iter 2618/3747 - loss 0.09633338 - samples/sec: 18.53 - lr: 0.000002
2023-06-03 04:51:53,422 epoch 6 - iter 2992/3747 - loss 0.09642785 - samples/sec: 17.08 - lr: 0.000002
2023-06-03 04:53:21,185 epoch 6 - iter 3366/3747 - loss 0.09590085 - samples/sec: 17.05 - lr: 0.000002
2023-06-03 04:54:48,615 epoch 6 - iter 3740/3747 - loss 0.09447428 - samples/sec: 17.12 - lr: 0.000002
2023-06-03 04:54:50,260 ----------------------------------------------------------------------------------------------------
2023-06-03 04:54:50,261 EPOCH 6 done: loss 0.0945 - lr 0.000002
2023-06-03 04:56:15,733 Evaluating as a multi-label problem: False
2023-06-03 04:56:15,802 DEV : loss 0.09585413336753845 - f1-score (micro avg)  0.966
2023-06-03 04:56:15,909 BAD EPOCHS (no improvement): 4
2023-06-03 04:56:15,912 ----------------------------------------------------------------------------------------------------
2023-06-03 04:57:42,988 epoch 7 - iter 374/3747 - loss 0.09103274 - samples/sec: 17.19 - lr: 0.000002
2023-06-03 04:59:11,873 epoch 7 - iter 748/3747 - loss 0.08258860 - samples/sec: 16.84 - lr: 0.000002
2023-06-03 05:00:38,324 epoch 7 - iter 1122/3747 - loss 0.08536404 - samples/sec: 17.31 - lr: 0.000002
2023-06-03 05:02:05,601 epoch 7 - iter 1496/3747 - loss 0.08538113 - samples/sec: 17.15 - lr: 0.000002
2023-06-03 05:03:31,988 epoch 7 - iter 1870/3747 - loss 0.08400960 - samples/sec: 17.33 - lr: 0.000002
2023-06-03 05:04:58,075 epoch 7 - iter 2244/3747 - loss 0.08347697 - samples/sec: 17.39 - lr: 0.000002
2023-06-03 05:06:29,029 epoch 7 - iter 2618/3747 - loss 0.08146884 - samples/sec: 16.46 - lr: 0.000002
2023-06-03 05:07:53,110 epoch 7 - iter 2992/3747 - loss 0.08219048 - samples/sec: 17.80 - lr: 0.000002
2023-06-03 05:09:17,666 epoch 7 - iter 3366/3747 - loss 0.08270180 - samples/sec: 17.70 - lr: 0.000002
2023-06-03 05:10:40,080 epoch 7 - iter 3740/3747 - loss 0.08246459 - samples/sec: 18.16 - lr: 0.000002
2023-06-03 05:10:41,656 ----------------------------------------------------------------------------------------------------
2023-06-03 05:10:41,656 EPOCH 7 done: loss 0.0824 - lr 0.000002
2023-06-03 05:12:07,880 Evaluating as a multi-label problem: False
2023-06-03 05:12:07,941 DEV : loss 0.08557754009962082 - f1-score (micro avg)  0.97
2023-06-03 05:12:08,048 BAD EPOCHS (no improvement): 4
2023-06-03 05:12:08,052 ----------------------------------------------------------------------------------------------------
2023-06-03 05:13:39,924 epoch 8 - iter 374/3747 - loss 0.07223077 - samples/sec: 16.29 - lr: 0.000002
2023-06-03 05:15:07,385 epoch 8 - iter 748/3747 - loss 0.07876446 - samples/sec: 17.11 - lr: 0.000002
2023-06-03 05:16:34,174 epoch 8 - iter 1122/3747 - loss 0.07810443 - samples/sec: 17.25 - lr: 0.000002
2023-06-03 05:18:01,827 epoch 8 - iter 1496/3747 - loss 0.07951184 - samples/sec: 17.08 - lr: 0.000001
2023-06-03 05:19:28,194 epoch 8 - iter 1870/3747 - loss 0.07641405 - samples/sec: 17.33 - lr: 0.000001
2023-06-03 05:20:55,187 epoch 8 - iter 2244/3747 - loss 0.07765528 - samples/sec: 17.20 - lr: 0.000001
2023-06-03 05:22:17,338 epoch 8 - iter 2618/3747 - loss 0.07525332 - samples/sec: 18.22 - lr: 0.000001
2023-06-03 05:23:39,784 epoch 8 - iter 2992/3747 - loss 0.07548488 - samples/sec: 18.15 - lr: 0.000001
2023-06-03 05:25:15,235 epoch 8 - iter 3366/3747 - loss 0.07427652 - samples/sec: 15.68 - lr: 0.000001
2023-06-03 05:26:51,733 epoch 8 - iter 3740/3747 - loss 0.07604084 - samples/sec: 15.51 - lr: 0.000001
2023-06-03 05:26:53,263 ----------------------------------------------------------------------------------------------------
2023-06-03 05:26:53,264 EPOCH 8 done: loss 0.0761 - lr 0.000001
2023-06-03 05:28:07,778 Evaluating as a multi-label problem: False
2023-06-03 05:28:07,823 DEV : loss 0.0973721593618393 - f1-score (micro avg)  0.968
2023-06-03 05:28:07,917 BAD EPOCHS (no improvement): 4
2023-06-03 05:28:07,920 ----------------------------------------------------------------------------------------------------
2023-06-03 05:29:34,104 epoch 9 - iter 374/3747 - loss 0.06596243 - samples/sec: 17.37 - lr: 0.000001
2023-06-03 05:31:06,352 epoch 9 - iter 748/3747 - loss 0.07544530 - samples/sec: 16.22 - lr: 0.000001
2023-06-03 05:32:35,307 epoch 9 - iter 1122/3747 - loss 0.07053678 - samples/sec: 16.83 - lr: 0.000001
2023-06-03 05:34:02,777 epoch 9 - iter 1496/3747 - loss 0.07108556 - samples/sec: 17.11 - lr: 0.000001
2023-06-03 05:35:27,463 epoch 9 - iter 1870/3747 - loss 0.06973698 - samples/sec: 17.67 - lr: 0.000001
2023-06-03 05:36:51,392 epoch 9 - iter 2244/3747 - loss 0.07076829 - samples/sec: 17.83 - lr: 0.000001
2023-06-03 05:38:17,853 epoch 9 - iter 2618/3747 - loss 0.07089675 - samples/sec: 17.31 - lr: 0.000001
2023-06-03 05:39:41,898 epoch 9 - iter 2992/3747 - loss 0.06957178 - samples/sec: 17.81 - lr: 0.000001
2023-06-03 05:41:08,889 epoch 9 - iter 3366/3747 - loss 0.06852188 - samples/sec: 17.21 - lr: 0.000001
2023-06-03 05:42:37,452 epoch 9 - iter 3740/3747 - loss 0.06866586 - samples/sec: 16.90 - lr: 0.000001
2023-06-03 05:42:39,046 ----------------------------------------------------------------------------------------------------
2023-06-03 05:42:39,046 EPOCH 9 done: loss 0.0687 - lr 0.000001
2023-06-03 05:44:05,375 Evaluating as a multi-label problem: False
2023-06-03 05:44:05,438 DEV : loss 0.10100679844617844 - f1-score (micro avg)  0.9699
2023-06-03 05:44:05,554 BAD EPOCHS (no improvement): 4
2023-06-03 05:44:05,594 ----------------------------------------------------------------------------------------------------
2023-06-03 05:45:33,665 epoch 10 - iter 374/3747 - loss 0.06648975 - samples/sec: 17.00 - lr: 0.000001
2023-06-03 05:46:55,483 epoch 10 - iter 748/3747 - loss 0.06328479 - samples/sec: 18.29 - lr: 0.000000
2023-06-03 05:48:21,037 epoch 10 - iter 1122/3747 - loss 0.06411298 - samples/sec: 17.49 - lr: 0.000000
2023-06-03 05:49:47,182 epoch 10 - iter 1496/3747 - loss 0.06304244 - samples/sec: 17.37 - lr: 0.000000
2023-06-03 05:51:12,832 epoch 10 - iter 1870/3747 - loss 0.06473935 - samples/sec: 17.47 - lr: 0.000000
2023-06-03 05:52:35,453 epoch 10 - iter 2244/3747 - loss 0.06359514 - samples/sec: 18.11 - lr: 0.000000
2023-06-03 05:54:04,556 epoch 10 - iter 2618/3747 - loss 0.06218104 - samples/sec: 16.80 - lr: 0.000000
2023-06-03 05:55:33,373 epoch 10 - iter 2992/3747 - loss 0.06250609 - samples/sec: 16.85 - lr: 0.000000
2023-06-03 05:57:01,872 epoch 10 - iter 3366/3747 - loss 0.06299593 - samples/sec: 16.91 - lr: 0.000000
2023-06-03 05:58:27,752 epoch 10 - iter 3740/3747 - loss 0.06309790 - samples/sec: 17.43 - lr: 0.000000
2023-06-03 05:58:29,357 ----------------------------------------------------------------------------------------------------
2023-06-03 05:58:29,357 EPOCH 10 done: loss 0.0630 - lr 0.000000
2023-06-03 05:59:53,567 Evaluating as a multi-label problem: False
2023-06-03 05:59:53,636 DEV : loss 0.10291402041912079 - f1-score (micro avg)  0.9706
2023-06-03 05:59:53,752 BAD EPOCHS (no improvement): 4
2023-06-03 06:00:06,119 ----------------------------------------------------------------------------------------------------
2023-06-03 06:00:06,123 Testing using last state of model ...
2023-06-03 06:01:30,392 Evaluating as a multi-label problem: False
2023-06-03 06:01:30,459 0.9273	0.9442	0.9357	0.9027
2023-06-03 06:01:30,459 
Results:
- F-score (micro) 0.9357
- F-score (macro) 0.9225
- Accuracy 0.9027

By class:
              precision    recall  f1-score   support

         ORG     0.9129    0.9404    0.9265      1661
         LOC     0.9414    0.9436    0.9425      1668
         PER     0.9827    0.9808    0.9817      1617
        MISC     0.8103    0.8704    0.8393       702

   micro avg     0.9273    0.9442    0.9357      5648
   macro avg     0.9118    0.9338    0.9225      5648
weighted avg     0.9285    0.9442    0.9362      5648

2023-06-03 06:01:30,460 ----------------------------------------------------------------------------------------------------
