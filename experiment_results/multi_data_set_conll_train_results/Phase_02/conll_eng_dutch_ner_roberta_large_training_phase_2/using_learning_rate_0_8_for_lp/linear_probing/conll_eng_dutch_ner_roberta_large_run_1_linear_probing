2023-06-05 11:51:32,113 ----------------------------------------------------------------------------------------------------
2023-06-05 11:51:32,117 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-05 11:51:32,121 ----------------------------------------------------------------------------------------------------
2023-06-05 11:51:32,122 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-05 11:51:32,124 ----------------------------------------------------------------------------------------------------
2023-06-05 11:51:32,124 Parameters:
2023-06-05 11:51:32,124  - learning_rate: "0.800000"
2023-06-05 11:51:32,124  - mini_batch_size: "32"
2023-06-05 11:51:32,124  - patience: "3"
2023-06-05 11:51:32,124  - anneal_factor: "0.5"
2023-06-05 11:51:32,124  - max_epochs: "10"
2023-06-05 11:51:32,124  - shuffle: "True"
2023-06-05 11:51:32,124  - train_with_dev: "False"
2023-06-05 11:51:32,124  - batch_growth_annealing: "False"
2023-06-05 11:51:32,126 ----------------------------------------------------------------------------------------------------
2023-06-05 11:51:32,126 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_1_linear_probing"
2023-06-05 11:51:32,126 ----------------------------------------------------------------------------------------------------
2023-06-05 11:51:32,126 Device: cuda:1
2023-06-05 11:51:32,126 ----------------------------------------------------------------------------------------------------
2023-06-05 11:51:32,126 Embeddings storage mode: none
2023-06-05 11:51:32,126 ----------------------------------------------------------------------------------------------------
2023-06-05 11:52:22,790 epoch 1 - iter 97/972 - loss 0.82539639 - samples/sec: 61.29 - lr: 0.079835
2023-06-05 11:53:04,106 epoch 1 - iter 194/972 - loss 1.05015259 - samples/sec: 75.15 - lr: 0.159671
2023-06-05 11:53:48,299 epoch 1 - iter 291/972 - loss 1.63788437 - samples/sec: 70.26 - lr: 0.239506
2023-06-05 11:54:36,509 epoch 1 - iter 388/972 - loss 2.19384970 - samples/sec: 64.41 - lr: 0.319342
2023-06-05 11:55:32,076 epoch 1 - iter 485/972 - loss 3.14162253 - samples/sec: 55.88 - lr: 0.399177
2023-06-05 11:56:17,254 epoch 1 - iter 582/972 - loss 4.15345897 - samples/sec: 68.73 - lr: 0.479012
2023-06-05 11:57:07,235 epoch 1 - iter 679/972 - loss 5.04733076 - samples/sec: 62.13 - lr: 0.558848
2023-06-05 11:57:48,908 epoch 1 - iter 776/972 - loss 5.78147931 - samples/sec: 74.51 - lr: 0.638683
2023-06-05 11:58:30,307 epoch 1 - iter 873/972 - loss 6.46067548 - samples/sec: 75.00 - lr: 0.718519
2023-06-05 11:59:12,420 epoch 1 - iter 970/972 - loss 6.90471695 - samples/sec: 73.73 - lr: 0.798354
2023-06-05 11:59:12,785 ----------------------------------------------------------------------------------------------------
2023-06-05 11:59:12,785 EPOCH 1 done: loss 6.9118 - lr 0.798354
2023-06-05 12:01:20,717 Evaluating as a multi-label problem: False
2023-06-05 12:01:20,795 DEV : loss 3.516543388366699 - f1-score (micro avg)  0.5922
2023-06-05 12:01:20,905 BAD EPOCHS (no improvement): 4
2023-06-05 12:01:20,909 ----------------------------------------------------------------------------------------------------
2023-06-05 12:02:06,782 epoch 2 - iter 97/972 - loss 10.02032922 - samples/sec: 67.69 - lr: 0.791131
2023-06-05 12:02:58,058 epoch 2 - iter 194/972 - loss 11.38081916 - samples/sec: 60.56 - lr: 0.782263
2023-06-05 12:03:44,446 epoch 2 - iter 291/972 - loss 12.00506638 - samples/sec: 66.94 - lr: 0.773394
2023-06-05 12:04:29,064 epoch 2 - iter 388/972 - loss 12.06195606 - samples/sec: 69.59 - lr: 0.764526
2023-06-05 12:05:22,622 epoch 2 - iter 485/972 - loss 12.16321136 - samples/sec: 57.98 - lr: 0.755657
2023-06-05 12:06:13,641 epoch 2 - iter 582/972 - loss 12.24016123 - samples/sec: 60.87 - lr: 0.746789
2023-06-05 12:07:05,176 epoch 2 - iter 679/972 - loss 12.29419696 - samples/sec: 60.25 - lr: 0.737920
2023-06-05 12:07:50,717 epoch 2 - iter 776/972 - loss 12.27796521 - samples/sec: 68.18 - lr: 0.729051
2023-06-05 12:08:39,295 epoch 2 - iter 873/972 - loss 12.11225677 - samples/sec: 63.92 - lr: 0.720183
2023-06-05 12:09:28,898 epoch 2 - iter 970/972 - loss 12.03434690 - samples/sec: 62.60 - lr: 0.711314
2023-06-05 12:09:29,529 ----------------------------------------------------------------------------------------------------
2023-06-05 12:09:29,529 EPOCH 2 done: loss 12.0365 - lr 0.711314
2023-06-05 12:12:05,220 Evaluating as a multi-label problem: False
2023-06-05 12:12:05,320 DEV : loss 4.520676612854004 - f1-score (micro avg)  0.5891
2023-06-05 12:12:05,524 BAD EPOCHS (no improvement): 4
2023-06-05 12:12:05,527 ----------------------------------------------------------------------------------------------------
2023-06-05 12:13:00,776 epoch 3 - iter 97/972 - loss 11.18449699 - samples/sec: 56.21 - lr: 0.702263
2023-06-05 12:13:51,088 epoch 3 - iter 194/972 - loss 11.35838198 - samples/sec: 61.72 - lr: 0.693394
2023-06-05 12:14:46,022 epoch 3 - iter 291/972 - loss 11.29760118 - samples/sec: 56.52 - lr: 0.684526
2023-06-05 12:15:29,924 epoch 3 - iter 388/972 - loss 11.03872432 - samples/sec: 70.73 - lr: 0.675657
2023-06-05 12:16:24,212 epoch 3 - iter 485/972 - loss 10.88795267 - samples/sec: 57.20 - lr: 0.666789
2023-06-05 12:17:14,885 epoch 3 - iter 582/972 - loss 10.77214812 - samples/sec: 61.28 - lr: 0.657920
2023-06-05 12:18:07,968 epoch 3 - iter 679/972 - loss 10.69298207 - samples/sec: 58.50 - lr: 0.649051
2023-06-05 12:18:56,599 epoch 3 - iter 776/972 - loss 10.71358706 - samples/sec: 63.85 - lr: 0.640183
2023-06-05 12:19:47,014 epoch 3 - iter 873/972 - loss 10.68501913 - samples/sec: 61.59 - lr: 0.631314
2023-06-05 12:20:38,837 epoch 3 - iter 970/972 - loss 10.54583983 - samples/sec: 59.92 - lr: 0.622446
2023-06-05 12:20:39,568 ----------------------------------------------------------------------------------------------------
2023-06-05 12:20:39,568 EPOCH 3 done: loss 10.5448 - lr 0.622446
2023-06-05 12:23:14,235 Evaluating as a multi-label problem: False
2023-06-05 12:23:14,336 DEV : loss 4.180838584899902 - f1-score (micro avg)  0.5732
2023-06-05 12:23:14,518 BAD EPOCHS (no improvement): 4
2023-06-05 12:23:14,523 ----------------------------------------------------------------------------------------------------
2023-06-05 12:24:10,035 epoch 4 - iter 97/972 - loss 9.41830659 - samples/sec: 55.94 - lr: 0.613394
2023-06-05 12:25:01,019 epoch 4 - iter 194/972 - loss 9.58679763 - samples/sec: 60.91 - lr: 0.604526
2023-06-05 12:25:51,005 epoch 4 - iter 291/972 - loss 9.57159213 - samples/sec: 62.12 - lr: 0.595657
2023-06-05 12:26:42,098 epoch 4 - iter 388/972 - loss 9.53098917 - samples/sec: 60.77 - lr: 0.586789
2023-06-05 12:27:32,339 epoch 4 - iter 485/972 - loss 9.50115553 - samples/sec: 61.81 - lr: 0.577920
2023-06-05 12:28:24,619 epoch 4 - iter 582/972 - loss 9.33438560 - samples/sec: 59.40 - lr: 0.569051
2023-06-05 12:29:16,932 epoch 4 - iter 679/972 - loss 9.25541402 - samples/sec: 59.36 - lr: 0.560183
2023-06-05 12:30:13,510 epoch 4 - iter 776/972 - loss 9.34351882 - samples/sec: 54.88 - lr: 0.551314
2023-06-05 12:31:00,919 epoch 4 - iter 873/972 - loss 9.28555771 - samples/sec: 65.50 - lr: 0.542446
2023-06-05 12:31:47,402 epoch 4 - iter 970/972 - loss 9.25090579 - samples/sec: 66.80 - lr: 0.533577
2023-06-05 12:31:47,934 ----------------------------------------------------------------------------------------------------
2023-06-05 12:31:47,935 EPOCH 4 done: loss 9.2494 - lr 0.533577
2023-06-05 12:34:21,847 Evaluating as a multi-label problem: False
2023-06-05 12:34:21,926 DEV : loss 3.491849422454834 - f1-score (micro avg)  0.5766
2023-06-05 12:34:22,096 BAD EPOCHS (no improvement): 4
2023-06-05 12:34:22,099 ----------------------------------------------------------------------------------------------------
2023-06-05 12:35:15,706 epoch 5 - iter 97/972 - loss 8.96476566 - samples/sec: 57.93 - lr: 0.524526
2023-06-05 12:36:04,558 epoch 5 - iter 194/972 - loss 9.14100611 - samples/sec: 63.57 - lr: 0.515657
2023-06-05 12:36:55,704 epoch 5 - iter 291/972 - loss 8.95669569 - samples/sec: 60.72 - lr: 0.506789
2023-06-05 12:37:48,648 epoch 5 - iter 388/972 - loss 8.94853392 - samples/sec: 58.65 - lr: 0.497920
2023-06-05 12:38:38,216 epoch 5 - iter 485/972 - loss 9.06857472 - samples/sec: 62.65 - lr: 0.489051
2023-06-05 12:39:33,019 epoch 5 - iter 582/972 - loss 8.95978608 - samples/sec: 56.66 - lr: 0.480183
2023-06-05 12:40:19,269 epoch 5 - iter 679/972 - loss 8.73847417 - samples/sec: 67.14 - lr: 0.471314
2023-06-05 12:41:16,291 epoch 5 - iter 776/972 - loss 8.54581588 - samples/sec: 54.46 - lr: 0.462446
2023-06-05 12:42:02,257 epoch 5 - iter 873/972 - loss 8.48056921 - samples/sec: 67.56 - lr: 0.453577
2023-06-05 12:42:53,789 epoch 5 - iter 970/972 - loss 8.39370480 - samples/sec: 60.26 - lr: 0.444709
2023-06-05 12:42:54,439 ----------------------------------------------------------------------------------------------------
2023-06-05 12:42:54,439 EPOCH 5 done: loss 8.3956 - lr 0.444709
2023-06-05 12:45:27,571 Evaluating as a multi-label problem: False
2023-06-05 12:45:27,669 DEV : loss 4.270164489746094 - f1-score (micro avg)  0.5337
2023-06-05 12:45:27,838 BAD EPOCHS (no improvement): 4
2023-06-05 12:45:27,845 ----------------------------------------------------------------------------------------------------
2023-06-05 12:46:18,628 epoch 6 - iter 97/972 - loss 8.34284169 - samples/sec: 61.15 - lr: 0.435657
2023-06-05 12:47:14,149 epoch 6 - iter 194/972 - loss 8.19097874 - samples/sec: 55.93 - lr: 0.426789
2023-06-05 12:48:00,297 epoch 6 - iter 291/972 - loss 7.90232000 - samples/sec: 67.29 - lr: 0.417920
2023-06-05 12:48:53,256 epoch 6 - iter 388/972 - loss 7.74159537 - samples/sec: 58.63 - lr: 0.409051
2023-06-05 12:49:42,744 epoch 6 - iter 485/972 - loss 7.47084063 - samples/sec: 62.75 - lr: 0.400183
2023-06-05 12:50:33,917 epoch 6 - iter 582/972 - loss 7.28097017 - samples/sec: 60.68 - lr: 0.391314
2023-06-05 12:51:20,714 epoch 6 - iter 679/972 - loss 7.13222699 - samples/sec: 66.35 - lr: 0.382446
2023-06-05 12:52:06,600 epoch 6 - iter 776/972 - loss 7.08790159 - samples/sec: 67.67 - lr: 0.373577
2023-06-05 12:53:00,752 epoch 6 - iter 873/972 - loss 7.02388319 - samples/sec: 57.34 - lr: 0.364709
2023-06-05 12:53:51,790 epoch 6 - iter 970/972 - loss 6.98060323 - samples/sec: 60.84 - lr: 0.355840
2023-06-05 12:53:52,465 ----------------------------------------------------------------------------------------------------
2023-06-05 12:53:52,465 EPOCH 6 done: loss 6.9791 - lr 0.355840
2023-06-05 12:56:33,702 Evaluating as a multi-label problem: False
2023-06-05 12:56:33,818 DEV : loss 3.119302988052368 - f1-score (micro avg)  0.5818
2023-06-05 12:56:33,988 BAD EPOCHS (no improvement): 4
2023-06-05 12:56:33,991 ----------------------------------------------------------------------------------------------------
2023-06-05 12:57:27,003 epoch 7 - iter 97/972 - loss 6.26055242 - samples/sec: 58.58 - lr: 0.346789
2023-06-05 12:58:24,102 epoch 7 - iter 194/972 - loss 6.56107183 - samples/sec: 54.38 - lr: 0.337920
2023-06-05 12:59:15,739 epoch 7 - iter 291/972 - loss 6.44977328 - samples/sec: 60.13 - lr: 0.329051
2023-06-05 13:00:08,177 epoch 7 - iter 388/972 - loss 6.36828880 - samples/sec: 59.22 - lr: 0.320183
2023-06-05 13:01:04,452 epoch 7 - iter 485/972 - loss 6.30797602 - samples/sec: 55.17 - lr: 0.311314
2023-06-05 13:01:57,468 epoch 7 - iter 582/972 - loss 6.17044254 - samples/sec: 58.57 - lr: 0.302446
2023-06-05 13:02:56,131 epoch 7 - iter 679/972 - loss 6.15328266 - samples/sec: 52.93 - lr: 0.293577
2023-06-05 13:03:47,168 epoch 7 - iter 776/972 - loss 6.13872039 - samples/sec: 60.84 - lr: 0.284709
2023-06-05 13:04:42,786 epoch 7 - iter 873/972 - loss 6.12524389 - samples/sec: 55.83 - lr: 0.275840
2023-06-05 13:05:32,723 epoch 7 - iter 970/972 - loss 6.04866567 - samples/sec: 62.19 - lr: 0.266971
2023-06-05 13:05:33,389 ----------------------------------------------------------------------------------------------------
2023-06-05 13:05:33,389 EPOCH 7 done: loss 6.0458 - lr 0.266971
2023-06-05 13:08:09,455 Evaluating as a multi-label problem: False
2023-06-05 13:08:09,555 DEV : loss 1.9527806043624878 - f1-score (micro avg)  0.685
2023-06-05 13:08:09,748 BAD EPOCHS (no improvement): 4
2023-06-05 13:08:09,751 ----------------------------------------------------------------------------------------------------
2023-06-05 13:09:03,509 epoch 8 - iter 97/972 - loss 4.83547214 - samples/sec: 57.76 - lr: 0.257920
2023-06-05 13:09:52,028 epoch 8 - iter 194/972 - loss 4.97361360 - samples/sec: 64.00 - lr: 0.249051
2023-06-05 13:10:39,994 epoch 8 - iter 291/972 - loss 4.94376786 - samples/sec: 64.73 - lr: 0.240183
2023-06-05 13:11:30,721 epoch 8 - iter 388/972 - loss 4.86287964 - samples/sec: 61.22 - lr: 0.231314
2023-06-05 13:12:21,354 epoch 8 - iter 485/972 - loss 4.76192514 - samples/sec: 61.32 - lr: 0.222446
2023-06-05 13:13:05,108 epoch 8 - iter 582/972 - loss 4.71119204 - samples/sec: 70.97 - lr: 0.213577
2023-06-05 13:13:58,333 epoch 8 - iter 679/972 - loss 4.67289084 - samples/sec: 58.34 - lr: 0.204709
2023-06-05 13:14:49,296 epoch 8 - iter 776/972 - loss 4.62289705 - samples/sec: 60.93 - lr: 0.195840
2023-06-05 13:15:39,187 epoch 8 - iter 873/972 - loss 4.56250520 - samples/sec: 62.24 - lr: 0.186971
2023-06-05 13:16:32,065 epoch 8 - iter 970/972 - loss 4.48215619 - samples/sec: 58.72 - lr: 0.178103
2023-06-05 13:16:32,754 ----------------------------------------------------------------------------------------------------
2023-06-05 13:16:32,754 EPOCH 8 done: loss 4.4810 - lr 0.178103
2023-06-05 13:19:06,594 Evaluating as a multi-label problem: False
2023-06-05 13:19:06,706 DEV : loss 1.4919624328613281 - f1-score (micro avg)  0.6515
2023-06-05 13:19:06,923 BAD EPOCHS (no improvement): 4
2023-06-05 13:19:06,926 ----------------------------------------------------------------------------------------------------
2023-06-05 13:20:02,203 epoch 9 - iter 97/972 - loss 3.51910161 - samples/sec: 56.18 - lr: 0.169051
2023-06-05 13:20:53,289 epoch 9 - iter 194/972 - loss 3.53326378 - samples/sec: 60.78 - lr: 0.160183
2023-06-05 13:21:48,762 epoch 9 - iter 291/972 - loss 3.47765027 - samples/sec: 55.98 - lr: 0.151314
2023-06-05 13:22:39,518 epoch 9 - iter 388/972 - loss 3.43493976 - samples/sec: 61.18 - lr: 0.142446
2023-06-05 13:23:30,329 epoch 9 - iter 485/972 - loss 3.38111834 - samples/sec: 61.11 - lr: 0.133577
2023-06-05 13:24:24,942 epoch 9 - iter 582/972 - loss 3.31073982 - samples/sec: 56.85 - lr: 0.124709
2023-06-05 13:25:16,138 epoch 9 - iter 679/972 - loss 3.25672305 - samples/sec: 60.65 - lr: 0.115840
2023-06-05 13:26:07,125 epoch 9 - iter 776/972 - loss 3.18044242 - samples/sec: 60.90 - lr: 0.106971
2023-06-05 13:26:49,575 epoch 9 - iter 873/972 - loss 3.11473468 - samples/sec: 73.15 - lr: 0.098103
2023-06-05 13:27:41,498 epoch 9 - iter 970/972 - loss 3.03713942 - samples/sec: 59.80 - lr: 0.089234
2023-06-05 13:27:42,022 ----------------------------------------------------------------------------------------------------
2023-06-05 13:27:42,022 EPOCH 9 done: loss 3.0359 - lr 0.089234
2023-06-05 13:30:13,245 Evaluating as a multi-label problem: False
2023-06-05 13:30:13,341 DEV : loss 0.8691738843917847 - f1-score (micro avg)  0.7013
2023-06-05 13:30:13,520 BAD EPOCHS (no improvement): 4
2023-06-05 13:30:13,525 ----------------------------------------------------------------------------------------------------
2023-06-05 13:31:04,394 epoch 10 - iter 97/972 - loss 2.25095529 - samples/sec: 61.05 - lr: 0.080183
2023-06-05 13:31:59,690 epoch 10 - iter 194/972 - loss 2.24963716 - samples/sec: 56.15 - lr: 0.071314
2023-06-05 13:32:49,013 epoch 10 - iter 291/972 - loss 2.14127494 - samples/sec: 62.96 - lr: 0.062446
2023-06-05 13:33:35,935 epoch 10 - iter 388/972 - loss 2.07023898 - samples/sec: 66.18 - lr: 0.053577
2023-06-05 13:34:21,686 epoch 10 - iter 485/972 - loss 1.98758582 - samples/sec: 67.87 - lr: 0.044709
2023-06-05 13:35:09,238 epoch 10 - iter 582/972 - loss 1.90171422 - samples/sec: 65.30 - lr: 0.035840
2023-06-05 13:36:00,215 epoch 10 - iter 679/972 - loss 1.81154299 - samples/sec: 60.91 - lr: 0.026971
2023-06-05 13:36:54,563 epoch 10 - iter 776/972 - loss 1.73586086 - samples/sec: 57.13 - lr: 0.018103
2023-06-05 13:37:45,881 epoch 10 - iter 873/972 - loss 1.66235108 - samples/sec: 60.51 - lr: 0.009234
2023-06-05 13:38:28,701 epoch 10 - iter 970/972 - loss 1.60156980 - samples/sec: 72.52 - lr: 0.000366
2023-06-05 13:38:29,239 ----------------------------------------------------------------------------------------------------
2023-06-05 13:38:29,239 EPOCH 10 done: loss 1.6005 - lr 0.000366
2023-06-05 13:40:47,592 Evaluating as a multi-label problem: False
2023-06-05 13:40:47,686 DEV : loss 0.37038275599479675 - f1-score (micro avg)  0.7539
2023-06-05 13:40:47,850 BAD EPOCHS (no improvement): 4
2023-06-05 13:41:00,066 ----------------------------------------------------------------------------------------------------
2023-06-05 13:41:00,069 Testing using last state of model ...
2023-06-05 13:44:26,914 Evaluating as a multi-label problem: False
2023-06-05 13:44:26,986 0.7424	0.6951	0.7179	0.6002
2023-06-05 13:44:26,986 
Results:
- F-score (micro) 0.7179
- F-score (macro) 0.7003
- Accuracy 0.6002

By class:
              precision    recall  f1-score   support

         PER     0.8592    0.8832    0.8710      2715
         LOC     0.7501    0.7535    0.7518      2442
         ORG     0.6426    0.5918    0.6162      2543
        MISC     0.6624    0.4881    0.5620      1889

   micro avg     0.7424    0.6951    0.7179      9589
   macro avg     0.7286    0.6792    0.7003      9589
weighted avg     0.7352    0.6951    0.7122      9589

2023-06-05 13:44:26,986 ----------------------------------------------------------------------------------------------------
2023-06-05 13:44:26,986 ----------------------------------------------------------------------------------------------------
2023-06-05 13:46:33,747 Evaluating as a multi-label problem: False
2023-06-05 13:46:33,796 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-05 13:46:33,797 0.7095	0.6092	0.6556	0.5305
2023-06-05 13:46:33,797 ----------------------------------------------------------------------------------------------------
2023-06-05 13:47:57,179 Evaluating as a multi-label problem: False
2023-06-05 13:47:57,251 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-05 13:47:57,251 0.7626	0.7551	0.7588	0.6486
