2023-05-28 08:01:28,742 ----------------------------------------------------------------------------------------------------
2023-05-28 08:01:28,747 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-28 08:01:28,749 ----------------------------------------------------------------------------------------------------
2023-05-28 08:01:28,749 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-28 08:01:28,749 ----------------------------------------------------------------------------------------------------
2023-05-28 08:01:28,749 Parameters:
2023-05-28 08:01:28,749  - learning_rate: "0.800000"
2023-05-28 08:01:28,749  - mini_batch_size: "32"
2023-05-28 08:01:28,749  - patience: "3"
2023-05-28 08:01:28,752  - anneal_factor: "0.5"
2023-05-28 08:01:28,752  - max_epochs: "10"
2023-05-28 08:01:28,752  - shuffle: "True"
2023-05-28 08:01:28,752  - train_with_dev: "False"
2023-05-28 08:01:28,752  - batch_growth_annealing: "False"
2023-05-28 08:01:28,752 ----------------------------------------------------------------------------------------------------
2023-05-28 08:01:28,752 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_3_linear_probing"
2023-05-28 08:01:28,752 ----------------------------------------------------------------------------------------------------
2023-05-28 08:01:28,753 Device: cuda:3
2023-05-28 08:01:28,753 ----------------------------------------------------------------------------------------------------
2023-05-28 08:01:28,753 Embeddings storage mode: none
2023-05-28 08:01:28,753 ----------------------------------------------------------------------------------------------------
2023-05-28 08:02:20,265 epoch 1 - iter 97/972 - loss 0.85191889 - samples/sec: 60.28 - lr: 0.079835
2023-05-28 08:03:10,057 epoch 1 - iter 194/972 - loss 1.18382129 - samples/sec: 62.36 - lr: 0.159671
2023-05-28 08:04:07,251 epoch 1 - iter 291/972 - loss 1.75579349 - samples/sec: 54.29 - lr: 0.239506
2023-05-28 08:04:59,954 epoch 1 - iter 388/972 - loss 2.40554357 - samples/sec: 58.92 - lr: 0.319342
2023-05-28 08:05:59,311 epoch 1 - iter 485/972 - loss 3.18207897 - samples/sec: 52.31 - lr: 0.399177
2023-05-28 08:06:49,858 epoch 1 - iter 582/972 - loss 4.20741551 - samples/sec: 61.43 - lr: 0.479012
2023-05-28 08:07:31,284 epoch 1 - iter 679/972 - loss 4.98904017 - samples/sec: 74.95 - lr: 0.558848
2023-05-28 08:08:24,011 epoch 1 - iter 776/972 - loss 5.62729297 - samples/sec: 58.89 - lr: 0.638683
2023-05-28 08:09:11,597 epoch 1 - iter 873/972 - loss 6.38492042 - samples/sec: 65.25 - lr: 0.718519
2023-05-28 08:10:05,462 epoch 1 - iter 970/972 - loss 7.04023445 - samples/sec: 57.65 - lr: 0.798354
2023-05-28 08:10:05,952 ----------------------------------------------------------------------------------------------------
2023-05-28 08:10:05,953 EPOCH 1 done: loss 7.0481 - lr 0.798354
2023-05-28 08:12:46,626 Evaluating as a multi-label problem: False
2023-05-28 08:12:46,703 DEV : loss 3.7305822372436523 - f1-score (micro avg)  0.5926
2023-05-28 08:12:46,885 BAD EPOCHS (no improvement): 4
2023-05-28 08:12:46,888 ----------------------------------------------------------------------------------------------------
2023-05-28 08:13:41,248 epoch 2 - iter 97/972 - loss 12.41256908 - samples/sec: 57.13 - lr: 0.791131
2023-05-28 08:14:32,039 epoch 2 - iter 194/972 - loss 12.19628721 - samples/sec: 61.14 - lr: 0.782263
2023-05-28 08:15:30,365 epoch 2 - iter 291/972 - loss 12.17101232 - samples/sec: 53.24 - lr: 0.773394
2023-05-28 08:16:18,887 epoch 2 - iter 388/972 - loss 12.16634125 - samples/sec: 64.00 - lr: 0.764526
2023-05-28 08:17:11,737 epoch 2 - iter 485/972 - loss 12.15005769 - samples/sec: 58.75 - lr: 0.755657
2023-05-28 08:18:04,338 epoch 2 - iter 582/972 - loss 11.95612494 - samples/sec: 59.03 - lr: 0.746789
2023-05-28 08:18:54,825 epoch 2 - iter 679/972 - loss 11.73220941 - samples/sec: 61.51 - lr: 0.737920
2023-05-28 08:19:48,909 epoch 2 - iter 776/972 - loss 11.68635930 - samples/sec: 57.41 - lr: 0.729051
2023-05-28 08:20:37,842 epoch 2 - iter 873/972 - loss 11.73045749 - samples/sec: 63.46 - lr: 0.720183
2023-05-28 08:21:37,143 epoch 2 - iter 970/972 - loss 11.80100856 - samples/sec: 52.36 - lr: 0.711314
2023-05-28 08:21:37,809 ----------------------------------------------------------------------------------------------------
2023-05-28 08:21:37,809 EPOCH 2 done: loss 11.7962 - lr 0.711314
2023-05-28 08:24:12,492 Evaluating as a multi-label problem: False
2023-05-28 08:24:12,587 DEV : loss 4.050731182098389 - f1-score (micro avg)  0.6296
2023-05-28 08:24:12,789 BAD EPOCHS (no improvement): 4
2023-05-28 08:24:12,791 ----------------------------------------------------------------------------------------------------
2023-05-28 08:25:02,784 epoch 3 - iter 97/972 - loss 10.52730663 - samples/sec: 62.12 - lr: 0.702263
2023-05-28 08:25:52,143 epoch 3 - iter 194/972 - loss 10.79205675 - samples/sec: 62.91 - lr: 0.693394
2023-05-28 08:26:48,029 epoch 3 - iter 291/972 - loss 11.05168917 - samples/sec: 55.56 - lr: 0.684526
2023-05-28 08:27:39,197 epoch 3 - iter 388/972 - loss 11.07022568 - samples/sec: 60.69 - lr: 0.675657
2023-05-28 08:28:36,333 epoch 3 - iter 485/972 - loss 11.01705656 - samples/sec: 54.35 - lr: 0.666789
2023-05-28 08:29:25,749 epoch 3 - iter 582/972 - loss 10.84794107 - samples/sec: 62.84 - lr: 0.657920
2023-05-28 08:30:15,836 epoch 3 - iter 679/972 - loss 10.68553567 - samples/sec: 62.00 - lr: 0.649051
2023-05-28 08:31:12,380 epoch 3 - iter 776/972 - loss 10.59082914 - samples/sec: 54.91 - lr: 0.640183
2023-05-28 08:32:01,370 epoch 3 - iter 873/972 - loss 10.52020685 - samples/sec: 63.38 - lr: 0.631314
2023-05-28 08:32:54,657 epoch 3 - iter 970/972 - loss 10.38396701 - samples/sec: 58.27 - lr: 0.622446
2023-05-28 08:32:55,352 ----------------------------------------------------------------------------------------------------
2023-05-28 08:32:55,352 EPOCH 3 done: loss 10.3798 - lr 0.622446
2023-05-28 08:35:37,161 Evaluating as a multi-label problem: False
2023-05-28 08:35:37,264 DEV : loss 3.5871312618255615 - f1-score (micro avg)  0.5779
2023-05-28 08:35:37,480 BAD EPOCHS (no improvement): 4
2023-05-28 08:35:37,483 ----------------------------------------------------------------------------------------------------
2023-05-28 08:36:27,428 epoch 4 - iter 97/972 - loss 10.10781564 - samples/sec: 62.18 - lr: 0.613394
2023-05-28 08:37:19,291 epoch 4 - iter 194/972 - loss 10.05456464 - samples/sec: 59.87 - lr: 0.604526
2023-05-28 08:38:14,848 epoch 4 - iter 291/972 - loss 9.85936182 - samples/sec: 55.89 - lr: 0.595657
2023-05-28 08:39:06,030 epoch 4 - iter 388/972 - loss 9.70125884 - samples/sec: 60.67 - lr: 0.586789
2023-05-28 08:40:03,256 epoch 4 - iter 485/972 - loss 9.64977120 - samples/sec: 54.26 - lr: 0.577920
2023-05-28 08:40:55,618 epoch 4 - iter 582/972 - loss 9.63946759 - samples/sec: 59.30 - lr: 0.569051
2023-05-28 08:41:48,200 epoch 4 - iter 679/972 - loss 9.67373492 - samples/sec: 59.06 - lr: 0.560183
2023-05-28 08:42:44,481 epoch 4 - iter 776/972 - loss 9.69076843 - samples/sec: 55.17 - lr: 0.551314
2023-05-28 08:43:36,050 epoch 4 - iter 873/972 - loss 9.61986016 - samples/sec: 60.21 - lr: 0.542446
2023-05-28 08:44:33,786 epoch 4 - iter 970/972 - loss 9.50418052 - samples/sec: 53.78 - lr: 0.533577
2023-05-28 08:44:34,364 ----------------------------------------------------------------------------------------------------
2023-05-28 08:44:34,364 EPOCH 4 done: loss 9.5021 - lr 0.533577
2023-05-28 08:47:13,414 Evaluating as a multi-label problem: False
2023-05-28 08:47:13,521 DEV : loss 3.0108883380889893 - f1-score (micro avg)  0.6522
2023-05-28 08:47:13,756 BAD EPOCHS (no improvement): 4
2023-05-28 08:47:13,759 ----------------------------------------------------------------------------------------------------
2023-05-28 08:48:02,759 epoch 5 - iter 97/972 - loss 8.32096192 - samples/sec: 63.37 - lr: 0.524526
2023-05-28 08:48:55,129 epoch 5 - iter 194/972 - loss 8.73530904 - samples/sec: 59.29 - lr: 0.515657
2023-05-28 08:49:52,028 epoch 5 - iter 291/972 - loss 8.54685476 - samples/sec: 54.57 - lr: 0.506789
2023-05-28 08:50:42,349 epoch 5 - iter 388/972 - loss 8.43896356 - samples/sec: 61.71 - lr: 0.497920
2023-05-28 08:51:37,207 epoch 5 - iter 485/972 - loss 8.49227886 - samples/sec: 56.60 - lr: 0.489051
2023-05-28 08:52:28,491 epoch 5 - iter 582/972 - loss 8.47507000 - samples/sec: 60.55 - lr: 0.480183
2023-05-28 08:53:21,699 epoch 5 - iter 679/972 - loss 8.41289640 - samples/sec: 58.36 - lr: 0.471314
2023-05-28 08:54:18,333 epoch 5 - iter 776/972 - loss 8.40089044 - samples/sec: 54.82 - lr: 0.462446
2023-05-28 08:55:07,959 epoch 5 - iter 873/972 - loss 8.38152554 - samples/sec: 62.57 - lr: 0.453577
2023-05-28 08:56:02,814 epoch 5 - iter 970/972 - loss 8.32204309 - samples/sec: 56.61 - lr: 0.444709
2023-05-28 08:56:03,486 ----------------------------------------------------------------------------------------------------
2023-05-28 08:56:03,486 EPOCH 5 done: loss 8.3221 - lr 0.444709
2023-05-28 08:58:47,418 Evaluating as a multi-label problem: False
2023-05-28 08:58:47,521 DEV : loss 3.381502628326416 - f1-score (micro avg)  0.5795
2023-05-28 08:58:47,719 BAD EPOCHS (no improvement): 4
2023-05-28 08:58:47,722 ----------------------------------------------------------------------------------------------------
2023-05-28 08:59:39,665 epoch 6 - iter 97/972 - loss 7.68026793 - samples/sec: 59.78 - lr: 0.435657
2023-05-28 09:00:26,898 epoch 6 - iter 194/972 - loss 7.29971341 - samples/sec: 65.74 - lr: 0.426789
2023-05-28 09:01:23,206 epoch 6 - iter 291/972 - loss 7.36429923 - samples/sec: 55.14 - lr: 0.417920
2023-05-28 09:02:15,080 epoch 6 - iter 388/972 - loss 7.33691302 - samples/sec: 59.86 - lr: 0.409051
2023-05-28 09:03:10,809 epoch 6 - iter 485/972 - loss 7.21971407 - samples/sec: 55.72 - lr: 0.400183
2023-05-28 09:04:02,490 epoch 6 - iter 582/972 - loss 7.16178374 - samples/sec: 60.08 - lr: 0.391314
2023-05-28 09:04:49,958 epoch 6 - iter 679/972 - loss 7.08179738 - samples/sec: 65.42 - lr: 0.382446
2023-05-28 09:05:45,827 epoch 6 - iter 776/972 - loss 7.05744395 - samples/sec: 55.58 - lr: 0.373577
2023-05-28 09:06:38,426 epoch 6 - iter 873/972 - loss 7.02088613 - samples/sec: 59.04 - lr: 0.364709
2023-05-28 09:07:36,090 epoch 6 - iter 970/972 - loss 6.94858051 - samples/sec: 53.85 - lr: 0.355840
2023-05-28 09:07:36,742 ----------------------------------------------------------------------------------------------------
2023-05-28 09:07:36,743 EPOCH 6 done: loss 6.9512 - lr 0.355840
2023-05-28 09:10:16,865 Evaluating as a multi-label problem: False
2023-05-28 09:10:16,967 DEV : loss 2.1543936729431152 - f1-score (micro avg)  0.6239
2023-05-28 09:10:17,165 BAD EPOCHS (no improvement): 4
2023-05-28 09:10:17,168 ----------------------------------------------------------------------------------------------------
2023-05-28 09:11:08,718 epoch 7 - iter 97/972 - loss 6.20673466 - samples/sec: 60.24 - lr: 0.346789
2023-05-28 09:12:05,936 epoch 7 - iter 194/972 - loss 6.25906420 - samples/sec: 54.27 - lr: 0.337920
2023-05-28 09:12:54,445 epoch 7 - iter 291/972 - loss 6.18663525 - samples/sec: 64.01 - lr: 0.329051
2023-05-28 09:13:46,343 epoch 7 - iter 388/972 - loss 6.08254219 - samples/sec: 59.83 - lr: 0.320183
2023-05-28 09:14:43,808 epoch 7 - iter 485/972 - loss 6.09653686 - samples/sec: 54.03 - lr: 0.311314
2023-05-28 09:15:34,367 epoch 7 - iter 582/972 - loss 6.06205301 - samples/sec: 61.42 - lr: 0.302446
2023-05-28 09:16:26,391 epoch 7 - iter 679/972 - loss 6.01621721 - samples/sec: 59.69 - lr: 0.293577
2023-05-28 09:17:15,724 epoch 7 - iter 776/972 - loss 5.96349709 - samples/sec: 62.94 - lr: 0.284709
2023-05-28 09:18:07,176 epoch 7 - iter 873/972 - loss 5.89661705 - samples/sec: 60.35 - lr: 0.275840
2023-05-28 09:19:04,637 epoch 7 - iter 970/972 - loss 5.84339306 - samples/sec: 54.04 - lr: 0.266971
2023-05-28 09:19:05,342 ----------------------------------------------------------------------------------------------------
2023-05-28 09:19:05,342 EPOCH 7 done: loss 5.8404 - lr 0.266971
2023-05-28 09:21:56,118 Evaluating as a multi-label problem: False
2023-05-28 09:21:56,221 DEV : loss 2.7076005935668945 - f1-score (micro avg)  0.5732
2023-05-28 09:21:56,416 BAD EPOCHS (no improvement): 4
2023-05-28 09:21:56,419 ----------------------------------------------------------------------------------------------------
2023-05-28 09:22:46,365 epoch 8 - iter 97/972 - loss 5.41064413 - samples/sec: 62.17 - lr: 0.257920
2023-05-28 09:23:42,844 epoch 8 - iter 194/972 - loss 5.20416051 - samples/sec: 54.98 - lr: 0.249051
2023-05-28 09:24:32,903 epoch 8 - iter 291/972 - loss 5.17096843 - samples/sec: 62.03 - lr: 0.240183
2023-05-28 09:25:23,009 epoch 8 - iter 388/972 - loss 5.03265115 - samples/sec: 61.97 - lr: 0.231314
2023-05-28 09:26:20,286 epoch 8 - iter 485/972 - loss 4.99476525 - samples/sec: 54.21 - lr: 0.222446
2023-05-28 09:27:10,775 epoch 8 - iter 582/972 - loss 4.90610623 - samples/sec: 61.50 - lr: 0.213577
2023-05-28 09:28:05,846 epoch 8 - iter 679/972 - loss 4.81375058 - samples/sec: 56.38 - lr: 0.204709
2023-05-28 09:28:52,660 epoch 8 - iter 776/972 - loss 4.72009574 - samples/sec: 66.33 - lr: 0.195840
2023-05-28 09:29:40,712 epoch 8 - iter 873/972 - loss 4.65264393 - samples/sec: 64.62 - lr: 0.186971
2023-05-28 09:30:35,643 epoch 8 - iter 970/972 - loss 4.59137715 - samples/sec: 56.53 - lr: 0.178103
2023-05-28 09:30:36,306 ----------------------------------------------------------------------------------------------------
2023-05-28 09:30:36,306 EPOCH 8 done: loss 4.5907 - lr 0.178103
2023-05-28 09:33:15,074 Evaluating as a multi-label problem: False
2023-05-28 09:33:15,179 DEV : loss 1.8530187606811523 - f1-score (micro avg)  0.6134
2023-05-28 09:33:15,428 BAD EPOCHS (no improvement): 4
2023-05-28 09:33:15,431 ----------------------------------------------------------------------------------------------------
2023-05-28 09:34:06,768 epoch 9 - iter 97/972 - loss 3.82270035 - samples/sec: 60.49 - lr: 0.169051
2023-05-28 09:35:02,009 epoch 9 - iter 194/972 - loss 3.59393635 - samples/sec: 56.21 - lr: 0.160183
2023-05-28 09:35:52,178 epoch 9 - iter 291/972 - loss 3.47186371 - samples/sec: 61.89 - lr: 0.151314
2023-05-28 09:36:43,029 epoch 9 - iter 388/972 - loss 3.41125780 - samples/sec: 61.06 - lr: 0.142446
2023-05-28 09:37:38,501 epoch 9 - iter 485/972 - loss 3.36379166 - samples/sec: 55.97 - lr: 0.133577
2023-05-28 09:38:28,412 epoch 9 - iter 582/972 - loss 3.30881865 - samples/sec: 62.21 - lr: 0.124709
2023-05-28 09:39:22,448 epoch 9 - iter 679/972 - loss 3.23505079 - samples/sec: 57.46 - lr: 0.115840
2023-05-28 09:40:13,048 epoch 9 - iter 776/972 - loss 3.16859845 - samples/sec: 61.37 - lr: 0.106971
2023-05-28 09:41:02,598 epoch 9 - iter 873/972 - loss 3.10547579 - samples/sec: 62.67 - lr: 0.098103
2023-05-28 09:41:58,294 epoch 9 - iter 970/972 - loss 3.03590984 - samples/sec: 55.75 - lr: 0.089234
2023-05-28 09:41:58,753 ----------------------------------------------------------------------------------------------------
2023-05-28 09:41:58,753 EPOCH 9 done: loss 3.0356 - lr 0.089234
2023-05-28 09:44:31,170 Evaluating as a multi-label problem: False
2023-05-28 09:44:31,269 DEV : loss 0.9256150126457214 - f1-score (micro avg)  0.7103
2023-05-28 09:44:31,463 BAD EPOCHS (no improvement): 4
2023-05-28 09:44:31,466 ----------------------------------------------------------------------------------------------------
2023-05-28 09:45:21,567 epoch 10 - iter 97/972 - loss 2.20265332 - samples/sec: 61.98 - lr: 0.080183
2023-05-28 09:46:18,569 epoch 10 - iter 194/972 - loss 2.14258259 - samples/sec: 54.47 - lr: 0.071314
2023-05-28 09:47:08,032 epoch 10 - iter 291/972 - loss 2.04920431 - samples/sec: 62.78 - lr: 0.062446
2023-05-28 09:48:00,692 epoch 10 - iter 388/972 - loss 1.98850975 - samples/sec: 58.97 - lr: 0.053577
2023-05-28 09:48:55,188 epoch 10 - iter 485/972 - loss 1.92196408 - samples/sec: 56.97 - lr: 0.044709
2023-05-28 09:49:43,244 epoch 10 - iter 582/972 - loss 1.84878290 - samples/sec: 64.62 - lr: 0.035840
2023-05-28 09:50:38,706 epoch 10 - iter 679/972 - loss 1.78118683 - samples/sec: 55.98 - lr: 0.026971
2023-05-28 09:51:26,742 epoch 10 - iter 776/972 - loss 1.70676506 - samples/sec: 64.64 - lr: 0.018103
2023-05-28 09:52:13,714 epoch 10 - iter 873/972 - loss 1.64700846 - samples/sec: 66.11 - lr: 0.009234
2023-05-28 09:53:08,404 epoch 10 - iter 970/972 - loss 1.59293567 - samples/sec: 56.77 - lr: 0.000366
2023-05-28 09:53:09,054 ----------------------------------------------------------------------------------------------------
2023-05-28 09:53:09,054 EPOCH 10 done: loss 1.5925 - lr 0.000366
2023-05-28 09:55:43,703 Evaluating as a multi-label problem: False
2023-05-28 09:55:43,801 DEV : loss 0.3788428008556366 - f1-score (micro avg)  0.7656
2023-05-28 09:55:43,995 BAD EPOCHS (no improvement): 4
2023-05-28 09:55:56,684 ----------------------------------------------------------------------------------------------------
2023-05-28 09:55:56,688 Testing using last state of model ...
2023-05-28 09:59:30,672 Evaluating as a multi-label problem: False
2023-05-28 09:59:30,787 0.7544	0.7029	0.7277	0.6144
2023-05-28 09:59:30,787 
Results:
- F-score (micro) 0.7277
- F-score (macro) 0.7119
- Accuracy 0.6144

By class:
              precision    recall  f1-score   support

         PER     0.8928    0.9020    0.8974      2715
         ORG     0.6200    0.5922    0.6058      2543
         LOC     0.7663    0.7465    0.7563      2442
        MISC     0.6956    0.5093    0.5880      1889

   micro avg     0.7544    0.7029    0.7277      9589
   macro avg     0.7437    0.6875    0.7119      9589
weighted avg     0.7494    0.7029    0.7232      9589

2023-05-28 09:59:30,787 ----------------------------------------------------------------------------------------------------
2023-05-28 09:59:30,787 ----------------------------------------------------------------------------------------------------
2023-05-28 10:01:46,809 Evaluating as a multi-label problem: False
2023-05-28 10:01:46,864 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-28 10:01:46,864 0.7244	0.6242	0.6706	0.55
2023-05-28 10:01:46,865 ----------------------------------------------------------------------------------------------------
2023-05-28 10:03:05,985 Evaluating as a multi-label problem: False
2023-05-28 10:03:06,051 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-28 10:03:06,051 0.7728	0.7578	0.7652	0.6588
