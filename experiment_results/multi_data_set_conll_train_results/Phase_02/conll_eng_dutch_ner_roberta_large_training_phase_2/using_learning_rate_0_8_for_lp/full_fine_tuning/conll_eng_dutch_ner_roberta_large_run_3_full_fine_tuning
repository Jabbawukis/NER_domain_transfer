2023-05-28 10:03:24,137 ----------------------------------------------------------------------------------------------------
2023-05-28 10:03:24,142 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-28 10:03:24,146 ----------------------------------------------------------------------------------------------------
2023-05-28 10:03:24,146 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-28 10:03:24,146 ----------------------------------------------------------------------------------------------------
2023-05-28 10:03:24,146 Parameters:
2023-05-28 10:03:24,146  - learning_rate: "0.000005"
2023-05-28 10:03:24,146  - mini_batch_size: "4"
2023-05-28 10:03:24,146  - patience: "3"
2023-05-28 10:03:24,146  - anneal_factor: "0.5"
2023-05-28 10:03:24,147  - max_epochs: "10"
2023-05-28 10:03:24,147  - shuffle: "True"
2023-05-28 10:03:24,147  - train_with_dev: "False"
2023-05-28 10:03:24,147  - batch_growth_annealing: "False"
2023-05-28 10:03:24,147 ----------------------------------------------------------------------------------------------------
2023-05-28 10:03:24,147 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_3_full_fine_tuning"
2023-05-28 10:03:24,147 ----------------------------------------------------------------------------------------------------
2023-05-28 10:03:24,147 Device: cuda:3
2023-05-28 10:03:24,147 ----------------------------------------------------------------------------------------------------
2023-05-28 10:03:24,147 Embeddings storage mode: none
2023-05-28 10:03:24,147 ----------------------------------------------------------------------------------------------------
2023-05-28 10:06:25,332 epoch 1 - iter 777/7770 - loss 0.78946799 - samples/sec: 17.16 - lr: 0.000001
2023-05-28 10:09:26,041 epoch 1 - iter 1554/7770 - loss 0.59302629 - samples/sec: 17.21 - lr: 0.000001
2023-05-28 10:12:25,915 epoch 1 - iter 2331/7770 - loss 0.51188319 - samples/sec: 17.29 - lr: 0.000002
2023-05-28 10:15:26,853 epoch 1 - iter 3108/7770 - loss 0.43917269 - samples/sec: 17.18 - lr: 0.000002
2023-05-28 10:18:27,724 epoch 1 - iter 3885/7770 - loss 0.39224661 - samples/sec: 17.19 - lr: 0.000003
2023-05-28 10:21:25,177 epoch 1 - iter 4662/7770 - loss 0.38035285 - samples/sec: 17.52 - lr: 0.000003
2023-05-28 10:24:19,357 epoch 1 - iter 5439/7770 - loss 0.36921201 - samples/sec: 17.85 - lr: 0.000003
2023-05-28 10:27:17,720 epoch 1 - iter 6216/7770 - loss 0.35639385 - samples/sec: 17.43 - lr: 0.000004
2023-05-28 10:30:15,385 epoch 1 - iter 6993/7770 - loss 0.34561603 - samples/sec: 17.50 - lr: 0.000005
2023-05-28 10:33:09,992 epoch 1 - iter 7770/7770 - loss 0.33151830 - samples/sec: 17.81 - lr: 0.000005
2023-05-28 10:33:09,995 ----------------------------------------------------------------------------------------------------
2023-05-28 10:33:09,995 EPOCH 1 done: loss 0.3315 - lr 0.000005
2023-05-28 10:35:58,479 Evaluating as a multi-label problem: False
2023-05-28 10:35:58,569 DEV : loss 0.12539716064929962 - f1-score (micro avg)  0.9144
2023-05-28 10:35:58,797 BAD EPOCHS (no improvement): 4
2023-05-28 10:35:58,800 ----------------------------------------------------------------------------------------------------
2023-05-28 10:39:01,595 epoch 2 - iter 777/7770 - loss 0.19421919 - samples/sec: 17.01 - lr: 0.000005
2023-05-28 10:42:05,201 epoch 2 - iter 1554/7770 - loss 0.18174052 - samples/sec: 16.94 - lr: 0.000005
2023-05-28 10:45:09,420 epoch 2 - iter 2331/7770 - loss 0.17884581 - samples/sec: 16.88 - lr: 0.000005
2023-05-28 10:48:13,166 epoch 2 - iter 3108/7770 - loss 0.18254842 - samples/sec: 16.92 - lr: 0.000005
2023-05-28 10:51:12,846 epoch 2 - iter 3885/7770 - loss 0.18406904 - samples/sec: 17.31 - lr: 0.000005
2023-05-28 10:54:11,884 epoch 2 - iter 4662/7770 - loss 0.18273960 - samples/sec: 17.37 - lr: 0.000005
2023-05-28 10:57:11,195 epoch 2 - iter 5439/7770 - loss 0.18120821 - samples/sec: 17.34 - lr: 0.000005
2023-05-28 11:00:10,495 epoch 2 - iter 6216/7770 - loss 0.17819475 - samples/sec: 17.34 - lr: 0.000005
2023-05-28 11:03:08,622 epoch 2 - iter 6993/7770 - loss 0.17680468 - samples/sec: 17.46 - lr: 0.000005
2023-05-28 11:06:09,464 epoch 2 - iter 7770/7770 - loss 0.17437059 - samples/sec: 17.19 - lr: 0.000004
2023-05-28 11:06:09,467 ----------------------------------------------------------------------------------------------------
2023-05-28 11:06:09,467 EPOCH 2 done: loss 0.1744 - lr 0.000004
2023-05-28 11:08:50,091 Evaluating as a multi-label problem: False
2023-05-28 11:08:50,181 DEV : loss 0.08655548840761185 - f1-score (micro avg)  0.953
2023-05-28 11:08:50,430 BAD EPOCHS (no improvement): 4
2023-05-28 11:08:50,433 ----------------------------------------------------------------------------------------------------
2023-05-28 11:11:51,946 epoch 3 - iter 777/7770 - loss 0.14045167 - samples/sec: 17.13 - lr: 0.000004
2023-05-28 11:14:55,070 epoch 3 - iter 1554/7770 - loss 0.12796292 - samples/sec: 16.98 - lr: 0.000004
2023-05-28 11:17:57,305 epoch 3 - iter 2331/7770 - loss 0.12748711 - samples/sec: 17.06 - lr: 0.000004
2023-05-28 11:20:54,931 epoch 3 - iter 3108/7770 - loss 0.12874887 - samples/sec: 17.51 - lr: 0.000004
2023-05-28 11:23:53,181 epoch 3 - iter 3885/7770 - loss 0.12772616 - samples/sec: 17.44 - lr: 0.000004
2023-05-28 11:26:52,084 epoch 3 - iter 4662/7770 - loss 0.13009941 - samples/sec: 17.38 - lr: 0.000004
2023-05-28 11:29:59,243 epoch 3 - iter 5439/7770 - loss 0.13186888 - samples/sec: 16.61 - lr: 0.000004
2023-05-28 11:33:02,260 epoch 3 - iter 6216/7770 - loss 0.13087365 - samples/sec: 16.99 - lr: 0.000004
2023-05-28 11:36:00,859 epoch 3 - iter 6993/7770 - loss 0.12980098 - samples/sec: 17.41 - lr: 0.000004
2023-05-28 11:39:01,740 epoch 3 - iter 7770/7770 - loss 0.12888973 - samples/sec: 17.19 - lr: 0.000004
2023-05-28 11:39:01,744 ----------------------------------------------------------------------------------------------------
2023-05-28 11:39:01,744 EPOCH 3 done: loss 0.1289 - lr 0.000004
2023-05-28 11:41:28,391 Evaluating as a multi-label problem: False
2023-05-28 11:41:28,481 DEV : loss 0.09156160056591034 - f1-score (micro avg)  0.9555
2023-05-28 11:41:28,720 BAD EPOCHS (no improvement): 4
2023-05-28 11:41:28,723 ----------------------------------------------------------------------------------------------------
2023-05-28 11:44:30,734 epoch 4 - iter 777/7770 - loss 0.09220886 - samples/sec: 17.08 - lr: 0.000004
2023-05-28 11:47:32,029 epoch 4 - iter 1554/7770 - loss 0.09907976 - samples/sec: 17.15 - lr: 0.000004
2023-05-28 11:50:34,166 epoch 4 - iter 2331/7770 - loss 0.10312783 - samples/sec: 17.07 - lr: 0.000004
2023-05-28 11:53:30,370 epoch 4 - iter 3108/7770 - loss 0.10343910 - samples/sec: 17.65 - lr: 0.000004
2023-05-28 11:56:29,716 epoch 4 - iter 3885/7770 - loss 0.10011525 - samples/sec: 17.34 - lr: 0.000004
2023-05-28 11:59:30,218 epoch 4 - iter 4662/7770 - loss 0.10017082 - samples/sec: 17.23 - lr: 0.000004
2023-05-28 12:02:26,633 epoch 4 - iter 5439/7770 - loss 0.10075755 - samples/sec: 17.63 - lr: 0.000004
2023-05-28 12:05:23,504 epoch 4 - iter 6216/7770 - loss 0.09875678 - samples/sec: 17.58 - lr: 0.000003
2023-05-28 12:08:22,365 epoch 4 - iter 6993/7770 - loss 0.09961977 - samples/sec: 17.39 - lr: 0.000003
2023-05-28 12:11:20,804 epoch 4 - iter 7770/7770 - loss 0.09941977 - samples/sec: 17.43 - lr: 0.000003
2023-05-28 12:11:20,807 ----------------------------------------------------------------------------------------------------
2023-05-28 12:11:20,807 EPOCH 4 done: loss 0.0994 - lr 0.000003
2023-05-28 12:14:01,840 Evaluating as a multi-label problem: False
2023-05-28 12:14:01,899 DEV : loss 0.09086278825998306 - f1-score (micro avg)  0.9609
2023-05-28 12:14:02,077 BAD EPOCHS (no improvement): 4
2023-05-28 12:14:02,080 ----------------------------------------------------------------------------------------------------
2023-05-28 12:17:04,914 epoch 5 - iter 777/7770 - loss 0.09712628 - samples/sec: 17.01 - lr: 0.000003
2023-05-28 12:20:04,745 epoch 5 - iter 1554/7770 - loss 0.09008718 - samples/sec: 17.29 - lr: 0.000003
2023-05-28 12:23:01,837 epoch 5 - iter 2331/7770 - loss 0.08629119 - samples/sec: 17.56 - lr: 0.000003
2023-05-28 12:26:08,123 epoch 5 - iter 3108/7770 - loss 0.08447104 - samples/sec: 16.69 - lr: 0.000003
2023-05-28 12:29:10,819 epoch 5 - iter 3885/7770 - loss 0.08345965 - samples/sec: 17.02 - lr: 0.000003
2023-05-28 12:32:10,923 epoch 5 - iter 4662/7770 - loss 0.08350004 - samples/sec: 17.26 - lr: 0.000003
2023-05-28 12:35:07,690 epoch 5 - iter 5439/7770 - loss 0.08411515 - samples/sec: 17.59 - lr: 0.000003
2023-05-28 12:38:07,283 epoch 5 - iter 6216/7770 - loss 0.08424806 - samples/sec: 17.31 - lr: 0.000003
2023-05-28 12:41:06,113 epoch 5 - iter 6993/7770 - loss 0.08317713 - samples/sec: 17.39 - lr: 0.000003
2023-05-28 12:44:03,671 epoch 5 - iter 7770/7770 - loss 0.08407101 - samples/sec: 17.51 - lr: 0.000003
2023-05-28 12:44:03,674 ----------------------------------------------------------------------------------------------------
2023-05-28 12:44:03,674 EPOCH 5 done: loss 0.0841 - lr 0.000003
2023-05-28 12:46:42,846 Evaluating as a multi-label problem: False
2023-05-28 12:46:42,940 DEV : loss 0.08570393919944763 - f1-score (micro avg)  0.965
2023-05-28 12:46:43,201 BAD EPOCHS (no improvement): 4
2023-05-28 12:46:43,204 ----------------------------------------------------------------------------------------------------
2023-05-28 12:49:45,713 epoch 6 - iter 777/7770 - loss 0.06837891 - samples/sec: 17.04 - lr: 0.000003
2023-05-28 12:52:46,036 epoch 6 - iter 1554/7770 - loss 0.06844989 - samples/sec: 17.24 - lr: 0.000003
2023-05-28 12:55:47,541 epoch 6 - iter 2331/7770 - loss 0.07046913 - samples/sec: 17.13 - lr: 0.000003
2023-05-28 12:58:43,503 epoch 6 - iter 3108/7770 - loss 0.07056643 - samples/sec: 17.67 - lr: 0.000003
2023-05-28 13:01:42,463 epoch 6 - iter 3885/7770 - loss 0.07026789 - samples/sec: 17.38 - lr: 0.000003
2023-05-28 13:04:41,042 epoch 6 - iter 4662/7770 - loss 0.07043060 - samples/sec: 17.41 - lr: 0.000002
2023-05-28 13:07:48,548 epoch 6 - iter 5439/7770 - loss 0.07044876 - samples/sec: 16.58 - lr: 0.000002
2023-05-28 13:10:49,338 epoch 6 - iter 6216/7770 - loss 0.07020272 - samples/sec: 17.20 - lr: 0.000002
2023-05-28 13:13:49,906 epoch 6 - iter 6993/7770 - loss 0.07029569 - samples/sec: 17.22 - lr: 0.000002
2023-05-28 13:16:48,773 epoch 6 - iter 7770/7770 - loss 0.07051389 - samples/sec: 17.38 - lr: 0.000002
2023-05-28 13:16:48,777 ----------------------------------------------------------------------------------------------------
2023-05-28 13:16:48,777 EPOCH 6 done: loss 0.0705 - lr 0.000002
2023-05-28 13:19:21,431 Evaluating as a multi-label problem: False
2023-05-28 13:19:21,526 DEV : loss 0.08313093334436417 - f1-score (micro avg)  0.9635
2023-05-28 13:19:21,759 BAD EPOCHS (no improvement): 4
2023-05-28 13:19:21,764 ----------------------------------------------------------------------------------------------------
2023-05-28 13:22:23,515 epoch 7 - iter 777/7770 - loss 0.07025300 - samples/sec: 17.11 - lr: 0.000002
2023-05-28 13:25:22,734 epoch 7 - iter 1554/7770 - loss 0.06542461 - samples/sec: 17.35 - lr: 0.000002
2023-05-28 13:28:23,970 epoch 7 - iter 2331/7770 - loss 0.06361278 - samples/sec: 17.16 - lr: 0.000002
2023-05-28 13:31:23,011 epoch 7 - iter 3108/7770 - loss 0.06604041 - samples/sec: 17.37 - lr: 0.000002
2023-05-28 13:34:24,466 epoch 7 - iter 3885/7770 - loss 0.06461329 - samples/sec: 17.14 - lr: 0.000002
2023-05-28 13:37:22,912 epoch 7 - iter 4662/7770 - loss 0.06767764 - samples/sec: 17.43 - lr: 0.000002
2023-05-28 13:40:21,950 epoch 7 - iter 5439/7770 - loss 0.06712983 - samples/sec: 17.37 - lr: 0.000002
2023-05-28 13:43:19,539 epoch 7 - iter 6216/7770 - loss 0.06742111 - samples/sec: 17.51 - lr: 0.000002
2023-05-28 13:46:16,713 epoch 7 - iter 6993/7770 - loss 0.06494900 - samples/sec: 17.55 - lr: 0.000002
2023-05-28 13:49:14,534 epoch 7 - iter 7770/7770 - loss 0.06461718 - samples/sec: 17.49 - lr: 0.000002
2023-05-28 13:49:14,538 ----------------------------------------------------------------------------------------------------
2023-05-28 13:49:14,539 EPOCH 7 done: loss 0.0646 - lr 0.000002
2023-05-28 13:52:02,481 Evaluating as a multi-label problem: False
2023-05-28 13:52:02,554 DEV : loss 0.0864194706082344 - f1-score (micro avg)  0.9639
2023-05-28 13:52:02,780 BAD EPOCHS (no improvement): 4
2023-05-28 13:52:02,783 ----------------------------------------------------------------------------------------------------
2023-05-28 13:55:05,822 epoch 8 - iter 777/7770 - loss 0.05077595 - samples/sec: 16.99 - lr: 0.000002
2023-05-28 13:58:07,373 epoch 8 - iter 1554/7770 - loss 0.05225387 - samples/sec: 17.13 - lr: 0.000002
2023-05-28 14:01:06,563 epoch 8 - iter 2331/7770 - loss 0.05333545 - samples/sec: 17.35 - lr: 0.000002
2023-05-28 14:04:12,585 epoch 8 - iter 3108/7770 - loss 0.05572566 - samples/sec: 16.72 - lr: 0.000001
2023-05-28 14:07:10,051 epoch 8 - iter 3885/7770 - loss 0.05622032 - samples/sec: 17.52 - lr: 0.000001
2023-05-28 14:10:05,291 epoch 8 - iter 4662/7770 - loss 0.05564385 - samples/sec: 17.74 - lr: 0.000001
2023-05-28 14:13:03,753 epoch 8 - iter 5439/7770 - loss 0.05680688 - samples/sec: 17.42 - lr: 0.000001
2023-05-28 14:16:03,429 epoch 8 - iter 6216/7770 - loss 0.05661959 - samples/sec: 17.31 - lr: 0.000001
2023-05-28 14:18:58,559 epoch 8 - iter 6993/7770 - loss 0.05812786 - samples/sec: 17.76 - lr: 0.000001
2023-05-28 14:21:56,834 epoch 8 - iter 7770/7770 - loss 0.05768657 - samples/sec: 17.44 - lr: 0.000001
2023-05-28 14:21:56,837 ----------------------------------------------------------------------------------------------------
2023-05-28 14:21:56,837 EPOCH 8 done: loss 0.0577 - lr 0.000001
2023-05-28 14:24:32,615 Evaluating as a multi-label problem: False
2023-05-28 14:24:32,678 DEV : loss 0.08687278628349304 - f1-score (micro avg)  0.966
2023-05-28 14:24:32,874 BAD EPOCHS (no improvement): 4
2023-05-28 14:24:32,877 ----------------------------------------------------------------------------------------------------
2023-05-28 14:27:31,517 epoch 9 - iter 777/7770 - loss 0.06080518 - samples/sec: 17.41 - lr: 0.000001
2023-05-28 14:30:31,437 epoch 9 - iter 1554/7770 - loss 0.05479819 - samples/sec: 17.28 - lr: 0.000001
2023-05-28 14:33:31,494 epoch 9 - iter 2331/7770 - loss 0.05334356 - samples/sec: 17.27 - lr: 0.000001
2023-05-28 14:36:30,371 epoch 9 - iter 3108/7770 - loss 0.05436900 - samples/sec: 17.38 - lr: 0.000001
2023-05-28 14:39:30,910 epoch 9 - iter 3885/7770 - loss 0.05392061 - samples/sec: 17.22 - lr: 0.000001
2023-05-28 14:42:27,619 epoch 9 - iter 4662/7770 - loss 0.05443415 - samples/sec: 17.60 - lr: 0.000001
2023-05-28 14:45:33,453 epoch 9 - iter 5439/7770 - loss 0.05291599 - samples/sec: 16.73 - lr: 0.000001
2023-05-28 14:48:33,101 epoch 9 - iter 6216/7770 - loss 0.05291529 - samples/sec: 17.31 - lr: 0.000001
2023-05-28 14:51:33,812 epoch 9 - iter 6993/7770 - loss 0.05275665 - samples/sec: 17.21 - lr: 0.000001
2023-05-28 14:54:34,173 epoch 9 - iter 7770/7770 - loss 0.05235173 - samples/sec: 17.24 - lr: 0.000001
2023-05-28 14:54:34,176 ----------------------------------------------------------------------------------------------------
2023-05-28 14:54:34,176 EPOCH 9 done: loss 0.0524 - lr 0.000001
2023-05-28 14:57:04,669 Evaluating as a multi-label problem: False
2023-05-28 14:57:04,726 DEV : loss 0.11057411134243011 - f1-score (micro avg)  0.9639
2023-05-28 14:57:04,920 BAD EPOCHS (no improvement): 4
2023-05-28 14:57:04,923 ----------------------------------------------------------------------------------------------------
2023-05-28 15:00:03,629 epoch 10 - iter 777/7770 - loss 0.05052521 - samples/sec: 17.40 - lr: 0.000001
2023-05-28 15:03:02,524 epoch 10 - iter 1554/7770 - loss 0.04886513 - samples/sec: 17.38 - lr: 0.000000
2023-05-28 15:06:01,343 epoch 10 - iter 2331/7770 - loss 0.05318493 - samples/sec: 17.39 - lr: 0.000000
2023-05-28 15:08:59,640 epoch 10 - iter 3108/7770 - loss 0.05100455 - samples/sec: 17.44 - lr: 0.000000
2023-05-28 15:11:56,034 epoch 10 - iter 3885/7770 - loss 0.05151156 - samples/sec: 17.63 - lr: 0.000000
2023-05-28 15:14:54,593 epoch 10 - iter 4662/7770 - loss 0.05138023 - samples/sec: 17.41 - lr: 0.000000
2023-05-28 15:17:52,635 epoch 10 - iter 5439/7770 - loss 0.05174330 - samples/sec: 17.47 - lr: 0.000000
2023-05-28 15:20:52,320 epoch 10 - iter 6216/7770 - loss 0.05174539 - samples/sec: 17.31 - lr: 0.000000
2023-05-28 15:23:49,351 epoch 10 - iter 6993/7770 - loss 0.05164883 - samples/sec: 17.56 - lr: 0.000000
2023-05-28 15:26:48,567 epoch 10 - iter 7770/7770 - loss 0.05134815 - samples/sec: 17.35 - lr: 0.000000
2023-05-28 15:26:48,573 ----------------------------------------------------------------------------------------------------
2023-05-28 15:26:48,574 EPOCH 10 done: loss 0.0513 - lr 0.000000
2023-05-28 15:29:34,747 Evaluating as a multi-label problem: False
2023-05-28 15:29:34,840 DEV : loss 0.10612010210752487 - f1-score (micro avg)  0.9642
2023-05-28 15:29:35,109 BAD EPOCHS (no improvement): 4
2023-05-28 15:29:46,825 ----------------------------------------------------------------------------------------------------
2023-05-28 15:29:46,827 Testing using last state of model ...
2023-05-28 15:33:27,526 Evaluating as a multi-label problem: False
2023-05-28 15:33:27,602 0.935	0.9446	0.9398	0.9153
2023-05-28 15:33:27,602 
Results:
- F-score (micro) 0.9398
- F-score (macro) 0.937
- Accuracy 0.9153

By class:
              precision    recall  f1-score   support

         PER     0.9780    0.9805    0.9792      2715
         ORG     0.9079    0.9379    0.9226      2543
         LOC     0.9504    0.9345    0.9424      2442
        MISC     0.8922    0.9153    0.9036      1889

   micro avg     0.9350    0.9446    0.9398      9589
   macro avg     0.9321    0.9420    0.9370      9589
weighted avg     0.9355    0.9446    0.9399      9589

2023-05-28 15:33:27,602 ----------------------------------------------------------------------------------------------------
2023-05-28 15:33:27,602 ----------------------------------------------------------------------------------------------------
2023-05-28 15:35:38,700 Evaluating as a multi-label problem: False
2023-05-28 15:35:38,746 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-28 15:35:38,746 0.9433	0.9449	0.9441	0.9291
2023-05-28 15:35:38,746 ----------------------------------------------------------------------------------------------------
2023-05-28 15:37:05,733 Evaluating as a multi-label problem: False
2023-05-28 15:37:05,795 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-28 15:37:05,795 0.9293	0.9444	0.9368	0.9059
