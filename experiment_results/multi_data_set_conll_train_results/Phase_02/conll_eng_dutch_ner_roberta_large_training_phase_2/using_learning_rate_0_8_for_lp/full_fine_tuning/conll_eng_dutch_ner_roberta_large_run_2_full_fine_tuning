2023-05-28 02:21:32,531 ----------------------------------------------------------------------------------------------------
2023-05-28 02:21:32,536 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-28 02:21:32,539 ----------------------------------------------------------------------------------------------------
2023-05-28 02:21:32,540 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-28 02:21:32,540 ----------------------------------------------------------------------------------------------------
2023-05-28 02:21:32,540 Parameters:
2023-05-28 02:21:32,540  - learning_rate: "0.000005"
2023-05-28 02:21:32,540  - mini_batch_size: "4"
2023-05-28 02:21:32,540  - patience: "3"
2023-05-28 02:21:32,540  - anneal_factor: "0.5"
2023-05-28 02:21:32,540  - max_epochs: "10"
2023-05-28 02:21:32,540  - shuffle: "True"
2023-05-28 02:21:32,540  - train_with_dev: "False"
2023-05-28 02:21:32,540  - batch_growth_annealing: "False"
2023-05-28 02:21:32,540 ----------------------------------------------------------------------------------------------------
2023-05-28 02:21:32,540 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_2_full_fine_tuning"
2023-05-28 02:21:32,540 ----------------------------------------------------------------------------------------------------
2023-05-28 02:21:32,540 Device: cuda:3
2023-05-28 02:21:32,540 ----------------------------------------------------------------------------------------------------
2023-05-28 02:21:32,540 Embeddings storage mode: none
2023-05-28 02:21:32,541 ----------------------------------------------------------------------------------------------------
2023-05-28 02:24:41,046 epoch 1 - iter 777/7770 - loss 0.80533487 - samples/sec: 16.49 - lr: 0.000001
2023-05-28 02:27:47,941 epoch 1 - iter 1554/7770 - loss 0.59419760 - samples/sec: 16.64 - lr: 0.000001
2023-05-28 02:30:53,118 epoch 1 - iter 2331/7770 - loss 0.50766662 - samples/sec: 16.79 - lr: 0.000002
2023-05-28 02:33:55,290 epoch 1 - iter 3108/7770 - loss 0.43493396 - samples/sec: 17.07 - lr: 0.000002
2023-05-28 02:37:03,476 epoch 1 - iter 3885/7770 - loss 0.39230615 - samples/sec: 16.52 - lr: 0.000003
2023-05-28 02:40:05,463 epoch 1 - iter 4662/7770 - loss 0.37071728 - samples/sec: 17.09 - lr: 0.000003
2023-05-28 02:43:03,167 epoch 1 - iter 5439/7770 - loss 0.36035344 - samples/sec: 17.50 - lr: 0.000003
2023-05-28 02:46:01,592 epoch 1 - iter 6216/7770 - loss 0.34464248 - samples/sec: 17.43 - lr: 0.000004
2023-05-28 02:49:03,758 epoch 1 - iter 6993/7770 - loss 0.33397950 - samples/sec: 17.07 - lr: 0.000005
2023-05-28 02:52:03,401 epoch 1 - iter 7770/7770 - loss 0.32147984 - samples/sec: 17.31 - lr: 0.000005
2023-05-28 02:52:03,404 ----------------------------------------------------------------------------------------------------
2023-05-28 02:52:03,404 EPOCH 1 done: loss 0.3215 - lr 0.000005
2023-05-28 02:54:57,666 Evaluating as a multi-label problem: False
2023-05-28 02:54:57,759 DEV : loss 0.1627732813358307 - f1-score (micro avg)  0.9025
2023-05-28 02:54:58,018 BAD EPOCHS (no improvement): 4
2023-05-28 02:54:58,021 ----------------------------------------------------------------------------------------------------
2023-05-28 02:58:02,907 epoch 2 - iter 777/7770 - loss 0.18308044 - samples/sec: 16.82 - lr: 0.000005
2023-05-28 03:01:09,806 epoch 2 - iter 1554/7770 - loss 0.18477304 - samples/sec: 16.64 - lr: 0.000005
2023-05-28 03:04:20,262 epoch 2 - iter 2331/7770 - loss 0.19348274 - samples/sec: 16.33 - lr: 0.000005
2023-05-28 03:07:26,208 epoch 2 - iter 3108/7770 - loss 0.18626073 - samples/sec: 16.72 - lr: 0.000005
2023-05-28 03:10:29,337 epoch 2 - iter 3885/7770 - loss 0.18073388 - samples/sec: 16.98 - lr: 0.000005
2023-05-28 03:13:34,551 epoch 2 - iter 4662/7770 - loss 0.17798854 - samples/sec: 16.79 - lr: 0.000005
2023-05-28 03:16:38,287 epoch 2 - iter 5439/7770 - loss 0.17661196 - samples/sec: 16.92 - lr: 0.000005
2023-05-28 03:19:40,654 epoch 2 - iter 6216/7770 - loss 0.17717817 - samples/sec: 17.05 - lr: 0.000005
2023-05-28 03:22:43,982 epoch 2 - iter 6993/7770 - loss 0.17520998 - samples/sec: 16.96 - lr: 0.000005
2023-05-28 03:25:46,089 epoch 2 - iter 7770/7770 - loss 0.17170444 - samples/sec: 17.08 - lr: 0.000004
2023-05-28 03:25:46,093 ----------------------------------------------------------------------------------------------------
2023-05-28 03:25:46,093 EPOCH 2 done: loss 0.1717 - lr 0.000004
2023-05-28 03:28:34,779 Evaluating as a multi-label problem: False
2023-05-28 03:28:34,851 DEV : loss 0.0844799354672432 - f1-score (micro avg)  0.9519
2023-05-28 03:28:35,071 BAD EPOCHS (no improvement): 4
2023-05-28 03:28:35,074 ----------------------------------------------------------------------------------------------------
2023-05-28 03:31:40,820 epoch 3 - iter 777/7770 - loss 0.11272192 - samples/sec: 16.74 - lr: 0.000004
2023-05-28 03:34:47,965 epoch 3 - iter 1554/7770 - loss 0.13766085 - samples/sec: 16.62 - lr: 0.000004
2023-05-28 03:37:52,899 epoch 3 - iter 2331/7770 - loss 0.13700093 - samples/sec: 16.81 - lr: 0.000004
2023-05-28 03:40:53,889 epoch 3 - iter 3108/7770 - loss 0.13006492 - samples/sec: 17.18 - lr: 0.000004
2023-05-28 03:43:52,167 epoch 3 - iter 3885/7770 - loss 0.13255852 - samples/sec: 17.44 - lr: 0.000004
2023-05-28 03:46:59,581 epoch 3 - iter 4662/7770 - loss 0.13477248 - samples/sec: 16.59 - lr: 0.000004
2023-05-28 03:50:03,114 epoch 3 - iter 5439/7770 - loss 0.13269244 - samples/sec: 16.94 - lr: 0.000004
2023-05-28 03:53:05,486 epoch 3 - iter 6216/7770 - loss 0.13181443 - samples/sec: 17.05 - lr: 0.000004
2023-05-28 03:56:06,429 epoch 3 - iter 6993/7770 - loss 0.13212424 - samples/sec: 17.18 - lr: 0.000004
2023-05-28 03:59:09,670 epoch 3 - iter 7770/7770 - loss 0.13243907 - samples/sec: 16.97 - lr: 0.000004
2023-05-28 03:59:09,673 ----------------------------------------------------------------------------------------------------
2023-05-28 03:59:09,673 EPOCH 3 done: loss 0.1324 - lr 0.000004
2023-05-28 04:01:50,799 Evaluating as a multi-label problem: False
2023-05-28 04:01:50,892 DEV : loss 0.0721546858549118 - f1-score (micro avg)  0.9574
2023-05-28 04:01:51,084 BAD EPOCHS (no improvement): 4
2023-05-28 04:01:51,087 ----------------------------------------------------------------------------------------------------
2023-05-28 04:04:54,559 epoch 4 - iter 777/7770 - loss 0.11422465 - samples/sec: 16.95 - lr: 0.000004
2023-05-28 04:07:56,225 epoch 4 - iter 1554/7770 - loss 0.10992981 - samples/sec: 17.12 - lr: 0.000004
2023-05-28 04:10:56,294 epoch 4 - iter 2331/7770 - loss 0.10378270 - samples/sec: 17.27 - lr: 0.000004
2023-05-28 04:13:55,538 epoch 4 - iter 3108/7770 - loss 0.10208959 - samples/sec: 17.35 - lr: 0.000004
2023-05-28 04:16:56,885 epoch 4 - iter 3885/7770 - loss 0.10064827 - samples/sec: 17.15 - lr: 0.000004
2023-05-28 04:19:56,558 epoch 4 - iter 4662/7770 - loss 0.10065031 - samples/sec: 17.31 - lr: 0.000004
2023-05-28 04:22:58,102 epoch 4 - iter 5439/7770 - loss 0.09882283 - samples/sec: 17.13 - lr: 0.000004
2023-05-28 04:25:59,322 epoch 4 - iter 6216/7770 - loss 0.10110935 - samples/sec: 17.16 - lr: 0.000003
2023-05-28 04:29:01,068 epoch 4 - iter 6993/7770 - loss 0.10200408 - samples/sec: 17.11 - lr: 0.000003
2023-05-28 04:32:03,788 epoch 4 - iter 7770/7770 - loss 0.10204123 - samples/sec: 17.02 - lr: 0.000003
2023-05-28 04:32:03,793 ----------------------------------------------------------------------------------------------------
2023-05-28 04:32:03,793 EPOCH 4 done: loss 0.1020 - lr 0.000003
2023-05-28 04:35:02,794 Evaluating as a multi-label problem: False
2023-05-28 04:35:02,885 DEV : loss 0.07406193763017654 - f1-score (micro avg)  0.96
2023-05-28 04:35:03,135 BAD EPOCHS (no improvement): 4
2023-05-28 04:35:03,138 ----------------------------------------------------------------------------------------------------
2023-05-28 04:38:02,528 epoch 5 - iter 777/7770 - loss 0.09355962 - samples/sec: 17.33 - lr: 0.000003
2023-05-28 04:41:06,374 epoch 5 - iter 1554/7770 - loss 0.08935389 - samples/sec: 16.91 - lr: 0.000003
2023-05-28 04:44:14,665 epoch 5 - iter 2331/7770 - loss 0.09148162 - samples/sec: 16.51 - lr: 0.000003
2023-05-28 04:47:14,470 epoch 5 - iter 3108/7770 - loss 0.09036472 - samples/sec: 17.29 - lr: 0.000003
2023-05-28 04:50:16,277 epoch 5 - iter 3885/7770 - loss 0.08820688 - samples/sec: 17.10 - lr: 0.000003
2023-05-28 04:53:17,318 epoch 5 - iter 4662/7770 - loss 0.08635459 - samples/sec: 17.18 - lr: 0.000003
2023-05-28 04:56:17,418 epoch 5 - iter 5439/7770 - loss 0.08472074 - samples/sec: 17.27 - lr: 0.000003
2023-05-28 04:59:21,172 epoch 5 - iter 6216/7770 - loss 0.08645675 - samples/sec: 16.92 - lr: 0.000003
2023-05-28 05:02:23,264 epoch 5 - iter 6993/7770 - loss 0.08562833 - samples/sec: 17.08 - lr: 0.000003
2023-05-28 05:05:25,562 epoch 5 - iter 7770/7770 - loss 0.08616003 - samples/sec: 17.06 - lr: 0.000003
2023-05-28 05:05:25,567 ----------------------------------------------------------------------------------------------------
2023-05-28 05:05:25,567 EPOCH 5 done: loss 0.0862 - lr 0.000003
2023-05-28 05:08:14,455 Evaluating as a multi-label problem: False
2023-05-28 05:08:14,549 DEV : loss 0.08954928070306778 - f1-score (micro avg)  0.9635
2023-05-28 05:08:14,779 BAD EPOCHS (no improvement): 4
2023-05-28 05:08:14,782 ----------------------------------------------------------------------------------------------------
2023-05-28 05:11:20,978 epoch 6 - iter 777/7770 - loss 0.07484723 - samples/sec: 16.70 - lr: 0.000003
2023-05-28 05:14:22,469 epoch 6 - iter 1554/7770 - loss 0.07302083 - samples/sec: 17.13 - lr: 0.000003
2023-05-28 05:17:24,772 epoch 6 - iter 2331/7770 - loss 0.07199558 - samples/sec: 17.06 - lr: 0.000003
2023-05-28 05:20:25,742 epoch 6 - iter 3108/7770 - loss 0.07068535 - samples/sec: 17.18 - lr: 0.000003
2023-05-28 05:23:26,481 epoch 6 - iter 3885/7770 - loss 0.06983784 - samples/sec: 17.20 - lr: 0.000003
2023-05-28 05:26:29,192 epoch 6 - iter 4662/7770 - loss 0.07061802 - samples/sec: 17.02 - lr: 0.000002
2023-05-28 05:29:34,082 epoch 6 - iter 5439/7770 - loss 0.07069932 - samples/sec: 16.82 - lr: 0.000002
2023-05-28 05:32:37,506 epoch 6 - iter 6216/7770 - loss 0.07153274 - samples/sec: 16.95 - lr: 0.000002
2023-05-28 05:35:39,023 epoch 6 - iter 6993/7770 - loss 0.07140791 - samples/sec: 17.13 - lr: 0.000002
2023-05-28 05:38:39,354 epoch 6 - iter 7770/7770 - loss 0.07149362 - samples/sec: 17.24 - lr: 0.000002
2023-05-28 05:38:39,358 ----------------------------------------------------------------------------------------------------
2023-05-28 05:38:39,358 EPOCH 6 done: loss 0.0715 - lr 0.000002
2023-05-28 05:41:17,812 Evaluating as a multi-label problem: False
2023-05-28 05:41:17,907 DEV : loss 0.09407635033130646 - f1-score (micro avg)  0.9613
2023-05-28 05:41:18,169 BAD EPOCHS (no improvement): 4
2023-05-28 05:41:18,172 ----------------------------------------------------------------------------------------------------
2023-05-28 05:44:21,414 epoch 7 - iter 777/7770 - loss 0.06607289 - samples/sec: 16.97 - lr: 0.000002
2023-05-28 05:47:20,469 epoch 7 - iter 1554/7770 - loss 0.06639084 - samples/sec: 17.37 - lr: 0.000002
2023-05-28 05:50:21,130 epoch 7 - iter 2331/7770 - loss 0.06659022 - samples/sec: 17.21 - lr: 0.000002
2023-05-28 05:53:24,995 epoch 7 - iter 3108/7770 - loss 0.06574820 - samples/sec: 16.91 - lr: 0.000002
2023-05-28 05:56:25,722 epoch 7 - iter 3885/7770 - loss 0.06605742 - samples/sec: 17.21 - lr: 0.000002
2023-05-28 05:59:25,033 epoch 7 - iter 4662/7770 - loss 0.06517746 - samples/sec: 17.34 - lr: 0.000002
2023-05-28 06:02:22,367 epoch 7 - iter 5439/7770 - loss 0.06557807 - samples/sec: 17.53 - lr: 0.000002
2023-05-28 06:05:26,539 epoch 7 - iter 6216/7770 - loss 0.06433090 - samples/sec: 16.88 - lr: 0.000002
2023-05-28 06:08:26,705 epoch 7 - iter 6993/7770 - loss 0.06458514 - samples/sec: 17.26 - lr: 0.000002
2023-05-28 06:11:24,553 epoch 7 - iter 7770/7770 - loss 0.06432394 - samples/sec: 17.48 - lr: 0.000002
2023-05-28 06:11:24,558 ----------------------------------------------------------------------------------------------------
2023-05-28 06:11:24,558 EPOCH 7 done: loss 0.0643 - lr 0.000002
2023-05-28 06:14:18,441 Evaluating as a multi-label problem: False
2023-05-28 06:14:18,522 DEV : loss 0.09609740227460861 - f1-score (micro avg)  0.9646
2023-05-28 06:14:18,749 BAD EPOCHS (no improvement): 4
2023-05-28 06:14:18,752 ----------------------------------------------------------------------------------------------------
2023-05-28 06:17:20,743 epoch 8 - iter 777/7770 - loss 0.06067043 - samples/sec: 17.09 - lr: 0.000002
2023-05-28 06:20:22,780 epoch 8 - iter 1554/7770 - loss 0.06160795 - samples/sec: 17.08 - lr: 0.000002
2023-05-28 06:23:29,026 epoch 8 - iter 2331/7770 - loss 0.05943437 - samples/sec: 16.70 - lr: 0.000002
2023-05-28 06:26:31,670 epoch 8 - iter 3108/7770 - loss 0.05960075 - samples/sec: 17.02 - lr: 0.000001
2023-05-28 06:29:30,233 epoch 8 - iter 3885/7770 - loss 0.05928983 - samples/sec: 17.41 - lr: 0.000001
2023-05-28 06:32:29,508 epoch 8 - iter 4662/7770 - loss 0.05902231 - samples/sec: 17.34 - lr: 0.000001
2023-05-28 06:35:27,973 epoch 8 - iter 5439/7770 - loss 0.05885734 - samples/sec: 17.42 - lr: 0.000001
2023-05-28 06:38:26,956 epoch 8 - iter 6216/7770 - loss 0.05803655 - samples/sec: 17.37 - lr: 0.000001
2023-05-28 06:41:22,897 epoch 8 - iter 6993/7770 - loss 0.05823612 - samples/sec: 17.67 - lr: 0.000001
2023-05-28 06:44:21,470 epoch 8 - iter 7770/7770 - loss 0.05778259 - samples/sec: 17.41 - lr: 0.000001
2023-05-28 06:44:21,474 ----------------------------------------------------------------------------------------------------
2023-05-28 06:44:21,474 EPOCH 8 done: loss 0.0578 - lr 0.000001
2023-05-28 06:47:11,488 Evaluating as a multi-label problem: False
2023-05-28 06:47:11,583 DEV : loss 0.10503947734832764 - f1-score (micro avg)  0.9655
2023-05-28 06:47:11,834 BAD EPOCHS (no improvement): 4
2023-05-28 06:47:11,837 ----------------------------------------------------------------------------------------------------
2023-05-28 06:50:15,401 epoch 9 - iter 777/7770 - loss 0.05554830 - samples/sec: 16.94 - lr: 0.000001
2023-05-28 06:53:18,719 epoch 9 - iter 1554/7770 - loss 0.05402626 - samples/sec: 16.96 - lr: 0.000001
2023-05-28 06:56:18,467 epoch 9 - iter 2331/7770 - loss 0.05247871 - samples/sec: 17.30 - lr: 0.000001
2023-05-28 06:59:16,976 epoch 9 - iter 3108/7770 - loss 0.05290262 - samples/sec: 17.42 - lr: 0.000001
2023-05-28 07:02:17,505 epoch 9 - iter 3885/7770 - loss 0.05332002 - samples/sec: 17.22 - lr: 0.000001
2023-05-28 07:05:20,751 epoch 9 - iter 4662/7770 - loss 0.05360828 - samples/sec: 16.97 - lr: 0.000001
2023-05-28 07:08:28,306 epoch 9 - iter 5439/7770 - loss 0.05409754 - samples/sec: 16.58 - lr: 0.000001
2023-05-28 07:11:30,777 epoch 9 - iter 6216/7770 - loss 0.05407473 - samples/sec: 17.04 - lr: 0.000001
2023-05-28 07:14:31,547 epoch 9 - iter 6993/7770 - loss 0.05256826 - samples/sec: 17.20 - lr: 0.000001
2023-05-28 07:17:34,033 epoch 9 - iter 7770/7770 - loss 0.05294368 - samples/sec: 17.04 - lr: 0.000001
2023-05-28 07:17:34,038 ----------------------------------------------------------------------------------------------------
2023-05-28 07:17:34,038 EPOCH 9 done: loss 0.0529 - lr 0.000001
2023-05-28 07:20:16,439 Evaluating as a multi-label problem: False
2023-05-28 07:20:16,534 DEV : loss 0.10932403802871704 - f1-score (micro avg)  0.9649
2023-05-28 07:20:16,798 BAD EPOCHS (no improvement): 4
2023-05-28 07:20:16,801 ----------------------------------------------------------------------------------------------------
2023-05-28 07:23:17,223 epoch 10 - iter 777/7770 - loss 0.04646427 - samples/sec: 17.24 - lr: 0.000001
2023-05-28 07:26:17,349 epoch 10 - iter 1554/7770 - loss 0.04662391 - samples/sec: 17.26 - lr: 0.000000
2023-05-28 07:29:17,088 epoch 10 - iter 2331/7770 - loss 0.04848673 - samples/sec: 17.30 - lr: 0.000000
2023-05-28 07:32:16,461 epoch 10 - iter 3108/7770 - loss 0.04779479 - samples/sec: 17.34 - lr: 0.000000
2023-05-28 07:35:15,830 epoch 10 - iter 3885/7770 - loss 0.04872300 - samples/sec: 17.34 - lr: 0.000000
2023-05-28 07:38:15,914 epoch 10 - iter 4662/7770 - loss 0.04892235 - samples/sec: 17.27 - lr: 0.000000
2023-05-28 07:41:14,730 epoch 10 - iter 5439/7770 - loss 0.04841706 - samples/sec: 17.39 - lr: 0.000000
2023-05-28 07:44:16,765 epoch 10 - iter 6216/7770 - loss 0.04827123 - samples/sec: 17.08 - lr: 0.000000
2023-05-28 07:47:13,297 epoch 10 - iter 6993/7770 - loss 0.04855781 - samples/sec: 17.61 - lr: 0.000000
2023-05-28 07:50:16,252 epoch 10 - iter 7770/7770 - loss 0.04785922 - samples/sec: 17.00 - lr: 0.000000
2023-05-28 07:50:16,255 ----------------------------------------------------------------------------------------------------
2023-05-28 07:50:16,255 EPOCH 10 done: loss 0.0479 - lr 0.000000
2023-05-28 07:53:20,003 Evaluating as a multi-label problem: False
2023-05-28 07:53:20,103 DEV : loss 0.11044669151306152 - f1-score (micro avg)  0.9657
2023-05-28 07:53:20,374 BAD EPOCHS (no improvement): 4
2023-05-28 07:53:32,835 ----------------------------------------------------------------------------------------------------
2023-05-28 07:53:32,839 Testing using last state of model ...
2023-05-28 07:57:31,739 Evaluating as a multi-label problem: False
2023-05-28 07:57:31,842 0.934	0.9429	0.9384	0.9144
2023-05-28 07:57:31,842 
Results:
- F-score (micro) 0.9384
- F-score (macro) 0.9353
- Accuracy 0.9144

By class:
              precision    recall  f1-score   support

         PER     0.9804    0.9779    0.9792      2715
         ORG     0.9030    0.9442    0.9231      2543
         LOC     0.9496    0.9333    0.9413      2442
        MISC     0.8918    0.9031    0.8974      1889

   micro avg     0.9340    0.9429    0.9384      9589
   macro avg     0.9312    0.9396    0.9353      9589
weighted avg     0.9346    0.9429    0.9386      9589

2023-05-28 07:57:31,842 ----------------------------------------------------------------------------------------------------
2023-05-28 07:57:31,842 ----------------------------------------------------------------------------------------------------
2023-05-28 07:59:55,012 Evaluating as a multi-label problem: False
2023-05-28 07:59:55,061 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-28 07:59:55,061 0.9397	0.9409	0.9403	0.9245
2023-05-28 07:59:55,061 ----------------------------------------------------------------------------------------------------
2023-05-28 08:01:28,383 Evaluating as a multi-label problem: False
2023-05-28 08:01:28,452 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-28 08:01:28,452 0.9301	0.9442	0.9371	0.9076
