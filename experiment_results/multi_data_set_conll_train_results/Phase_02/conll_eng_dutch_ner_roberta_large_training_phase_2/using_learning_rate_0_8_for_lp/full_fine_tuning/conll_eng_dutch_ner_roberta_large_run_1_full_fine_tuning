2023-06-05 13:48:11,393 ----------------------------------------------------------------------------------------------------
2023-06-05 13:48:11,407 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-05 13:48:11,410 ----------------------------------------------------------------------------------------------------
2023-06-05 13:48:11,411 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-05 13:48:11,412 ----------------------------------------------------------------------------------------------------
2023-06-05 13:48:11,412 Parameters:
2023-06-05 13:48:11,412  - learning_rate: "0.000005"
2023-06-05 13:48:11,412  - mini_batch_size: "4"
2023-06-05 13:48:11,412  - patience: "3"
2023-06-05 13:48:11,412  - anneal_factor: "0.5"
2023-06-05 13:48:11,413  - max_epochs: "10"
2023-06-05 13:48:11,413  - shuffle: "True"
2023-06-05 13:48:11,413  - train_with_dev: "False"
2023-06-05 13:48:11,413  - batch_growth_annealing: "False"
2023-06-05 13:48:11,413 ----------------------------------------------------------------------------------------------------
2023-06-05 13:48:11,413 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_1_full_fine_tuning"
2023-06-05 13:48:11,413 ----------------------------------------------------------------------------------------------------
2023-06-05 13:48:11,414 Device: cuda:1
2023-06-05 13:48:11,414 ----------------------------------------------------------------------------------------------------
2023-06-05 13:48:11,414 Embeddings storage mode: none
2023-06-05 13:48:11,414 ----------------------------------------------------------------------------------------------------
2023-06-05 13:51:16,710 epoch 1 - iter 777/7770 - loss 0.75551066 - samples/sec: 16.78 - lr: 0.000001
2023-06-05 13:54:17,139 epoch 1 - iter 1554/7770 - loss 0.59342867 - samples/sec: 17.23 - lr: 0.000001
2023-06-05 13:57:19,925 epoch 1 - iter 2331/7770 - loss 0.50461729 - samples/sec: 17.01 - lr: 0.000002
2023-06-05 14:00:25,964 epoch 1 - iter 3108/7770 - loss 0.42851642 - samples/sec: 16.71 - lr: 0.000002
2023-06-05 14:03:33,266 epoch 1 - iter 3885/7770 - loss 0.38302096 - samples/sec: 16.60 - lr: 0.000003
2023-06-05 14:06:36,716 epoch 1 - iter 4662/7770 - loss 0.36698829 - samples/sec: 16.95 - lr: 0.000003
2023-06-05 14:09:33,159 epoch 1 - iter 5439/7770 - loss 0.35873708 - samples/sec: 17.62 - lr: 0.000003
2023-06-05 14:12:31,314 epoch 1 - iter 6216/7770 - loss 0.34509548 - samples/sec: 17.45 - lr: 0.000004
2023-06-05 14:15:32,593 epoch 1 - iter 6993/7770 - loss 0.33387915 - samples/sec: 17.15 - lr: 0.000005
2023-06-05 14:18:26,941 epoch 1 - iter 7770/7770 - loss 0.32263509 - samples/sec: 17.83 - lr: 0.000005
2023-06-05 14:18:26,943 ----------------------------------------------------------------------------------------------------
2023-06-05 14:18:26,943 EPOCH 1 done: loss 0.3226 - lr 0.000005
2023-06-05 14:21:01,328 Evaluating as a multi-label problem: False
2023-06-05 14:21:01,392 DEV : loss 0.11422009021043777 - f1-score (micro avg)  0.9122
2023-06-05 14:21:01,599 BAD EPOCHS (no improvement): 4
2023-06-05 14:21:01,601 ----------------------------------------------------------------------------------------------------
2023-06-05 14:24:08,745 epoch 2 - iter 777/7770 - loss 0.19502903 - samples/sec: 16.62 - lr: 0.000005
2023-06-05 14:27:14,155 epoch 2 - iter 1554/7770 - loss 0.19513374 - samples/sec: 16.77 - lr: 0.000005
2023-06-05 14:30:16,053 epoch 2 - iter 2331/7770 - loss 0.19231639 - samples/sec: 17.10 - lr: 0.000005
2023-06-05 14:33:17,640 epoch 2 - iter 3108/7770 - loss 0.19352899 - samples/sec: 17.13 - lr: 0.000005
2023-06-05 14:37:08,137 epoch 2 - iter 3885/7770 - loss 0.19275698 - samples/sec: 13.49 - lr: 0.000005
2023-06-05 14:41:21,286 epoch 2 - iter 4662/7770 - loss 0.19043302 - samples/sec: 12.29 - lr: 0.000005
2023-06-05 14:45:35,067 epoch 2 - iter 5439/7770 - loss 0.18882849 - samples/sec: 12.25 - lr: 0.000005
2023-06-05 14:49:44,680 epoch 2 - iter 6216/7770 - loss 0.18444654 - samples/sec: 12.46 - lr: 0.000005
2023-06-05 14:52:45,197 epoch 2 - iter 6993/7770 - loss 0.18194497 - samples/sec: 17.23 - lr: 0.000005
2023-06-05 14:55:46,052 epoch 2 - iter 7770/7770 - loss 0.17809602 - samples/sec: 17.19 - lr: 0.000004
2023-06-05 14:55:46,057 ----------------------------------------------------------------------------------------------------
2023-06-05 14:55:46,057 EPOCH 2 done: loss 0.1781 - lr 0.000004
2023-06-05 14:58:38,361 Evaluating as a multi-label problem: False
2023-06-05 14:58:38,454 DEV : loss 0.10795356333255768 - f1-score (micro avg)  0.9449
2023-06-05 14:58:38,666 BAD EPOCHS (no improvement): 4
2023-06-05 14:58:38,680 ----------------------------------------------------------------------------------------------------
2023-06-05 15:01:45,573 epoch 3 - iter 777/7770 - loss 0.12666840 - samples/sec: 16.64 - lr: 0.000004
2023-06-05 15:04:49,348 epoch 3 - iter 1554/7770 - loss 0.13425465 - samples/sec: 16.92 - lr: 0.000004
2023-06-05 15:07:49,540 epoch 3 - iter 2331/7770 - loss 0.13506262 - samples/sec: 17.26 - lr: 0.000004
2023-06-05 15:10:53,596 epoch 3 - iter 3108/7770 - loss 0.13312688 - samples/sec: 16.90 - lr: 0.000004
2023-06-05 15:13:55,786 epoch 3 - iter 3885/7770 - loss 0.12978656 - samples/sec: 17.07 - lr: 0.000004
2023-06-05 15:17:02,523 epoch 3 - iter 4662/7770 - loss 0.13158655 - samples/sec: 16.65 - lr: 0.000004
2023-06-05 15:20:07,437 epoch 3 - iter 5439/7770 - loss 0.13471968 - samples/sec: 16.82 - lr: 0.000004
2023-06-05 15:23:10,175 epoch 3 - iter 6216/7770 - loss 0.13456176 - samples/sec: 17.02 - lr: 0.000004
2023-06-05 15:26:10,556 epoch 3 - iter 6993/7770 - loss 0.13373468 - samples/sec: 17.24 - lr: 0.000004
2023-06-05 15:29:11,520 epoch 3 - iter 7770/7770 - loss 0.13195719 - samples/sec: 17.18 - lr: 0.000004
2023-06-05 15:29:11,523 ----------------------------------------------------------------------------------------------------
2023-06-05 15:29:11,523 EPOCH 3 done: loss 0.1320 - lr 0.000004
2023-06-05 15:31:58,178 Evaluating as a multi-label problem: False
2023-06-05 15:31:58,274 DEV : loss 0.09321655333042145 - f1-score (micro avg)  0.9582
2023-06-05 15:31:58,453 BAD EPOCHS (no improvement): 4
2023-06-05 15:31:58,467 ----------------------------------------------------------------------------------------------------
2023-06-05 15:35:01,024 epoch 4 - iter 777/7770 - loss 0.07923649 - samples/sec: 17.03 - lr: 0.000004
2023-06-05 15:38:01,377 epoch 4 - iter 1554/7770 - loss 0.09382225 - samples/sec: 17.24 - lr: 0.000004
2023-06-05 15:41:05,764 epoch 4 - iter 2331/7770 - loss 0.09651598 - samples/sec: 16.87 - lr: 0.000004
2023-06-05 15:44:09,806 epoch 4 - iter 3108/7770 - loss 0.09921502 - samples/sec: 16.90 - lr: 0.000004
2023-06-05 15:47:11,423 epoch 4 - iter 3885/7770 - loss 0.09705938 - samples/sec: 17.12 - lr: 0.000004
2023-06-05 15:50:10,054 epoch 4 - iter 4662/7770 - loss 0.09748937 - samples/sec: 17.41 - lr: 0.000004
2023-06-05 15:53:10,410 epoch 4 - iter 5439/7770 - loss 0.09796709 - samples/sec: 17.24 - lr: 0.000004
2023-06-05 15:56:09,939 epoch 4 - iter 6216/7770 - loss 0.09852484 - samples/sec: 17.32 - lr: 0.000003
2023-06-05 15:59:10,265 epoch 4 - iter 6993/7770 - loss 0.09799634 - samples/sec: 17.25 - lr: 0.000003
2023-06-05 16:02:16,196 epoch 4 - iter 7770/7770 - loss 0.09812447 - samples/sec: 16.72 - lr: 0.000003
2023-06-05 16:02:16,200 ----------------------------------------------------------------------------------------------------
2023-06-05 16:02:16,201 EPOCH 4 done: loss 0.0981 - lr 0.000003
2023-06-05 16:05:12,961 Evaluating as a multi-label problem: False
2023-06-05 16:05:13,073 DEV : loss 0.09965171664953232 - f1-score (micro avg)  0.9621
2023-06-05 16:05:13,317 BAD EPOCHS (no improvement): 4
2023-06-05 16:05:27,061 ----------------------------------------------------------------------------------------------------
2023-06-05 16:08:27,804 epoch 5 - iter 777/7770 - loss 0.08486833 - samples/sec: 17.25 - lr: 0.000003
2023-06-05 16:11:34,989 epoch 5 - iter 1554/7770 - loss 0.08513550 - samples/sec: 16.61 - lr: 0.000003
2023-06-05 16:14:37,640 epoch 5 - iter 2331/7770 - loss 0.08594912 - samples/sec: 17.03 - lr: 0.000003
2023-06-05 16:17:38,079 epoch 5 - iter 3108/7770 - loss 0.08472925 - samples/sec: 17.23 - lr: 0.000003
2023-06-05 16:20:39,640 epoch 5 - iter 3885/7770 - loss 0.08603785 - samples/sec: 17.13 - lr: 0.000003
2023-06-05 16:23:40,080 epoch 5 - iter 4662/7770 - loss 0.08451620 - samples/sec: 17.23 - lr: 0.000003
2023-06-05 16:26:41,386 epoch 5 - iter 5439/7770 - loss 0.08428238 - samples/sec: 17.15 - lr: 0.000003
2023-06-05 16:29:41,899 epoch 5 - iter 6216/7770 - loss 0.08448019 - samples/sec: 17.23 - lr: 0.000003
2023-06-05 16:32:42,033 epoch 5 - iter 6993/7770 - loss 0.08433227 - samples/sec: 17.26 - lr: 0.000003
2023-06-05 16:35:42,400 epoch 5 - iter 7770/7770 - loss 0.08423012 - samples/sec: 17.24 - lr: 0.000003
2023-06-05 16:35:42,405 ----------------------------------------------------------------------------------------------------
2023-06-05 16:35:42,405 EPOCH 5 done: loss 0.0842 - lr 0.000003
2023-06-05 16:38:25,361 Evaluating as a multi-label problem: False
2023-06-05 16:38:25,463 DEV : loss 0.08290319889783859 - f1-score (micro avg)  0.9633
2023-06-05 16:38:25,695 BAD EPOCHS (no improvement): 4
2023-06-05 16:38:25,708 ----------------------------------------------------------------------------------------------------
2023-06-05 16:41:31,499 epoch 6 - iter 777/7770 - loss 0.08185459 - samples/sec: 16.74 - lr: 0.000003
2023-06-05 16:44:33,646 epoch 6 - iter 1554/7770 - loss 0.07193808 - samples/sec: 17.07 - lr: 0.000003
2023-06-05 16:47:33,205 epoch 6 - iter 2331/7770 - loss 0.06824908 - samples/sec: 17.32 - lr: 0.000003
2023-06-05 16:50:35,372 epoch 6 - iter 3108/7770 - loss 0.06832802 - samples/sec: 17.07 - lr: 0.000003
2023-06-05 16:53:38,318 epoch 6 - iter 3885/7770 - loss 0.07039224 - samples/sec: 17.00 - lr: 0.000003
2023-06-05 16:56:45,878 epoch 6 - iter 4662/7770 - loss 0.06897021 - samples/sec: 16.58 - lr: 0.000002
2023-06-05 16:59:48,346 epoch 6 - iter 5439/7770 - loss 0.06999817 - samples/sec: 17.04 - lr: 0.000002
2023-06-05 17:02:50,804 epoch 6 - iter 6216/7770 - loss 0.07058053 - samples/sec: 17.04 - lr: 0.000002
2023-06-05 17:05:51,013 epoch 6 - iter 6993/7770 - loss 0.07015661 - samples/sec: 17.26 - lr: 0.000002
2023-06-05 17:08:53,087 epoch 6 - iter 7770/7770 - loss 0.07066771 - samples/sec: 17.08 - lr: 0.000002
2023-06-05 17:08:53,091 ----------------------------------------------------------------------------------------------------
2023-06-05 17:08:53,092 EPOCH 6 done: loss 0.0707 - lr 0.000002
2023-06-05 17:11:25,454 Evaluating as a multi-label problem: False
2023-06-05 17:11:25,539 DEV : loss 0.08443548530340195 - f1-score (micro avg)  0.9611
2023-06-05 17:11:25,682 BAD EPOCHS (no improvement): 4
2023-06-05 17:11:25,716 ----------------------------------------------------------------------------------------------------
2023-06-05 17:14:29,921 epoch 7 - iter 777/7770 - loss 0.06051893 - samples/sec: 16.88 - lr: 0.000002
2023-06-05 17:17:34,316 epoch 7 - iter 1554/7770 - loss 0.06357607 - samples/sec: 16.86 - lr: 0.000002
2023-06-05 17:20:36,907 epoch 7 - iter 2331/7770 - loss 0.06001352 - samples/sec: 17.03 - lr: 0.000002
2023-06-05 17:23:43,501 epoch 7 - iter 3108/7770 - loss 0.06031387 - samples/sec: 16.67 - lr: 0.000002
2023-06-05 17:26:51,623 epoch 7 - iter 3885/7770 - loss 0.06123546 - samples/sec: 16.53 - lr: 0.000002
2023-06-05 17:29:58,016 epoch 7 - iter 4662/7770 - loss 0.06089274 - samples/sec: 16.68 - lr: 0.000002
2023-06-05 17:33:04,212 epoch 7 - iter 5439/7770 - loss 0.06091261 - samples/sec: 16.70 - lr: 0.000002
2023-06-05 17:36:14,893 epoch 7 - iter 6216/7770 - loss 0.05979755 - samples/sec: 16.31 - lr: 0.000002
2023-06-05 17:39:24,681 epoch 7 - iter 6993/7770 - loss 0.06046356 - samples/sec: 16.39 - lr: 0.000002
2023-06-05 17:42:44,684 epoch 7 - iter 7770/7770 - loss 0.06031656 - samples/sec: 15.55 - lr: 0.000002
2023-06-05 17:42:44,689 ----------------------------------------------------------------------------------------------------
2023-06-05 17:42:44,689 EPOCH 7 done: loss 0.0603 - lr 0.000002
2023-06-05 17:45:36,428 Evaluating as a multi-label problem: False
2023-06-05 17:45:36,532 DEV : loss 0.08400242030620575 - f1-score (micro avg)  0.9651
2023-06-05 17:45:36,780 BAD EPOCHS (no improvement): 4
2023-06-05 17:45:36,783 ----------------------------------------------------------------------------------------------------
2023-06-05 17:48:44,313 epoch 8 - iter 777/7770 - loss 0.06256414 - samples/sec: 16.58 - lr: 0.000002
2023-06-05 17:51:56,983 epoch 8 - iter 1554/7770 - loss 0.06018206 - samples/sec: 16.14 - lr: 0.000002
2023-06-05 17:55:04,580 epoch 8 - iter 2331/7770 - loss 0.05782043 - samples/sec: 16.58 - lr: 0.000002
2023-06-05 17:58:08,723 epoch 8 - iter 3108/7770 - loss 0.05844088 - samples/sec: 16.89 - lr: 0.000001
2023-06-05 18:01:13,818 epoch 8 - iter 3885/7770 - loss 0.05875566 - samples/sec: 16.80 - lr: 0.000001
2023-06-05 18:04:24,128 epoch 8 - iter 4662/7770 - loss 0.05897663 - samples/sec: 16.34 - lr: 0.000001
2023-06-05 18:07:28,563 epoch 8 - iter 5439/7770 - loss 0.05933095 - samples/sec: 16.86 - lr: 0.000001
2023-06-05 18:10:34,135 epoch 8 - iter 6216/7770 - loss 0.05879866 - samples/sec: 16.76 - lr: 0.000001
2023-06-05 18:13:42,946 epoch 8 - iter 6993/7770 - loss 0.05833484 - samples/sec: 16.47 - lr: 0.000001
2023-06-05 18:16:49,315 epoch 8 - iter 7770/7770 - loss 0.05750379 - samples/sec: 16.68 - lr: 0.000001
2023-06-05 18:16:49,320 ----------------------------------------------------------------------------------------------------
2023-06-05 18:16:49,320 EPOCH 8 done: loss 0.0575 - lr 0.000001
2023-06-05 18:19:46,247 Evaluating as a multi-label problem: False
2023-06-05 18:19:46,341 DEV : loss 0.08965238928794861 - f1-score (micro avg)  0.9654
2023-06-05 18:19:46,583 BAD EPOCHS (no improvement): 4
2023-06-05 18:19:46,586 ----------------------------------------------------------------------------------------------------
2023-06-05 18:22:58,832 epoch 9 - iter 777/7770 - loss 0.05450553 - samples/sec: 16.17 - lr: 0.000001
2023-06-05 18:26:04,914 epoch 9 - iter 1554/7770 - loss 0.05130828 - samples/sec: 16.71 - lr: 0.000001
2023-06-05 18:29:16,711 epoch 9 - iter 2331/7770 - loss 0.05173918 - samples/sec: 16.21 - lr: 0.000001
2023-06-05 18:32:24,820 epoch 9 - iter 3108/7770 - loss 0.05046382 - samples/sec: 16.53 - lr: 0.000001
2023-06-05 18:35:31,791 epoch 9 - iter 3885/7770 - loss 0.05069102 - samples/sec: 16.63 - lr: 0.000001
2023-06-05 18:38:46,863 epoch 9 - iter 4662/7770 - loss 0.05044697 - samples/sec: 15.94 - lr: 0.000001
2023-06-05 18:42:01,034 epoch 9 - iter 5439/7770 - loss 0.05029776 - samples/sec: 16.01 - lr: 0.000001
2023-06-05 18:45:07,731 epoch 9 - iter 6216/7770 - loss 0.05073757 - samples/sec: 16.66 - lr: 0.000001
2023-06-05 18:48:13,555 epoch 9 - iter 6993/7770 - loss 0.05099832 - samples/sec: 16.73 - lr: 0.000001
2023-06-05 18:51:19,142 epoch 9 - iter 7770/7770 - loss 0.05046319 - samples/sec: 16.76 - lr: 0.000001
2023-06-05 18:51:19,146 ----------------------------------------------------------------------------------------------------
2023-06-05 18:51:19,147 EPOCH 9 done: loss 0.0505 - lr 0.000001
2023-06-05 18:54:09,586 Evaluating as a multi-label problem: False
2023-06-05 18:54:09,690 DEV : loss 0.10151299089193344 - f1-score (micro avg)  0.9638
2023-06-05 18:54:09,973 BAD EPOCHS (no improvement): 4
2023-06-05 18:54:09,976 ----------------------------------------------------------------------------------------------------
2023-06-05 18:57:18,459 epoch 10 - iter 777/7770 - loss 0.04663549 - samples/sec: 16.50 - lr: 0.000001
2023-06-05 19:00:27,025 epoch 10 - iter 1554/7770 - loss 0.04774615 - samples/sec: 16.49 - lr: 0.000000
2023-06-05 19:03:35,591 epoch 10 - iter 2331/7770 - loss 0.04538197 - samples/sec: 16.49 - lr: 0.000000
2023-06-05 19:06:49,511 epoch 10 - iter 3108/7770 - loss 0.04552443 - samples/sec: 16.03 - lr: 0.000000
2023-06-05 19:09:55,711 epoch 10 - iter 3885/7770 - loss 0.04549686 - samples/sec: 16.70 - lr: 0.000000
2023-06-05 19:12:59,401 epoch 10 - iter 4662/7770 - loss 0.04533306 - samples/sec: 16.93 - lr: 0.000000
2023-06-05 19:16:02,909 epoch 10 - iter 5439/7770 - loss 0.04572189 - samples/sec: 16.94 - lr: 0.000000
2023-06-05 19:19:10,487 epoch 10 - iter 6216/7770 - loss 0.04596539 - samples/sec: 16.58 - lr: 0.000000
2023-06-05 19:22:13,995 epoch 10 - iter 6993/7770 - loss 0.04663423 - samples/sec: 16.94 - lr: 0.000000
2023-06-05 19:25:18,426 epoch 10 - iter 7770/7770 - loss 0.04737159 - samples/sec: 16.86 - lr: 0.000000
2023-06-05 19:25:18,430 ----------------------------------------------------------------------------------------------------
2023-06-05 19:25:18,430 EPOCH 10 done: loss 0.0474 - lr 0.000000
2023-06-05 19:28:19,653 Evaluating as a multi-label problem: False
2023-06-05 19:28:19,753 DEV : loss 0.1051110178232193 - f1-score (micro avg)  0.9656
2023-06-05 19:28:19,995 BAD EPOCHS (no improvement): 4
2023-06-05 19:28:32,883 ----------------------------------------------------------------------------------------------------
2023-06-05 19:28:32,887 Testing using last state of model ...
2023-06-05 19:32:30,618 Evaluating as a multi-label problem: False
2023-06-05 19:32:30,726 0.9365	0.945	0.9408	0.9176
2023-06-05 19:32:30,727 
Results:
- F-score (micro) 0.9408
- F-score (macro) 0.9378
- Accuracy 0.9176

By class:
              precision    recall  f1-score   support

         PER     0.9805    0.9805    0.9805      2715
         ORG     0.9009    0.9469    0.9233      2543
         LOC     0.9506    0.9386    0.9446      2442
        MISC     0.9057    0.8999    0.9028      1889

   micro avg     0.9365    0.9450    0.9408      9589
   macro avg     0.9344    0.9415    0.9378      9589
weighted avg     0.9370    0.9450    0.9409      9589

2023-06-05 19:32:30,727 ----------------------------------------------------------------------------------------------------
2023-06-05 19:32:30,727 ----------------------------------------------------------------------------------------------------
2023-06-05 19:34:57,619 Evaluating as a multi-label problem: False
2023-06-05 19:34:57,674 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-05 19:34:57,674 0.9426	0.9421	0.9424	0.9273
2023-06-05 19:34:57,674 ----------------------------------------------------------------------------------------------------
2023-06-05 19:36:34,235 Evaluating as a multi-label problem: False
2023-06-05 19:36:34,302 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-05 19:36:34,303 0.9324	0.9471	0.9397	0.9109
