2023-05-27 18:36:19,795 ----------------------------------------------------------------------------------------------------
2023-05-27 18:36:19,800 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-27 18:36:19,802 ----------------------------------------------------------------------------------------------------
2023-05-27 18:36:19,802 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-27 18:36:19,803 ----------------------------------------------------------------------------------------------------
2023-05-27 18:36:19,803 Parameters:
2023-05-27 18:36:19,803  - learning_rate: "0.000005"
2023-05-27 18:36:19,803  - mini_batch_size: "4"
2023-05-27 18:36:19,803  - patience: "3"
2023-05-27 18:36:19,803  - anneal_factor: "0.5"
2023-05-27 18:36:19,803  - max_epochs: "10"
2023-05-27 18:36:19,803  - shuffle: "True"
2023-05-27 18:36:19,803  - train_with_dev: "False"
2023-05-27 18:36:19,803  - batch_growth_annealing: "False"
2023-05-27 18:36:19,803 ----------------------------------------------------------------------------------------------------
2023-05-27 18:36:19,803 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_1_full_fine_tuning"
2023-05-27 18:36:19,803 ----------------------------------------------------------------------------------------------------
2023-05-27 18:36:19,803 Device: cuda:3
2023-05-27 18:36:19,803 ----------------------------------------------------------------------------------------------------
2023-05-27 18:36:19,803 Embeddings storage mode: none
2023-05-27 18:36:19,804 ----------------------------------------------------------------------------------------------------
2023-05-27 18:39:25,737 epoch 1 - iter 777/7770 - loss 0.76500502 - samples/sec: 16.72 - lr: 0.000001
2023-05-27 18:42:28,661 epoch 1 - iter 1554/7770 - loss 0.57699624 - samples/sec: 17.00 - lr: 0.000001
2023-05-27 18:45:31,529 epoch 1 - iter 2331/7770 - loss 0.49857611 - samples/sec: 17.00 - lr: 0.000002
2023-05-27 18:48:38,553 epoch 1 - iter 3108/7770 - loss 0.42608701 - samples/sec: 16.63 - lr: 0.000002
2023-05-27 18:51:45,098 epoch 1 - iter 3885/7770 - loss 0.38157082 - samples/sec: 16.67 - lr: 0.000003
2023-05-27 18:54:45,449 epoch 1 - iter 4662/7770 - loss 0.36250340 - samples/sec: 17.24 - lr: 0.000003
2023-05-27 18:57:41,801 epoch 1 - iter 5439/7770 - loss 0.35090096 - samples/sec: 17.63 - lr: 0.000003
2023-05-27 19:00:41,466 epoch 1 - iter 6216/7770 - loss 0.33339024 - samples/sec: 17.31 - lr: 0.000004
2023-05-27 19:03:41,311 epoch 1 - iter 6993/7770 - loss 0.32203310 - samples/sec: 17.29 - lr: 0.000005
2023-05-27 19:06:42,711 epoch 1 - iter 7770/7770 - loss 0.30878492 - samples/sec: 17.14 - lr: 0.000005
2023-05-27 19:06:42,714 ----------------------------------------------------------------------------------------------------
2023-05-27 19:06:42,714 EPOCH 1 done: loss 0.3088 - lr 0.000005
2023-05-27 19:09:39,852 Evaluating as a multi-label problem: False
2023-05-27 19:09:39,956 DEV : loss 0.12801292538642883 - f1-score (micro avg)  0.9132
2023-05-27 19:09:40,167 BAD EPOCHS (no improvement): 4
2023-05-27 19:09:40,169 ----------------------------------------------------------------------------------------------------
2023-05-27 19:12:47,597 epoch 2 - iter 777/7770 - loss 0.18779314 - samples/sec: 16.59 - lr: 0.000005
2023-05-27 19:15:51,794 epoch 2 - iter 1554/7770 - loss 0.18411593 - samples/sec: 16.88 - lr: 0.000005
2023-05-27 19:18:59,767 epoch 2 - iter 2331/7770 - loss 0.18556420 - samples/sec: 16.54 - lr: 0.000005
2023-05-27 19:22:01,987 epoch 2 - iter 3108/7770 - loss 0.18589694 - samples/sec: 17.06 - lr: 0.000005
2023-05-27 19:25:05,797 epoch 2 - iter 3885/7770 - loss 0.17942939 - samples/sec: 16.92 - lr: 0.000005
2023-05-27 19:28:08,074 epoch 2 - iter 4662/7770 - loss 0.18005276 - samples/sec: 17.06 - lr: 0.000005
2023-05-27 19:31:13,010 epoch 2 - iter 5439/7770 - loss 0.17916905 - samples/sec: 16.81 - lr: 0.000005
2023-05-27 19:34:15,254 epoch 2 - iter 6216/7770 - loss 0.17735673 - samples/sec: 17.06 - lr: 0.000005
2023-05-27 19:37:19,473 epoch 2 - iter 6993/7770 - loss 0.17541508 - samples/sec: 16.88 - lr: 0.000005
2023-05-27 19:40:21,443 epoch 2 - iter 7770/7770 - loss 0.17142506 - samples/sec: 17.09 - lr: 0.000004
2023-05-27 19:40:21,446 ----------------------------------------------------------------------------------------------------
2023-05-27 19:40:21,446 EPOCH 2 done: loss 0.1714 - lr 0.000004
2023-05-27 19:43:08,687 Evaluating as a multi-label problem: False
2023-05-27 19:43:08,782 DEV : loss 0.09524528682231903 - f1-score (micro avg)  0.9515
2023-05-27 19:43:08,993 BAD EPOCHS (no improvement): 4
2023-05-27 19:43:08,996 ----------------------------------------------------------------------------------------------------
2023-05-27 19:46:13,332 epoch 3 - iter 777/7770 - loss 0.12496495 - samples/sec: 16.87 - lr: 0.000004
2023-05-27 19:49:18,684 epoch 3 - iter 1554/7770 - loss 0.12465459 - samples/sec: 16.78 - lr: 0.000004
2023-05-27 19:52:20,574 epoch 3 - iter 2331/7770 - loss 0.11992695 - samples/sec: 17.10 - lr: 0.000004
2023-05-27 19:55:22,245 epoch 3 - iter 3108/7770 - loss 0.12118932 - samples/sec: 17.12 - lr: 0.000004
2023-05-27 19:58:22,438 epoch 3 - iter 3885/7770 - loss 0.12278023 - samples/sec: 17.26 - lr: 0.000004
2023-05-27 20:01:22,858 epoch 3 - iter 4662/7770 - loss 0.12612989 - samples/sec: 17.23 - lr: 0.000004
2023-05-27 20:04:28,642 epoch 3 - iter 5439/7770 - loss 0.12688338 - samples/sec: 16.74 - lr: 0.000004
2023-05-27 20:07:38,421 epoch 3 - iter 6216/7770 - loss 0.12515765 - samples/sec: 16.38 - lr: 0.000004
2023-05-27 20:10:43,834 epoch 3 - iter 6993/7770 - loss 0.12579747 - samples/sec: 16.77 - lr: 0.000004
2023-05-27 20:13:45,954 epoch 3 - iter 7770/7770 - loss 0.12616663 - samples/sec: 17.07 - lr: 0.000004
2023-05-27 20:13:45,957 ----------------------------------------------------------------------------------------------------
2023-05-27 20:13:45,957 EPOCH 3 done: loss 0.1262 - lr 0.000004
2023-05-27 20:16:23,452 Evaluating as a multi-label problem: False
2023-05-27 20:16:23,550 DEV : loss 0.0787774845957756 - f1-score (micro avg)  0.9524
2023-05-27 20:16:23,737 BAD EPOCHS (no improvement): 4
2023-05-27 20:16:23,740 ----------------------------------------------------------------------------------------------------
2023-05-27 20:19:27,704 epoch 4 - iter 777/7770 - loss 0.12027587 - samples/sec: 16.90 - lr: 0.000004
2023-05-27 20:22:30,546 epoch 4 - iter 1554/7770 - loss 0.10992842 - samples/sec: 17.01 - lr: 0.000004
2023-05-27 20:25:31,776 epoch 4 - iter 2331/7770 - loss 0.10823095 - samples/sec: 17.16 - lr: 0.000004
2023-05-27 20:28:34,691 epoch 4 - iter 3108/7770 - loss 0.11465983 - samples/sec: 17.00 - lr: 0.000004
2023-05-27 20:31:39,087 epoch 4 - iter 3885/7770 - loss 0.11033291 - samples/sec: 16.86 - lr: 0.000004
2023-05-27 20:34:40,364 epoch 4 - iter 4662/7770 - loss 0.10757535 - samples/sec: 17.15 - lr: 0.000004
2023-05-27 20:37:40,947 epoch 4 - iter 5439/7770 - loss 0.10688123 - samples/sec: 17.22 - lr: 0.000004
2023-05-27 20:40:44,578 epoch 4 - iter 6216/7770 - loss 0.10600900 - samples/sec: 16.93 - lr: 0.000003
2023-05-27 20:43:44,837 epoch 4 - iter 6993/7770 - loss 0.10354709 - samples/sec: 17.25 - lr: 0.000003
2023-05-27 20:46:48,082 epoch 4 - iter 7770/7770 - loss 0.10355767 - samples/sec: 16.97 - lr: 0.000003
2023-05-27 20:46:48,086 ----------------------------------------------------------------------------------------------------
2023-05-27 20:46:48,087 EPOCH 4 done: loss 0.1036 - lr 0.000003
2023-05-27 20:49:43,313 Evaluating as a multi-label problem: False
2023-05-27 20:49:43,372 DEV : loss 0.09444382041692734 - f1-score (micro avg)  0.9555
2023-05-27 20:49:43,518 BAD EPOCHS (no improvement): 4
2023-05-27 20:49:43,521 ----------------------------------------------------------------------------------------------------
2023-05-27 20:52:51,359 epoch 5 - iter 777/7770 - loss 0.07176145 - samples/sec: 16.55 - lr: 0.000003
2023-05-27 20:55:52,548 epoch 5 - iter 1554/7770 - loss 0.07541367 - samples/sec: 17.16 - lr: 0.000003
2023-05-27 20:59:03,313 epoch 5 - iter 2331/7770 - loss 0.07788739 - samples/sec: 16.30 - lr: 0.000003
2023-05-27 21:02:05,964 epoch 5 - iter 3108/7770 - loss 0.08125230 - samples/sec: 17.02 - lr: 0.000003
2023-05-27 21:05:09,808 epoch 5 - iter 3885/7770 - loss 0.08325313 - samples/sec: 16.91 - lr: 0.000003
2023-05-27 21:08:09,394 epoch 5 - iter 4662/7770 - loss 0.08269448 - samples/sec: 17.31 - lr: 0.000003
2023-05-27 21:11:12,007 epoch 5 - iter 5439/7770 - loss 0.08420685 - samples/sec: 17.03 - lr: 0.000003
2023-05-27 21:14:14,183 epoch 5 - iter 6216/7770 - loss 0.08505170 - samples/sec: 17.07 - lr: 0.000003
2023-05-27 21:17:14,812 epoch 5 - iter 6993/7770 - loss 0.08313512 - samples/sec: 17.21 - lr: 0.000003
2023-05-27 21:20:16,100 epoch 5 - iter 7770/7770 - loss 0.08244210 - samples/sec: 17.15 - lr: 0.000003
2023-05-27 21:20:16,104 ----------------------------------------------------------------------------------------------------
2023-05-27 21:20:16,104 EPOCH 5 done: loss 0.0824 - lr 0.000003
2023-05-27 21:23:00,477 Evaluating as a multi-label problem: False
2023-05-27 21:23:00,542 DEV : loss 0.10137532651424408 - f1-score (micro avg)  0.9572
2023-05-27 21:23:00,702 BAD EPOCHS (no improvement): 4
2023-05-27 21:23:00,708 ----------------------------------------------------------------------------------------------------
2023-05-27 21:26:05,744 epoch 6 - iter 777/7770 - loss 0.07138169 - samples/sec: 16.81 - lr: 0.000003
2023-05-27 21:29:13,222 epoch 6 - iter 1554/7770 - loss 0.07529366 - samples/sec: 16.59 - lr: 0.000003
2023-05-27 21:32:14,673 epoch 6 - iter 2331/7770 - loss 0.07704807 - samples/sec: 17.14 - lr: 0.000003
2023-05-27 21:35:14,130 epoch 6 - iter 3108/7770 - loss 0.07824456 - samples/sec: 17.33 - lr: 0.000003
2023-05-27 21:38:13,955 epoch 6 - iter 3885/7770 - loss 0.07762464 - samples/sec: 17.29 - lr: 0.000003
2023-05-27 21:41:13,911 epoch 6 - iter 4662/7770 - loss 0.07654047 - samples/sec: 17.28 - lr: 0.000002
2023-05-27 21:44:22,606 epoch 6 - iter 5439/7770 - loss 0.07722836 - samples/sec: 16.48 - lr: 0.000002
2023-05-27 21:47:27,576 epoch 6 - iter 6216/7770 - loss 0.07604371 - samples/sec: 16.81 - lr: 0.000002
2023-05-27 21:50:30,329 epoch 6 - iter 6993/7770 - loss 0.07613369 - samples/sec: 17.01 - lr: 0.000002
2023-05-27 21:53:30,673 epoch 6 - iter 7770/7770 - loss 0.07642884 - samples/sec: 17.24 - lr: 0.000002
2023-05-27 21:53:30,677 ----------------------------------------------------------------------------------------------------
2023-05-27 21:53:30,677 EPOCH 6 done: loss 0.0764 - lr 0.000002
2023-05-27 21:56:15,367 Evaluating as a multi-label problem: False
2023-05-27 21:56:15,465 DEV : loss 0.09946619719266891 - f1-score (micro avg)  0.9502
2023-05-27 21:56:15,682 BAD EPOCHS (no improvement): 4
2023-05-27 21:56:15,685 ----------------------------------------------------------------------------------------------------
2023-05-27 21:59:19,910 epoch 7 - iter 777/7770 - loss 0.08150221 - samples/sec: 16.88 - lr: 0.000002
2023-05-27 22:02:21,363 epoch 7 - iter 1554/7770 - loss 0.07412597 - samples/sec: 17.14 - lr: 0.000002
2023-05-27 22:05:20,502 epoch 7 - iter 2331/7770 - loss 0.07316730 - samples/sec: 17.36 - lr: 0.000002
2023-05-27 22:08:20,691 epoch 7 - iter 3108/7770 - loss 0.07225574 - samples/sec: 17.26 - lr: 0.000002
2023-05-27 22:11:19,651 epoch 7 - iter 3885/7770 - loss 0.06832078 - samples/sec: 17.38 - lr: 0.000002
2023-05-27 22:14:23,195 epoch 7 - iter 4662/7770 - loss 0.06678804 - samples/sec: 16.94 - lr: 0.000002
2023-05-27 22:17:26,080 epoch 7 - iter 5439/7770 - loss 0.06701539 - samples/sec: 17.00 - lr: 0.000002
2023-05-27 22:20:27,611 epoch 7 - iter 6216/7770 - loss 0.06657973 - samples/sec: 17.13 - lr: 0.000002
2023-05-27 22:23:28,264 epoch 7 - iter 6993/7770 - loss 0.06587714 - samples/sec: 17.21 - lr: 0.000002
2023-05-27 22:26:31,075 epoch 7 - iter 7770/7770 - loss 0.06567124 - samples/sec: 17.01 - lr: 0.000002
2023-05-27 22:26:31,080 ----------------------------------------------------------------------------------------------------
2023-05-27 22:26:31,080 EPOCH 7 done: loss 0.0657 - lr 0.000002
2023-05-27 22:29:31,549 Evaluating as a multi-label problem: False
2023-05-27 22:29:31,650 DEV : loss 0.10105200111865997 - f1-score (micro avg)  0.9594
2023-05-27 22:29:31,906 BAD EPOCHS (no improvement): 4
2023-05-27 22:29:31,909 ----------------------------------------------------------------------------------------------------
2023-05-27 22:32:35,236 epoch 8 - iter 777/7770 - loss 0.05260347 - samples/sec: 16.96 - lr: 0.000002
2023-05-27 22:35:40,441 epoch 8 - iter 1554/7770 - loss 0.05437027 - samples/sec: 16.79 - lr: 0.000002
2023-05-27 22:38:48,889 epoch 8 - iter 2331/7770 - loss 0.05524220 - samples/sec: 16.50 - lr: 0.000002
2023-05-27 22:41:51,100 epoch 8 - iter 3108/7770 - loss 0.05578976 - samples/sec: 17.07 - lr: 0.000001
2023-05-27 22:44:53,490 epoch 8 - iter 3885/7770 - loss 0.05509081 - samples/sec: 17.05 - lr: 0.000001
2023-05-27 22:47:52,915 epoch 8 - iter 4662/7770 - loss 0.05553147 - samples/sec: 17.33 - lr: 0.000001
2023-05-27 22:50:53,845 epoch 8 - iter 5439/7770 - loss 0.05692700 - samples/sec: 17.19 - lr: 0.000001
2023-05-27 22:53:53,638 epoch 8 - iter 6216/7770 - loss 0.05707565 - samples/sec: 17.30 - lr: 0.000001
2023-05-27 22:56:55,446 epoch 8 - iter 6993/7770 - loss 0.05659283 - samples/sec: 17.10 - lr: 0.000001
2023-05-27 22:59:55,949 epoch 8 - iter 7770/7770 - loss 0.05607548 - samples/sec: 17.23 - lr: 0.000001
2023-05-27 22:59:55,953 ----------------------------------------------------------------------------------------------------
2023-05-27 22:59:55,953 EPOCH 8 done: loss 0.0561 - lr 0.000001
2023-05-27 23:02:51,054 Evaluating as a multi-label problem: False
2023-05-27 23:02:51,158 DEV : loss 0.10615609586238861 - f1-score (micro avg)  0.9633
2023-05-27 23:02:51,395 BAD EPOCHS (no improvement): 4
2023-05-27 23:02:51,398 ----------------------------------------------------------------------------------------------------
2023-05-27 23:05:55,484 epoch 9 - iter 777/7770 - loss 0.04840547 - samples/sec: 16.89 - lr: 0.000001
2023-05-27 23:08:58,848 epoch 9 - iter 1554/7770 - loss 0.05303353 - samples/sec: 16.96 - lr: 0.000001
2023-05-27 23:11:59,334 epoch 9 - iter 2331/7770 - loss 0.05313363 - samples/sec: 17.23 - lr: 0.000001
2023-05-27 23:15:01,389 epoch 9 - iter 3108/7770 - loss 0.05328320 - samples/sec: 17.08 - lr: 0.000001
2023-05-27 23:18:03,916 epoch 9 - iter 3885/7770 - loss 0.05252059 - samples/sec: 17.04 - lr: 0.000001
2023-05-27 23:21:01,103 epoch 9 - iter 4662/7770 - loss 0.05163910 - samples/sec: 17.55 - lr: 0.000001
2023-05-27 23:24:10,120 epoch 9 - iter 5439/7770 - loss 0.05195393 - samples/sec: 16.45 - lr: 0.000001
2023-05-27 23:27:14,690 epoch 9 - iter 6216/7770 - loss 0.05227985 - samples/sec: 16.85 - lr: 0.000001
2023-05-27 23:30:16,261 epoch 9 - iter 6993/7770 - loss 0.05261930 - samples/sec: 17.13 - lr: 0.000001
2023-05-27 23:33:18,279 epoch 9 - iter 7770/7770 - loss 0.05331215 - samples/sec: 17.08 - lr: 0.000001
2023-05-27 23:33:18,282 ----------------------------------------------------------------------------------------------------
2023-05-27 23:33:18,282 EPOCH 9 done: loss 0.0533 - lr 0.000001
2023-05-27 23:35:56,742 Evaluating as a multi-label problem: False
2023-05-27 23:35:56,842 DEV : loss 0.1050281822681427 - f1-score (micro avg)  0.9629
2023-05-27 23:35:57,079 BAD EPOCHS (no improvement): 4
2023-05-27 23:35:57,082 ----------------------------------------------------------------------------------------------------
2023-05-27 23:39:02,555 epoch 10 - iter 777/7770 - loss 0.05390245 - samples/sec: 16.77 - lr: 0.000001
2023-05-27 23:42:03,286 epoch 10 - iter 1554/7770 - loss 0.05272386 - samples/sec: 17.20 - lr: 0.000000
2023-05-27 23:45:06,802 epoch 10 - iter 2331/7770 - loss 0.05287224 - samples/sec: 16.94 - lr: 0.000000
2023-05-27 23:48:07,349 epoch 10 - iter 3108/7770 - loss 0.05160658 - samples/sec: 17.22 - lr: 0.000000
2023-05-27 23:51:08,187 epoch 10 - iter 3885/7770 - loss 0.05279897 - samples/sec: 17.20 - lr: 0.000000
2023-05-27 23:54:10,573 epoch 10 - iter 4662/7770 - loss 0.05177137 - samples/sec: 17.05 - lr: 0.000000
2023-05-27 23:57:11,380 epoch 10 - iter 5439/7770 - loss 0.05146177 - samples/sec: 17.20 - lr: 0.000000
2023-05-28 00:00:09,391 epoch 10 - iter 6216/7770 - loss 0.05130302 - samples/sec: 17.47 - lr: 0.000000
2023-05-28 00:03:09,991 epoch 10 - iter 6993/7770 - loss 0.05043435 - samples/sec: 17.22 - lr: 0.000000
2023-05-28 00:06:12,117 epoch 10 - iter 7770/7770 - loss 0.05029015 - samples/sec: 17.07 - lr: 0.000000
2023-05-28 00:06:12,121 ----------------------------------------------------------------------------------------------------
2023-05-28 00:06:12,121 EPOCH 10 done: loss 0.0503 - lr 0.000000
2023-05-28 00:09:00,209 Evaluating as a multi-label problem: False
2023-05-28 00:09:00,310 DEV : loss 0.10833484679460526 - f1-score (micro avg)  0.9629
2023-05-28 00:09:00,545 BAD EPOCHS (no improvement): 4
2023-05-28 00:09:12,194 ----------------------------------------------------------------------------------------------------
2023-05-28 00:09:12,197 Testing using last state of model ...
2023-05-28 00:13:05,760 Evaluating as a multi-label problem: False
2023-05-28 00:13:05,874 0.9326	0.9411	0.9368	0.9111
2023-05-28 00:13:05,875 
Results:
- F-score (micro) 0.9368
- F-score (macro) 0.9338
- Accuracy 0.9111

By class:
              precision    recall  f1-score   support

         PER     0.9819    0.9779    0.9799      2715
         ORG     0.8940    0.9418    0.9173      2543
         LOC     0.9495    0.9324    0.9409      2442
        MISC     0.8955    0.8984    0.8969      1889

   micro avg     0.9326    0.9411    0.9368      9589
   macro avg     0.9302    0.9376    0.9338      9589
weighted avg     0.9333    0.9411    0.9370      9589

2023-05-28 00:13:05,875 ----------------------------------------------------------------------------------------------------
2023-05-28 00:13:05,875 ----------------------------------------------------------------------------------------------------
2023-05-28 00:15:21,270 Evaluating as a multi-label problem: False
2023-05-28 00:15:21,317 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-28 00:15:21,317 0.9402	0.9383	0.9393	0.9217
2023-05-28 00:15:21,317 ----------------------------------------------------------------------------------------------------
2023-05-28 00:16:46,060 Evaluating as a multi-label problem: False
2023-05-28 00:16:46,099 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-28 00:16:46,100 0.9274	0.943	0.9351	0.9039
