2023-06-02 16:38:12,090 ----------------------------------------------------------------------------------------------------
2023-06-02 16:38:12,094 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-02 16:38:12,103 ----------------------------------------------------------------------------------------------------
2023-06-02 16:38:12,104 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-02 16:38:12,104 ----------------------------------------------------------------------------------------------------
2023-06-02 16:38:12,104 Parameters:
2023-06-02 16:38:12,104  - learning_rate: "0.300000"
2023-06-02 16:38:12,104  - mini_batch_size: "32"
2023-06-02 16:38:12,104  - patience: "3"
2023-06-02 16:38:12,104  - anneal_factor: "0.5"
2023-06-02 16:38:12,104  - max_epochs: "10"
2023-06-02 16:38:12,104  - shuffle: "True"
2023-06-02 16:38:12,104  - train_with_dev: "False"
2023-06-02 16:38:12,104  - batch_growth_annealing: "False"
2023-06-02 16:38:12,104 ----------------------------------------------------------------------------------------------------
2023-06-02 16:38:12,104 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_2_linear_probing"
2023-06-02 16:38:12,104 ----------------------------------------------------------------------------------------------------
2023-06-02 16:38:12,104 Device: cuda:1
2023-06-02 16:38:12,105 ----------------------------------------------------------------------------------------------------
2023-06-02 16:38:12,105 Embeddings storage mode: none
2023-06-02 16:38:12,105 ----------------------------------------------------------------------------------------------------
2023-06-02 16:39:17,220 epoch 1 - iter 97/972 - loss 0.79768304 - samples/sec: 47.68 - lr: 0.029938
2023-06-02 16:40:23,606 epoch 1 - iter 194/972 - loss 0.70498698 - samples/sec: 46.77 - lr: 0.059877
2023-06-02 16:41:28,094 epoch 1 - iter 291/972 - loss 0.89980360 - samples/sec: 48.15 - lr: 0.089815
2023-06-02 16:42:31,067 epoch 1 - iter 388/972 - loss 1.09285642 - samples/sec: 49.31 - lr: 0.119753
2023-06-02 16:43:41,050 epoch 1 - iter 485/972 - loss 1.35679948 - samples/sec: 44.36 - lr: 0.149691
2023-06-02 16:44:39,671 epoch 1 - iter 582/972 - loss 1.84695358 - samples/sec: 52.97 - lr: 0.179630
2023-06-02 16:45:41,978 epoch 1 - iter 679/972 - loss 2.18765986 - samples/sec: 49.83 - lr: 0.209568
2023-06-02 16:46:40,205 epoch 1 - iter 776/972 - loss 2.49405756 - samples/sec: 53.32 - lr: 0.239506
2023-06-02 16:47:39,303 epoch 1 - iter 873/972 - loss 2.83264740 - samples/sec: 52.54 - lr: 0.269444
2023-06-02 16:48:42,395 epoch 1 - iter 970/972 - loss 3.09748880 - samples/sec: 49.21 - lr: 0.299383
2023-06-02 16:48:43,122 ----------------------------------------------------------------------------------------------------
2023-06-02 16:48:43,122 EPOCH 1 done: loss 3.1015 - lr 0.299383
2023-06-02 16:51:42,976 Evaluating as a multi-label problem: False
2023-06-02 16:51:43,086 DEV : loss 2.6126341819763184 - f1-score (micro avg)  0.5648
2023-06-02 16:51:43,267 BAD EPOCHS (no improvement): 4
2023-06-02 16:51:43,270 ----------------------------------------------------------------------------------------------------
2023-06-02 16:52:50,477 epoch 2 - iter 97/972 - loss 5.81627662 - samples/sec: 46.20 - lr: 0.296674
2023-06-02 16:54:00,185 epoch 2 - iter 194/972 - loss 5.53111594 - samples/sec: 44.54 - lr: 0.293349
2023-06-02 16:55:04,830 epoch 2 - iter 291/972 - loss 5.37589197 - samples/sec: 48.03 - lr: 0.290023
2023-06-02 16:56:07,673 epoch 2 - iter 388/972 - loss 5.32649814 - samples/sec: 49.41 - lr: 0.286697
2023-06-02 16:57:16,034 epoch 2 - iter 485/972 - loss 5.37785028 - samples/sec: 45.42 - lr: 0.283371
2023-06-02 16:58:20,336 epoch 2 - iter 582/972 - loss 5.41659109 - samples/sec: 48.29 - lr: 0.280046
2023-06-02 16:59:32,424 epoch 2 - iter 679/972 - loss 5.42414671 - samples/sec: 43.07 - lr: 0.276720
2023-06-02 17:00:37,475 epoch 2 - iter 776/972 - loss 5.42145090 - samples/sec: 47.73 - lr: 0.273394
2023-06-02 17:01:41,692 epoch 2 - iter 873/972 - loss 5.44503722 - samples/sec: 48.35 - lr: 0.270069
2023-06-02 17:02:50,686 epoch 2 - iter 970/972 - loss 5.45059389 - samples/sec: 45.00 - lr: 0.266743
2023-06-02 17:02:51,588 ----------------------------------------------------------------------------------------------------
2023-06-02 17:02:51,588 EPOCH 2 done: loss 5.4517 - lr 0.266743
2023-06-02 17:05:55,009 Evaluating as a multi-label problem: False
2023-06-02 17:05:55,117 DEV : loss 2.38079571723938 - f1-score (micro avg)  0.6197
2023-06-02 17:05:55,300 BAD EPOCHS (no improvement): 4
2023-06-02 17:05:55,303 ----------------------------------------------------------------------------------------------------
2023-06-02 17:06:57,858 epoch 3 - iter 97/972 - loss 5.40441480 - samples/sec: 49.64 - lr: 0.263349
2023-06-02 17:08:08,447 epoch 3 - iter 194/972 - loss 5.24993062 - samples/sec: 43.99 - lr: 0.260023
2023-06-02 17:09:14,619 epoch 3 - iter 291/972 - loss 5.15772983 - samples/sec: 46.92 - lr: 0.256697
2023-06-02 17:10:18,630 epoch 3 - iter 388/972 - loss 5.07376443 - samples/sec: 48.51 - lr: 0.253371
2023-06-02 17:11:29,586 epoch 3 - iter 485/972 - loss 4.94446975 - samples/sec: 43.76 - lr: 0.250046
2023-06-02 17:12:33,879 epoch 3 - iter 582/972 - loss 4.90401806 - samples/sec: 48.30 - lr: 0.246720
2023-06-02 17:13:42,737 epoch 3 - iter 679/972 - loss 4.87969142 - samples/sec: 45.09 - lr: 0.243394
2023-06-02 17:14:45,992 epoch 3 - iter 776/972 - loss 4.87255349 - samples/sec: 49.09 - lr: 0.240069
2023-06-02 17:15:48,544 epoch 3 - iter 873/972 - loss 4.84641704 - samples/sec: 49.64 - lr: 0.236743
2023-06-02 17:16:55,475 epoch 3 - iter 970/972 - loss 4.83853671 - samples/sec: 46.39 - lr: 0.233417
2023-06-02 17:16:56,267 ----------------------------------------------------------------------------------------------------
2023-06-02 17:16:56,267 EPOCH 3 done: loss 4.8386 - lr 0.233417
2023-06-02 17:19:57,205 Evaluating as a multi-label problem: False
2023-06-02 17:19:57,308 DEV : loss 1.6814665794372559 - f1-score (micro avg)  0.6869
2023-06-02 17:19:57,482 BAD EPOCHS (no improvement): 4
2023-06-02 17:19:57,486 ----------------------------------------------------------------------------------------------------
2023-06-02 17:20:59,886 epoch 4 - iter 97/972 - loss 4.64883332 - samples/sec: 49.76 - lr: 0.230023
2023-06-02 17:22:09,072 epoch 4 - iter 194/972 - loss 4.54241183 - samples/sec: 44.88 - lr: 0.226697
2023-06-02 17:23:14,095 epoch 4 - iter 291/972 - loss 4.55043943 - samples/sec: 47.75 - lr: 0.223371
2023-06-02 17:24:18,238 epoch 4 - iter 388/972 - loss 4.57756700 - samples/sec: 48.41 - lr: 0.220046
2023-06-02 17:25:29,079 epoch 4 - iter 485/972 - loss 4.58736061 - samples/sec: 43.83 - lr: 0.216720
2023-06-02 17:26:32,129 epoch 4 - iter 582/972 - loss 4.53733889 - samples/sec: 49.25 - lr: 0.213394
2023-06-02 17:27:39,929 epoch 4 - iter 679/972 - loss 4.49942598 - samples/sec: 45.79 - lr: 0.210069
2023-06-02 17:28:44,926 epoch 4 - iter 776/972 - loss 4.47284219 - samples/sec: 47.77 - lr: 0.206743
2023-06-02 17:29:47,988 epoch 4 - iter 873/972 - loss 4.45876169 - samples/sec: 49.24 - lr: 0.203417
2023-06-02 17:30:59,232 epoch 4 - iter 970/972 - loss 4.43048926 - samples/sec: 43.58 - lr: 0.200091
2023-06-02 17:31:00,010 ----------------------------------------------------------------------------------------------------
2023-06-02 17:31:00,010 EPOCH 4 done: loss 4.4282 - lr 0.200091
2023-06-02 17:33:56,634 Evaluating as a multi-label problem: False
2023-06-02 17:33:56,742 DEV : loss 1.5079022645950317 - f1-score (micro avg)  0.6637
2023-06-02 17:33:56,920 BAD EPOCHS (no improvement): 4
2023-06-02 17:33:56,923 ----------------------------------------------------------------------------------------------------
2023-06-02 17:35:04,040 epoch 5 - iter 97/972 - loss 4.07435960 - samples/sec: 46.27 - lr: 0.196697
2023-06-02 17:36:16,329 epoch 5 - iter 194/972 - loss 3.93194104 - samples/sec: 42.95 - lr: 0.193371
2023-06-02 17:37:20,449 epoch 5 - iter 291/972 - loss 3.90623125 - samples/sec: 48.43 - lr: 0.190046
2023-06-02 17:38:29,883 epoch 5 - iter 388/972 - loss 3.92935519 - samples/sec: 44.72 - lr: 0.186720
2023-06-02 17:39:32,576 epoch 5 - iter 485/972 - loss 3.90591801 - samples/sec: 49.53 - lr: 0.183394
2023-06-02 17:40:35,366 epoch 5 - iter 582/972 - loss 3.88145766 - samples/sec: 49.45 - lr: 0.180069
2023-06-02 17:41:47,693 epoch 5 - iter 679/972 - loss 3.84571969 - samples/sec: 42.93 - lr: 0.176743
2023-06-02 17:42:55,296 epoch 5 - iter 776/972 - loss 3.84474027 - samples/sec: 45.93 - lr: 0.173417
2023-06-02 17:44:05,086 epoch 5 - iter 873/972 - loss 3.81511849 - samples/sec: 44.49 - lr: 0.170091
2023-06-02 17:45:11,332 epoch 5 - iter 970/972 - loss 3.76791927 - samples/sec: 46.87 - lr: 0.166766
2023-06-02 17:45:12,041 ----------------------------------------------------------------------------------------------------
2023-06-02 17:45:12,041 EPOCH 5 done: loss 3.7698 - lr 0.166766
2023-06-02 17:48:16,786 Evaluating as a multi-label problem: False
2023-06-02 17:48:16,884 DEV : loss 1.3480409383773804 - f1-score (micro avg)  0.659
2023-06-02 17:48:17,094 BAD EPOCHS (no improvement): 4
2023-06-02 17:48:17,096 ----------------------------------------------------------------------------------------------------
2023-06-02 17:49:24,030 epoch 6 - iter 97/972 - loss 3.71600472 - samples/sec: 46.39 - lr: 0.163371
2023-06-02 17:50:34,497 epoch 6 - iter 194/972 - loss 3.55974848 - samples/sec: 44.06 - lr: 0.160046
2023-06-02 17:51:35,340 epoch 6 - iter 291/972 - loss 3.47790932 - samples/sec: 51.03 - lr: 0.156720
2023-06-02 17:52:41,706 epoch 6 - iter 388/972 - loss 3.44885022 - samples/sec: 46.78 - lr: 0.153394
2023-06-02 17:53:48,993 epoch 6 - iter 485/972 - loss 3.36180162 - samples/sec: 46.15 - lr: 0.150069
2023-06-02 17:54:53,596 epoch 6 - iter 582/972 - loss 3.32315582 - samples/sec: 48.06 - lr: 0.146743
2023-06-02 17:56:05,279 epoch 6 - iter 679/972 - loss 3.28637081 - samples/sec: 43.31 - lr: 0.143417
2023-06-02 17:57:11,184 epoch 6 - iter 776/972 - loss 3.26549225 - samples/sec: 47.11 - lr: 0.140091
2023-06-02 17:58:21,396 epoch 6 - iter 873/972 - loss 3.24428039 - samples/sec: 44.22 - lr: 0.136766
2023-06-02 17:59:25,539 epoch 6 - iter 970/972 - loss 3.20564929 - samples/sec: 48.41 - lr: 0.133440
2023-06-02 17:59:26,381 ----------------------------------------------------------------------------------------------------
2023-06-02 17:59:26,381 EPOCH 6 done: loss 3.2058 - lr 0.133440
2023-06-02 18:02:30,317 Evaluating as a multi-label problem: False
2023-06-02 18:02:30,416 DEV : loss 1.2295584678649902 - f1-score (micro avg)  0.6559
2023-06-02 18:02:30,603 BAD EPOCHS (no improvement): 4
2023-06-02 18:02:30,607 ----------------------------------------------------------------------------------------------------
2023-06-02 18:03:34,560 epoch 7 - iter 97/972 - loss 2.82798596 - samples/sec: 48.55 - lr: 0.130046
2023-06-02 18:04:41,466 epoch 7 - iter 194/972 - loss 2.72818850 - samples/sec: 46.41 - lr: 0.126720
2023-06-02 18:05:47,401 epoch 7 - iter 291/972 - loss 2.75949413 - samples/sec: 47.09 - lr: 0.123394
2023-06-02 18:06:53,543 epoch 7 - iter 388/972 - loss 2.75994235 - samples/sec: 46.94 - lr: 0.120069
2023-06-02 18:07:54,750 epoch 7 - iter 485/972 - loss 2.78542715 - samples/sec: 50.73 - lr: 0.116743
2023-06-02 18:09:00,261 epoch 7 - iter 582/972 - loss 2.74555365 - samples/sec: 47.40 - lr: 0.113417
2023-06-02 18:10:11,814 epoch 7 - iter 679/972 - loss 2.73071033 - samples/sec: 43.39 - lr: 0.110091
2023-06-02 18:11:17,843 epoch 7 - iter 776/972 - loss 2.71975009 - samples/sec: 47.03 - lr: 0.106766
2023-06-02 18:12:27,492 epoch 7 - iter 873/972 - loss 2.68782193 - samples/sec: 44.58 - lr: 0.103440
2023-06-02 18:13:32,208 epoch 7 - iter 970/972 - loss 2.65554453 - samples/sec: 47.98 - lr: 0.100114
2023-06-02 18:13:33,118 ----------------------------------------------------------------------------------------------------
2023-06-02 18:13:33,118 EPOCH 7 done: loss 2.6552 - lr 0.100114
2023-06-02 18:16:39,233 Evaluating as a multi-label problem: False
2023-06-02 18:16:39,334 DEV : loss 0.9361079931259155 - f1-score (micro avg)  0.6751
2023-06-02 18:16:39,537 BAD EPOCHS (no improvement): 4
2023-06-02 18:16:39,540 ----------------------------------------------------------------------------------------------------
2023-06-02 18:17:51,062 epoch 8 - iter 97/972 - loss 2.32128451 - samples/sec: 43.41 - lr: 0.096720
2023-06-02 18:18:57,056 epoch 8 - iter 194/972 - loss 2.30376832 - samples/sec: 47.05 - lr: 0.093394
2023-06-02 18:20:03,019 epoch 8 - iter 291/972 - loss 2.29460380 - samples/sec: 47.07 - lr: 0.090069
2023-06-02 18:21:14,026 epoch 8 - iter 388/972 - loss 2.21347387 - samples/sec: 43.73 - lr: 0.086743
2023-06-02 18:22:18,748 epoch 8 - iter 485/972 - loss 2.17269524 - samples/sec: 47.97 - lr: 0.083417
2023-06-02 18:23:28,131 epoch 8 - iter 582/972 - loss 2.14688591 - samples/sec: 44.75 - lr: 0.080091
2023-06-02 18:24:35,483 epoch 8 - iter 679/972 - loss 2.11454092 - samples/sec: 46.10 - lr: 0.076766
2023-06-02 18:25:41,745 epoch 8 - iter 776/972 - loss 2.08939846 - samples/sec: 46.86 - lr: 0.073440
2023-06-02 18:26:50,328 epoch 8 - iter 873/972 - loss 2.04704920 - samples/sec: 45.27 - lr: 0.070114
2023-06-02 18:27:57,322 epoch 8 - iter 970/972 - loss 2.01202209 - samples/sec: 46.35 - lr: 0.066789
2023-06-02 18:27:58,418 ----------------------------------------------------------------------------------------------------
2023-06-02 18:27:58,418 EPOCH 8 done: loss 2.0106 - lr 0.066789
2023-06-02 18:30:56,898 Evaluating as a multi-label problem: False
2023-06-02 18:30:56,998 DEV : loss 0.7109947204589844 - f1-score (micro avg)  0.678
2023-06-02 18:30:57,185 BAD EPOCHS (no improvement): 4
2023-06-02 18:30:57,188 ----------------------------------------------------------------------------------------------------
2023-06-02 18:32:07,480 epoch 9 - iter 97/972 - loss 1.71774713 - samples/sec: 44.17 - lr: 0.063394
2023-06-02 18:33:11,666 epoch 9 - iter 194/972 - loss 1.69923424 - samples/sec: 48.37 - lr: 0.060069
2023-06-02 18:34:16,768 epoch 9 - iter 291/972 - loss 1.63777907 - samples/sec: 47.69 - lr: 0.056743
2023-06-02 18:35:27,131 epoch 9 - iter 388/972 - loss 1.59712424 - samples/sec: 44.13 - lr: 0.053417
2023-06-02 18:36:30,650 epoch 9 - iter 485/972 - loss 1.55479371 - samples/sec: 48.88 - lr: 0.050091
2023-06-02 18:37:40,403 epoch 9 - iter 582/972 - loss 1.52685683 - samples/sec: 44.51 - lr: 0.046766
2023-06-02 18:38:42,071 epoch 9 - iter 679/972 - loss 1.50307600 - samples/sec: 50.35 - lr: 0.043440
2023-06-02 18:39:47,684 epoch 9 - iter 776/972 - loss 1.46877133 - samples/sec: 47.32 - lr: 0.040114
2023-06-02 18:40:56,448 epoch 9 - iter 873/972 - loss 1.43385474 - samples/sec: 45.15 - lr: 0.036789
2023-06-02 18:41:59,448 epoch 9 - iter 970/972 - loss 1.39999504 - samples/sec: 49.29 - lr: 0.033463
2023-06-02 18:42:00,229 ----------------------------------------------------------------------------------------------------
2023-06-02 18:42:00,229 EPOCH 9 done: loss 1.4015 - lr 0.033463
2023-06-02 18:44:48,836 Evaluating as a multi-label problem: False
2023-06-02 18:44:48,937 DEV : loss 0.4207947850227356 - f1-score (micro avg)  0.7067
2023-06-02 18:44:49,148 BAD EPOCHS (no improvement): 4
2023-06-02 18:44:49,151 ----------------------------------------------------------------------------------------------------
2023-06-02 18:46:02,121 epoch 10 - iter 97/972 - loss 1.04596429 - samples/sec: 42.55 - lr: 0.030069
2023-06-02 18:47:06,248 epoch 10 - iter 194/972 - loss 1.02903283 - samples/sec: 48.42 - lr: 0.026743
2023-06-02 18:48:17,855 epoch 10 - iter 291/972 - loss 0.99095634 - samples/sec: 43.36 - lr: 0.023417
2023-06-02 18:49:19,371 epoch 10 - iter 388/972 - loss 0.97143451 - samples/sec: 50.47 - lr: 0.020091
2023-06-02 18:50:25,273 epoch 10 - iter 485/972 - loss 0.94870773 - samples/sec: 47.12 - lr: 0.016766
2023-06-02 18:51:34,867 epoch 10 - iter 582/972 - loss 0.91831538 - samples/sec: 44.61 - lr: 0.013440
2023-06-02 18:52:37,359 epoch 10 - iter 679/972 - loss 0.88761055 - samples/sec: 49.69 - lr: 0.010114
2023-06-02 18:53:41,443 epoch 10 - iter 776/972 - loss 0.85867741 - samples/sec: 48.45 - lr: 0.006789
2023-06-02 18:54:48,855 epoch 10 - iter 873/972 - loss 0.83218448 - samples/sec: 46.06 - lr: 0.003463
2023-06-02 18:55:52,711 epoch 10 - iter 970/972 - loss 0.81080509 - samples/sec: 48.62 - lr: 0.000137
2023-06-02 18:55:53,602 ----------------------------------------------------------------------------------------------------
2023-06-02 18:55:53,602 EPOCH 10 done: loss 0.8102 - lr 0.000137
2023-06-02 18:58:59,713 Evaluating as a multi-label problem: False
2023-06-02 18:58:59,809 DEV : loss 0.2354847490787506 - f1-score (micro avg)  0.7644
2023-06-02 18:59:00,020 BAD EPOCHS (no improvement): 4
2023-06-02 18:59:12,022 ----------------------------------------------------------------------------------------------------
2023-06-02 18:59:12,025 Testing using last state of model ...
2023-06-02 19:03:29,489 Evaluating as a multi-label problem: False
2023-06-02 19:03:29,597 0.77	0.6961	0.7312	0.6127
2023-06-02 19:03:29,597 
Results:
- F-score (micro) 0.7312
- F-score (macro) 0.7132
- Accuracy 0.6127

By class:
              precision    recall  f1-score   support

         PER     0.8776    0.8950    0.8862      2715
         LOC     0.7693    0.7523    0.7607      2442
         ORG     0.6545    0.5855    0.6181      2543
        MISC     0.7429    0.4865    0.5880      1889

   micro avg     0.7700    0.6961    0.7312      9589
   macro avg     0.7611    0.6798    0.7132      9589
weighted avg     0.7643    0.6961    0.7244      9589

2023-06-02 19:03:29,597 ----------------------------------------------------------------------------------------------------
2023-06-02 19:03:29,597 ----------------------------------------------------------------------------------------------------
2023-06-02 19:06:08,752 Evaluating as a multi-label problem: False
2023-06-02 19:06:08,800 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-02 19:06:08,801 0.7276	0.6052	0.6608	0.5322
2023-06-02 19:06:08,801 ----------------------------------------------------------------------------------------------------
2023-06-02 19:07:47,368 Evaluating as a multi-label problem: False
2023-06-02 19:07:47,438 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-02 19:07:47,438 0.796	0.7597	0.7774	0.6691
