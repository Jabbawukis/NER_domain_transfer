2023-06-02 19:07:47,465 ----------------------------------------------------------------------------------------------------
2023-06-02 19:07:47,469 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-02 19:07:47,471 ----------------------------------------------------------------------------------------------------
2023-06-02 19:07:47,471 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-02 19:07:47,471 ----------------------------------------------------------------------------------------------------
2023-06-02 19:07:47,471 Parameters:
2023-06-02 19:07:47,471  - learning_rate: "0.300000"
2023-06-02 19:07:47,471  - mini_batch_size: "32"
2023-06-02 19:07:47,471  - patience: "3"
2023-06-02 19:07:47,471  - anneal_factor: "0.5"
2023-06-02 19:07:47,471  - max_epochs: "10"
2023-06-02 19:07:47,472  - shuffle: "True"
2023-06-02 19:07:47,472  - train_with_dev: "False"
2023-06-02 19:07:47,472  - batch_growth_annealing: "False"
2023-06-02 19:07:47,472 ----------------------------------------------------------------------------------------------------
2023-06-02 19:07:47,472 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_3_linear_probing"
2023-06-02 19:07:47,472 ----------------------------------------------------------------------------------------------------
2023-06-02 19:07:47,472 Device: cuda:1
2023-06-02 19:07:47,472 ----------------------------------------------------------------------------------------------------
2023-06-02 19:07:47,472 Embeddings storage mode: none
2023-06-02 19:07:47,472 ----------------------------------------------------------------------------------------------------
2023-06-02 19:08:50,204 epoch 1 - iter 97/972 - loss 0.80525982 - samples/sec: 49.49 - lr: 0.029938
2023-06-02 19:09:58,574 epoch 1 - iter 194/972 - loss 0.69482616 - samples/sec: 45.41 - lr: 0.059877
2023-06-02 19:10:59,295 epoch 1 - iter 291/972 - loss 0.88938324 - samples/sec: 51.13 - lr: 0.089815
2023-06-02 19:12:07,592 epoch 1 - iter 388/972 - loss 1.05428208 - samples/sec: 45.46 - lr: 0.119753
2023-06-02 19:13:08,618 epoch 1 - iter 485/972 - loss 1.35433217 - samples/sec: 50.88 - lr: 0.149691
2023-06-02 19:14:08,543 epoch 1 - iter 582/972 - loss 1.91832465 - samples/sec: 51.81 - lr: 0.179630
2023-06-02 19:15:07,057 epoch 1 - iter 679/972 - loss 2.25377044 - samples/sec: 53.06 - lr: 0.209568
2023-06-02 19:16:07,060 epoch 1 - iter 776/972 - loss 2.63530718 - samples/sec: 51.75 - lr: 0.239506
2023-06-02 19:17:12,706 epoch 1 - iter 873/972 - loss 2.94940028 - samples/sec: 47.30 - lr: 0.269444
2023-06-02 19:18:13,643 epoch 1 - iter 970/972 - loss 3.11493655 - samples/sec: 50.95 - lr: 0.299383
2023-06-02 19:18:14,227 ----------------------------------------------------------------------------------------------------
2023-06-02 19:18:14,227 EPOCH 1 done: loss 3.1165 - lr 0.299383
2023-06-02 19:21:02,691 Evaluating as a multi-label problem: False
2023-06-02 19:21:02,793 DEV : loss 1.8153290748596191 - f1-score (micro avg)  0.6539
2023-06-02 19:21:02,991 BAD EPOCHS (no improvement): 4
2023-06-02 19:21:02,994 ----------------------------------------------------------------------------------------------------
2023-06-02 19:22:09,882 epoch 2 - iter 97/972 - loss 5.20900359 - samples/sec: 46.42 - lr: 0.296674
2023-06-02 19:23:20,113 epoch 2 - iter 194/972 - loss 5.35770081 - samples/sec: 44.21 - lr: 0.293349
2023-06-02 19:24:22,136 epoch 2 - iter 291/972 - loss 5.50271371 - samples/sec: 50.06 - lr: 0.290023
2023-06-02 19:25:32,070 epoch 2 - iter 388/972 - loss 5.70877048 - samples/sec: 44.40 - lr: 0.286697
2023-06-02 19:26:37,042 epoch 2 - iter 485/972 - loss 5.67798805 - samples/sec: 47.79 - lr: 0.283371
2023-06-02 19:27:40,945 epoch 2 - iter 582/972 - loss 5.67701406 - samples/sec: 48.59 - lr: 0.280046
2023-06-02 19:28:52,746 epoch 2 - iter 679/972 - loss 5.60018526 - samples/sec: 43.24 - lr: 0.276720
2023-06-02 19:29:56,336 epoch 2 - iter 776/972 - loss 5.56058519 - samples/sec: 48.83 - lr: 0.273394
2023-06-02 19:31:06,864 epoch 2 - iter 873/972 - loss 5.53479548 - samples/sec: 44.02 - lr: 0.270069
2023-06-02 19:32:12,524 epoch 2 - iter 970/972 - loss 5.48494610 - samples/sec: 47.29 - lr: 0.266743
2023-06-02 19:32:13,369 ----------------------------------------------------------------------------------------------------
2023-06-02 19:32:13,369 EPOCH 2 done: loss 5.4850 - lr 0.266743
2023-06-02 19:35:17,877 Evaluating as a multi-label problem: False
2023-06-02 19:35:17,989 DEV : loss 1.6881036758422852 - f1-score (micro avg)  0.6327
2023-06-02 19:35:18,206 BAD EPOCHS (no improvement): 4
2023-06-02 19:35:18,208 ----------------------------------------------------------------------------------------------------
2023-06-02 19:36:14,105 epoch 3 - iter 97/972 - loss 5.05746373 - samples/sec: 55.56 - lr: 0.263349
2023-06-02 19:37:08,794 epoch 3 - iter 194/972 - loss 4.91560448 - samples/sec: 56.78 - lr: 0.260023
2023-06-02 19:37:59,425 epoch 3 - iter 291/972 - loss 4.92558888 - samples/sec: 61.33 - lr: 0.256697
2023-06-02 19:38:52,373 epoch 3 - iter 388/972 - loss 4.95094420 - samples/sec: 58.65 - lr: 0.253371
2023-06-02 19:39:36,096 epoch 3 - iter 485/972 - loss 4.96679241 - samples/sec: 71.02 - lr: 0.250046
2023-06-02 19:40:26,152 epoch 3 - iter 582/972 - loss 5.03250237 - samples/sec: 62.04 - lr: 0.246720
2023-06-02 19:41:21,966 epoch 3 - iter 679/972 - loss 4.99356493 - samples/sec: 55.63 - lr: 0.243394
2023-06-02 19:42:05,034 epoch 3 - iter 776/972 - loss 4.98599413 - samples/sec: 72.10 - lr: 0.240069
2023-06-02 19:42:56,485 epoch 3 - iter 873/972 - loss 5.01887317 - samples/sec: 60.35 - lr: 0.236743
2023-06-02 19:43:46,301 epoch 3 - iter 970/972 - loss 5.00287437 - samples/sec: 62.33 - lr: 0.233417
2023-06-02 19:43:46,758 ----------------------------------------------------------------------------------------------------
2023-06-02 19:43:46,758 EPOCH 3 done: loss 5.0053 - lr 0.233417
2023-06-02 19:45:43,262 Evaluating as a multi-label problem: False
2023-06-02 19:45:43,331 DEV : loss 2.6071202754974365 - f1-score (micro avg)  0.5325
2023-06-02 19:45:43,460 BAD EPOCHS (no improvement): 4
2023-06-02 19:45:43,462 ----------------------------------------------------------------------------------------------------
2023-06-02 19:46:28,030 epoch 4 - iter 97/972 - loss 4.63818041 - samples/sec: 69.68 - lr: 0.230023
2023-06-02 19:47:17,849 epoch 4 - iter 194/972 - loss 4.73341958 - samples/sec: 62.32 - lr: 0.226697
2023-06-02 19:48:04,930 epoch 4 - iter 291/972 - loss 4.67149085 - samples/sec: 65.95 - lr: 0.223371
2023-06-02 19:48:55,198 epoch 4 - iter 388/972 - loss 4.50933571 - samples/sec: 61.77 - lr: 0.220046
2023-06-02 19:49:44,864 epoch 4 - iter 485/972 - loss 4.48511969 - samples/sec: 62.52 - lr: 0.216720
2023-06-02 19:50:31,611 epoch 4 - iter 582/972 - loss 4.44440230 - samples/sec: 66.42 - lr: 0.213394
2023-06-02 19:51:22,705 epoch 4 - iter 679/972 - loss 4.39375497 - samples/sec: 60.77 - lr: 0.210069
2023-06-02 19:52:12,233 epoch 4 - iter 776/972 - loss 4.36133125 - samples/sec: 62.70 - lr: 0.206743
2023-06-02 19:53:07,308 epoch 4 - iter 873/972 - loss 4.31978969 - samples/sec: 56.38 - lr: 0.203417
2023-06-02 19:53:52,189 epoch 4 - iter 970/972 - loss 4.26865204 - samples/sec: 69.19 - lr: 0.200091
2023-06-02 19:53:52,881 ----------------------------------------------------------------------------------------------------
2023-06-02 19:53:52,881 EPOCH 4 done: loss 4.2694 - lr 0.200091
2023-06-02 19:56:27,058 Evaluating as a multi-label problem: False
2023-06-02 19:56:27,154 DEV : loss 1.5822453498840332 - f1-score (micro avg)  0.681
2023-06-02 19:56:27,346 BAD EPOCHS (no improvement): 4
2023-06-02 19:56:27,349 ----------------------------------------------------------------------------------------------------
2023-06-02 19:57:19,794 epoch 5 - iter 97/972 - loss 3.84435713 - samples/sec: 59.21 - lr: 0.196697
2023-06-02 19:58:05,246 epoch 5 - iter 194/972 - loss 3.84329142 - samples/sec: 68.31 - lr: 0.193371
2023-06-02 19:58:55,324 epoch 5 - iter 291/972 - loss 3.83345951 - samples/sec: 62.01 - lr: 0.190046
2023-06-02 19:59:43,233 epoch 5 - iter 388/972 - loss 3.82149347 - samples/sec: 64.81 - lr: 0.186720
2023-06-02 20:00:23,430 epoch 5 - iter 485/972 - loss 3.83948928 - samples/sec: 77.24 - lr: 0.183394
2023-06-02 20:01:09,222 epoch 5 - iter 582/972 - loss 3.84139661 - samples/sec: 67.80 - lr: 0.180069
2023-06-02 20:01:54,622 epoch 5 - iter 679/972 - loss 3.81850049 - samples/sec: 68.39 - lr: 0.176743
2023-06-02 20:02:43,972 epoch 5 - iter 776/972 - loss 3.83225466 - samples/sec: 62.92 - lr: 0.173417
2023-06-02 20:03:36,977 epoch 5 - iter 873/972 - loss 3.82279410 - samples/sec: 58.58 - lr: 0.170091
2023-06-02 20:04:26,718 epoch 5 - iter 970/972 - loss 3.79447635 - samples/sec: 62.43 - lr: 0.166766
2023-06-02 20:04:27,345 ----------------------------------------------------------------------------------------------------
2023-06-02 20:04:27,345 EPOCH 5 done: loss 3.7944 - lr 0.166766
2023-06-02 20:07:01,431 Evaluating as a multi-label problem: False
2023-06-02 20:07:01,538 DEV : loss 1.3586480617523193 - f1-score (micro avg)  0.5862
2023-06-02 20:07:01,723 BAD EPOCHS (no improvement): 4
2023-06-02 20:07:01,726 ----------------------------------------------------------------------------------------------------
2023-06-02 20:07:53,997 epoch 6 - iter 97/972 - loss 3.37053813 - samples/sec: 59.41 - lr: 0.163371
2023-06-02 20:08:44,396 epoch 6 - iter 194/972 - loss 3.34011349 - samples/sec: 61.61 - lr: 0.160046
2023-06-02 20:09:30,522 epoch 6 - iter 291/972 - loss 3.33828071 - samples/sec: 67.32 - lr: 0.156720
2023-06-02 20:10:21,208 epoch 6 - iter 388/972 - loss 3.34962121 - samples/sec: 61.26 - lr: 0.153394
2023-06-02 20:11:10,783 epoch 6 - iter 485/972 - loss 3.32221186 - samples/sec: 62.64 - lr: 0.150069
2023-06-02 20:12:03,928 epoch 6 - iter 582/972 - loss 3.31368288 - samples/sec: 58.43 - lr: 0.146743
2023-06-02 20:12:53,769 epoch 6 - iter 679/972 - loss 3.27980688 - samples/sec: 62.30 - lr: 0.143417
2023-06-02 20:13:44,257 epoch 6 - iter 776/972 - loss 3.27667456 - samples/sec: 61.50 - lr: 0.140091
2023-06-02 20:14:41,294 epoch 6 - iter 873/972 - loss 3.26519785 - samples/sec: 54.44 - lr: 0.136766
2023-06-02 20:15:24,656 epoch 6 - iter 970/972 - loss 3.25077100 - samples/sec: 71.61 - lr: 0.133440
2023-06-02 20:15:25,231 ----------------------------------------------------------------------------------------------------
2023-06-02 20:15:25,231 EPOCH 6 done: loss 3.2500 - lr 0.133440
2023-06-02 20:17:49,426 Evaluating as a multi-label problem: False
2023-06-02 20:17:49,524 DEV : loss 1.0971136093139648 - f1-score (micro avg)  0.6584
2023-06-02 20:17:49,712 BAD EPOCHS (no improvement): 4
2023-06-02 20:17:49,714 ----------------------------------------------------------------------------------------------------
2023-06-02 20:18:45,294 epoch 7 - iter 97/972 - loss 2.80126408 - samples/sec: 55.87 - lr: 0.130046
2023-06-02 20:19:33,933 epoch 7 - iter 194/972 - loss 2.86397182 - samples/sec: 63.84 - lr: 0.126720
2023-06-02 20:20:23,235 epoch 7 - iter 291/972 - loss 2.81455713 - samples/sec: 62.98 - lr: 0.123394
2023-06-02 20:21:16,365 epoch 7 - iter 388/972 - loss 2.82977857 - samples/sec: 58.44 - lr: 0.120069
2023-06-02 20:22:03,364 epoch 7 - iter 485/972 - loss 2.80531002 - samples/sec: 66.07 - lr: 0.116743
2023-06-02 20:22:56,916 epoch 7 - iter 582/972 - loss 2.77408611 - samples/sec: 57.98 - lr: 0.113417
2023-06-02 20:23:48,359 epoch 7 - iter 679/972 - loss 2.74196307 - samples/sec: 60.36 - lr: 0.110091
2023-06-02 20:24:33,395 epoch 7 - iter 776/972 - loss 2.70201472 - samples/sec: 68.95 - lr: 0.106766
2023-06-02 20:25:22,466 epoch 7 - iter 873/972 - loss 2.67051955 - samples/sec: 63.28 - lr: 0.103440
2023-06-02 20:26:07,952 epoch 7 - iter 970/972 - loss 2.64040492 - samples/sec: 68.27 - lr: 0.100114
2023-06-02 20:26:08,458 ----------------------------------------------------------------------------------------------------
2023-06-02 20:26:08,459 EPOCH 7 done: loss 2.6398 - lr 0.100114
2023-06-02 20:28:30,546 Evaluating as a multi-label problem: False
2023-06-02 20:28:30,649 DEV : loss 0.8292779922485352 - f1-score (micro avg)  0.6809
2023-06-02 20:28:30,847 BAD EPOCHS (no improvement): 4
2023-06-02 20:28:30,850 ----------------------------------------------------------------------------------------------------
2023-06-02 20:29:26,755 epoch 8 - iter 97/972 - loss 2.30313055 - samples/sec: 55.55 - lr: 0.096720
2023-06-02 20:30:15,220 epoch 8 - iter 194/972 - loss 2.29813243 - samples/sec: 64.07 - lr: 0.093394
2023-06-02 20:30:57,446 epoch 8 - iter 291/972 - loss 2.24220096 - samples/sec: 73.54 - lr: 0.090069
2023-06-02 20:31:42,823 epoch 8 - iter 388/972 - loss 2.19840868 - samples/sec: 68.43 - lr: 0.086743
2023-06-02 20:32:26,833 epoch 8 - iter 485/972 - loss 2.20416200 - samples/sec: 70.56 - lr: 0.083417
2023-06-02 20:33:23,235 epoch 8 - iter 582/972 - loss 2.16983837 - samples/sec: 55.05 - lr: 0.080091
2023-06-02 20:34:13,937 epoch 8 - iter 679/972 - loss 2.14218309 - samples/sec: 61.25 - lr: 0.076766
2023-06-02 20:34:59,528 epoch 8 - iter 776/972 - loss 2.10579467 - samples/sec: 68.11 - lr: 0.073440
2023-06-02 20:35:54,384 epoch 8 - iter 873/972 - loss 2.07812638 - samples/sec: 56.61 - lr: 0.070114
2023-06-02 20:36:44,948 epoch 8 - iter 970/972 - loss 2.04811474 - samples/sec: 61.41 - lr: 0.066789
2023-06-02 20:36:45,700 ----------------------------------------------------------------------------------------------------
2023-06-02 20:36:45,700 EPOCH 8 done: loss 2.0467 - lr 0.066789
2023-06-02 20:38:59,101 Evaluating as a multi-label problem: False
2023-06-02 20:38:59,161 DEV : loss 0.6230725049972534 - f1-score (micro avg)  0.6973
2023-06-02 20:38:59,301 BAD EPOCHS (no improvement): 4
2023-06-02 20:38:59,303 ----------------------------------------------------------------------------------------------------
2023-06-02 20:39:51,996 epoch 9 - iter 97/972 - loss 1.78040151 - samples/sec: 58.93 - lr: 0.063394
2023-06-02 20:40:40,814 epoch 9 - iter 194/972 - loss 1.72048871 - samples/sec: 63.61 - lr: 0.060069
2023-06-02 20:41:22,662 epoch 9 - iter 291/972 - loss 1.65190282 - samples/sec: 74.20 - lr: 0.056743
2023-06-02 20:42:11,276 epoch 9 - iter 388/972 - loss 1.61710182 - samples/sec: 63.87 - lr: 0.053417
2023-06-02 20:43:01,883 epoch 9 - iter 485/972 - loss 1.57989472 - samples/sec: 61.36 - lr: 0.050091
2023-06-02 20:43:51,596 epoch 9 - iter 582/972 - loss 1.54530459 - samples/sec: 62.46 - lr: 0.046766
2023-06-02 20:44:39,187 epoch 9 - iter 679/972 - loss 1.51426167 - samples/sec: 65.25 - lr: 0.043440
2023-06-02 20:45:29,395 epoch 9 - iter 776/972 - loss 1.47874877 - samples/sec: 61.85 - lr: 0.040114
2023-06-02 20:46:22,015 epoch 9 - iter 873/972 - loss 1.44937692 - samples/sec: 59.01 - lr: 0.036789
2023-06-02 20:47:12,898 epoch 9 - iter 970/972 - loss 1.41708947 - samples/sec: 61.03 - lr: 0.033463
2023-06-02 20:47:13,518 ----------------------------------------------------------------------------------------------------
2023-06-02 20:47:13,518 EPOCH 9 done: loss 1.4165 - lr 0.033463
2023-06-02 20:49:26,761 Evaluating as a multi-label problem: False
2023-06-02 20:49:26,865 DEV : loss 0.38709935545921326 - f1-score (micro avg)  0.7062
2023-06-02 20:49:27,081 BAD EPOCHS (no improvement): 4
2023-06-02 20:49:27,084 ----------------------------------------------------------------------------------------------------
2023-06-02 20:50:16,514 epoch 10 - iter 97/972 - loss 1.07274120 - samples/sec: 62.82 - lr: 0.030069
2023-06-02 20:51:04,189 epoch 10 - iter 194/972 - loss 1.04274254 - samples/sec: 65.13 - lr: 0.026743
2023-06-02 20:51:53,324 epoch 10 - iter 291/972 - loss 1.01631832 - samples/sec: 63.19 - lr: 0.023417
2023-06-02 20:52:44,488 epoch 10 - iter 388/972 - loss 0.97339119 - samples/sec: 60.69 - lr: 0.020091
2023-06-02 20:53:34,817 epoch 10 - iter 485/972 - loss 0.94535979 - samples/sec: 61.70 - lr: 0.016766
2023-06-02 20:54:30,841 epoch 10 - iter 582/972 - loss 0.91479592 - samples/sec: 55.43 - lr: 0.013440
2023-06-02 20:55:15,016 epoch 10 - iter 679/972 - loss 0.88846923 - samples/sec: 70.29 - lr: 0.010114
2023-06-02 20:56:05,033 epoch 10 - iter 776/972 - loss 0.86122544 - samples/sec: 62.08 - lr: 0.006789
2023-06-02 20:56:55,393 epoch 10 - iter 873/972 - loss 0.83940640 - samples/sec: 61.66 - lr: 0.003463
2023-06-02 20:57:42,828 epoch 10 - iter 970/972 - loss 0.81749821 - samples/sec: 65.46 - lr: 0.000137
2023-06-02 20:57:43,456 ----------------------------------------------------------------------------------------------------
2023-06-02 20:57:43,456 EPOCH 10 done: loss 0.8170 - lr 0.000137
2023-06-02 20:59:58,780 Evaluating as a multi-label problem: False
2023-06-02 20:59:58,873 DEV : loss 0.23276962339878082 - f1-score (micro avg)  0.7585
2023-06-02 20:59:59,061 BAD EPOCHS (no improvement): 4
2023-06-02 21:00:12,242 ----------------------------------------------------------------------------------------------------
2023-06-02 21:00:12,245 Testing using last state of model ...
2023-06-02 21:03:40,446 Evaluating as a multi-label problem: False
2023-06-02 21:03:40,536 0.7647	0.6952	0.7283	0.61
2023-06-02 21:03:40,536 
Results:
- F-score (micro) 0.7283
- F-score (macro) 0.7106
- Accuracy 0.61

By class:
              precision    recall  f1-score   support

         PER     0.8722    0.8895    0.8807      2715
         LOC     0.7754    0.7678    0.7716      2442
         ORG     0.6549    0.5611    0.6044      2543
        MISC     0.7024    0.5024    0.5858      1889

   micro avg     0.7647    0.6952    0.7283      9589
   macro avg     0.7512    0.6802    0.7106      9589
weighted avg     0.7565    0.6952    0.7216      9589

2023-06-02 21:03:40,536 ----------------------------------------------------------------------------------------------------
2023-06-02 21:03:40,536 ----------------------------------------------------------------------------------------------------
2023-06-02 21:05:39,452 Evaluating as a multi-label problem: False
2023-06-02 21:05:39,500 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-02 21:05:39,500 0.7294	0.6039	0.6607	0.5315
2023-06-02 21:05:39,500 ----------------------------------------------------------------------------------------------------
2023-06-02 21:07:01,762 Evaluating as a multi-label problem: False
2023-06-02 21:07:01,831 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-02 21:07:01,832 0.7859	0.759	0.7722	0.6648
