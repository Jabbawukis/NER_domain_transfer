2023-06-03 09:26:19,277 ----------------------------------------------------------------------------------------------------
2023-06-03 09:26:19,283 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-03 09:26:19,286 ----------------------------------------------------------------------------------------------------
2023-06-03 09:26:19,286 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-03 09:26:19,287 ----------------------------------------------------------------------------------------------------
2023-06-03 09:26:19,287 Parameters:
2023-06-03 09:26:19,287  - learning_rate: "0.000005"
2023-06-03 09:26:19,287  - mini_batch_size: "4"
2023-06-03 09:26:19,287  - patience: "3"
2023-06-03 09:26:19,287  - anneal_factor: "0.5"
2023-06-03 09:26:19,287  - max_epochs: "10"
2023-06-03 09:26:19,287  - shuffle: "True"
2023-06-03 09:26:19,287  - train_with_dev: "False"
2023-06-03 09:26:19,287  - batch_growth_annealing: "False"
2023-06-03 09:26:19,287 ----------------------------------------------------------------------------------------------------
2023-06-03 09:26:19,287 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_3_full_fine_tuning"
2023-06-03 09:26:19,287 ----------------------------------------------------------------------------------------------------
2023-06-03 09:26:19,287 Device: cuda:3
2023-06-03 09:26:19,288 ----------------------------------------------------------------------------------------------------
2023-06-03 09:26:19,288 Embeddings storage mode: none
2023-06-03 09:26:19,288 ----------------------------------------------------------------------------------------------------
2023-06-03 09:29:23,586 epoch 1 - iter 777/7770 - loss 0.47010598 - samples/sec: 16.87 - lr: 0.000001
2023-06-03 09:32:28,200 epoch 1 - iter 1554/7770 - loss 0.37380801 - samples/sec: 16.84 - lr: 0.000001
2023-06-03 09:35:39,048 epoch 1 - iter 2331/7770 - loss 0.33327837 - samples/sec: 16.29 - lr: 0.000002
2023-06-03 09:38:47,048 epoch 1 - iter 3108/7770 - loss 0.29637977 - samples/sec: 16.54 - lr: 0.000002
2023-06-03 09:41:55,691 epoch 1 - iter 3885/7770 - loss 0.27296552 - samples/sec: 16.48 - lr: 0.000003
2023-06-03 09:45:04,718 epoch 1 - iter 4662/7770 - loss 0.26909319 - samples/sec: 16.45 - lr: 0.000003
2023-06-03 09:48:01,485 epoch 1 - iter 5439/7770 - loss 0.26758443 - samples/sec: 17.59 - lr: 0.000003
2023-06-03 09:50:55,473 epoch 1 - iter 6216/7770 - loss 0.25725351 - samples/sec: 17.87 - lr: 0.000004
2023-06-03 09:53:50,848 epoch 1 - iter 6993/7770 - loss 0.25243036 - samples/sec: 17.73 - lr: 0.000005
2023-06-03 09:56:50,655 epoch 1 - iter 7770/7770 - loss 0.24618043 - samples/sec: 17.29 - lr: 0.000005
2023-06-03 09:56:50,657 ----------------------------------------------------------------------------------------------------
2023-06-03 09:56:50,657 EPOCH 1 done: loss 0.2462 - lr 0.000005
2023-06-03 09:59:29,314 Evaluating as a multi-label problem: False
2023-06-03 09:59:29,413 DEV : loss 0.13152246177196503 - f1-score (micro avg)  0.9136
2023-06-03 09:59:29,676 BAD EPOCHS (no improvement): 4
2023-06-03 09:59:29,679 ----------------------------------------------------------------------------------------------------
2023-06-03 10:02:27,509 epoch 2 - iter 777/7770 - loss 0.17521808 - samples/sec: 17.49 - lr: 0.000005
2023-06-03 10:05:28,802 epoch 2 - iter 1554/7770 - loss 0.17414907 - samples/sec: 17.15 - lr: 0.000005
2023-06-03 10:08:34,270 epoch 2 - iter 2331/7770 - loss 0.17442324 - samples/sec: 16.77 - lr: 0.000005
2023-06-03 10:11:40,149 epoch 2 - iter 3108/7770 - loss 0.17052065 - samples/sec: 16.73 - lr: 0.000005
2023-06-03 10:14:38,459 epoch 2 - iter 3885/7770 - loss 0.16833908 - samples/sec: 17.44 - lr: 0.000005
2023-06-03 10:17:39,369 epoch 2 - iter 4662/7770 - loss 0.16583819 - samples/sec: 17.19 - lr: 0.000005
2023-06-03 10:20:45,619 epoch 2 - iter 5439/7770 - loss 0.16617933 - samples/sec: 16.69 - lr: 0.000005
2023-06-03 10:23:46,742 epoch 2 - iter 6216/7770 - loss 0.16359007 - samples/sec: 17.17 - lr: 0.000005
2023-06-03 10:26:37,880 epoch 2 - iter 6993/7770 - loss 0.15840998 - samples/sec: 18.17 - lr: 0.000005
2023-06-03 10:29:37,488 epoch 2 - iter 7770/7770 - loss 0.15569778 - samples/sec: 17.31 - lr: 0.000004
2023-06-03 10:29:37,493 ----------------------------------------------------------------------------------------------------
2023-06-03 10:29:37,493 EPOCH 2 done: loss 0.1557 - lr 0.000004
2023-06-03 10:32:20,518 Evaluating as a multi-label problem: False
2023-06-03 10:32:20,612 DEV : loss 0.0907268151640892 - f1-score (micro avg)  0.947
2023-06-03 10:32:20,810 BAD EPOCHS (no improvement): 4
2023-06-03 10:32:20,818 ----------------------------------------------------------------------------------------------------
2023-06-03 10:35:14,180 epoch 3 - iter 777/7770 - loss 0.12800425 - samples/sec: 17.94 - lr: 0.000004
2023-06-03 10:38:14,735 epoch 3 - iter 1554/7770 - loss 0.12330957 - samples/sec: 17.22 - lr: 0.000004
2023-06-03 10:41:09,968 epoch 3 - iter 2331/7770 - loss 0.12408094 - samples/sec: 17.74 - lr: 0.000004
2023-06-03 10:44:10,864 epoch 3 - iter 3108/7770 - loss 0.11894503 - samples/sec: 17.19 - lr: 0.000004
2023-06-03 10:47:14,517 epoch 3 - iter 3885/7770 - loss 0.11496573 - samples/sec: 16.93 - lr: 0.000004
2023-06-03 10:50:20,475 epoch 3 - iter 4662/7770 - loss 0.11335150 - samples/sec: 16.72 - lr: 0.000004
2023-06-03 10:53:23,768 epoch 3 - iter 5439/7770 - loss 0.11361597 - samples/sec: 16.96 - lr: 0.000004
2023-06-03 10:56:31,558 epoch 3 - iter 6216/7770 - loss 0.11275313 - samples/sec: 16.56 - lr: 0.000004
2023-06-03 10:59:22,317 epoch 3 - iter 6993/7770 - loss 0.11258770 - samples/sec: 18.21 - lr: 0.000004
2023-06-03 11:02:20,261 epoch 3 - iter 7770/7770 - loss 0.11156062 - samples/sec: 17.47 - lr: 0.000004
2023-06-03 11:02:20,264 ----------------------------------------------------------------------------------------------------
2023-06-03 11:02:20,264 EPOCH 3 done: loss 0.1116 - lr 0.000004
2023-06-03 11:05:09,459 Evaluating as a multi-label problem: False
2023-06-03 11:05:09,567 DEV : loss 0.06718409806489944 - f1-score (micro avg)  0.9591
2023-06-03 11:05:09,841 BAD EPOCHS (no improvement): 4
2023-06-03 11:05:09,844 ----------------------------------------------------------------------------------------------------
2023-06-03 11:08:10,248 epoch 4 - iter 777/7770 - loss 0.10364134 - samples/sec: 17.24 - lr: 0.000004
2023-06-03 11:11:11,794 epoch 4 - iter 1554/7770 - loss 0.09844128 - samples/sec: 17.13 - lr: 0.000004
2023-06-03 11:14:18,300 epoch 4 - iter 2331/7770 - loss 0.09908749 - samples/sec: 16.67 - lr: 0.000004
2023-06-03 11:17:21,070 epoch 4 - iter 3108/7770 - loss 0.09537051 - samples/sec: 17.01 - lr: 0.000004
2023-06-03 11:20:18,991 epoch 4 - iter 3885/7770 - loss 0.09397989 - samples/sec: 17.48 - lr: 0.000004
2023-06-03 11:23:18,816 epoch 4 - iter 4662/7770 - loss 0.09479258 - samples/sec: 17.29 - lr: 0.000004
2023-06-03 11:26:19,159 epoch 4 - iter 5439/7770 - loss 0.09352110 - samples/sec: 17.24 - lr: 0.000004
2023-06-03 11:29:10,986 epoch 4 - iter 6216/7770 - loss 0.09285149 - samples/sec: 18.10 - lr: 0.000003
2023-06-03 11:32:13,470 epoch 4 - iter 6993/7770 - loss 0.09359881 - samples/sec: 17.04 - lr: 0.000003
2023-06-03 11:35:08,426 epoch 4 - iter 7770/7770 - loss 0.09323457 - samples/sec: 17.77 - lr: 0.000003
2023-06-03 11:35:08,429 ----------------------------------------------------------------------------------------------------
2023-06-03 11:35:08,429 EPOCH 4 done: loss 0.0932 - lr 0.000003
2023-06-03 11:37:19,479 Evaluating as a multi-label problem: False
2023-06-03 11:37:19,545 DEV : loss 0.07129940390586853 - f1-score (micro avg)  0.9626
2023-06-03 11:37:19,722 BAD EPOCHS (no improvement): 4
2023-06-03 11:37:19,724 ----------------------------------------------------------------------------------------------------
2023-06-03 11:40:16,133 epoch 5 - iter 777/7770 - loss 0.06910981 - samples/sec: 17.63 - lr: 0.000003
2023-06-03 11:43:19,579 epoch 5 - iter 1554/7770 - loss 0.07222813 - samples/sec: 16.95 - lr: 0.000003
2023-06-03 11:46:23,088 epoch 5 - iter 2331/7770 - loss 0.07620779 - samples/sec: 16.94 - lr: 0.000003
2023-06-03 11:49:22,694 epoch 5 - iter 3108/7770 - loss 0.07574327 - samples/sec: 17.31 - lr: 0.000003
2023-06-03 11:52:24,740 epoch 5 - iter 3885/7770 - loss 0.07455044 - samples/sec: 17.08 - lr: 0.000003
2023-06-03 11:55:25,562 epoch 5 - iter 4662/7770 - loss 0.07590206 - samples/sec: 17.20 - lr: 0.000003
2023-06-03 11:58:37,651 epoch 5 - iter 5439/7770 - loss 0.07613521 - samples/sec: 16.19 - lr: 0.000003
2023-06-03 12:01:40,314 epoch 5 - iter 6216/7770 - loss 0.07516789 - samples/sec: 17.02 - lr: 0.000003
2023-06-03 12:04:37,567 epoch 5 - iter 6993/7770 - loss 0.07566997 - samples/sec: 17.54 - lr: 0.000003
2023-06-03 12:07:44,396 epoch 5 - iter 7770/7770 - loss 0.07499575 - samples/sec: 16.64 - lr: 0.000003
2023-06-03 12:07:44,401 ----------------------------------------------------------------------------------------------------
2023-06-03 12:07:44,401 EPOCH 5 done: loss 0.0750 - lr 0.000003
2023-06-03 12:10:26,106 Evaluating as a multi-label problem: False
2023-06-03 12:10:26,198 DEV : loss 0.08115171641111374 - f1-score (micro avg)  0.9614
2023-06-03 12:10:26,399 BAD EPOCHS (no improvement): 4
2023-06-03 12:10:26,402 ----------------------------------------------------------------------------------------------------
2023-06-03 12:13:31,861 epoch 6 - iter 777/7770 - loss 0.06550765 - samples/sec: 16.77 - lr: 0.000003
2023-06-03 12:16:31,704 epoch 6 - iter 1554/7770 - loss 0.06920450 - samples/sec: 17.29 - lr: 0.000003
2023-06-03 12:19:32,449 epoch 6 - iter 2331/7770 - loss 0.07158530 - samples/sec: 17.20 - lr: 0.000003
2023-06-03 12:22:25,877 epoch 6 - iter 3108/7770 - loss 0.06936929 - samples/sec: 17.93 - lr: 0.000003
2023-06-03 12:25:20,425 epoch 6 - iter 3885/7770 - loss 0.06741164 - samples/sec: 17.81 - lr: 0.000003
2023-06-03 12:28:22,475 epoch 6 - iter 4662/7770 - loss 0.06606937 - samples/sec: 17.08 - lr: 0.000002
2023-06-03 12:31:19,293 epoch 6 - iter 5439/7770 - loss 0.06801505 - samples/sec: 17.59 - lr: 0.000002
2023-06-03 12:34:18,556 epoch 6 - iter 6216/7770 - loss 0.06974991 - samples/sec: 17.35 - lr: 0.000002
2023-06-03 12:37:12,866 epoch 6 - iter 6993/7770 - loss 0.06991101 - samples/sec: 17.84 - lr: 0.000002
2023-06-03 12:40:10,282 epoch 6 - iter 7770/7770 - loss 0.06935823 - samples/sec: 17.53 - lr: 0.000002
2023-06-03 12:40:10,287 ----------------------------------------------------------------------------------------------------
2023-06-03 12:40:10,287 EPOCH 6 done: loss 0.0694 - lr 0.000002
2023-06-03 12:42:40,040 Evaluating as a multi-label problem: False
2023-06-03 12:42:40,140 DEV : loss 0.0739581286907196 - f1-score (micro avg)  0.9639
2023-06-03 12:42:40,389 BAD EPOCHS (no improvement): 4
2023-06-03 12:42:40,392 ----------------------------------------------------------------------------------------------------
2023-06-03 12:45:40,249 epoch 7 - iter 777/7770 - loss 0.06606569 - samples/sec: 17.29 - lr: 0.000002
2023-06-03 12:48:35,927 epoch 7 - iter 1554/7770 - loss 0.06297119 - samples/sec: 17.70 - lr: 0.000002
2023-06-03 12:51:34,549 epoch 7 - iter 2331/7770 - loss 0.06288088 - samples/sec: 17.41 - lr: 0.000002
2023-06-03 12:54:38,112 epoch 7 - iter 3108/7770 - loss 0.06385232 - samples/sec: 16.94 - lr: 0.000002
2023-06-03 12:57:34,347 epoch 7 - iter 3885/7770 - loss 0.06233436 - samples/sec: 17.64 - lr: 0.000002
2023-06-03 13:00:33,138 epoch 7 - iter 4662/7770 - loss 0.06154667 - samples/sec: 17.39 - lr: 0.000002
2023-06-03 13:03:33,521 epoch 7 - iter 5439/7770 - loss 0.06091893 - samples/sec: 17.24 - lr: 0.000002
2023-06-03 13:06:25,624 epoch 7 - iter 6216/7770 - loss 0.06126320 - samples/sec: 18.07 - lr: 0.000002
2023-06-03 13:09:25,152 epoch 7 - iter 6993/7770 - loss 0.06168121 - samples/sec: 17.32 - lr: 0.000002
2023-06-03 13:12:21,065 epoch 7 - iter 7770/7770 - loss 0.06198529 - samples/sec: 17.68 - lr: 0.000002
2023-06-03 13:12:21,069 ----------------------------------------------------------------------------------------------------
2023-06-03 13:12:21,069 EPOCH 7 done: loss 0.0620 - lr 0.000002
2023-06-03 13:15:07,056 Evaluating as a multi-label problem: False
2023-06-03 13:15:07,158 DEV : loss 0.08186762779951096 - f1-score (micro avg)  0.9636
2023-06-03 13:15:07,417 BAD EPOCHS (no improvement): 4
2023-06-03 13:15:07,436 ----------------------------------------------------------------------------------------------------
2023-06-03 13:18:06,073 epoch 8 - iter 777/7770 - loss 0.05243768 - samples/sec: 17.41 - lr: 0.000002
2023-06-03 13:21:01,696 epoch 8 - iter 1554/7770 - loss 0.05451430 - samples/sec: 17.71 - lr: 0.000002
2023-06-03 13:23:56,064 epoch 8 - iter 2331/7770 - loss 0.05800159 - samples/sec: 17.83 - lr: 0.000002
2023-06-03 13:26:53,027 epoch 8 - iter 3108/7770 - loss 0.05807875 - samples/sec: 17.57 - lr: 0.000001
2023-06-03 13:29:46,015 epoch 8 - iter 3885/7770 - loss 0.05680142 - samples/sec: 17.98 - lr: 0.000001
2023-06-03 13:32:44,396 epoch 8 - iter 4662/7770 - loss 0.05602981 - samples/sec: 17.43 - lr: 0.000001
2023-06-03 13:35:53,282 epoch 8 - iter 5439/7770 - loss 0.05661750 - samples/sec: 16.46 - lr: 0.000001
2023-06-03 13:38:53,350 epoch 8 - iter 6216/7770 - loss 0.05604222 - samples/sec: 17.27 - lr: 0.000001
2023-06-03 13:41:47,484 epoch 8 - iter 6993/7770 - loss 0.05562129 - samples/sec: 17.86 - lr: 0.000001
2023-06-03 13:44:42,390 epoch 8 - iter 7770/7770 - loss 0.05570212 - samples/sec: 17.78 - lr: 0.000001
2023-06-03 13:44:42,395 ----------------------------------------------------------------------------------------------------
2023-06-03 13:44:42,395 EPOCH 8 done: loss 0.0557 - lr 0.000001
2023-06-03 13:46:46,455 Evaluating as a multi-label problem: False
2023-06-03 13:46:46,510 DEV : loss 0.08763845264911652 - f1-score (micro avg)  0.9612
2023-06-03 13:46:46,689 BAD EPOCHS (no improvement): 4
2023-06-03 13:46:46,700 ----------------------------------------------------------------------------------------------------
2023-06-03 13:49:31,867 epoch 9 - iter 777/7770 - loss 0.05657455 - samples/sec: 18.83 - lr: 0.000001
2023-06-03 13:52:23,870 epoch 9 - iter 1554/7770 - loss 0.05559997 - samples/sec: 18.08 - lr: 0.000001
2023-06-03 13:55:25,644 epoch 9 - iter 2331/7770 - loss 0.05364511 - samples/sec: 17.11 - lr: 0.000001
2023-06-03 13:58:20,986 epoch 9 - iter 3108/7770 - loss 0.05313847 - samples/sec: 17.73 - lr: 0.000001
2023-06-03 14:01:22,447 epoch 9 - iter 3885/7770 - loss 0.05160283 - samples/sec: 17.14 - lr: 0.000001
2023-06-03 14:04:24,338 epoch 9 - iter 4662/7770 - loss 0.05172890 - samples/sec: 17.10 - lr: 0.000001
2023-06-03 14:07:21,872 epoch 9 - iter 5439/7770 - loss 0.05218874 - samples/sec: 17.52 - lr: 0.000001
2023-06-03 14:10:19,696 epoch 9 - iter 6216/7770 - loss 0.05268648 - samples/sec: 17.49 - lr: 0.000001
2023-06-03 14:13:21,345 epoch 9 - iter 6993/7770 - loss 0.05308986 - samples/sec: 17.12 - lr: 0.000001
2023-06-03 14:16:17,650 epoch 9 - iter 7770/7770 - loss 0.05327076 - samples/sec: 17.64 - lr: 0.000001
2023-06-03 14:16:17,654 ----------------------------------------------------------------------------------------------------
2023-06-03 14:16:17,654 EPOCH 9 done: loss 0.0533 - lr 0.000001
2023-06-03 14:19:05,434 Evaluating as a multi-label problem: False
2023-06-03 14:19:05,533 DEV : loss 0.0945436954498291 - f1-score (micro avg)  0.9611
2023-06-03 14:19:05,788 BAD EPOCHS (no improvement): 4
2023-06-03 14:19:05,791 ----------------------------------------------------------------------------------------------------
2023-06-03 14:22:00,290 epoch 10 - iter 777/7770 - loss 0.05181930 - samples/sec: 17.82 - lr: 0.000001
2023-06-03 14:25:02,987 epoch 10 - iter 1554/7770 - loss 0.04924550 - samples/sec: 17.02 - lr: 0.000000
2023-06-03 14:28:10,849 epoch 10 - iter 2331/7770 - loss 0.04714935 - samples/sec: 16.55 - lr: 0.000000
2023-06-03 14:31:08,398 epoch 10 - iter 3108/7770 - loss 0.04736838 - samples/sec: 17.51 - lr: 0.000000
2023-06-03 14:34:07,998 epoch 10 - iter 3885/7770 - loss 0.04834139 - samples/sec: 17.31 - lr: 0.000000
2023-06-03 14:37:09,148 epoch 10 - iter 4662/7770 - loss 0.05001740 - samples/sec: 17.17 - lr: 0.000000
2023-06-03 14:40:04,031 epoch 10 - iter 5439/7770 - loss 0.04954513 - samples/sec: 17.78 - lr: 0.000000
2023-06-03 14:43:02,928 epoch 10 - iter 6216/7770 - loss 0.04963767 - samples/sec: 17.38 - lr: 0.000000
2023-06-03 14:45:56,391 epoch 10 - iter 6993/7770 - loss 0.04994630 - samples/sec: 17.93 - lr: 0.000000
2023-06-03 14:48:53,964 epoch 10 - iter 7770/7770 - loss 0.04970509 - samples/sec: 17.51 - lr: 0.000000
2023-06-03 14:48:53,967 ----------------------------------------------------------------------------------------------------
2023-06-03 14:48:53,967 EPOCH 10 done: loss 0.0497 - lr 0.000000
2023-06-03 14:51:10,630 Evaluating as a multi-label problem: False
2023-06-03 14:51:10,727 DEV : loss 0.09517557919025421 - f1-score (micro avg)  0.961
2023-06-03 14:51:10,955 BAD EPOCHS (no improvement): 4
2023-06-03 14:51:24,346 ----------------------------------------------------------------------------------------------------
2023-06-03 14:51:24,352 Testing using last state of model ...
2023-06-03 14:55:09,270 Evaluating as a multi-label problem: False
2023-06-03 14:55:09,368 0.9322	0.9432	0.9376	0.9121
2023-06-03 14:55:09,368 
Results:
- F-score (micro) 0.9376
- F-score (macro) 0.9344
- Accuracy 0.9121

By class:
              precision    recall  f1-score   support

         PER     0.9773    0.9816    0.9794      2715
         ORG     0.8981    0.9426    0.9198      2543
         LOC     0.9504    0.9345    0.9424      2442
        MISC     0.8924    0.8999    0.8962      1889

   micro avg     0.9322    0.9432    0.9376      9589
   macro avg     0.9295    0.9396    0.9344      9589
weighted avg     0.9327    0.9432    0.9378      9589

2023-06-03 14:55:09,368 ----------------------------------------------------------------------------------------------------
2023-06-03 14:55:09,369 ----------------------------------------------------------------------------------------------------
2023-06-03 14:57:29,130 Evaluating as a multi-label problem: False
2023-06-03 14:57:29,179 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-03 14:57:29,179 0.9375	0.9394	0.9384	0.9223
2023-06-03 14:57:29,179 ----------------------------------------------------------------------------------------------------
2023-06-03 14:58:54,377 Evaluating as a multi-label problem: False
2023-06-03 14:58:54,441 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-03 14:58:54,442 0.9286	0.9458	0.9371	0.9051
