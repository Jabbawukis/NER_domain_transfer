2023-06-02 22:05:08,029 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:08,033 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-02 22:05:08,036 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:08,037 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-02 22:05:08,039 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:08,039 Parameters:
2023-06-02 22:05:08,039  - learning_rate: "0.000005"
2023-06-02 22:05:08,039  - mini_batch_size: "4"
2023-06-02 22:05:08,039  - patience: "3"
2023-06-02 22:05:08,039  - anneal_factor: "0.5"
2023-06-02 22:05:08,039  - max_epochs: "10"
2023-06-02 22:05:08,039  - shuffle: "True"
2023-06-02 22:05:08,039  - train_with_dev: "False"
2023-06-02 22:05:08,039  - batch_growth_annealing: "False"
2023-06-02 22:05:08,039 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:08,041 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_1_full_fine_tuning"
2023-06-02 22:05:08,041 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:08,041 Device: cuda:3
2023-06-02 22:05:08,041 ----------------------------------------------------------------------------------------------------
2023-06-02 22:05:08,041 Embeddings storage mode: none
2023-06-02 22:05:08,041 ----------------------------------------------------------------------------------------------------
2023-06-02 22:08:10,443 epoch 1 - iter 777/7770 - loss 0.48791141 - samples/sec: 17.05 - lr: 0.000001
2023-06-02 22:11:14,584 epoch 1 - iter 1554/7770 - loss 0.38772847 - samples/sec: 16.89 - lr: 0.000001
2023-06-02 22:14:16,806 epoch 1 - iter 2331/7770 - loss 0.34542415 - samples/sec: 17.06 - lr: 0.000002
2023-06-02 22:17:20,202 epoch 1 - iter 3108/7770 - loss 0.30415550 - samples/sec: 16.95 - lr: 0.000002
2023-06-02 22:20:36,327 epoch 1 - iter 3885/7770 - loss 0.28252269 - samples/sec: 15.85 - lr: 0.000003
2023-06-02 22:23:38,194 epoch 1 - iter 4662/7770 - loss 0.27557602 - samples/sec: 17.10 - lr: 0.000003
2023-06-02 22:26:31,141 epoch 1 - iter 5439/7770 - loss 0.27066385 - samples/sec: 17.98 - lr: 0.000003
2023-06-02 22:29:29,144 epoch 1 - iter 6216/7770 - loss 0.26255521 - samples/sec: 17.47 - lr: 0.000004
2023-06-02 22:32:30,628 epoch 1 - iter 6993/7770 - loss 0.25898484 - samples/sec: 17.13 - lr: 0.000005
2023-06-02 22:35:44,895 epoch 1 - iter 7770/7770 - loss 0.25273486 - samples/sec: 16.01 - lr: 0.000005
2023-06-02 22:35:44,898 ----------------------------------------------------------------------------------------------------
2023-06-02 22:35:44,898 EPOCH 1 done: loss 0.2527 - lr 0.000005
2023-06-02 22:38:51,681 Evaluating as a multi-label problem: False
2023-06-02 22:38:51,777 DEV : loss 0.12638454139232635 - f1-score (micro avg)  0.9086
2023-06-02 22:38:51,971 BAD EPOCHS (no improvement): 4
2023-06-02 22:38:51,974 ----------------------------------------------------------------------------------------------------
2023-06-02 22:42:00,192 epoch 2 - iter 777/7770 - loss 0.17299076 - samples/sec: 16.52 - lr: 0.000005
2023-06-02 22:45:03,969 epoch 2 - iter 1554/7770 - loss 0.17573928 - samples/sec: 16.92 - lr: 0.000005
2023-06-02 22:48:06,385 epoch 2 - iter 2331/7770 - loss 0.16944531 - samples/sec: 17.05 - lr: 0.000005
2023-06-02 22:51:08,049 epoch 2 - iter 3108/7770 - loss 0.16368271 - samples/sec: 17.12 - lr: 0.000005
2023-06-02 22:54:20,295 epoch 2 - iter 3885/7770 - loss 0.16412626 - samples/sec: 16.17 - lr: 0.000005
2023-06-02 22:57:19,616 epoch 2 - iter 4662/7770 - loss 0.16055405 - samples/sec: 17.34 - lr: 0.000005
2023-06-02 23:00:28,577 epoch 2 - iter 5439/7770 - loss 0.15913600 - samples/sec: 16.46 - lr: 0.000005
2023-06-02 23:03:32,506 epoch 2 - iter 6216/7770 - loss 0.15762706 - samples/sec: 16.91 - lr: 0.000005
2023-06-02 23:06:38,106 epoch 2 - iter 6993/7770 - loss 0.15645032 - samples/sec: 16.75 - lr: 0.000005
2023-06-02 23:09:40,028 epoch 2 - iter 7770/7770 - loss 0.15319933 - samples/sec: 17.09 - lr: 0.000004
2023-06-02 23:09:40,032 ----------------------------------------------------------------------------------------------------
2023-06-02 23:09:40,032 EPOCH 2 done: loss 0.1532 - lr 0.000004
2023-06-02 23:12:26,043 Evaluating as a multi-label problem: False
2023-06-02 23:12:26,148 DEV : loss 0.09240561723709106 - f1-score (micro avg)  0.9455
2023-06-02 23:12:26,339 BAD EPOCHS (no improvement): 4
2023-06-02 23:12:26,342 ----------------------------------------------------------------------------------------------------
2023-06-02 23:15:30,455 epoch 3 - iter 777/7770 - loss 0.13253276 - samples/sec: 16.89 - lr: 0.000004
2023-06-02 23:18:35,014 epoch 3 - iter 1554/7770 - loss 0.12182492 - samples/sec: 16.85 - lr: 0.000004
2023-06-02 23:21:40,150 epoch 3 - iter 2331/7770 - loss 0.11627779 - samples/sec: 16.80 - lr: 0.000004
2023-06-02 23:24:42,138 epoch 3 - iter 3108/7770 - loss 0.11314877 - samples/sec: 17.09 - lr: 0.000004
2023-06-02 23:27:44,543 epoch 3 - iter 3885/7770 - loss 0.11322988 - samples/sec: 17.05 - lr: 0.000004
2023-06-02 23:30:53,337 epoch 3 - iter 4662/7770 - loss 0.11248888 - samples/sec: 16.47 - lr: 0.000004
2023-06-02 23:33:56,098 epoch 3 - iter 5439/7770 - loss 0.11139091 - samples/sec: 17.01 - lr: 0.000004
2023-06-02 23:36:55,773 epoch 3 - iter 6216/7770 - loss 0.11275216 - samples/sec: 17.31 - lr: 0.000004
2023-06-02 23:39:59,268 epoch 3 - iter 6993/7770 - loss 0.11430029 - samples/sec: 16.95 - lr: 0.000004
2023-06-02 23:43:04,270 epoch 3 - iter 7770/7770 - loss 0.11459924 - samples/sec: 16.81 - lr: 0.000004
2023-06-02 23:43:04,274 ----------------------------------------------------------------------------------------------------
2023-06-02 23:43:04,274 EPOCH 3 done: loss 0.1146 - lr 0.000004
2023-06-02 23:45:45,946 Evaluating as a multi-label problem: False
2023-06-02 23:45:46,039 DEV : loss 0.07679660618305206 - f1-score (micro avg)  0.9581
2023-06-02 23:45:46,243 BAD EPOCHS (no improvement): 4
2023-06-02 23:45:46,246 ----------------------------------------------------------------------------------------------------
2023-06-02 23:48:53,091 epoch 4 - iter 777/7770 - loss 0.09182431 - samples/sec: 16.64 - lr: 0.000004
2023-06-02 23:51:58,523 epoch 4 - iter 1554/7770 - loss 0.08887611 - samples/sec: 16.77 - lr: 0.000004
2023-06-02 23:55:01,577 epoch 4 - iter 2331/7770 - loss 0.08328270 - samples/sec: 16.99 - lr: 0.000004
2023-06-02 23:58:03,539 epoch 4 - iter 3108/7770 - loss 0.08683088 - samples/sec: 17.09 - lr: 0.000004
2023-06-03 00:01:07,990 epoch 4 - iter 3885/7770 - loss 0.08787356 - samples/sec: 16.86 - lr: 0.000004
2023-06-03 00:04:10,077 epoch 4 - iter 4662/7770 - loss 0.08660104 - samples/sec: 17.08 - lr: 0.000004
2023-06-03 00:07:11,003 epoch 4 - iter 5439/7770 - loss 0.08767143 - samples/sec: 17.19 - lr: 0.000004
2023-06-03 00:10:23,832 epoch 4 - iter 6216/7770 - loss 0.08890350 - samples/sec: 16.13 - lr: 0.000003
2023-06-03 00:13:27,333 epoch 4 - iter 6993/7770 - loss 0.08853377 - samples/sec: 16.95 - lr: 0.000003
2023-06-03 00:16:26,466 epoch 4 - iter 7770/7770 - loss 0.08900369 - samples/sec: 17.36 - lr: 0.000003
2023-06-03 00:16:26,470 ----------------------------------------------------------------------------------------------------
2023-06-03 00:16:26,470 EPOCH 4 done: loss 0.0890 - lr 0.000003
2023-06-03 00:19:07,523 Evaluating as a multi-label problem: False
2023-06-03 00:19:07,616 DEV : loss 0.0847315788269043 - f1-score (micro avg)  0.9584
2023-06-03 00:19:07,820 BAD EPOCHS (no improvement): 4
2023-06-03 00:19:07,822 ----------------------------------------------------------------------------------------------------
2023-06-03 00:22:16,850 epoch 5 - iter 777/7770 - loss 0.08925410 - samples/sec: 16.45 - lr: 0.000003
2023-06-03 00:25:21,467 epoch 5 - iter 1554/7770 - loss 0.08443011 - samples/sec: 16.84 - lr: 0.000003
2023-06-03 00:28:27,859 epoch 5 - iter 2331/7770 - loss 0.08194212 - samples/sec: 16.68 - lr: 0.000003
2023-06-03 00:31:30,613 epoch 5 - iter 3108/7770 - loss 0.08098056 - samples/sec: 17.01 - lr: 0.000003
2023-06-03 00:34:28,413 epoch 5 - iter 3885/7770 - loss 0.07952630 - samples/sec: 17.49 - lr: 0.000003
2023-06-03 00:37:28,726 epoch 5 - iter 4662/7770 - loss 0.07857640 - samples/sec: 17.25 - lr: 0.000003
2023-06-03 00:40:31,049 epoch 5 - iter 5439/7770 - loss 0.07841634 - samples/sec: 17.06 - lr: 0.000003
2023-06-03 00:43:38,677 epoch 5 - iter 6216/7770 - loss 0.07860373 - samples/sec: 16.57 - lr: 0.000003
2023-06-03 00:46:50,439 epoch 5 - iter 6993/7770 - loss 0.07906192 - samples/sec: 16.22 - lr: 0.000003
2023-06-03 00:49:58,894 epoch 5 - iter 7770/7770 - loss 0.08005553 - samples/sec: 16.50 - lr: 0.000003
2023-06-03 00:49:58,898 ----------------------------------------------------------------------------------------------------
2023-06-03 00:49:58,898 EPOCH 5 done: loss 0.0801 - lr 0.000003
2023-06-03 00:52:30,360 Evaluating as a multi-label problem: False
2023-06-03 00:52:30,461 DEV : loss 0.06522072106599808 - f1-score (micro avg)  0.9592
2023-06-03 00:52:30,640 BAD EPOCHS (no improvement): 4
2023-06-03 00:52:30,643 ----------------------------------------------------------------------------------------------------
2023-06-03 00:55:36,389 epoch 6 - iter 777/7770 - loss 0.05994662 - samples/sec: 16.74 - lr: 0.000003
2023-06-03 00:58:40,515 epoch 6 - iter 1554/7770 - loss 0.06109774 - samples/sec: 16.89 - lr: 0.000003
2023-06-03 01:01:46,821 epoch 6 - iter 2331/7770 - loss 0.06271375 - samples/sec: 16.69 - lr: 0.000003
2023-06-03 01:04:55,564 epoch 6 - iter 3108/7770 - loss 0.06809354 - samples/sec: 16.47 - lr: 0.000003
2023-06-03 01:07:57,251 epoch 6 - iter 3885/7770 - loss 0.06752598 - samples/sec: 17.11 - lr: 0.000003
2023-06-03 01:11:00,800 epoch 6 - iter 4662/7770 - loss 0.06640679 - samples/sec: 16.94 - lr: 0.000002
2023-06-03 01:14:02,140 epoch 6 - iter 5439/7770 - loss 0.06694125 - samples/sec: 17.15 - lr: 0.000002
2023-06-03 01:17:09,172 epoch 6 - iter 6216/7770 - loss 0.06730519 - samples/sec: 16.63 - lr: 0.000002
2023-06-03 01:20:07,396 epoch 6 - iter 6993/7770 - loss 0.06597209 - samples/sec: 17.45 - lr: 0.000002
2023-06-03 01:23:19,688 epoch 6 - iter 7770/7770 - loss 0.06676874 - samples/sec: 16.17 - lr: 0.000002
2023-06-03 01:23:19,692 ----------------------------------------------------------------------------------------------------
2023-06-03 01:23:19,692 EPOCH 6 done: loss 0.0668 - lr 0.000002
2023-06-03 01:25:47,715 Evaluating as a multi-label problem: False
2023-06-03 01:25:47,810 DEV : loss 0.08393879979848862 - f1-score (micro avg)  0.965
2023-06-03 01:25:48,011 BAD EPOCHS (no improvement): 4
2023-06-03 01:25:48,016 ----------------------------------------------------------------------------------------------------
2023-06-03 01:28:55,503 epoch 7 - iter 777/7770 - loss 0.05604992 - samples/sec: 16.59 - lr: 0.000002
2023-06-03 01:31:59,416 epoch 7 - iter 1554/7770 - loss 0.05815865 - samples/sec: 16.91 - lr: 0.000002
2023-06-03 01:35:02,436 epoch 7 - iter 2331/7770 - loss 0.06031101 - samples/sec: 16.99 - lr: 0.000002
2023-06-03 01:38:05,182 epoch 7 - iter 3108/7770 - loss 0.06123465 - samples/sec: 17.02 - lr: 0.000002
2023-06-03 01:41:06,545 epoch 7 - iter 3885/7770 - loss 0.05991259 - samples/sec: 17.15 - lr: 0.000002
2023-06-03 01:44:04,874 epoch 7 - iter 4662/7770 - loss 0.05910818 - samples/sec: 17.44 - lr: 0.000002
2023-06-03 01:47:07,659 epoch 7 - iter 5439/7770 - loss 0.06048776 - samples/sec: 17.01 - lr: 0.000002
2023-06-03 01:50:07,536 epoch 7 - iter 6216/7770 - loss 0.05930175 - samples/sec: 17.29 - lr: 0.000002
2023-06-03 01:53:10,925 epoch 7 - iter 6993/7770 - loss 0.05954755 - samples/sec: 16.96 - lr: 0.000002
2023-06-03 01:56:17,738 epoch 7 - iter 7770/7770 - loss 0.05883816 - samples/sec: 16.65 - lr: 0.000002
2023-06-03 01:56:17,742 ----------------------------------------------------------------------------------------------------
2023-06-03 01:56:17,742 EPOCH 7 done: loss 0.0588 - lr 0.000002
2023-06-03 01:59:07,150 Evaluating as a multi-label problem: False
2023-06-03 01:59:07,216 DEV : loss 0.09600862115621567 - f1-score (micro avg)  0.964
2023-06-03 01:59:07,383 BAD EPOCHS (no improvement): 4
2023-06-03 01:59:07,386 ----------------------------------------------------------------------------------------------------
2023-06-03 02:02:12,354 epoch 8 - iter 777/7770 - loss 0.05588184 - samples/sec: 16.81 - lr: 0.000002
2023-06-03 02:05:13,823 epoch 8 - iter 1554/7770 - loss 0.05893929 - samples/sec: 17.13 - lr: 0.000002
2023-06-03 02:08:17,238 epoch 8 - iter 2331/7770 - loss 0.05899540 - samples/sec: 16.95 - lr: 0.000002
2023-06-03 02:11:19,131 epoch 8 - iter 3108/7770 - loss 0.05656267 - samples/sec: 17.10 - lr: 0.000001
2023-06-03 02:14:24,513 epoch 8 - iter 3885/7770 - loss 0.05654874 - samples/sec: 16.77 - lr: 0.000001
2023-06-03 02:17:23,300 epoch 8 - iter 4662/7770 - loss 0.05506720 - samples/sec: 17.39 - lr: 0.000001
2023-06-03 02:20:22,513 epoch 8 - iter 5439/7770 - loss 0.05426225 - samples/sec: 17.35 - lr: 0.000001
2023-06-03 02:23:28,211 epoch 8 - iter 6216/7770 - loss 0.05388418 - samples/sec: 16.74 - lr: 0.000001
2023-06-03 02:26:28,448 epoch 8 - iter 6993/7770 - loss 0.05414046 - samples/sec: 17.25 - lr: 0.000001
2023-06-03 02:29:30,984 epoch 8 - iter 7770/7770 - loss 0.05347571 - samples/sec: 17.04 - lr: 0.000001
2023-06-03 02:29:30,988 ----------------------------------------------------------------------------------------------------
2023-06-03 02:29:30,988 EPOCH 8 done: loss 0.0535 - lr 0.000001
2023-06-03 02:31:57,553 Evaluating as a multi-label problem: False
2023-06-03 02:31:57,615 DEV : loss 0.09615368396043777 - f1-score (micro avg)  0.9651
2023-06-03 02:31:57,774 BAD EPOCHS (no improvement): 4
2023-06-03 02:31:57,777 ----------------------------------------------------------------------------------------------------
2023-06-03 02:34:58,969 epoch 9 - iter 777/7770 - loss 0.05672650 - samples/sec: 17.16 - lr: 0.000001
2023-06-03 02:37:59,531 epoch 9 - iter 1554/7770 - loss 0.05253023 - samples/sec: 17.22 - lr: 0.000001
2023-06-03 02:41:13,455 epoch 9 - iter 2331/7770 - loss 0.04948762 - samples/sec: 16.03 - lr: 0.000001
2023-06-03 02:44:17,225 epoch 9 - iter 3108/7770 - loss 0.04846500 - samples/sec: 16.92 - lr: 0.000001
2023-06-03 02:47:13,913 epoch 9 - iter 3885/7770 - loss 0.04810208 - samples/sec: 17.60 - lr: 0.000001
2023-06-03 02:50:13,443 epoch 9 - iter 4662/7770 - loss 0.04786616 - samples/sec: 17.32 - lr: 0.000001
2023-06-03 02:53:11,601 epoch 9 - iter 5439/7770 - loss 0.04767734 - samples/sec: 17.45 - lr: 0.000001
2023-06-03 02:56:09,644 epoch 9 - iter 6216/7770 - loss 0.04766513 - samples/sec: 17.46 - lr: 0.000001
2023-06-03 02:59:11,057 epoch 9 - iter 6993/7770 - loss 0.04844081 - samples/sec: 17.14 - lr: 0.000001
2023-06-03 03:02:14,775 epoch 9 - iter 7770/7770 - loss 0.04860142 - samples/sec: 16.93 - lr: 0.000001
2023-06-03 03:02:14,779 ----------------------------------------------------------------------------------------------------
2023-06-03 03:02:14,779 EPOCH 9 done: loss 0.0486 - lr 0.000001
2023-06-03 03:05:03,202 Evaluating as a multi-label problem: False
2023-06-03 03:05:03,299 DEV : loss 0.09405522048473358 - f1-score (micro avg)  0.9662
2023-06-03 03:05:03,514 BAD EPOCHS (no improvement): 4
2023-06-03 03:05:03,517 ----------------------------------------------------------------------------------------------------
2023-06-03 03:08:04,286 epoch 10 - iter 777/7770 - loss 0.05290568 - samples/sec: 17.20 - lr: 0.000001
2023-06-03 03:11:10,887 epoch 10 - iter 1554/7770 - loss 0.05009033 - samples/sec: 16.66 - lr: 0.000000
2023-06-03 03:14:10,040 epoch 10 - iter 2331/7770 - loss 0.04950987 - samples/sec: 17.36 - lr: 0.000000
2023-06-03 03:17:19,090 epoch 10 - iter 3108/7770 - loss 0.04741417 - samples/sec: 16.45 - lr: 0.000000
2023-06-03 03:20:22,895 epoch 10 - iter 3885/7770 - loss 0.04670983 - samples/sec: 16.92 - lr: 0.000000
2023-06-03 03:23:23,341 epoch 10 - iter 4662/7770 - loss 0.04745004 - samples/sec: 17.23 - lr: 0.000000
2023-06-03 03:26:22,765 epoch 10 - iter 5439/7770 - loss 0.04678575 - samples/sec: 17.33 - lr: 0.000000
2023-06-03 03:29:25,660 epoch 10 - iter 6216/7770 - loss 0.04665211 - samples/sec: 17.00 - lr: 0.000000
2023-06-03 03:32:27,864 epoch 10 - iter 6993/7770 - loss 0.04655917 - samples/sec: 17.07 - lr: 0.000000
2023-06-03 03:35:30,454 epoch 10 - iter 7770/7770 - loss 0.04638141 - samples/sec: 17.03 - lr: 0.000000
2023-06-03 03:35:30,459 ----------------------------------------------------------------------------------------------------
2023-06-03 03:35:30,459 EPOCH 10 done: loss 0.0464 - lr 0.000000
2023-06-03 03:38:17,599 Evaluating as a multi-label problem: False
2023-06-03 03:38:17,694 DEV : loss 0.09990628063678741 - f1-score (micro avg)  0.9657
2023-06-03 03:38:17,923 BAD EPOCHS (no improvement): 4
2023-06-03 03:38:32,672 ----------------------------------------------------------------------------------------------------
2023-06-03 03:38:32,675 Testing using last state of model ...
2023-06-03 03:42:17,684 Evaluating as a multi-label problem: False
2023-06-03 03:42:17,801 0.9368	0.9468	0.9418	0.9185
2023-06-03 03:42:17,802 
Results:
- F-score (micro) 0.9418
- F-score (macro) 0.9389
- Accuracy 0.9185

By class:
              precision    recall  f1-score   support

         PER     0.9808    0.9801    0.9805      2715
         ORG     0.9058    0.9453    0.9251      2543
         LOC     0.9560    0.9353    0.9456      2442
        MISC     0.8936    0.9158    0.9046      1889

   micro avg     0.9368    0.9468    0.9418      9589
   macro avg     0.9341    0.9441    0.9389      9589
weighted avg     0.9374    0.9468    0.9420      9589

2023-06-03 03:42:17,802 ----------------------------------------------------------------------------------------------------
2023-06-03 03:42:17,802 ----------------------------------------------------------------------------------------------------
2023-06-03 03:44:39,858 Evaluating as a multi-label problem: False
2023-06-03 03:44:39,910 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-03 03:44:39,910 0.9469	0.9465	0.9467	0.9334
2023-06-03 03:44:39,910 ----------------------------------------------------------------------------------------------------
2023-06-03 03:46:21,073 Evaluating as a multi-label problem: False
2023-06-03 03:46:21,145 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-03 03:46:21,146 0.9298	0.9471	0.9383	0.9083
