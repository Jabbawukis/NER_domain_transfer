2023-05-28 00:16:46,323 ----------------------------------------------------------------------------------------------------
2023-05-28 00:16:46,327 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-28 00:16:46,329 ----------------------------------------------------------------------------------------------------
2023-05-28 00:16:46,330 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-28 00:16:46,330 ----------------------------------------------------------------------------------------------------
2023-05-28 00:16:46,330 Parameters:
2023-05-28 00:16:46,330  - learning_rate: "0.800000"
2023-05-28 00:16:46,330  - mini_batch_size: "32"
2023-05-28 00:16:46,330  - patience: "3"
2023-05-28 00:16:46,330  - anneal_factor: "0.5"
2023-05-28 00:16:46,330  - max_epochs: "10"
2023-05-28 00:16:46,330  - shuffle: "True"
2023-05-28 00:16:46,330  - train_with_dev: "False"
2023-05-28 00:16:46,330  - batch_growth_annealing: "False"
2023-05-28 00:16:46,330 ----------------------------------------------------------------------------------------------------
2023-05-28 00:16:46,330 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_2_linear_probing"
2023-05-28 00:16:46,330 ----------------------------------------------------------------------------------------------------
2023-05-28 00:16:46,330 Device: cuda:3
2023-05-28 00:16:46,331 ----------------------------------------------------------------------------------------------------
2023-05-28 00:16:46,331 Embeddings storage mode: none
2023-05-28 00:16:46,331 ----------------------------------------------------------------------------------------------------
2023-05-28 00:17:41,825 epoch 1 - iter 97/972 - loss 0.89536918 - samples/sec: 55.95 - lr: 0.079835
2023-05-28 00:18:35,048 epoch 1 - iter 194/972 - loss 1.16767030 - samples/sec: 58.34 - lr: 0.159671
2023-05-28 00:19:30,580 epoch 1 - iter 291/972 - loss 1.76022502 - samples/sec: 55.91 - lr: 0.239506
2023-05-28 00:20:23,441 epoch 1 - iter 388/972 - loss 2.33513251 - samples/sec: 58.74 - lr: 0.319342
2023-05-28 00:21:21,482 epoch 1 - iter 485/972 - loss 3.02068840 - samples/sec: 53.50 - lr: 0.399177
2023-05-28 00:22:11,527 epoch 1 - iter 582/972 - loss 4.14468933 - samples/sec: 62.05 - lr: 0.479012
2023-05-28 00:22:55,317 epoch 1 - iter 679/972 - loss 4.91927372 - samples/sec: 70.91 - lr: 0.558848
2023-05-28 00:23:45,328 epoch 1 - iter 776/972 - loss 5.71137279 - samples/sec: 62.09 - lr: 0.638683
2023-05-28 00:24:31,454 epoch 1 - iter 873/972 - loss 6.44896466 - samples/sec: 67.32 - lr: 0.718519
2023-05-28 00:25:26,048 epoch 1 - iter 970/972 - loss 6.94253090 - samples/sec: 56.87 - lr: 0.798354
2023-05-28 00:25:26,571 ----------------------------------------------------------------------------------------------------
2023-05-28 00:25:26,571 EPOCH 1 done: loss 6.9504 - lr 0.798354
2023-05-28 00:28:08,206 Evaluating as a multi-label problem: False
2023-05-28 00:28:08,321 DEV : loss 4.102582931518555 - f1-score (micro avg)  0.6057
2023-05-28 00:28:08,517 BAD EPOCHS (no improvement): 4
2023-05-28 00:28:08,520 ----------------------------------------------------------------------------------------------------
2023-05-28 00:29:02,055 epoch 2 - iter 97/972 - loss 12.34505011 - samples/sec: 58.01 - lr: 0.791131
2023-05-28 00:29:55,661 epoch 2 - iter 194/972 - loss 12.14015013 - samples/sec: 57.93 - lr: 0.782263
2023-05-28 00:30:53,857 epoch 2 - iter 291/972 - loss 12.17267343 - samples/sec: 53.35 - lr: 0.773394
2023-05-28 00:31:44,920 epoch 2 - iter 388/972 - loss 12.03056496 - samples/sec: 60.81 - lr: 0.764526
2023-05-28 00:32:42,355 epoch 2 - iter 485/972 - loss 11.87289185 - samples/sec: 54.06 - lr: 0.755657
2023-05-28 00:33:34,491 epoch 2 - iter 582/972 - loss 11.83678600 - samples/sec: 59.56 - lr: 0.746789
2023-05-28 00:34:27,796 epoch 2 - iter 679/972 - loss 11.81569174 - samples/sec: 58.26 - lr: 0.737920
2023-05-28 00:35:24,980 epoch 2 - iter 776/972 - loss 11.96857865 - samples/sec: 54.30 - lr: 0.729051
2023-05-28 00:36:16,763 epoch 2 - iter 873/972 - loss 11.79814861 - samples/sec: 59.97 - lr: 0.720183
2023-05-28 00:37:14,600 epoch 2 - iter 970/972 - loss 11.85715051 - samples/sec: 53.69 - lr: 0.711314
2023-05-28 00:37:15,238 ----------------------------------------------------------------------------------------------------
2023-05-28 00:37:15,238 EPOCH 2 done: loss 11.8533 - lr 0.711314
2023-05-28 00:39:59,113 Evaluating as a multi-label problem: False
2023-05-28 00:39:59,221 DEV : loss 5.048276901245117 - f1-score (micro avg)  0.5547
2023-05-28 00:39:59,452 BAD EPOCHS (no improvement): 4
2023-05-28 00:39:59,454 ----------------------------------------------------------------------------------------------------
2023-05-28 00:40:50,356 epoch 3 - iter 97/972 - loss 11.33222114 - samples/sec: 61.01 - lr: 0.702263
2023-05-28 00:41:42,739 epoch 3 - iter 194/972 - loss 11.52626108 - samples/sec: 59.28 - lr: 0.693394
2023-05-28 00:42:39,351 epoch 3 - iter 291/972 - loss 11.03089109 - samples/sec: 54.85 - lr: 0.684526
2023-05-28 00:43:30,318 epoch 3 - iter 388/972 - loss 10.99794207 - samples/sec: 60.93 - lr: 0.675657
2023-05-28 00:44:28,734 epoch 3 - iter 485/972 - loss 10.95051350 - samples/sec: 53.16 - lr: 0.666789
2023-05-28 00:45:21,758 epoch 3 - iter 582/972 - loss 10.88269727 - samples/sec: 58.56 - lr: 0.657920
2023-05-28 00:46:14,068 epoch 3 - iter 679/972 - loss 10.86554356 - samples/sec: 59.36 - lr: 0.649051
2023-05-28 00:47:14,252 epoch 3 - iter 776/972 - loss 10.75612208 - samples/sec: 51.59 - lr: 0.640183
2023-05-28 00:48:07,486 epoch 3 - iter 873/972 - loss 10.61133648 - samples/sec: 58.33 - lr: 0.631314
2023-05-28 00:49:05,818 epoch 3 - iter 970/972 - loss 10.54886616 - samples/sec: 53.23 - lr: 0.622446
2023-05-28 00:49:06,524 ----------------------------------------------------------------------------------------------------
2023-05-28 00:49:06,524 EPOCH 3 done: loss 10.5517 - lr 0.622446
2023-05-28 00:51:47,223 Evaluating as a multi-label problem: False
2023-05-28 00:51:47,342 DEV : loss 3.72989559173584 - f1-score (micro avg)  0.5075
2023-05-28 00:51:47,600 BAD EPOCHS (no improvement): 4
2023-05-28 00:51:47,602 ----------------------------------------------------------------------------------------------------
2023-05-28 00:52:39,354 epoch 4 - iter 97/972 - loss 10.54310640 - samples/sec: 60.01 - lr: 0.613394
2023-05-28 00:53:29,785 epoch 4 - iter 194/972 - loss 10.02446820 - samples/sec: 61.57 - lr: 0.604526
2023-05-28 00:54:26,695 epoch 4 - iter 291/972 - loss 9.74869740 - samples/sec: 54.56 - lr: 0.595657
2023-05-28 00:55:19,737 epoch 4 - iter 388/972 - loss 9.77409471 - samples/sec: 58.54 - lr: 0.586789
2023-05-28 00:56:13,887 epoch 4 - iter 485/972 - loss 9.52226532 - samples/sec: 57.34 - lr: 0.577920
2023-05-28 00:57:06,839 epoch 4 - iter 582/972 - loss 9.47907289 - samples/sec: 58.64 - lr: 0.569051
2023-05-28 00:57:59,381 epoch 4 - iter 679/972 - loss 9.47164629 - samples/sec: 59.10 - lr: 0.560183
2023-05-28 00:58:56,743 epoch 4 - iter 776/972 - loss 9.41191066 - samples/sec: 54.13 - lr: 0.551314
2023-05-28 00:59:48,123 epoch 4 - iter 873/972 - loss 9.31988960 - samples/sec: 60.44 - lr: 0.542446
2023-05-28 01:00:44,816 epoch 4 - iter 970/972 - loss 9.31118437 - samples/sec: 54.77 - lr: 0.533577
2023-05-28 01:00:45,391 ----------------------------------------------------------------------------------------------------
2023-05-28 01:00:45,391 EPOCH 4 done: loss 9.3066 - lr 0.533577
2023-05-28 01:03:25,870 Evaluating as a multi-label problem: False
2023-05-28 01:03:25,972 DEV : loss 2.665550947189331 - f1-score (micro avg)  0.6546
2023-05-28 01:03:26,215 BAD EPOCHS (no improvement): 4
2023-05-28 01:03:26,218 ----------------------------------------------------------------------------------------------------
2023-05-28 01:04:18,557 epoch 5 - iter 97/972 - loss 8.42748543 - samples/sec: 59.33 - lr: 0.524526
2023-05-28 01:05:09,127 epoch 5 - iter 194/972 - loss 8.11023305 - samples/sec: 61.40 - lr: 0.515657
2023-05-28 01:06:05,885 epoch 5 - iter 291/972 - loss 8.36773837 - samples/sec: 54.70 - lr: 0.506789
2023-05-28 01:06:56,462 epoch 5 - iter 388/972 - loss 8.28174528 - samples/sec: 61.39 - lr: 0.497920
2023-05-28 01:07:53,290 epoch 5 - iter 485/972 - loss 8.36783467 - samples/sec: 54.64 - lr: 0.489051
2023-05-28 01:08:46,266 epoch 5 - iter 582/972 - loss 8.36485567 - samples/sec: 58.61 - lr: 0.480183
2023-05-28 01:09:36,106 epoch 5 - iter 679/972 - loss 8.39556401 - samples/sec: 62.30 - lr: 0.471314
2023-05-28 01:10:33,212 epoch 5 - iter 776/972 - loss 8.30728137 - samples/sec: 54.37 - lr: 0.462446
2023-05-28 01:11:25,231 epoch 5 - iter 873/972 - loss 8.21853912 - samples/sec: 59.69 - lr: 0.453577
2023-05-28 01:12:22,278 epoch 5 - iter 970/972 - loss 8.18012991 - samples/sec: 54.43 - lr: 0.444709
2023-05-28 01:12:22,920 ----------------------------------------------------------------------------------------------------
2023-05-28 01:12:22,920 EPOCH 5 done: loss 8.1757 - lr 0.444709
2023-05-28 01:15:03,878 Evaluating as a multi-label problem: False
2023-05-28 01:15:03,984 DEV : loss 3.3733201026916504 - f1-score (micro avg)  0.6364
2023-05-28 01:15:04,213 BAD EPOCHS (no improvement): 4
2023-05-28 01:15:04,216 ----------------------------------------------------------------------------------------------------
2023-05-28 01:15:56,080 epoch 6 - iter 97/972 - loss 8.62435546 - samples/sec: 59.88 - lr: 0.435657
2023-05-28 01:16:52,786 epoch 6 - iter 194/972 - loss 8.18434997 - samples/sec: 54.76 - lr: 0.426789
2023-05-28 01:17:45,218 epoch 6 - iter 291/972 - loss 8.20725202 - samples/sec: 59.22 - lr: 0.417920
2023-05-28 01:18:36,748 epoch 6 - iter 388/972 - loss 8.24010620 - samples/sec: 60.26 - lr: 0.409051
2023-05-28 01:19:34,757 epoch 6 - iter 485/972 - loss 8.04715359 - samples/sec: 53.53 - lr: 0.400183
2023-05-28 01:20:26,237 epoch 6 - iter 582/972 - loss 7.83004087 - samples/sec: 60.32 - lr: 0.391314
2023-05-28 01:21:23,364 epoch 6 - iter 679/972 - loss 7.57145102 - samples/sec: 54.35 - lr: 0.382446
2023-05-28 01:22:16,277 epoch 6 - iter 776/972 - loss 7.47322722 - samples/sec: 58.69 - lr: 0.373577
2023-05-28 01:23:08,588 epoch 6 - iter 873/972 - loss 7.38221585 - samples/sec: 59.36 - lr: 0.364709
2023-05-28 01:24:06,800 epoch 6 - iter 970/972 - loss 7.27288491 - samples/sec: 53.34 - lr: 0.355840
2023-05-28 01:24:07,442 ----------------------------------------------------------------------------------------------------
2023-05-28 01:24:07,442 EPOCH 6 done: loss 7.2694 - lr 0.355840
2023-05-28 01:26:48,010 Evaluating as a multi-label problem: False
2023-05-28 01:26:48,118 DEV : loss 3.16190505027771 - f1-score (micro avg)  0.5281
2023-05-28 01:26:48,304 BAD EPOCHS (no improvement): 4
2023-05-28 01:26:48,307 ----------------------------------------------------------------------------------------------------
2023-05-28 01:27:38,695 epoch 7 - iter 97/972 - loss 5.98835797 - samples/sec: 61.63 - lr: 0.346789
2023-05-28 01:28:36,899 epoch 7 - iter 194/972 - loss 5.91574145 - samples/sec: 53.35 - lr: 0.337920
2023-05-28 01:29:27,984 epoch 7 - iter 291/972 - loss 5.92804723 - samples/sec: 60.78 - lr: 0.329051
2023-05-28 01:30:21,081 epoch 7 - iter 388/972 - loss 5.93800516 - samples/sec: 58.48 - lr: 0.320183
2023-05-28 01:31:18,326 epoch 7 - iter 485/972 - loss 5.95708481 - samples/sec: 54.24 - lr: 0.311314
2023-05-28 01:32:09,160 epoch 7 - iter 582/972 - loss 5.92459174 - samples/sec: 61.09 - lr: 0.302446
2023-05-28 01:33:06,742 epoch 7 - iter 679/972 - loss 5.85316375 - samples/sec: 53.92 - lr: 0.293577
2023-05-28 01:33:58,817 epoch 7 - iter 776/972 - loss 5.84243145 - samples/sec: 59.63 - lr: 0.284709
2023-05-28 01:34:50,601 epoch 7 - iter 873/972 - loss 5.78546476 - samples/sec: 59.96 - lr: 0.275840
2023-05-28 01:35:45,325 epoch 7 - iter 970/972 - loss 5.70883882 - samples/sec: 56.74 - lr: 0.266971
2023-05-28 01:35:46,064 ----------------------------------------------------------------------------------------------------
2023-05-28 01:35:46,064 EPOCH 7 done: loss 5.7074 - lr 0.266971
2023-05-28 01:38:26,164 Evaluating as a multi-label problem: False
2023-05-28 01:38:26,265 DEV : loss 2.3297345638275146 - f1-score (micro avg)  0.6102
2023-05-28 01:38:26,453 BAD EPOCHS (no improvement): 4
2023-05-28 01:38:26,456 ----------------------------------------------------------------------------------------------------
2023-05-28 01:39:18,586 epoch 8 - iter 97/972 - loss 4.96268994 - samples/sec: 59.57 - lr: 0.257920
2023-05-28 01:40:13,946 epoch 8 - iter 194/972 - loss 5.03186970 - samples/sec: 56.09 - lr: 0.249051
2023-05-28 01:41:07,157 epoch 8 - iter 291/972 - loss 4.90548992 - samples/sec: 58.35 - lr: 0.240183
2023-05-28 01:41:58,991 epoch 8 - iter 388/972 - loss 4.82414770 - samples/sec: 59.91 - lr: 0.231314
2023-05-28 01:42:56,303 epoch 8 - iter 485/972 - loss 4.80806592 - samples/sec: 54.18 - lr: 0.222446
2023-05-28 01:43:46,236 epoch 8 - iter 582/972 - loss 4.83816028 - samples/sec: 62.19 - lr: 0.213577
2023-05-28 01:44:43,161 epoch 8 - iter 679/972 - loss 4.78366982 - samples/sec: 54.55 - lr: 0.204709
2023-05-28 01:45:35,250 epoch 8 - iter 776/972 - loss 4.70134884 - samples/sec: 59.61 - lr: 0.195840
2023-05-28 01:46:28,583 epoch 8 - iter 873/972 - loss 4.60832720 - samples/sec: 58.22 - lr: 0.186971
2023-05-28 01:47:26,440 epoch 8 - iter 970/972 - loss 4.52484984 - samples/sec: 53.66 - lr: 0.178103
2023-05-28 01:47:27,132 ----------------------------------------------------------------------------------------------------
2023-05-28 01:47:27,132 EPOCH 8 done: loss 4.5247 - lr 0.178103
2023-05-28 01:50:10,331 Evaluating as a multi-label problem: False
2023-05-28 01:50:10,439 DEV : loss 1.7073806524276733 - f1-score (micro avg)  0.6224
2023-05-28 01:50:10,632 BAD EPOCHS (no improvement): 4
2023-05-28 01:50:10,635 ----------------------------------------------------------------------------------------------------
2023-05-28 01:51:04,171 epoch 9 - iter 97/972 - loss 3.94963873 - samples/sec: 58.01 - lr: 0.169051
2023-05-28 01:51:58,961 epoch 9 - iter 194/972 - loss 3.86609050 - samples/sec: 56.67 - lr: 0.160183
2023-05-28 01:52:51,585 epoch 9 - iter 291/972 - loss 3.70547311 - samples/sec: 59.01 - lr: 0.151314
2023-05-28 01:53:47,441 epoch 9 - iter 388/972 - loss 3.65175528 - samples/sec: 55.59 - lr: 0.142446
2023-05-28 01:54:39,940 epoch 9 - iter 485/972 - loss 3.52368968 - samples/sec: 59.15 - lr: 0.133577
2023-05-28 01:55:29,918 epoch 9 - iter 582/972 - loss 3.42235522 - samples/sec: 62.13 - lr: 0.124709
2023-05-28 01:56:27,511 epoch 9 - iter 679/972 - loss 3.31911166 - samples/sec: 53.91 - lr: 0.115840
2023-05-28 01:57:19,589 epoch 9 - iter 776/972 - loss 3.22014497 - samples/sec: 59.62 - lr: 0.106971
2023-05-28 01:58:15,619 epoch 9 - iter 873/972 - loss 3.14332727 - samples/sec: 55.42 - lr: 0.098103
2023-05-28 01:59:05,894 epoch 9 - iter 970/972 - loss 3.07802582 - samples/sec: 61.76 - lr: 0.089234
2023-05-28 01:59:06,548 ----------------------------------------------------------------------------------------------------
2023-05-28 01:59:06,548 EPOCH 9 done: loss 3.0768 - lr 0.089234
2023-05-28 02:01:44,745 Evaluating as a multi-label problem: False
2023-05-28 02:01:44,846 DEV : loss 0.9417978525161743 - f1-score (micro avg)  0.6804
2023-05-28 02:01:45,087 BAD EPOCHS (no improvement): 4
2023-05-28 02:01:45,090 ----------------------------------------------------------------------------------------------------
2023-05-28 02:02:36,571 epoch 10 - iter 97/972 - loss 2.10024314 - samples/sec: 60.32 - lr: 0.080183
2023-05-28 02:03:32,914 epoch 10 - iter 194/972 - loss 2.10732310 - samples/sec: 55.11 - lr: 0.071314
2023-05-28 02:04:24,923 epoch 10 - iter 291/972 - loss 2.02615282 - samples/sec: 59.71 - lr: 0.062446
2023-05-28 02:05:22,626 epoch 10 - iter 388/972 - loss 1.94776736 - samples/sec: 53.81 - lr: 0.053577
2023-05-28 02:06:16,161 epoch 10 - iter 485/972 - loss 1.88469171 - samples/sec: 58.00 - lr: 0.044709
2023-05-28 02:07:06,447 epoch 10 - iter 582/972 - loss 1.80225956 - samples/sec: 61.75 - lr: 0.035840
2023-05-28 02:08:03,219 epoch 10 - iter 679/972 - loss 1.75085495 - samples/sec: 54.69 - lr: 0.026971
2023-05-28 02:08:55,113 epoch 10 - iter 776/972 - loss 1.68657141 - samples/sec: 59.84 - lr: 0.018103
2023-05-28 02:09:53,069 epoch 10 - iter 873/972 - loss 1.62458125 - samples/sec: 53.58 - lr: 0.009234
2023-05-28 02:10:44,401 epoch 10 - iter 970/972 - loss 1.56432973 - samples/sec: 60.50 - lr: 0.000366
2023-05-28 02:10:44,984 ----------------------------------------------------------------------------------------------------
2023-05-28 02:10:44,984 EPOCH 10 done: loss 1.5643 - lr 0.000366
2023-05-28 02:13:26,942 Evaluating as a multi-label problem: False
2023-05-28 02:13:27,013 DEV : loss 0.3707886338233948 - f1-score (micro avg)  0.7614
2023-05-28 02:13:27,206 BAD EPOCHS (no improvement): 4
2023-05-28 02:13:39,453 ----------------------------------------------------------------------------------------------------
2023-05-28 02:13:39,456 Testing using last state of model ...
2023-05-28 02:17:28,004 Evaluating as a multi-label problem: False
2023-05-28 02:17:28,116 0.757	0.6993	0.727	0.6099
2023-05-28 02:17:28,117 
Results:
- F-score (micro) 0.727
- F-score (macro) 0.7119
- Accuracy 0.6099

By class:
              precision    recall  f1-score   support

         PER     0.8697    0.8899    0.8797      2715
         ORG     0.6321    0.6013    0.6163      2543
         LOC     0.7892    0.7330    0.7601      2442
        MISC     0.6966    0.5140    0.5915      1889

   micro avg     0.7570    0.6993    0.7270      9589
   macro avg     0.7469    0.6845    0.7119      9589
weighted avg     0.7521    0.6993    0.7226      9589

2023-05-28 02:17:28,117 ----------------------------------------------------------------------------------------------------
2023-05-28 02:17:28,117 ----------------------------------------------------------------------------------------------------
2023-05-28 02:19:50,009 Evaluating as a multi-label problem: False
2023-05-28 02:19:50,056 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-28 02:19:50,057 0.7191	0.6166	0.6639	0.5374
2023-05-28 02:19:50,057 ----------------------------------------------------------------------------------------------------
2023-05-28 02:21:14,463 Evaluating as a multi-label problem: False
2023-05-28 02:21:14,536 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-28 02:21:14,537 0.7803	0.7571	0.7685	0.6606
