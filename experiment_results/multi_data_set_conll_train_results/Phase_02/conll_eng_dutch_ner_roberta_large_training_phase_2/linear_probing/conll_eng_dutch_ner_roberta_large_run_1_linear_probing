2023-05-27 16:36:05,085 ----------------------------------------------------------------------------------------------------
2023-05-27 16:36:05,090 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-27 16:36:05,091 ----------------------------------------------------------------------------------------------------
2023-05-27 16:36:05,092 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-27 16:36:05,095 ----------------------------------------------------------------------------------------------------
2023-05-27 16:36:05,095 Parameters:
2023-05-27 16:36:05,095  - learning_rate: "0.800000"
2023-05-27 16:36:05,095  - mini_batch_size: "32"
2023-05-27 16:36:05,095  - patience: "3"
2023-05-27 16:36:05,095  - anneal_factor: "0.5"
2023-05-27 16:36:05,095  - max_epochs: "10"
2023-05-27 16:36:05,095  - shuffle: "True"
2023-05-27 16:36:05,096  - train_with_dev: "False"
2023-05-27 16:36:05,096  - batch_growth_annealing: "False"
2023-05-27 16:36:05,096 ----------------------------------------------------------------------------------------------------
2023-05-27 16:36:05,096 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_1_linear_probing"
2023-05-27 16:36:05,096 ----------------------------------------------------------------------------------------------------
2023-05-27 16:36:05,096 Device: cuda:3
2023-05-27 16:36:05,096 ----------------------------------------------------------------------------------------------------
2023-05-27 16:36:05,097 Embeddings storage mode: none
2023-05-27 16:36:05,098 ----------------------------------------------------------------------------------------------------
2023-05-27 16:36:59,639 epoch 1 - iter 97/972 - loss 0.85814485 - samples/sec: 56.93 - lr: 0.079835
2023-05-27 16:37:52,139 epoch 1 - iter 194/972 - loss 1.11011559 - samples/sec: 59.15 - lr: 0.159671
2023-05-27 16:38:45,768 epoch 1 - iter 291/972 - loss 1.69923153 - samples/sec: 57.90 - lr: 0.239506
2023-05-27 16:39:37,300 epoch 1 - iter 388/972 - loss 2.27764989 - samples/sec: 60.26 - lr: 0.319342
2023-05-27 16:40:32,864 epoch 1 - iter 485/972 - loss 2.97711407 - samples/sec: 55.88 - lr: 0.399177
2023-05-27 16:41:21,505 epoch 1 - iter 582/972 - loss 4.02133580 - samples/sec: 63.84 - lr: 0.479012
2023-05-27 16:42:11,632 epoch 1 - iter 679/972 - loss 4.85917933 - samples/sec: 61.95 - lr: 0.558848
2023-05-27 16:42:58,369 epoch 1 - iter 776/972 - loss 5.61813840 - samples/sec: 66.44 - lr: 0.638683
2023-05-27 16:43:50,088 epoch 1 - iter 873/972 - loss 6.35661743 - samples/sec: 60.04 - lr: 0.718519
2023-05-27 16:44:35,017 epoch 1 - iter 970/972 - loss 7.00614207 - samples/sec: 69.11 - lr: 0.798354
2023-05-27 16:44:35,439 ----------------------------------------------------------------------------------------------------
2023-05-27 16:44:35,439 EPOCH 1 done: loss 7.0148 - lr 0.798354
2023-05-27 16:47:11,866 Evaluating as a multi-label problem: False
2023-05-27 16:47:11,932 DEV : loss 4.485673904418945 - f1-score (micro avg)  0.608
2023-05-27 16:47:12,086 BAD EPOCHS (no improvement): 4
2023-05-27 16:47:12,166 ----------------------------------------------------------------------------------------------------
2023-05-27 16:48:02,545 epoch 2 - iter 97/972 - loss 12.12185750 - samples/sec: 61.65 - lr: 0.791131
2023-05-27 16:48:54,112 epoch 2 - iter 194/972 - loss 11.97748020 - samples/sec: 60.22 - lr: 0.782263
2023-05-27 16:49:50,939 epoch 2 - iter 291/972 - loss 11.70717745 - samples/sec: 54.64 - lr: 0.773394
2023-05-27 16:50:41,556 epoch 2 - iter 388/972 - loss 11.61420606 - samples/sec: 61.35 - lr: 0.764526
2023-05-27 16:51:37,221 epoch 2 - iter 485/972 - loss 11.59832453 - samples/sec: 55.78 - lr: 0.755657
2023-05-27 16:52:28,591 epoch 2 - iter 582/972 - loss 11.70976765 - samples/sec: 60.45 - lr: 0.746789
2023-05-27 16:53:23,221 epoch 2 - iter 679/972 - loss 11.78629754 - samples/sec: 56.84 - lr: 0.737920
2023-05-27 16:54:13,552 epoch 2 - iter 776/972 - loss 11.76828803 - samples/sec: 61.70 - lr: 0.729051
2023-05-27 16:55:09,436 epoch 2 - iter 873/972 - loss 11.73898361 - samples/sec: 55.56 - lr: 0.720183
2023-05-27 16:56:00,454 epoch 2 - iter 970/972 - loss 11.53610845 - samples/sec: 60.87 - lr: 0.711314
2023-05-27 16:56:01,158 ----------------------------------------------------------------------------------------------------
2023-05-27 16:56:01,158 EPOCH 2 done: loss 11.5329 - lr 0.711314
2023-05-27 16:58:36,519 Evaluating as a multi-label problem: False
2023-05-27 16:58:36,632 DEV : loss 4.0223517417907715 - f1-score (micro avg)  0.5476
2023-05-27 16:58:36,790 BAD EPOCHS (no improvement): 4
2023-05-27 16:58:36,792 ----------------------------------------------------------------------------------------------------
2023-05-27 16:59:31,857 epoch 3 - iter 97/972 - loss 10.05692766 - samples/sec: 56.39 - lr: 0.702263
2023-05-27 17:00:23,083 epoch 3 - iter 194/972 - loss 10.39200815 - samples/sec: 60.62 - lr: 0.693394
2023-05-27 17:01:19,062 epoch 3 - iter 291/972 - loss 10.60399366 - samples/sec: 55.47 - lr: 0.684526
2023-05-27 17:02:10,109 epoch 3 - iter 388/972 - loss 10.78358525 - samples/sec: 60.83 - lr: 0.675657
2023-05-27 17:03:04,562 epoch 3 - iter 485/972 - loss 10.88511436 - samples/sec: 57.03 - lr: 0.666789
2023-05-27 17:03:56,102 epoch 3 - iter 582/972 - loss 10.72585850 - samples/sec: 60.25 - lr: 0.657920
2023-05-27 17:04:50,598 epoch 3 - iter 679/972 - loss 10.53847074 - samples/sec: 56.98 - lr: 0.649051
2023-05-27 17:05:41,615 epoch 3 - iter 776/972 - loss 10.48366578 - samples/sec: 60.87 - lr: 0.640183
2023-05-27 17:06:31,007 epoch 3 - iter 873/972 - loss 10.49455942 - samples/sec: 62.87 - lr: 0.631314
2023-05-27 17:07:25,972 epoch 3 - iter 970/972 - loss 10.44781336 - samples/sec: 56.49 - lr: 0.622446
2023-05-27 17:07:26,635 ----------------------------------------------------------------------------------------------------
2023-05-27 17:07:26,636 EPOCH 3 done: loss 10.4501 - lr 0.622446
2023-05-27 17:10:01,825 Evaluating as a multi-label problem: False
2023-05-27 17:10:01,927 DEV : loss 4.549117088317871 - f1-score (micro avg)  0.5767
2023-05-27 17:10:02,131 BAD EPOCHS (no improvement): 4
2023-05-27 17:10:02,133 ----------------------------------------------------------------------------------------------------
2023-05-27 17:10:57,219 epoch 4 - iter 97/972 - loss 9.70824289 - samples/sec: 56.37 - lr: 0.613394
2023-05-27 17:11:48,125 epoch 4 - iter 194/972 - loss 10.25517793 - samples/sec: 61.00 - lr: 0.604526
2023-05-27 17:12:38,680 epoch 4 - iter 291/972 - loss 10.08833586 - samples/sec: 61.42 - lr: 0.595657
2023-05-27 17:13:35,845 epoch 4 - iter 388/972 - loss 10.07850328 - samples/sec: 54.32 - lr: 0.586789
2023-05-27 17:14:27,143 epoch 4 - iter 485/972 - loss 9.95331472 - samples/sec: 60.54 - lr: 0.577920
2023-05-27 17:15:22,588 epoch 4 - iter 582/972 - loss 9.81977267 - samples/sec: 56.01 - lr: 0.569051
2023-05-27 17:16:13,671 epoch 4 - iter 679/972 - loss 9.74843314 - samples/sec: 60.79 - lr: 0.560183
2023-05-27 17:17:08,315 epoch 4 - iter 776/972 - loss 9.65785812 - samples/sec: 56.83 - lr: 0.551314
2023-05-27 17:17:59,396 epoch 4 - iter 873/972 - loss 9.52090751 - samples/sec: 60.79 - lr: 0.542446
2023-05-27 17:18:54,147 epoch 4 - iter 970/972 - loss 9.52586590 - samples/sec: 56.72 - lr: 0.533577
2023-05-27 17:18:54,820 ----------------------------------------------------------------------------------------------------
2023-05-27 17:18:54,820 EPOCH 4 done: loss 9.5238 - lr 0.533577
2023-05-27 17:21:30,907 Evaluating as a multi-label problem: False
2023-05-27 17:21:31,005 DEV : loss 3.831136703491211 - f1-score (micro avg)  0.6155
2023-05-27 17:21:31,195 BAD EPOCHS (no improvement): 4
2023-05-27 17:21:31,198 ----------------------------------------------------------------------------------------------------
2023-05-27 17:22:20,790 epoch 5 - iter 97/972 - loss 8.97683852 - samples/sec: 62.62 - lr: 0.524526
2023-05-27 17:23:17,473 epoch 5 - iter 194/972 - loss 8.84795872 - samples/sec: 54.78 - lr: 0.515657
2023-05-27 17:24:09,382 epoch 5 - iter 291/972 - loss 8.60753611 - samples/sec: 59.82 - lr: 0.506789
2023-05-27 17:25:04,925 epoch 5 - iter 388/972 - loss 8.45156829 - samples/sec: 55.91 - lr: 0.497920
2023-05-27 17:25:54,678 epoch 5 - iter 485/972 - loss 8.33812027 - samples/sec: 62.41 - lr: 0.489051
2023-05-27 17:26:50,369 epoch 5 - iter 582/972 - loss 8.27433366 - samples/sec: 55.76 - lr: 0.480183
2023-05-27 17:27:41,016 epoch 5 - iter 679/972 - loss 8.23444158 - samples/sec: 61.31 - lr: 0.471314
2023-05-27 17:28:35,932 epoch 5 - iter 776/972 - loss 8.10580198 - samples/sec: 56.54 - lr: 0.462446
2023-05-27 17:29:25,690 epoch 5 - iter 873/972 - loss 8.07390778 - samples/sec: 62.41 - lr: 0.453577
2023-05-27 17:30:18,770 epoch 5 - iter 970/972 - loss 8.04514803 - samples/sec: 58.50 - lr: 0.444709
2023-05-27 17:30:19,346 ----------------------------------------------------------------------------------------------------
2023-05-27 17:30:19,347 EPOCH 5 done: loss 8.0439 - lr 0.444709
2023-05-27 17:33:00,448 Evaluating as a multi-label problem: False
2023-05-27 17:33:00,546 DEV : loss 3.906965494155884 - f1-score (micro avg)  0.5665
2023-05-27 17:33:00,745 BAD EPOCHS (no improvement): 4
2023-05-27 17:33:00,747 ----------------------------------------------------------------------------------------------------
2023-05-27 17:33:50,872 epoch 6 - iter 97/972 - loss 8.06940007 - samples/sec: 61.96 - lr: 0.435657
2023-05-27 17:34:45,075 epoch 6 - iter 194/972 - loss 8.12403050 - samples/sec: 57.29 - lr: 0.426789
2023-05-27 17:35:36,337 epoch 6 - iter 291/972 - loss 8.00722122 - samples/sec: 60.58 - lr: 0.417920
2023-05-27 17:36:28,372 epoch 6 - iter 388/972 - loss 7.68841688 - samples/sec: 59.68 - lr: 0.409051
2023-05-27 17:37:14,704 epoch 6 - iter 485/972 - loss 7.51538261 - samples/sec: 67.02 - lr: 0.400183
2023-05-27 17:38:07,356 epoch 6 - iter 582/972 - loss 7.42588921 - samples/sec: 58.98 - lr: 0.391314
2023-05-27 17:38:59,355 epoch 6 - iter 679/972 - loss 7.30454638 - samples/sec: 59.71 - lr: 0.382446
2023-05-27 17:39:48,304 epoch 6 - iter 776/972 - loss 7.21406783 - samples/sec: 63.44 - lr: 0.373577
2023-05-27 17:40:43,605 epoch 6 - iter 873/972 - loss 7.15604836 - samples/sec: 56.15 - lr: 0.364709
2023-05-27 17:41:31,969 epoch 6 - iter 970/972 - loss 7.13101582 - samples/sec: 64.21 - lr: 0.355840
2023-05-27 17:41:32,626 ----------------------------------------------------------------------------------------------------
2023-05-27 17:41:32,626 EPOCH 6 done: loss 7.1375 - lr 0.355840
2023-05-27 17:44:08,748 Evaluating as a multi-label problem: False
2023-05-27 17:44:08,845 DEV : loss 3.2476391792297363 - f1-score (micro avg)  0.6449
2023-05-27 17:44:09,021 BAD EPOCHS (no improvement): 4
2023-05-27 17:44:09,024 ----------------------------------------------------------------------------------------------------
2023-05-27 17:45:01,296 epoch 7 - iter 97/972 - loss 6.69758246 - samples/sec: 59.41 - lr: 0.346789
2023-05-27 17:45:52,156 epoch 7 - iter 194/972 - loss 6.78199424 - samples/sec: 61.06 - lr: 0.337920
2023-05-27 17:46:43,088 epoch 7 - iter 291/972 - loss 6.52919701 - samples/sec: 60.96 - lr: 0.329051
2023-05-27 17:47:32,027 epoch 7 - iter 388/972 - loss 6.40118427 - samples/sec: 63.46 - lr: 0.320183
2023-05-27 17:48:29,663 epoch 7 - iter 485/972 - loss 6.28121677 - samples/sec: 53.87 - lr: 0.311314
2023-05-27 17:49:20,611 epoch 7 - iter 582/972 - loss 6.16356207 - samples/sec: 60.95 - lr: 0.302446
2023-05-27 17:50:15,874 epoch 7 - iter 679/972 - loss 6.11607091 - samples/sec: 56.19 - lr: 0.293577
2023-05-27 17:51:03,447 epoch 7 - iter 776/972 - loss 6.07290724 - samples/sec: 65.27 - lr: 0.284709
2023-05-27 17:51:58,356 epoch 7 - iter 873/972 - loss 6.00032222 - samples/sec: 56.55 - lr: 0.275840
2023-05-27 17:52:46,973 epoch 7 - iter 970/972 - loss 5.93874033 - samples/sec: 63.87 - lr: 0.266971
2023-05-27 17:52:47,529 ----------------------------------------------------------------------------------------------------
2023-05-27 17:52:47,529 EPOCH 7 done: loss 5.9422 - lr 0.266971
2023-05-27 17:55:14,213 Evaluating as a multi-label problem: False
2023-05-27 17:55:14,307 DEV : loss 1.9678252935409546 - f1-score (micro avg)  0.6548
2023-05-27 17:55:14,502 BAD EPOCHS (no improvement): 4
2023-05-27 17:55:14,504 ----------------------------------------------------------------------------------------------------
2023-05-27 17:56:08,657 epoch 8 - iter 97/972 - loss 5.17923712 - samples/sec: 57.34 - lr: 0.257920
2023-05-27 17:57:00,055 epoch 8 - iter 194/972 - loss 4.90605111 - samples/sec: 60.41 - lr: 0.249051
2023-05-27 17:57:54,659 epoch 8 - iter 291/972 - loss 4.84864791 - samples/sec: 56.86 - lr: 0.240183
2023-05-27 17:58:43,257 epoch 8 - iter 388/972 - loss 4.91253831 - samples/sec: 63.90 - lr: 0.231314
2023-05-27 17:59:37,616 epoch 8 - iter 485/972 - loss 4.81048370 - samples/sec: 57.12 - lr: 0.222446
2023-05-27 18:00:29,056 epoch 8 - iter 582/972 - loss 4.73212022 - samples/sec: 60.37 - lr: 0.213577
2023-05-27 18:01:20,417 epoch 8 - iter 679/972 - loss 4.68949830 - samples/sec: 60.46 - lr: 0.204709
2023-05-27 18:02:13,486 epoch 8 - iter 776/972 - loss 4.62460031 - samples/sec: 58.51 - lr: 0.195840
2023-05-27 18:03:03,104 epoch 8 - iter 873/972 - loss 4.56181592 - samples/sec: 62.58 - lr: 0.186971
2023-05-27 18:03:55,905 epoch 8 - iter 970/972 - loss 4.49652634 - samples/sec: 58.81 - lr: 0.178103
2023-05-27 18:03:56,570 ----------------------------------------------------------------------------------------------------
2023-05-27 18:03:56,570 EPOCH 8 done: loss 4.4935 - lr 0.178103
2023-05-27 18:06:27,686 Evaluating as a multi-label problem: False
2023-05-27 18:06:27,798 DEV : loss 1.4401040077209473 - f1-score (micro avg)  0.6563
2023-05-27 18:06:28,004 BAD EPOCHS (no improvement): 4
2023-05-27 18:06:28,006 ----------------------------------------------------------------------------------------------------
2023-05-27 18:07:22,250 epoch 9 - iter 97/972 - loss 4.00194384 - samples/sec: 57.25 - lr: 0.169051
2023-05-27 18:08:10,870 epoch 9 - iter 194/972 - loss 3.91080095 - samples/sec: 63.87 - lr: 0.160183
2023-05-27 18:09:00,210 epoch 9 - iter 291/972 - loss 3.77430998 - samples/sec: 62.94 - lr: 0.151314
2023-05-27 18:09:55,526 epoch 9 - iter 388/972 - loss 3.65939737 - samples/sec: 56.15 - lr: 0.142446
2023-05-27 18:10:47,051 epoch 9 - iter 485/972 - loss 3.55524244 - samples/sec: 60.27 - lr: 0.133577
2023-05-27 18:11:41,474 epoch 9 - iter 582/972 - loss 3.44715959 - samples/sec: 57.06 - lr: 0.124709
2023-05-27 18:12:30,067 epoch 9 - iter 679/972 - loss 3.33281126 - samples/sec: 63.90 - lr: 0.115840
2023-05-27 18:13:23,176 epoch 9 - iter 776/972 - loss 3.26055964 - samples/sec: 58.47 - lr: 0.106971
2023-05-27 18:14:12,472 epoch 9 - iter 873/972 - loss 3.16932531 - samples/sec: 62.99 - lr: 0.098103
2023-05-27 18:15:07,210 epoch 9 - iter 970/972 - loss 3.08848195 - samples/sec: 56.73 - lr: 0.089234
2023-05-27 18:15:07,898 ----------------------------------------------------------------------------------------------------
2023-05-27 18:15:07,898 EPOCH 9 done: loss 3.0864 - lr 0.089234
2023-05-27 18:17:39,765 Evaluating as a multi-label problem: False
2023-05-27 18:17:39,862 DEV : loss 0.7812352180480957 - f1-score (micro avg)  0.6909
2023-05-27 18:17:40,044 BAD EPOCHS (no improvement): 4
2023-05-27 18:17:40,056 ----------------------------------------------------------------------------------------------------
2023-05-27 18:18:31,340 epoch 10 - iter 97/972 - loss 2.22763607 - samples/sec: 60.56 - lr: 0.080183
2023-05-27 18:19:23,561 epoch 10 - iter 194/972 - loss 2.17750119 - samples/sec: 59.46 - lr: 0.071314
2023-05-27 18:20:09,774 epoch 10 - iter 291/972 - loss 2.09869013 - samples/sec: 67.19 - lr: 0.062446
2023-05-27 18:21:04,782 epoch 10 - iter 388/972 - loss 2.01948307 - samples/sec: 56.45 - lr: 0.053577
2023-05-27 18:21:55,465 epoch 10 - iter 485/972 - loss 1.93899113 - samples/sec: 61.27 - lr: 0.044709
2023-05-27 18:22:49,795 epoch 10 - iter 582/972 - loss 1.86003625 - samples/sec: 57.15 - lr: 0.035840
2023-05-27 18:23:40,227 epoch 10 - iter 679/972 - loss 1.79471369 - samples/sec: 61.57 - lr: 0.026971
2023-05-27 18:24:29,205 epoch 10 - iter 776/972 - loss 1.72423015 - samples/sec: 63.40 - lr: 0.018103
2023-05-27 18:25:23,381 epoch 10 - iter 873/972 - loss 1.65401358 - samples/sec: 57.31 - lr: 0.009234
2023-05-27 18:26:13,265 epoch 10 - iter 970/972 - loss 1.58987020 - samples/sec: 62.25 - lr: 0.000366
2023-05-27 18:26:13,906 ----------------------------------------------------------------------------------------------------
2023-05-27 18:26:13,907 EPOCH 10 done: loss 1.5892 - lr 0.000366
2023-05-27 18:28:47,197 Evaluating as a multi-label problem: False
2023-05-27 18:28:47,312 DEV : loss 0.3765501379966736 - f1-score (micro avg)  0.7504
2023-05-27 18:28:47,493 BAD EPOCHS (no improvement): 4
2023-05-27 18:29:00,176 ----------------------------------------------------------------------------------------------------
2023-05-27 18:29:00,178 Testing using last state of model ...
2023-05-27 18:32:32,622 Evaluating as a multi-label problem: False
2023-05-27 18:32:32,725 0.7444	0.6845	0.7132	0.5943
2023-05-27 18:32:32,725 
Results:
- F-score (micro) 0.7132
- F-score (macro) 0.6955
- Accuracy 0.5943

By class:
              precision    recall  f1-score   support

         PER     0.8589    0.8836    0.8711      2715
         LOC     0.7580    0.7490    0.7535      2442
         ORG     0.6209    0.5604    0.5891      2543
        MISC     0.6917    0.4823    0.5683      1889

   micro avg     0.7444    0.6845    0.7132      9589
   macro avg     0.7324    0.6688    0.6955      9589
weighted avg     0.7372    0.6845    0.7067      9589

2023-05-27 18:32:32,725 ----------------------------------------------------------------------------------------------------
2023-05-27 18:32:32,726 ----------------------------------------------------------------------------------------------------
2023-05-27 18:34:40,465 Evaluating as a multi-label problem: False
2023-05-27 18:34:40,503 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-27 18:34:40,504 0.7222	0.6054	0.6587	0.5309
2023-05-27 18:34:40,504 ----------------------------------------------------------------------------------------------------
2023-05-27 18:36:01,168 Evaluating as a multi-label problem: False
2023-05-27 18:36:01,231 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-27 18:36:01,231 0.7579	0.7399	0.7488	0.6379
