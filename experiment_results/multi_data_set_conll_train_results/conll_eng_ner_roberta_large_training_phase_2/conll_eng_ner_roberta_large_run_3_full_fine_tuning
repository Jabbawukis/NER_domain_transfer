2023-05-27 04:29:22,486 ----------------------------------------------------------------------------------------------------
2023-05-27 04:29:22,491 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-27 04:29:22,493 ----------------------------------------------------------------------------------------------------
2023-05-27 04:29:22,493 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-05-27 04:29:22,493 ----------------------------------------------------------------------------------------------------
2023-05-27 04:29:22,493 Parameters:
2023-05-27 04:29:22,493  - learning_rate: "0.000005"
2023-05-27 04:29:22,493  - mini_batch_size: "4"
2023-05-27 04:29:22,493  - patience: "3"
2023-05-27 04:29:22,493  - anneal_factor: "0.5"
2023-05-27 04:29:22,493  - max_epochs: "10"
2023-05-27 04:29:22,493  - shuffle: "True"
2023-05-27 04:29:22,494  - train_with_dev: "False"
2023-05-27 04:29:22,494  - batch_growth_annealing: "False"
2023-05-27 04:29:22,494 ----------------------------------------------------------------------------------------------------
2023-05-27 04:29:22,494 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_3_full_fine_tuning"
2023-05-27 04:29:22,494 ----------------------------------------------------------------------------------------------------
2023-05-27 04:29:22,494 Device: cuda:3
2023-05-27 04:29:22,494 ----------------------------------------------------------------------------------------------------
2023-05-27 04:29:22,494 Embeddings storage mode: none
2023-05-27 04:29:22,494 ----------------------------------------------------------------------------------------------------
2023-05-27 04:30:46,861 epoch 1 - iter 374/3747 - loss 1.39342897 - samples/sec: 17.74 - lr: 0.000000
2023-05-27 04:32:10,372 epoch 1 - iter 748/3747 - loss 1.21514099 - samples/sec: 17.92 - lr: 0.000001
2023-05-27 04:33:33,168 epoch 1 - iter 1122/3747 - loss 1.02571614 - samples/sec: 18.08 - lr: 0.000001
2023-05-27 04:34:54,665 epoch 1 - iter 1496/3747 - loss 0.89705153 - samples/sec: 18.36 - lr: 0.000002
2023-05-27 04:36:18,313 epoch 1 - iter 1870/3747 - loss 0.78802952 - samples/sec: 17.89 - lr: 0.000002
2023-05-27 04:37:42,870 epoch 1 - iter 2244/3747 - loss 0.69669821 - samples/sec: 17.70 - lr: 0.000003
2023-05-27 04:39:06,862 epoch 1 - iter 2618/3747 - loss 0.63337542 - samples/sec: 17.82 - lr: 0.000003
2023-05-27 04:40:34,318 epoch 1 - iter 2992/3747 - loss 0.58971256 - samples/sec: 17.11 - lr: 0.000004
2023-05-27 04:41:58,733 epoch 1 - iter 3366/3747 - loss 0.55102817 - samples/sec: 17.73 - lr: 0.000004
2023-05-27 04:43:22,748 epoch 1 - iter 3740/3747 - loss 0.51259194 - samples/sec: 17.81 - lr: 0.000005
2023-05-27 04:43:24,248 ----------------------------------------------------------------------------------------------------
2023-05-27 04:43:24,248 EPOCH 1 done: loss 0.5126 - lr 0.000005
2023-05-27 04:44:39,805 Evaluating as a multi-label problem: False
2023-05-27 04:44:39,871 DEV : loss 0.14666864275932312 - f1-score (micro avg)  0.9173
2023-05-27 04:44:39,971 BAD EPOCHS (no improvement): 4
2023-05-27 04:44:39,974 ----------------------------------------------------------------------------------------------------
2023-05-27 04:46:05,608 epoch 2 - iter 374/3747 - loss 0.24690818 - samples/sec: 17.48 - lr: 0.000005
2023-05-27 04:47:31,024 epoch 2 - iter 748/3747 - loss 0.25102571 - samples/sec: 17.52 - lr: 0.000005
2023-05-27 04:48:53,793 epoch 2 - iter 1122/3747 - loss 0.23574448 - samples/sec: 18.08 - lr: 0.000005
2023-05-27 04:50:17,866 epoch 2 - iter 1496/3747 - loss 0.23549215 - samples/sec: 17.80 - lr: 0.000005
2023-05-27 04:51:42,317 epoch 2 - iter 1870/3747 - loss 0.23211089 - samples/sec: 17.72 - lr: 0.000005
2023-05-27 04:53:06,707 epoch 2 - iter 2244/3747 - loss 0.22734270 - samples/sec: 17.73 - lr: 0.000005
2023-05-27 04:54:30,783 epoch 2 - iter 2618/3747 - loss 0.22854517 - samples/sec: 17.80 - lr: 0.000005
2023-05-27 04:55:55,093 epoch 2 - iter 2992/3747 - loss 0.22551461 - samples/sec: 17.75 - lr: 0.000005
2023-05-27 04:57:19,651 epoch 2 - iter 3366/3747 - loss 0.22523642 - samples/sec: 17.70 - lr: 0.000005
2023-05-27 04:58:42,438 epoch 2 - iter 3740/3747 - loss 0.21779636 - samples/sec: 18.08 - lr: 0.000004
2023-05-27 04:58:43,958 ----------------------------------------------------------------------------------------------------
2023-05-27 04:58:43,959 EPOCH 2 done: loss 0.2175 - lr 0.000004
2023-05-27 04:59:58,281 Evaluating as a multi-label problem: False
2023-05-27 04:59:58,348 DEV : loss 0.09420643746852875 - f1-score (micro avg)  0.9488
2023-05-27 04:59:58,447 BAD EPOCHS (no improvement): 4
2023-05-27 04:59:58,451 ----------------------------------------------------------------------------------------------------
2023-05-27 05:01:22,436 epoch 3 - iter 374/3747 - loss 0.14701366 - samples/sec: 17.82 - lr: 0.000004
2023-05-27 05:02:46,556 epoch 3 - iter 748/3747 - loss 0.15003214 - samples/sec: 17.79 - lr: 0.000004
2023-05-27 05:04:08,858 epoch 3 - iter 1122/3747 - loss 0.16086448 - samples/sec: 18.18 - lr: 0.000004
2023-05-27 05:05:32,015 epoch 3 - iter 1496/3747 - loss 0.17035383 - samples/sec: 18.00 - lr: 0.000004
2023-05-27 05:06:55,340 epoch 3 - iter 1870/3747 - loss 0.16596564 - samples/sec: 17.96 - lr: 0.000004
2023-05-27 05:08:19,896 epoch 3 - iter 2244/3747 - loss 0.16285055 - samples/sec: 17.70 - lr: 0.000004
2023-05-27 05:09:42,759 epoch 3 - iter 2618/3747 - loss 0.16330507 - samples/sec: 18.06 - lr: 0.000004
2023-05-27 05:11:07,609 epoch 3 - iter 2992/3747 - loss 0.16286053 - samples/sec: 17.64 - lr: 0.000004
2023-05-27 05:12:31,330 epoch 3 - iter 3366/3747 - loss 0.16261962 - samples/sec: 17.88 - lr: 0.000004
2023-05-27 05:13:53,472 epoch 3 - iter 3740/3747 - loss 0.16325843 - samples/sec: 18.22 - lr: 0.000004
2023-05-27 05:13:55,061 ----------------------------------------------------------------------------------------------------
2023-05-27 05:13:55,062 EPOCH 3 done: loss 0.1633 - lr 0.000004
2023-05-27 05:15:11,565 Evaluating as a multi-label problem: False
2023-05-27 05:15:11,627 DEV : loss 0.089652419090271 - f1-score (micro avg)  0.9544
2023-05-27 05:15:11,729 BAD EPOCHS (no improvement): 4
2023-05-27 05:15:11,731 ----------------------------------------------------------------------------------------------------
2023-05-27 05:16:36,813 epoch 4 - iter 374/3747 - loss 0.12165954 - samples/sec: 17.59 - lr: 0.000004
2023-05-27 05:17:59,585 epoch 4 - iter 748/3747 - loss 0.13664548 - samples/sec: 18.08 - lr: 0.000004
2023-05-27 05:19:22,385 epoch 4 - iter 1122/3747 - loss 0.15067807 - samples/sec: 18.08 - lr: 0.000004
2023-05-27 05:20:47,073 epoch 4 - iter 1496/3747 - loss 0.15064195 - samples/sec: 17.67 - lr: 0.000004
2023-05-27 05:22:09,413 epoch 4 - iter 1870/3747 - loss 0.14784027 - samples/sec: 18.18 - lr: 0.000004
2023-05-27 05:23:32,314 epoch 4 - iter 2244/3747 - loss 0.14580927 - samples/sec: 18.05 - lr: 0.000004
2023-05-27 05:24:56,233 epoch 4 - iter 2618/3747 - loss 0.14608173 - samples/sec: 17.83 - lr: 0.000004
2023-05-27 05:26:17,651 epoch 4 - iter 2992/3747 - loss 0.14272322 - samples/sec: 18.38 - lr: 0.000003
2023-05-27 05:27:39,794 epoch 4 - iter 3366/3747 - loss 0.14104092 - samples/sec: 18.22 - lr: 0.000003
2023-05-27 05:29:03,267 epoch 4 - iter 3740/3747 - loss 0.14004115 - samples/sec: 17.93 - lr: 0.000003
2023-05-27 05:29:04,768 ----------------------------------------------------------------------------------------------------
2023-05-27 05:29:04,768 EPOCH 4 done: loss 0.1399 - lr 0.000003
2023-05-27 05:30:21,838 Evaluating as a multi-label problem: False
2023-05-27 05:30:21,908 DEV : loss 0.0938301607966423 - f1-score (micro avg)  0.9618
2023-05-27 05:30:22,014 BAD EPOCHS (no improvement): 4
2023-05-27 05:30:22,017 ----------------------------------------------------------------------------------------------------
2023-05-27 05:31:45,925 epoch 5 - iter 374/3747 - loss 0.12525348 - samples/sec: 17.84 - lr: 0.000003
2023-05-27 05:33:10,710 epoch 5 - iter 748/3747 - loss 0.12244386 - samples/sec: 17.65 - lr: 0.000003
2023-05-27 05:34:32,957 epoch 5 - iter 1122/3747 - loss 0.12599675 - samples/sec: 18.20 - lr: 0.000003
2023-05-27 05:35:59,706 epoch 5 - iter 1496/3747 - loss 0.12407352 - samples/sec: 17.25 - lr: 0.000003
2023-05-27 05:37:23,052 epoch 5 - iter 1870/3747 - loss 0.13013186 - samples/sec: 17.96 - lr: 0.000003
2023-05-27 05:38:45,467 epoch 5 - iter 2244/3747 - loss 0.12664675 - samples/sec: 18.16 - lr: 0.000003
2023-05-27 05:40:07,293 epoch 5 - iter 2618/3747 - loss 0.12814414 - samples/sec: 18.29 - lr: 0.000003
2023-05-27 05:41:30,084 epoch 5 - iter 2992/3747 - loss 0.12523897 - samples/sec: 18.08 - lr: 0.000003
2023-05-27 05:42:53,865 epoch 5 - iter 3366/3747 - loss 0.12655932 - samples/sec: 17.86 - lr: 0.000003
2023-05-27 05:44:16,811 epoch 5 - iter 3740/3747 - loss 0.12603396 - samples/sec: 18.04 - lr: 0.000003
2023-05-27 05:44:18,228 ----------------------------------------------------------------------------------------------------
2023-05-27 05:44:18,228 EPOCH 5 done: loss 0.1259 - lr 0.000003
2023-05-27 05:45:33,095 Evaluating as a multi-label problem: False
2023-05-27 05:45:33,148 DEV : loss 0.08495156466960907 - f1-score (micro avg)  0.9642
2023-05-27 05:45:33,242 BAD EPOCHS (no improvement): 4
2023-05-27 05:45:33,245 ----------------------------------------------------------------------------------------------------
2023-05-27 05:46:57,648 epoch 6 - iter 374/3747 - loss 0.08861211 - samples/sec: 17.73 - lr: 0.000003
2023-05-27 05:48:21,194 epoch 6 - iter 748/3747 - loss 0.09328120 - samples/sec: 17.91 - lr: 0.000003
2023-05-27 05:49:45,314 epoch 6 - iter 1122/3747 - loss 0.09687441 - samples/sec: 17.79 - lr: 0.000003
2023-05-27 05:51:07,206 epoch 6 - iter 1496/3747 - loss 0.09515591 - samples/sec: 18.28 - lr: 0.000003
2023-05-27 05:52:30,860 epoch 6 - iter 1870/3747 - loss 0.09738069 - samples/sec: 17.89 - lr: 0.000003
2023-05-27 05:53:53,915 epoch 6 - iter 2244/3747 - loss 0.09694873 - samples/sec: 18.02 - lr: 0.000002
2023-05-27 05:55:17,635 epoch 6 - iter 2618/3747 - loss 0.09576950 - samples/sec: 17.88 - lr: 0.000002
2023-05-27 05:56:40,246 epoch 6 - iter 2992/3747 - loss 0.09648708 - samples/sec: 18.12 - lr: 0.000002
2023-05-27 05:58:02,988 epoch 6 - iter 3366/3747 - loss 0.09754732 - samples/sec: 18.09 - lr: 0.000002
2023-05-27 05:59:24,476 epoch 6 - iter 3740/3747 - loss 0.09998011 - samples/sec: 18.37 - lr: 0.000002
2023-05-27 05:59:25,961 ----------------------------------------------------------------------------------------------------
2023-05-27 05:59:25,961 EPOCH 6 done: loss 0.1001 - lr 0.000002
2023-05-27 06:00:48,159 Evaluating as a multi-label problem: False
2023-05-27 06:00:48,205 DEV : loss 0.08224068582057953 - f1-score (micro avg)  0.9658
2023-05-27 06:00:48,285 BAD EPOCHS (no improvement): 4
2023-05-27 06:00:48,288 ----------------------------------------------------------------------------------------------------
2023-05-27 06:02:11,884 epoch 7 - iter 374/3747 - loss 0.10928067 - samples/sec: 17.90 - lr: 0.000002
2023-05-27 06:03:36,053 epoch 7 - iter 748/3747 - loss 0.08485827 - samples/sec: 17.78 - lr: 0.000002
2023-05-27 06:05:01,449 epoch 7 - iter 1122/3747 - loss 0.08654177 - samples/sec: 17.53 - lr: 0.000002
2023-05-27 06:06:24,428 epoch 7 - iter 1496/3747 - loss 0.08748023 - samples/sec: 18.04 - lr: 0.000002
2023-05-27 06:07:47,960 epoch 7 - iter 1870/3747 - loss 0.08794211 - samples/sec: 17.92 - lr: 0.000002
2023-05-27 06:09:09,009 epoch 7 - iter 2244/3747 - loss 0.08883472 - samples/sec: 18.47 - lr: 0.000002
2023-05-27 06:10:32,734 epoch 7 - iter 2618/3747 - loss 0.08965464 - samples/sec: 17.88 - lr: 0.000002
2023-05-27 06:11:54,157 epoch 7 - iter 2992/3747 - loss 0.08892315 - samples/sec: 18.38 - lr: 0.000002
2023-05-27 06:13:16,295 epoch 7 - iter 3366/3747 - loss 0.09107992 - samples/sec: 18.22 - lr: 0.000002
2023-05-27 06:14:38,339 epoch 7 - iter 3740/3747 - loss 0.09186960 - samples/sec: 18.24 - lr: 0.000002
2023-05-27 06:14:39,773 ----------------------------------------------------------------------------------------------------
2023-05-27 06:14:39,773 EPOCH 7 done: loss 0.0919 - lr 0.000002
2023-05-27 06:15:57,711 Evaluating as a multi-label problem: False
2023-05-27 06:15:57,781 DEV : loss 0.09810381382703781 - f1-score (micro avg)  0.9672
2023-05-27 06:15:57,888 BAD EPOCHS (no improvement): 4
2023-05-27 06:15:57,898 ----------------------------------------------------------------------------------------------------
2023-05-27 06:17:22,927 epoch 8 - iter 374/3747 - loss 0.07596308 - samples/sec: 17.60 - lr: 0.000002
2023-05-27 06:18:46,563 epoch 8 - iter 748/3747 - loss 0.08052584 - samples/sec: 17.89 - lr: 0.000002
2023-05-27 06:20:09,563 epoch 8 - iter 1122/3747 - loss 0.07733366 - samples/sec: 18.03 - lr: 0.000002
2023-05-27 06:21:31,988 epoch 8 - iter 1496/3747 - loss 0.07756433 - samples/sec: 18.16 - lr: 0.000001
2023-05-27 06:22:54,502 epoch 8 - iter 1870/3747 - loss 0.07781026 - samples/sec: 18.14 - lr: 0.000001
2023-05-27 06:24:15,891 epoch 8 - iter 2244/3747 - loss 0.08103566 - samples/sec: 18.39 - lr: 0.000001
2023-05-27 06:25:37,501 epoch 8 - iter 2618/3747 - loss 0.07914907 - samples/sec: 18.34 - lr: 0.000001
2023-05-27 06:26:59,388 epoch 8 - iter 2992/3747 - loss 0.08136143 - samples/sec: 18.28 - lr: 0.000001
2023-05-27 06:28:22,063 epoch 8 - iter 3366/3747 - loss 0.08369515 - samples/sec: 18.10 - lr: 0.000001
2023-05-27 06:29:48,356 epoch 8 - iter 3740/3747 - loss 0.08774622 - samples/sec: 17.34 - lr: 0.000001
2023-05-27 06:29:49,816 ----------------------------------------------------------------------------------------------------
2023-05-27 06:29:49,817 EPOCH 8 done: loss 0.0876 - lr 0.000001
2023-05-27 06:31:07,045 Evaluating as a multi-label problem: False
2023-05-27 06:31:07,112 DEV : loss 0.09959782660007477 - f1-score (micro avg)  0.9698
2023-05-27 06:31:07,215 BAD EPOCHS (no improvement): 4
2023-05-27 06:31:07,217 ----------------------------------------------------------------------------------------------------
2023-05-27 06:32:31,868 epoch 9 - iter 374/3747 - loss 0.07678862 - samples/sec: 17.68 - lr: 0.000001
2023-05-27 06:33:53,489 epoch 9 - iter 748/3747 - loss 0.07832024 - samples/sec: 18.34 - lr: 0.000001
2023-05-27 06:35:17,483 epoch 9 - iter 1122/3747 - loss 0.07547489 - samples/sec: 17.82 - lr: 0.000001
2023-05-27 06:36:40,175 epoch 9 - iter 1496/3747 - loss 0.07689301 - samples/sec: 18.10 - lr: 0.000001
2023-05-27 06:38:02,179 epoch 9 - iter 1870/3747 - loss 0.07332540 - samples/sec: 18.25 - lr: 0.000001
2023-05-27 06:39:24,273 epoch 9 - iter 2244/3747 - loss 0.07261053 - samples/sec: 18.23 - lr: 0.000001
2023-05-27 06:40:45,981 epoch 9 - iter 2618/3747 - loss 0.07225533 - samples/sec: 18.32 - lr: 0.000001
2023-05-27 06:42:07,662 epoch 9 - iter 2992/3747 - loss 0.07047435 - samples/sec: 18.32 - lr: 0.000001
2023-05-27 06:43:29,193 epoch 9 - iter 3366/3747 - loss 0.07093204 - samples/sec: 18.36 - lr: 0.000001
2023-05-27 06:44:50,767 epoch 9 - iter 3740/3747 - loss 0.07285733 - samples/sec: 18.35 - lr: 0.000001
2023-05-27 06:44:52,298 ----------------------------------------------------------------------------------------------------
2023-05-27 06:44:52,298 EPOCH 9 done: loss 0.0728 - lr 0.000001
2023-05-27 06:46:14,801 Evaluating as a multi-label problem: False
2023-05-27 06:46:14,860 DEV : loss 0.09677618741989136 - f1-score (micro avg)  0.9673
2023-05-27 06:46:14,961 BAD EPOCHS (no improvement): 4
2023-05-27 06:46:14,963 ----------------------------------------------------------------------------------------------------
2023-05-27 06:47:39,453 epoch 10 - iter 374/3747 - loss 0.07230426 - samples/sec: 17.71 - lr: 0.000001
2023-05-27 06:49:02,254 epoch 10 - iter 748/3747 - loss 0.06345456 - samples/sec: 18.07 - lr: 0.000000
2023-05-27 06:50:23,218 epoch 10 - iter 1122/3747 - loss 0.06851334 - samples/sec: 18.48 - lr: 0.000000
2023-05-27 06:51:44,951 epoch 10 - iter 1496/3747 - loss 0.07070796 - samples/sec: 18.31 - lr: 0.000000
2023-05-27 06:53:06,223 epoch 10 - iter 1870/3747 - loss 0.06802477 - samples/sec: 18.41 - lr: 0.000000
2023-05-27 06:54:29,947 epoch 10 - iter 2244/3747 - loss 0.06800298 - samples/sec: 17.88 - lr: 0.000000
2023-05-27 06:55:52,197 epoch 10 - iter 2618/3747 - loss 0.06925920 - samples/sec: 18.20 - lr: 0.000000
2023-05-27 06:57:19,216 epoch 10 - iter 2992/3747 - loss 0.06944462 - samples/sec: 17.20 - lr: 0.000000
2023-05-27 06:58:41,297 epoch 10 - iter 3366/3747 - loss 0.06769585 - samples/sec: 18.23 - lr: 0.000000
2023-05-27 07:00:04,066 epoch 10 - iter 3740/3747 - loss 0.06894587 - samples/sec: 18.08 - lr: 0.000000
2023-05-27 07:00:05,614 ----------------------------------------------------------------------------------------------------
2023-05-27 07:00:05,614 EPOCH 10 done: loss 0.0689 - lr 0.000000
2023-05-27 07:01:18,545 Evaluating as a multi-label problem: False
2023-05-27 07:01:18,595 DEV : loss 0.10479529947042465 - f1-score (micro avg)  0.9673
2023-05-27 07:01:18,673 BAD EPOCHS (no improvement): 4
2023-05-27 07:01:30,710 ----------------------------------------------------------------------------------------------------
2023-05-27 07:01:30,713 Testing using last state of model ...
2023-05-27 07:02:46,508 Evaluating as a multi-label problem: False
2023-05-27 07:02:46,568 0.9259	0.9469	0.9363	0.9041
2023-05-27 07:02:46,568 
Results:
- F-score (micro) 0.9363
- F-score (macro) 0.9237
- Accuracy 0.9041

By class:
              precision    recall  f1-score   support

         ORG     0.9150    0.9398    0.9272      1661
         LOC     0.9386    0.9436    0.9411      1668
         PER     0.9839    0.9827    0.9833      1617
        MISC     0.8021    0.8889    0.8432       702

   micro avg     0.9259    0.9469    0.9363      5648
   macro avg     0.9099    0.9388    0.9237      5648
weighted avg     0.9277    0.9469    0.9369      5648

2023-05-27 07:02:46,569 ----------------------------------------------------------------------------------------------------
