2023-06-23 01:52:48,420 ----------------------------------------------------------------------------------------------------
2023-06-23 01:52:48,422 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-23 01:52:48,424 ----------------------------------------------------------------------------------------------------
2023-06-23 01:52:48,424 Corpus: "Corpus: 14987 train + 3160 dev + 3684 test sentences"
2023-06-23 01:52:48,424 ----------------------------------------------------------------------------------------------------
2023-06-23 01:52:48,424 Parameters:
2023-06-23 01:52:48,424  - learning_rate: "0.000005"
2023-06-23 01:52:48,424  - mini_batch_size: "4"
2023-06-23 01:52:48,424  - patience: "3"
2023-06-23 01:52:48,424  - anneal_factor: "0.5"
2023-06-23 01:52:48,424  - max_epochs: "10"
2023-06-23 01:52:48,424  - shuffle: "True"
2023-06-23 01:52:48,424  - train_with_dev: "False"
2023-06-23 01:52:48,424  - batch_growth_annealing: "False"
2023-06-23 01:52:48,424 ----------------------------------------------------------------------------------------------------
2023-06-23 01:52:48,424 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_3_ger_test_as_dev"
2023-06-23 01:52:48,424 ----------------------------------------------------------------------------------------------------
2023-06-23 01:52:48,424 Device: cuda:1
2023-06-23 01:52:48,424 ----------------------------------------------------------------------------------------------------
2023-06-23 01:52:48,425 Embeddings storage mode: none
2023-06-23 01:52:48,425 ----------------------------------------------------------------------------------------------------
2023-06-23 01:54:10,903 epoch 1 - iter 374/3747 - loss 3.00819943 - samples/sec: 18.14 - lr: 0.000000
2023-06-23 01:55:33,138 epoch 1 - iter 748/3747 - loss 2.08761595 - samples/sec: 18.20 - lr: 0.000001
2023-06-23 01:56:52,760 epoch 1 - iter 1122/3747 - loss 1.65698887 - samples/sec: 18.79 - lr: 0.000001
2023-06-23 01:58:14,495 epoch 1 - iter 1496/3747 - loss 1.42321745 - samples/sec: 18.31 - lr: 0.000002
2023-06-23 01:59:35,583 epoch 1 - iter 1870/3747 - loss 1.21934594 - samples/sec: 18.45 - lr: 0.000002
2023-06-23 02:00:58,333 epoch 1 - iter 2244/3747 - loss 1.05715353 - samples/sec: 18.08 - lr: 0.000003
2023-06-23 02:02:21,407 epoch 1 - iter 2618/3747 - loss 0.93562391 - samples/sec: 18.01 - lr: 0.000003
2023-06-23 02:03:43,531 epoch 1 - iter 2992/3747 - loss 0.85192625 - samples/sec: 18.22 - lr: 0.000004
2023-06-23 02:05:05,883 epoch 1 - iter 3366/3747 - loss 0.78143064 - samples/sec: 18.17 - lr: 0.000004
2023-06-23 02:06:27,891 epoch 1 - iter 3740/3747 - loss 0.71784330 - samples/sec: 18.25 - lr: 0.000005
2023-06-23 02:06:29,275 ----------------------------------------------------------------------------------------------------
2023-06-23 02:06:29,275 EPOCH 1 done: loss 0.7173 - lr 0.000005
2023-06-23 02:07:26,904 Evaluating as a multi-label problem: False
2023-06-23 02:07:26,932 DEV : loss 0.19934715330600739 - f1-score (micro avg)  0.7152
2023-06-23 02:08:22,306 Evaluating as a multi-label problem: False
2023-06-23 02:08:22,341 TEST : loss 0.15350143611431122 - f1-score (micro avg)  0.8764
2023-06-23 02:08:22,405 BAD EPOCHS (no improvement): 4
2023-06-23 02:08:22,408 ----------------------------------------------------------------------------------------------------
2023-06-23 02:09:44,927 epoch 2 - iter 374/3747 - loss 0.25260458 - samples/sec: 18.13 - lr: 0.000005
2023-06-23 02:11:07,828 epoch 2 - iter 748/3747 - loss 0.24649813 - samples/sec: 18.05 - lr: 0.000005
2023-06-23 02:12:31,024 epoch 2 - iter 1122/3747 - loss 0.24864325 - samples/sec: 17.99 - lr: 0.000005
2023-06-23 02:13:53,090 epoch 2 - iter 1496/3747 - loss 0.24684689 - samples/sec: 18.23 - lr: 0.000005
2023-06-23 02:15:15,663 epoch 2 - iter 1870/3747 - loss 0.24643771 - samples/sec: 18.12 - lr: 0.000005
2023-06-23 02:16:37,248 epoch 2 - iter 2244/3747 - loss 0.24588311 - samples/sec: 18.34 - lr: 0.000005
2023-06-23 02:17:58,703 epoch 2 - iter 2618/3747 - loss 0.24281426 - samples/sec: 18.37 - lr: 0.000005
2023-06-23 02:19:20,421 epoch 2 - iter 2992/3747 - loss 0.24080560 - samples/sec: 18.31 - lr: 0.000005
2023-06-23 02:20:42,535 epoch 2 - iter 3366/3747 - loss 0.23781237 - samples/sec: 18.22 - lr: 0.000005
2023-06-23 02:22:03,926 epoch 2 - iter 3740/3747 - loss 0.23619011 - samples/sec: 18.39 - lr: 0.000004
2023-06-23 02:22:05,407 ----------------------------------------------------------------------------------------------------
2023-06-23 02:22:05,407 EPOCH 2 done: loss 0.2361 - lr 0.000004
2023-06-23 02:23:04,898 Evaluating as a multi-label problem: False
2023-06-23 02:23:04,926 DEV : loss 0.2814212739467621 - f1-score (micro avg)  0.7269
2023-06-23 02:24:03,587 Evaluating as a multi-label problem: False
2023-06-23 02:24:03,621 TEST : loss 0.17035627365112305 - f1-score (micro avg)  0.909
2023-06-23 02:24:03,683 BAD EPOCHS (no improvement): 4
2023-06-23 02:24:03,685 ----------------------------------------------------------------------------------------------------
2023-06-23 02:25:26,094 epoch 3 - iter 374/3747 - loss 0.20233132 - samples/sec: 18.16 - lr: 0.000004
2023-06-23 02:26:47,786 epoch 3 - iter 748/3747 - loss 0.20537766 - samples/sec: 18.32 - lr: 0.000004
2023-06-23 02:28:10,284 epoch 3 - iter 1122/3747 - loss 0.21118558 - samples/sec: 18.14 - lr: 0.000004
2023-06-23 02:29:32,101 epoch 3 - iter 1496/3747 - loss 0.21317139 - samples/sec: 18.29 - lr: 0.000004
2023-06-23 02:30:54,200 epoch 3 - iter 1870/3747 - loss 0.21319085 - samples/sec: 18.23 - lr: 0.000004
2023-06-23 02:32:16,324 epoch 3 - iter 2244/3747 - loss 0.21043720 - samples/sec: 18.22 - lr: 0.000004
2023-06-23 02:33:37,690 epoch 3 - iter 2618/3747 - loss 0.21091011 - samples/sec: 18.39 - lr: 0.000004
2023-06-23 02:34:59,713 epoch 3 - iter 2992/3747 - loss 0.21061687 - samples/sec: 18.24 - lr: 0.000004
2023-06-23 02:36:21,218 epoch 3 - iter 3366/3747 - loss 0.20985810 - samples/sec: 18.36 - lr: 0.000004
2023-06-23 02:37:42,408 epoch 3 - iter 3740/3747 - loss 0.20855995 - samples/sec: 18.43 - lr: 0.000004
2023-06-23 02:37:43,900 ----------------------------------------------------------------------------------------------------
2023-06-23 02:37:43,900 EPOCH 3 done: loss 0.2085 - lr 0.000004
2023-06-23 02:38:44,213 Evaluating as a multi-label problem: False
2023-06-23 02:38:44,239 DEV : loss 0.2689870595932007 - f1-score (micro avg)  0.7556
2023-06-23 02:39:42,775 Evaluating as a multi-label problem: False
2023-06-23 02:39:42,809 TEST : loss 0.15809832513332367 - f1-score (micro avg)  0.9217
2023-06-23 02:39:42,870 BAD EPOCHS (no improvement): 4
2023-06-23 02:39:42,872 ----------------------------------------------------------------------------------------------------
2023-06-23 02:41:05,087 epoch 4 - iter 374/3747 - loss 0.20917776 - samples/sec: 18.20 - lr: 0.000004
2023-06-23 02:42:26,778 epoch 4 - iter 748/3747 - loss 0.19740165 - samples/sec: 18.32 - lr: 0.000004
2023-06-23 02:43:48,723 epoch 4 - iter 1122/3747 - loss 0.19171173 - samples/sec: 18.26 - lr: 0.000004
2023-06-23 02:45:09,888 epoch 4 - iter 1496/3747 - loss 0.19082953 - samples/sec: 18.44 - lr: 0.000004
2023-06-23 02:46:31,554 epoch 4 - iter 1870/3747 - loss 0.19128876 - samples/sec: 18.32 - lr: 0.000004
2023-06-23 02:47:53,570 epoch 4 - iter 2244/3747 - loss 0.18981220 - samples/sec: 18.25 - lr: 0.000004
2023-06-23 02:49:14,757 epoch 4 - iter 2618/3747 - loss 0.19103605 - samples/sec: 18.43 - lr: 0.000004
2023-06-23 02:50:35,825 epoch 4 - iter 2992/3747 - loss 0.18951897 - samples/sec: 18.46 - lr: 0.000003
2023-06-23 02:51:57,198 epoch 4 - iter 3366/3747 - loss 0.19001343 - samples/sec: 18.39 - lr: 0.000003
2023-06-23 02:53:18,241 epoch 4 - iter 3740/3747 - loss 0.18863421 - samples/sec: 18.46 - lr: 0.000003
2023-06-23 02:53:19,823 ----------------------------------------------------------------------------------------------------
2023-06-23 02:53:19,824 EPOCH 4 done: loss 0.1885 - lr 0.000003
2023-06-23 02:54:24,350 Evaluating as a multi-label problem: False
2023-06-23 02:54:24,391 DEV : loss 0.28203731775283813 - f1-score (micro avg)  0.7577
2023-06-23 02:55:26,701 Evaluating as a multi-label problem: False
2023-06-23 02:55:26,735 TEST : loss 0.15518984198570251 - f1-score (micro avg)  0.9323
2023-06-23 02:55:26,802 BAD EPOCHS (no improvement): 4
2023-06-23 02:55:26,805 ----------------------------------------------------------------------------------------------------
2023-06-23 02:56:48,654 epoch 5 - iter 374/3747 - loss 0.17976529 - samples/sec: 18.28 - lr: 0.000003
2023-06-23 02:58:11,594 epoch 5 - iter 748/3747 - loss 0.17839491 - samples/sec: 18.04 - lr: 0.000003
2023-06-23 02:59:33,799 epoch 5 - iter 1122/3747 - loss 0.17765840 - samples/sec: 18.20 - lr: 0.000003
2023-06-23 03:00:54,909 epoch 5 - iter 1496/3747 - loss 0.17716183 - samples/sec: 18.45 - lr: 0.000003
2023-06-23 03:02:16,751 epoch 5 - iter 1870/3747 - loss 0.17624916 - samples/sec: 18.29 - lr: 0.000003
2023-06-23 03:03:38,133 epoch 5 - iter 2244/3747 - loss 0.17545710 - samples/sec: 18.39 - lr: 0.000003
2023-06-23 03:04:55,838 epoch 5 - iter 2618/3747 - loss 0.17491562 - samples/sec: 19.26 - lr: 0.000003
2023-06-23 03:06:17,237 epoch 5 - iter 2992/3747 - loss 0.17546455 - samples/sec: 18.38 - lr: 0.000003
2023-06-23 03:07:38,059 epoch 5 - iter 3366/3747 - loss 0.17588085 - samples/sec: 18.52 - lr: 0.000003
2023-06-23 03:08:59,042 epoch 5 - iter 3740/3747 - loss 0.17620028 - samples/sec: 18.48 - lr: 0.000003
2023-06-23 03:09:00,565 ----------------------------------------------------------------------------------------------------
2023-06-23 03:09:00,565 EPOCH 5 done: loss 0.1762 - lr 0.000003
2023-06-23 03:09:58,757 Evaluating as a multi-label problem: False
2023-06-23 03:09:58,799 DEV : loss 0.27161112427711487 - f1-score (micro avg)  0.7628
2023-06-23 03:11:02,026 Evaluating as a multi-label problem: False
2023-06-23 03:11:02,066 TEST : loss 0.1675855815410614 - f1-score (micro avg)  0.9314
2023-06-23 03:11:02,145 BAD EPOCHS (no improvement): 4
2023-06-23 03:11:02,148 ----------------------------------------------------------------------------------------------------
2023-06-23 03:12:22,429 epoch 6 - iter 374/3747 - loss 0.17170113 - samples/sec: 18.64 - lr: 0.000003
2023-06-23 03:13:44,337 epoch 6 - iter 748/3747 - loss 0.17114631 - samples/sec: 18.27 - lr: 0.000003
2023-06-23 03:15:06,335 epoch 6 - iter 1122/3747 - loss 0.17172552 - samples/sec: 18.25 - lr: 0.000003
2023-06-23 03:16:28,360 epoch 6 - iter 1496/3747 - loss 0.16951939 - samples/sec: 18.24 - lr: 0.000003
2023-06-23 03:17:50,666 epoch 6 - iter 1870/3747 - loss 0.17118680 - samples/sec: 18.18 - lr: 0.000003
2023-06-23 03:19:11,266 epoch 6 - iter 2244/3747 - loss 0.17213330 - samples/sec: 18.57 - lr: 0.000002
2023-06-23 03:20:31,911 epoch 6 - iter 2618/3747 - loss 0.17224285 - samples/sec: 18.56 - lr: 0.000002
2023-06-23 03:21:55,547 epoch 6 - iter 2992/3747 - loss 0.17108503 - samples/sec: 17.89 - lr: 0.000002
2023-06-23 03:23:16,335 epoch 6 - iter 3366/3747 - loss 0.17056641 - samples/sec: 18.52 - lr: 0.000002
2023-06-23 03:24:38,418 epoch 6 - iter 3740/3747 - loss 0.16975639 - samples/sec: 18.23 - lr: 0.000002
2023-06-23 03:24:39,832 ----------------------------------------------------------------------------------------------------
2023-06-23 03:24:39,832 EPOCH 6 done: loss 0.1697 - lr 0.000002
2023-06-23 03:25:36,561 Evaluating as a multi-label problem: False
2023-06-23 03:25:36,616 DEV : loss 0.2667994499206543 - f1-score (micro avg)  0.7646
2023-06-23 03:26:34,968 Evaluating as a multi-label problem: False
2023-06-23 03:26:35,001 TEST : loss 0.16879616677761078 - f1-score (micro avg)  0.9314
2023-06-23 03:26:35,064 BAD EPOCHS (no improvement): 4
2023-06-23 03:26:35,067 ----------------------------------------------------------------------------------------------------
2023-06-23 03:27:56,696 epoch 7 - iter 374/3747 - loss 0.16083997 - samples/sec: 18.33 - lr: 0.000002
2023-06-23 03:29:17,329 epoch 7 - iter 748/3747 - loss 0.15911861 - samples/sec: 18.56 - lr: 0.000002
2023-06-23 03:30:34,755 epoch 7 - iter 1122/3747 - loss 0.16005888 - samples/sec: 19.33 - lr: 0.000002
2023-06-23 03:31:53,350 epoch 7 - iter 1496/3747 - loss 0.16122857 - samples/sec: 19.04 - lr: 0.000002
2023-06-23 03:33:16,416 epoch 7 - iter 1870/3747 - loss 0.16135619 - samples/sec: 18.01 - lr: 0.000002
2023-06-23 03:34:36,660 epoch 7 - iter 2244/3747 - loss 0.16369496 - samples/sec: 18.65 - lr: 0.000002
2023-06-23 03:35:57,688 epoch 7 - iter 2618/3747 - loss 0.16411808 - samples/sec: 18.47 - lr: 0.000002
2023-06-23 03:37:18,270 epoch 7 - iter 2992/3747 - loss 0.16337477 - samples/sec: 18.57 - lr: 0.000002
2023-06-23 03:38:39,319 epoch 7 - iter 3366/3747 - loss 0.16258846 - samples/sec: 18.46 - lr: 0.000002
2023-06-23 03:40:00,409 epoch 7 - iter 3740/3747 - loss 0.16262990 - samples/sec: 18.45 - lr: 0.000002
2023-06-23 03:40:01,918 ----------------------------------------------------------------------------------------------------
2023-06-23 03:40:01,919 EPOCH 7 done: loss 0.1625 - lr 0.000002
2023-06-23 03:40:59,317 Evaluating as a multi-label problem: False
2023-06-23 03:40:59,342 DEV : loss 0.2721961736679077 - f1-score (micro avg)  0.7719
2023-06-23 03:41:53,696 Evaluating as a multi-label problem: False
2023-06-23 03:41:53,729 TEST : loss 0.16218271851539612 - f1-score (micro avg)  0.935
2023-06-23 03:41:53,793 BAD EPOCHS (no improvement): 4
2023-06-23 03:41:53,797 ----------------------------------------------------------------------------------------------------
2023-06-23 03:43:14,872 epoch 8 - iter 374/3747 - loss 0.15491064 - samples/sec: 18.46 - lr: 0.000002
2023-06-23 03:44:37,698 epoch 8 - iter 748/3747 - loss 0.15571925 - samples/sec: 18.07 - lr: 0.000002
2023-06-23 03:45:59,733 epoch 8 - iter 1122/3747 - loss 0.15645204 - samples/sec: 18.24 - lr: 0.000002
2023-06-23 03:47:21,547 epoch 8 - iter 1496/3747 - loss 0.15607738 - samples/sec: 18.29 - lr: 0.000001
2023-06-23 03:48:42,840 epoch 8 - iter 1870/3747 - loss 0.15740088 - samples/sec: 18.41 - lr: 0.000001
2023-06-23 03:50:04,052 epoch 8 - iter 2244/3747 - loss 0.15648563 - samples/sec: 18.42 - lr: 0.000001
2023-06-23 03:51:24,469 epoch 8 - iter 2618/3747 - loss 0.15623819 - samples/sec: 18.61 - lr: 0.000001
2023-06-23 03:52:46,050 epoch 8 - iter 2992/3747 - loss 0.15619453 - samples/sec: 18.34 - lr: 0.000001
2023-06-23 03:54:06,533 epoch 8 - iter 3366/3747 - loss 0.15597692 - samples/sec: 18.59 - lr: 0.000001
2023-06-23 03:55:27,316 epoch 8 - iter 3740/3747 - loss 0.15671936 - samples/sec: 18.52 - lr: 0.000001
2023-06-23 03:55:28,765 ----------------------------------------------------------------------------------------------------
2023-06-23 03:55:28,766 EPOCH 8 done: loss 0.1569 - lr 0.000001
2023-06-23 03:56:27,129 Evaluating as a multi-label problem: False
2023-06-23 03:56:27,157 DEV : loss 0.28478479385375977 - f1-score (micro avg)  0.7666
2023-06-23 03:57:25,486 Evaluating as a multi-label problem: False
2023-06-23 03:57:25,519 TEST : loss 0.15748849511146545 - f1-score (micro avg)  0.9389
2023-06-23 03:57:25,583 BAD EPOCHS (no improvement): 4
2023-06-23 03:57:25,586 ----------------------------------------------------------------------------------------------------
2023-06-23 03:58:45,293 epoch 9 - iter 374/3747 - loss 0.14657508 - samples/sec: 18.77 - lr: 0.000001
2023-06-23 04:00:06,051 epoch 9 - iter 748/3747 - loss 0.15013764 - samples/sec: 18.53 - lr: 0.000001
2023-06-23 04:01:26,606 epoch 9 - iter 1122/3747 - loss 0.15127514 - samples/sec: 18.58 - lr: 0.000001
2023-06-23 04:02:47,155 epoch 9 - iter 1496/3747 - loss 0.15094800 - samples/sec: 18.58 - lr: 0.000001
2023-06-23 04:04:08,738 epoch 9 - iter 1870/3747 - loss 0.15117772 - samples/sec: 18.34 - lr: 0.000001
2023-06-23 04:05:30,273 epoch 9 - iter 2244/3747 - loss 0.15020295 - samples/sec: 18.35 - lr: 0.000001
2023-06-23 04:06:51,109 epoch 9 - iter 2618/3747 - loss 0.15132589 - samples/sec: 18.51 - lr: 0.000001
2023-06-23 04:08:11,435 epoch 9 - iter 2992/3747 - loss 0.15060224 - samples/sec: 18.63 - lr: 0.000001
2023-06-23 04:09:32,158 epoch 9 - iter 3366/3747 - loss 0.15057468 - samples/sec: 18.54 - lr: 0.000001
2023-06-23 04:10:53,469 epoch 9 - iter 3740/3747 - loss 0.15099033 - samples/sec: 18.40 - lr: 0.000001
2023-06-23 04:10:54,986 ----------------------------------------------------------------------------------------------------
2023-06-23 04:10:54,987 EPOCH 9 done: loss 0.1510 - lr 0.000001
2023-06-23 04:11:54,316 Evaluating as a multi-label problem: False
2023-06-23 04:11:54,342 DEV : loss 0.28538450598716736 - f1-score (micro avg)  0.7684
2023-06-23 04:12:52,208 Evaluating as a multi-label problem: False
2023-06-23 04:12:52,241 TEST : loss 0.16592256724834442 - f1-score (micro avg)  0.9369
2023-06-23 04:12:52,308 BAD EPOCHS (no improvement): 4
2023-06-23 04:12:52,311 ----------------------------------------------------------------------------------------------------
2023-06-23 04:14:13,289 epoch 10 - iter 374/3747 - loss 0.14993716 - samples/sec: 18.48 - lr: 0.000001
2023-06-23 04:15:34,821 epoch 10 - iter 748/3747 - loss 0.15808316 - samples/sec: 18.35 - lr: 0.000000
2023-06-23 04:16:55,841 epoch 10 - iter 1122/3747 - loss 0.15272422 - samples/sec: 18.47 - lr: 0.000000
2023-06-23 04:18:17,714 epoch 10 - iter 1496/3747 - loss 0.15149397 - samples/sec: 18.28 - lr: 0.000000
2023-06-23 04:19:39,100 epoch 10 - iter 1870/3747 - loss 0.15215176 - samples/sec: 18.39 - lr: 0.000000
2023-06-23 04:21:00,490 epoch 10 - iter 2244/3747 - loss 0.15371758 - samples/sec: 18.38 - lr: 0.000000
2023-06-23 04:22:18,795 epoch 10 - iter 2618/3747 - loss 0.15284828 - samples/sec: 19.11 - lr: 0.000000
2023-06-23 04:23:39,006 epoch 10 - iter 2992/3747 - loss 0.15354740 - samples/sec: 18.65 - lr: 0.000000
2023-06-23 04:24:59,476 epoch 10 - iter 3366/3747 - loss 0.15188482 - samples/sec: 18.59 - lr: 0.000000
2023-06-23 04:26:20,558 epoch 10 - iter 3740/3747 - loss 0.15126339 - samples/sec: 18.45 - lr: 0.000000
2023-06-23 04:26:22,066 ----------------------------------------------------------------------------------------------------
2023-06-23 04:26:22,066 EPOCH 10 done: loss 0.1513 - lr 0.000000
2023-06-23 04:27:20,753 Evaluating as a multi-label problem: False
2023-06-23 04:27:20,779 DEV : loss 0.292556494474411 - f1-score (micro avg)  0.7691
2023-06-23 04:28:17,485 Evaluating as a multi-label problem: False
2023-06-23 04:28:17,518 TEST : loss 0.16982367634773254 - f1-score (micro avg)  0.9366
2023-06-23 04:28:17,583 BAD EPOCHS (no improvement): 4
2023-06-23 04:28:29,710 ----------------------------------------------------------------------------------------------------
2023-06-23 04:28:29,716 Testing using last state of model ...
2023-06-23 04:29:26,550 Evaluating as a multi-label problem: False
2023-06-23 04:29:26,584 0.9295	0.9437	0.9366	0.9049
2023-06-23 04:29:26,584 
Results:
- F-score (micro) 0.9366
- F-score (macro) 0.9228
- Accuracy 0.9049

By class:
              precision    recall  f1-score   support

         ORG     0.9121    0.9368    0.9243      1661
         LOC     0.9489    0.9472    0.9481      1668
         PER     0.9833    0.9814    0.9824      1617
        MISC     0.8104    0.8647    0.8367       702

   micro avg     0.9295    0.9437    0.9366      5648
   macro avg     0.9137    0.9325    0.9228      5648
weighted avg     0.9307    0.9437    0.9370      5648

2023-06-23 04:29:26,584 ----------------------------------------------------------------------------------------------------
