2023-06-05 14:31:40,514 ----------------------------------------------------------------------------------------------------
2023-06-05 14:31:40,519 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-05 14:31:40,522 ----------------------------------------------------------------------------------------------------
2023-06-05 14:31:40,522 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-05 14:31:40,522 ----------------------------------------------------------------------------------------------------
2023-06-05 14:31:40,522 Parameters:
2023-06-05 14:31:40,522  - learning_rate: "0.000005"
2023-06-05 14:31:40,522  - mini_batch_size: "4"
2023-06-05 14:31:40,523  - patience: "3"
2023-06-05 14:31:40,523  - anneal_factor: "0.5"
2023-06-05 14:31:40,523  - max_epochs: "10"
2023-06-05 14:31:40,523  - shuffle: "True"
2023-06-05 14:31:40,523  - train_with_dev: "False"
2023-06-05 14:31:40,523  - batch_growth_annealing: "False"
2023-06-05 14:31:40,523 ----------------------------------------------------------------------------------------------------
2023-06-05 14:31:40,523 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_2"
2023-06-05 14:31:40,523 ----------------------------------------------------------------------------------------------------
2023-06-05 14:31:40,523 Device: cuda:3
2023-06-05 14:31:40,523 ----------------------------------------------------------------------------------------------------
2023-06-05 14:31:40,523 Embeddings storage mode: none
2023-06-05 14:31:40,523 ----------------------------------------------------------------------------------------------------
2023-06-05 14:33:11,387 epoch 1 - iter 374/3747 - loss 2.84221654 - samples/sec: 16.47 - lr: 0.000000
2023-06-05 14:34:41,102 epoch 1 - iter 748/3747 - loss 2.07579282 - samples/sec: 16.68 - lr: 0.000001
2023-06-05 14:36:09,155 epoch 1 - iter 1122/3747 - loss 1.65460091 - samples/sec: 17.00 - lr: 0.000001
2023-06-05 14:37:37,331 epoch 1 - iter 1496/3747 - loss 1.43681251 - samples/sec: 16.97 - lr: 0.000002
2023-06-05 14:39:08,278 epoch 1 - iter 1870/3747 - loss 1.23624796 - samples/sec: 16.46 - lr: 0.000002
2023-06-05 14:40:41,195 epoch 1 - iter 2244/3747 - loss 1.07457450 - samples/sec: 16.11 - lr: 0.000003
2023-06-05 14:42:14,336 epoch 1 - iter 2618/3747 - loss 0.95302606 - samples/sec: 16.07 - lr: 0.000003
2023-06-05 14:43:44,419 epoch 1 - iter 2992/3747 - loss 0.86780141 - samples/sec: 16.61 - lr: 0.000004
2023-06-05 14:45:15,315 epoch 1 - iter 3366/3747 - loss 0.79677098 - samples/sec: 16.46 - lr: 0.000004
2023-06-05 14:46:46,149 epoch 1 - iter 3740/3747 - loss 0.73108083 - samples/sec: 16.48 - lr: 0.000005
2023-06-05 14:46:47,718 ----------------------------------------------------------------------------------------------------
2023-06-05 14:46:47,718 EPOCH 1 done: loss 0.7304 - lr 0.000005
2023-06-05 14:48:23,946 Evaluating as a multi-label problem: False
2023-06-05 14:48:24,021 DEV : loss 0.12918415665626526 - f1-score (micro avg)  0.8906
2023-06-05 14:48:24,148 BAD EPOCHS (no improvement): 4
2023-06-05 14:48:24,154 ----------------------------------------------------------------------------------------------------
2023-06-05 14:49:57,242 epoch 2 - iter 374/3747 - loss 0.25014416 - samples/sec: 16.08 - lr: 0.000005
2023-06-05 14:51:27,015 epoch 2 - iter 748/3747 - loss 0.24377107 - samples/sec: 16.67 - lr: 0.000005
2023-06-05 14:52:56,327 epoch 2 - iter 1122/3747 - loss 0.24200232 - samples/sec: 16.76 - lr: 0.000005
2023-06-05 14:54:23,873 epoch 2 - iter 1496/3747 - loss 0.23898099 - samples/sec: 17.10 - lr: 0.000005
2023-06-05 14:55:54,068 epoch 2 - iter 1870/3747 - loss 0.23962461 - samples/sec: 16.59 - lr: 0.000005
2023-06-05 14:57:27,940 epoch 2 - iter 2244/3747 - loss 0.23862646 - samples/sec: 15.94 - lr: 0.000005
2023-06-05 14:58:57,365 epoch 2 - iter 2618/3747 - loss 0.23660182 - samples/sec: 16.74 - lr: 0.000005
2023-06-05 15:00:30,728 epoch 2 - iter 2992/3747 - loss 0.23493251 - samples/sec: 16.03 - lr: 0.000005
2023-06-05 15:02:02,095 epoch 2 - iter 3366/3747 - loss 0.23364177 - samples/sec: 16.38 - lr: 0.000005
2023-06-05 15:03:30,789 epoch 2 - iter 3740/3747 - loss 0.23405511 - samples/sec: 16.87 - lr: 0.000004
2023-06-05 15:03:32,590 ----------------------------------------------------------------------------------------------------
2023-06-05 15:03:32,591 EPOCH 2 done: loss 0.2340 - lr 0.000004
2023-06-05 15:05:00,925 Evaluating as a multi-label problem: False
2023-06-05 15:05:01,001 DEV : loss 0.07861700654029846 - f1-score (micro avg)  0.9491
2023-06-05 15:05:01,137 BAD EPOCHS (no improvement): 4
2023-06-05 15:05:01,143 ----------------------------------------------------------------------------------------------------
2023-06-05 15:06:31,467 epoch 3 - iter 374/3747 - loss 0.22547249 - samples/sec: 16.57 - lr: 0.000004
2023-06-05 15:08:01,088 epoch 3 - iter 748/3747 - loss 0.21750192 - samples/sec: 16.70 - lr: 0.000004
2023-06-05 15:09:30,394 epoch 3 - iter 1122/3747 - loss 0.21684747 - samples/sec: 16.76 - lr: 0.000004
2023-06-05 15:10:58,288 epoch 3 - iter 1496/3747 - loss 0.21529789 - samples/sec: 17.03 - lr: 0.000004
2023-06-05 15:12:29,040 epoch 3 - iter 1870/3747 - loss 0.21540031 - samples/sec: 16.49 - lr: 0.000004
2023-06-05 15:13:55,879 epoch 3 - iter 2244/3747 - loss 0.21310874 - samples/sec: 17.24 - lr: 0.000004
2023-06-05 15:15:25,073 epoch 3 - iter 2618/3747 - loss 0.21142006 - samples/sec: 16.78 - lr: 0.000004
2023-06-05 15:16:53,004 epoch 3 - iter 2992/3747 - loss 0.21293918 - samples/sec: 17.02 - lr: 0.000004
2023-06-05 15:18:19,675 epoch 3 - iter 3366/3747 - loss 0.21223387 - samples/sec: 17.27 - lr: 0.000004
2023-06-05 15:19:50,589 epoch 3 - iter 3740/3747 - loss 0.21260576 - samples/sec: 16.46 - lr: 0.000004
2023-06-05 15:19:52,288 ----------------------------------------------------------------------------------------------------
2023-06-05 15:19:52,288 EPOCH 3 done: loss 0.2125 - lr 0.000004
2023-06-05 15:21:22,369 Evaluating as a multi-label problem: False
2023-06-05 15:21:22,440 DEV : loss 0.09198608249425888 - f1-score (micro avg)  0.9457
2023-06-05 15:21:22,558 BAD EPOCHS (no improvement): 4
2023-06-05 15:21:22,561 ----------------------------------------------------------------------------------------------------
2023-06-05 15:22:51,517 epoch 4 - iter 374/3747 - loss 0.20896366 - samples/sec: 16.83 - lr: 0.000004
2023-06-05 15:24:20,110 epoch 4 - iter 748/3747 - loss 0.20173545 - samples/sec: 16.89 - lr: 0.000004
2023-06-05 15:25:46,968 epoch 4 - iter 1122/3747 - loss 0.19661251 - samples/sec: 17.23 - lr: 0.000004
2023-06-05 15:27:14,169 epoch 4 - iter 1496/3747 - loss 0.19625349 - samples/sec: 17.16 - lr: 0.000004
2023-06-05 15:28:45,258 epoch 4 - iter 1870/3747 - loss 0.19801241 - samples/sec: 16.43 - lr: 0.000004
2023-06-05 15:30:13,435 epoch 4 - iter 2244/3747 - loss 0.19573197 - samples/sec: 16.97 - lr: 0.000004
2023-06-05 15:31:38,023 epoch 4 - iter 2618/3747 - loss 0.19551100 - samples/sec: 17.69 - lr: 0.000004
2023-06-05 15:33:06,550 epoch 4 - iter 2992/3747 - loss 0.19517740 - samples/sec: 16.91 - lr: 0.000003
2023-06-05 15:34:33,914 epoch 4 - iter 3366/3747 - loss 0.19433330 - samples/sec: 17.13 - lr: 0.000003
2023-06-05 15:35:59,990 epoch 4 - iter 3740/3747 - loss 0.19341261 - samples/sec: 17.39 - lr: 0.000003
2023-06-05 15:36:01,449 ----------------------------------------------------------------------------------------------------
2023-06-05 15:36:01,449 EPOCH 4 done: loss 0.1934 - lr 0.000003
2023-06-05 15:37:27,780 Evaluating as a multi-label problem: False
2023-06-05 15:37:27,848 DEV : loss 0.06985490769147873 - f1-score (micro avg)  0.9548
2023-06-05 15:37:27,982 BAD EPOCHS (no improvement): 4
2023-06-05 15:37:27,984 ----------------------------------------------------------------------------------------------------
2023-06-05 15:38:56,138 epoch 5 - iter 374/3747 - loss 0.18048240 - samples/sec: 16.98 - lr: 0.000003
2023-06-05 15:40:25,599 epoch 5 - iter 748/3747 - loss 0.17875901 - samples/sec: 16.73 - lr: 0.000003
2023-06-05 15:41:54,455 epoch 5 - iter 1122/3747 - loss 0.17949265 - samples/sec: 16.84 - lr: 0.000003
2023-06-05 15:43:23,193 epoch 5 - iter 1496/3747 - loss 0.17826089 - samples/sec: 16.87 - lr: 0.000003
2023-06-05 15:44:52,625 epoch 5 - iter 1870/3747 - loss 0.17775669 - samples/sec: 16.74 - lr: 0.000003
2023-06-05 15:46:19,718 epoch 5 - iter 2244/3747 - loss 0.17907102 - samples/sec: 17.18 - lr: 0.000003
2023-06-05 15:47:48,283 epoch 5 - iter 2618/3747 - loss 0.17778999 - samples/sec: 16.90 - lr: 0.000003
2023-06-05 15:49:16,727 epoch 5 - iter 2992/3747 - loss 0.17862120 - samples/sec: 16.92 - lr: 0.000003
2023-06-05 15:50:45,092 epoch 5 - iter 3366/3747 - loss 0.17798015 - samples/sec: 16.94 - lr: 0.000003
2023-06-05 15:52:11,810 epoch 5 - iter 3740/3747 - loss 0.17878594 - samples/sec: 17.26 - lr: 0.000003
2023-06-05 15:52:13,422 ----------------------------------------------------------------------------------------------------
2023-06-05 15:52:13,422 EPOCH 5 done: loss 0.1789 - lr 0.000003
2023-06-05 15:53:46,178 Evaluating as a multi-label problem: False
2023-06-05 15:53:46,247 DEV : loss 0.07125000655651093 - f1-score (micro avg)  0.9533
2023-06-05 15:53:46,387 BAD EPOCHS (no improvement): 4
2023-06-05 15:53:46,389 ----------------------------------------------------------------------------------------------------
2023-06-05 15:55:19,670 epoch 6 - iter 374/3747 - loss 0.17718136 - samples/sec: 16.05 - lr: 0.000003
2023-06-05 15:56:48,892 epoch 6 - iter 748/3747 - loss 0.17282223 - samples/sec: 16.77 - lr: 0.000003
2023-06-05 15:58:21,454 epoch 6 - iter 1122/3747 - loss 0.16953529 - samples/sec: 16.17 - lr: 0.000003
2023-06-05 15:59:48,528 epoch 6 - iter 1496/3747 - loss 0.17135794 - samples/sec: 17.19 - lr: 0.000003
2023-06-05 16:01:14,287 epoch 6 - iter 1870/3747 - loss 0.17104475 - samples/sec: 17.45 - lr: 0.000003
2023-06-05 16:02:40,973 epoch 6 - iter 2244/3747 - loss 0.16956372 - samples/sec: 17.27 - lr: 0.000002
2023-06-05 16:04:08,147 epoch 6 - iter 2618/3747 - loss 0.16970176 - samples/sec: 17.17 - lr: 0.000002
2023-06-05 16:05:37,132 epoch 6 - iter 2992/3747 - loss 0.17065071 - samples/sec: 16.82 - lr: 0.000002
2023-06-05 16:07:10,668 epoch 6 - iter 3366/3747 - loss 0.17150032 - samples/sec: 16.00 - lr: 0.000002
2023-06-05 16:08:38,140 epoch 6 - iter 3740/3747 - loss 0.16997708 - samples/sec: 17.11 - lr: 0.000002
2023-06-05 16:08:39,884 ----------------------------------------------------------------------------------------------------
2023-06-05 16:08:39,884 EPOCH 6 done: loss 0.1699 - lr 0.000002
2023-06-05 16:10:09,239 Evaluating as a multi-label problem: False
2023-06-05 16:10:09,310 DEV : loss 0.07031400501728058 - f1-score (micro avg)  0.9658
2023-06-05 16:10:09,414 BAD EPOCHS (no improvement): 4
2023-06-05 16:10:09,417 ----------------------------------------------------------------------------------------------------
2023-06-05 16:11:37,434 epoch 7 - iter 374/3747 - loss 0.16996387 - samples/sec: 17.01 - lr: 0.000002
2023-06-05 16:13:07,200 epoch 7 - iter 748/3747 - loss 0.17536717 - samples/sec: 16.67 - lr: 0.000002
2023-06-05 16:14:34,887 epoch 7 - iter 1122/3747 - loss 0.17129254 - samples/sec: 17.07 - lr: 0.000002
2023-06-05 16:16:04,844 epoch 7 - iter 1496/3747 - loss 0.16827581 - samples/sec: 16.64 - lr: 0.000002
2023-06-05 16:17:33,547 epoch 7 - iter 1870/3747 - loss 0.16655469 - samples/sec: 16.87 - lr: 0.000002
2023-06-05 16:19:01,967 epoch 7 - iter 2244/3747 - loss 0.16540283 - samples/sec: 16.93 - lr: 0.000002
2023-06-05 16:20:30,678 epoch 7 - iter 2618/3747 - loss 0.16609708 - samples/sec: 16.87 - lr: 0.000002
2023-06-05 16:21:58,728 epoch 7 - iter 2992/3747 - loss 0.16630150 - samples/sec: 17.00 - lr: 0.000002
2023-06-05 16:23:30,275 epoch 7 - iter 3366/3747 - loss 0.16549781 - samples/sec: 16.35 - lr: 0.000002
2023-06-05 16:25:01,397 epoch 7 - iter 3740/3747 - loss 0.16430813 - samples/sec: 16.43 - lr: 0.000002
2023-06-05 16:25:03,102 ----------------------------------------------------------------------------------------------------
2023-06-05 16:25:03,102 EPOCH 7 done: loss 0.1642 - lr 0.000002
2023-06-05 16:26:29,515 Evaluating as a multi-label problem: False
2023-06-05 16:26:29,585 DEV : loss 0.06703164428472519 - f1-score (micro avg)  0.9671
2023-06-05 16:26:29,720 BAD EPOCHS (no improvement): 4
2023-06-05 16:26:29,744 ----------------------------------------------------------------------------------------------------
2023-06-05 16:28:02,447 epoch 8 - iter 374/3747 - loss 0.16626912 - samples/sec: 16.15 - lr: 0.000002
2023-06-05 16:29:30,884 epoch 8 - iter 748/3747 - loss 0.16091358 - samples/sec: 16.92 - lr: 0.000002
2023-06-05 16:31:00,053 epoch 8 - iter 1122/3747 - loss 0.15768382 - samples/sec: 16.79 - lr: 0.000002
2023-06-05 16:32:26,758 epoch 8 - iter 1496/3747 - loss 0.15889236 - samples/sec: 17.26 - lr: 0.000001
2023-06-05 16:33:53,722 epoch 8 - iter 1870/3747 - loss 0.15998428 - samples/sec: 17.21 - lr: 0.000001
2023-06-05 16:35:21,621 epoch 8 - iter 2244/3747 - loss 0.16053000 - samples/sec: 17.03 - lr: 0.000001
2023-06-05 16:36:48,284 epoch 8 - iter 2618/3747 - loss 0.16182965 - samples/sec: 17.27 - lr: 0.000001
2023-06-05 16:38:14,793 epoch 8 - iter 2992/3747 - loss 0.16077502 - samples/sec: 17.30 - lr: 0.000001
2023-06-05 16:39:42,430 epoch 8 - iter 3366/3747 - loss 0.16116019 - samples/sec: 17.08 - lr: 0.000001
2023-06-05 16:41:09,502 epoch 8 - iter 3740/3747 - loss 0.16030238 - samples/sec: 17.19 - lr: 0.000001
2023-06-05 16:41:11,190 ----------------------------------------------------------------------------------------------------
2023-06-05 16:41:11,190 EPOCH 8 done: loss 0.1603 - lr 0.000001
2023-06-05 16:42:38,858 Evaluating as a multi-label problem: False
2023-06-05 16:42:38,922 DEV : loss 0.06610605120658875 - f1-score (micro avg)  0.9684
2023-06-05 16:42:39,044 BAD EPOCHS (no improvement): 4
2023-06-05 16:42:39,052 ----------------------------------------------------------------------------------------------------
2023-06-05 16:44:07,577 epoch 9 - iter 374/3747 - loss 0.15228506 - samples/sec: 16.91 - lr: 0.000001
2023-06-05 16:45:36,291 epoch 9 - iter 748/3747 - loss 0.15470409 - samples/sec: 16.87 - lr: 0.000001
2023-06-05 16:47:03,013 epoch 9 - iter 1122/3747 - loss 0.15363370 - samples/sec: 17.26 - lr: 0.000001
2023-06-05 16:48:31,360 epoch 9 - iter 1496/3747 - loss 0.15584455 - samples/sec: 16.94 - lr: 0.000001
2023-06-05 16:49:59,651 epoch 9 - iter 1870/3747 - loss 0.15610858 - samples/sec: 16.95 - lr: 0.000001
2023-06-05 16:51:28,519 epoch 9 - iter 2244/3747 - loss 0.15635361 - samples/sec: 16.84 - lr: 0.000001
2023-06-05 16:53:01,540 epoch 9 - iter 2618/3747 - loss 0.15669425 - samples/sec: 16.09 - lr: 0.000001
2023-06-05 16:54:30,216 epoch 9 - iter 2992/3747 - loss 0.15685204 - samples/sec: 16.88 - lr: 0.000001
2023-06-05 16:55:57,996 epoch 9 - iter 3366/3747 - loss 0.15657860 - samples/sec: 17.05 - lr: 0.000001
2023-06-05 16:57:26,261 epoch 9 - iter 3740/3747 - loss 0.15538842 - samples/sec: 16.96 - lr: 0.000001
2023-06-05 16:57:27,876 ----------------------------------------------------------------------------------------------------
2023-06-05 16:57:27,877 EPOCH 9 done: loss 0.1553 - lr 0.000001
2023-06-05 16:58:55,004 Evaluating as a multi-label problem: False
2023-06-05 16:58:55,070 DEV : loss 0.06673501431941986 - f1-score (micro avg)  0.9685
2023-06-05 16:58:55,177 BAD EPOCHS (no improvement): 4
2023-06-05 16:58:55,185 ----------------------------------------------------------------------------------------------------
2023-06-05 17:00:24,126 epoch 10 - iter 374/3747 - loss 0.16496361 - samples/sec: 16.83 - lr: 0.000001
2023-06-05 17:01:53,383 epoch 10 - iter 748/3747 - loss 0.16218091 - samples/sec: 16.77 - lr: 0.000000
2023-06-05 17:03:21,042 epoch 10 - iter 1122/3747 - loss 0.15854879 - samples/sec: 17.07 - lr: 0.000000
2023-06-05 17:04:47,866 epoch 10 - iter 1496/3747 - loss 0.15634647 - samples/sec: 17.24 - lr: 0.000000
2023-06-05 17:06:14,107 epoch 10 - iter 1870/3747 - loss 0.15613505 - samples/sec: 17.35 - lr: 0.000000
2023-06-05 17:07:43,888 epoch 10 - iter 2244/3747 - loss 0.15570362 - samples/sec: 16.67 - lr: 0.000000
2023-06-05 17:09:11,038 epoch 10 - iter 2618/3747 - loss 0.15617539 - samples/sec: 17.17 - lr: 0.000000
2023-06-05 17:10:38,186 epoch 10 - iter 2992/3747 - loss 0.15487509 - samples/sec: 17.17 - lr: 0.000000
2023-06-05 17:12:04,931 epoch 10 - iter 3366/3747 - loss 0.15495183 - samples/sec: 17.25 - lr: 0.000000
2023-06-05 17:13:33,589 epoch 10 - iter 3740/3747 - loss 0.15475140 - samples/sec: 16.88 - lr: 0.000000
2023-06-05 17:13:35,242 ----------------------------------------------------------------------------------------------------
2023-06-05 17:13:35,242 EPOCH 10 done: loss 0.1547 - lr 0.000000
2023-06-05 17:15:08,502 Evaluating as a multi-label problem: False
2023-06-05 17:15:08,577 DEV : loss 0.06701811403036118 - f1-score (micro avg)  0.9679
2023-06-05 17:15:08,737 BAD EPOCHS (no improvement): 4
2023-06-05 17:15:21,574 ----------------------------------------------------------------------------------------------------
2023-06-05 17:15:21,578 Testing using last state of model ...
2023-06-05 17:16:54,577 Evaluating as a multi-label problem: False
2023-06-05 17:16:54,641 0.9307	0.9442	0.9374	0.9067
2023-06-05 17:16:54,642 
Results:
- F-score (micro) 0.9374
- F-score (macro) 0.9254
- Accuracy 0.9067

By class:
              precision    recall  f1-score   support

         ORG     0.9179    0.9362    0.9270      1661
         LOC     0.9402    0.9430    0.9416      1668
         PER     0.9869    0.9808    0.9839      1617
        MISC     0.8188    0.8818    0.8491       702

   micro avg     0.9307    0.9442    0.9374      5648
   macro avg     0.9160    0.9355    0.9254      5648
weighted avg     0.9320    0.9442    0.9379      5648

2023-06-05 17:16:54,642 ----------------------------------------------------------------------------------------------------
