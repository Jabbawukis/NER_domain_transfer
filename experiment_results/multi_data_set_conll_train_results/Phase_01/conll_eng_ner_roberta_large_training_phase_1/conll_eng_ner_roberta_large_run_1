2023-06-05 11:49:46,009 ----------------------------------------------------------------------------------------------------
2023-06-05 11:49:46,014 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-05 11:49:46,034 ----------------------------------------------------------------------------------------------------
2023-06-05 11:49:46,034 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-06-05 11:49:46,036 ----------------------------------------------------------------------------------------------------
2023-06-05 11:49:46,036 Parameters:
2023-06-05 11:49:46,036  - learning_rate: "0.000005"
2023-06-05 11:49:46,037  - mini_batch_size: "4"
2023-06-05 11:49:46,038  - patience: "3"
2023-06-05 11:49:46,038  - anneal_factor: "0.5"
2023-06-05 11:49:46,038  - max_epochs: "10"
2023-06-05 11:49:46,038  - shuffle: "True"
2023-06-05 11:49:46,038  - train_with_dev: "False"
2023-06-05 11:49:46,038  - batch_growth_annealing: "False"
2023-06-05 11:49:46,038 ----------------------------------------------------------------------------------------------------
2023-06-05 11:49:46,039 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_1"
2023-06-05 11:49:46,039 ----------------------------------------------------------------------------------------------------
2023-06-05 11:49:46,039 Device: cuda:3
2023-06-05 11:49:46,040 ----------------------------------------------------------------------------------------------------
2023-06-05 11:49:46,040 Embeddings storage mode: none
2023-06-05 11:49:46,041 ----------------------------------------------------------------------------------------------------
2023-06-05 11:51:10,384 epoch 1 - iter 374/3747 - loss 3.11998448 - samples/sec: 17.74 - lr: 0.000000
2023-06-05 11:52:34,987 epoch 1 - iter 748/3747 - loss 2.21960818 - samples/sec: 17.69 - lr: 0.000001
2023-06-05 11:53:58,451 epoch 1 - iter 1122/3747 - loss 1.75565509 - samples/sec: 17.93 - lr: 0.000001
2023-06-05 11:55:19,739 epoch 1 - iter 1496/3747 - loss 1.51224589 - samples/sec: 18.41 - lr: 0.000002
2023-06-05 11:56:45,293 epoch 1 - iter 1870/3747 - loss 1.29195050 - samples/sec: 17.49 - lr: 0.000002
2023-06-05 11:58:17,712 epoch 1 - iter 2244/3747 - loss 1.11910652 - samples/sec: 16.19 - lr: 0.000003
2023-06-05 11:59:46,733 epoch 1 - iter 2618/3747 - loss 0.98970098 - samples/sec: 16.81 - lr: 0.000003
2023-06-05 12:01:13,231 epoch 1 - iter 2992/3747 - loss 0.90125572 - samples/sec: 17.30 - lr: 0.000004
2023-06-05 12:02:41,393 epoch 1 - iter 3366/3747 - loss 0.82403361 - samples/sec: 16.98 - lr: 0.000004
2023-06-05 12:04:10,778 epoch 1 - iter 3740/3747 - loss 0.75470570 - samples/sec: 16.74 - lr: 0.000005
2023-06-05 12:04:12,331 ----------------------------------------------------------------------------------------------------
2023-06-05 12:04:12,331 EPOCH 1 done: loss 0.7541 - lr 0.000005
2023-06-05 12:05:28,178 Evaluating as a multi-label problem: False
2023-06-05 12:05:28,245 DEV : loss 0.11092130094766617 - f1-score (micro avg)  0.9132
2023-06-05 12:05:28,353 BAD EPOCHS (no improvement): 4
2023-06-05 12:05:28,357 ----------------------------------------------------------------------------------------------------
2023-06-05 12:06:59,468 epoch 2 - iter 374/3747 - loss 0.23563713 - samples/sec: 16.43 - lr: 0.000005
2023-06-05 12:08:27,962 epoch 2 - iter 748/3747 - loss 0.23620552 - samples/sec: 16.91 - lr: 0.000005
2023-06-05 12:09:53,521 epoch 2 - iter 1122/3747 - loss 0.24085939 - samples/sec: 17.49 - lr: 0.000005
2023-06-05 12:11:21,006 epoch 2 - iter 1496/3747 - loss 0.23830457 - samples/sec: 17.11 - lr: 0.000005
2023-06-05 12:12:49,977 epoch 2 - iter 1870/3747 - loss 0.23858045 - samples/sec: 16.82 - lr: 0.000005
2023-06-05 12:14:16,916 epoch 2 - iter 2244/3747 - loss 0.23799749 - samples/sec: 17.22 - lr: 0.000005
2023-06-05 12:15:46,966 epoch 2 - iter 2618/3747 - loss 0.23708289 - samples/sec: 16.62 - lr: 0.000005
2023-06-05 12:17:15,505 epoch 2 - iter 2992/3747 - loss 0.23572750 - samples/sec: 16.90 - lr: 0.000005
2023-06-05 12:18:42,773 epoch 2 - iter 3366/3747 - loss 0.23387495 - samples/sec: 17.15 - lr: 0.000005
2023-06-05 12:20:09,137 epoch 2 - iter 3740/3747 - loss 0.23468684 - samples/sec: 17.33 - lr: 0.000004
2023-06-05 12:20:10,744 ----------------------------------------------------------------------------------------------------
2023-06-05 12:20:10,744 EPOCH 2 done: loss 0.2347 - lr 0.000004
2023-06-05 12:21:32,303 Evaluating as a multi-label problem: False
2023-06-05 12:21:32,373 DEV : loss 0.06971760094165802 - f1-score (micro avg)  0.9535
2023-06-05 12:21:32,485 BAD EPOCHS (no improvement): 4
2023-06-05 12:21:32,488 ----------------------------------------------------------------------------------------------------
2023-06-05 12:22:59,826 epoch 3 - iter 374/3747 - loss 0.22183878 - samples/sec: 17.14 - lr: 0.000004
2023-06-05 12:24:27,837 epoch 3 - iter 748/3747 - loss 0.21523175 - samples/sec: 17.01 - lr: 0.000004
2023-06-05 12:25:55,391 epoch 3 - iter 1122/3747 - loss 0.21442941 - samples/sec: 17.09 - lr: 0.000004
2023-06-05 12:27:19,907 epoch 3 - iter 1496/3747 - loss 0.21580306 - samples/sec: 17.71 - lr: 0.000004
2023-06-05 12:28:47,149 epoch 3 - iter 1870/3747 - loss 0.21521099 - samples/sec: 17.16 - lr: 0.000004
2023-06-05 12:30:14,476 epoch 3 - iter 2244/3747 - loss 0.21339130 - samples/sec: 17.14 - lr: 0.000004
2023-06-05 12:31:41,111 epoch 3 - iter 2618/3747 - loss 0.21088849 - samples/sec: 17.28 - lr: 0.000004
2023-06-05 12:33:09,612 epoch 3 - iter 2992/3747 - loss 0.20973628 - samples/sec: 16.91 - lr: 0.000004
2023-06-05 12:34:38,912 epoch 3 - iter 3366/3747 - loss 0.20883742 - samples/sec: 16.76 - lr: 0.000004
2023-06-05 12:36:12,598 epoch 3 - iter 3740/3747 - loss 0.20836371 - samples/sec: 15.98 - lr: 0.000004
2023-06-05 12:36:14,219 ----------------------------------------------------------------------------------------------------
2023-06-05 12:36:14,219 EPOCH 3 done: loss 0.2083 - lr 0.000004
2023-06-05 12:37:36,108 Evaluating as a multi-label problem: False
2023-06-05 12:37:36,177 DEV : loss 0.08074226975440979 - f1-score (micro avg)  0.955
2023-06-05 12:37:36,288 BAD EPOCHS (no improvement): 4
2023-06-05 12:37:36,291 ----------------------------------------------------------------------------------------------------
2023-06-05 12:39:05,297 epoch 4 - iter 374/3747 - loss 0.19597549 - samples/sec: 16.82 - lr: 0.000004
2023-06-05 12:40:34,017 epoch 4 - iter 748/3747 - loss 0.19673478 - samples/sec: 16.87 - lr: 0.000004
2023-06-05 12:42:02,201 epoch 4 - iter 1122/3747 - loss 0.19702754 - samples/sec: 16.97 - lr: 0.000004
2023-06-05 12:43:30,764 epoch 4 - iter 1496/3747 - loss 0.19630017 - samples/sec: 16.90 - lr: 0.000004
2023-06-05 12:44:59,293 epoch 4 - iter 1870/3747 - loss 0.19747057 - samples/sec: 16.91 - lr: 0.000004
2023-06-05 12:46:25,947 epoch 4 - iter 2244/3747 - loss 0.19579608 - samples/sec: 17.27 - lr: 0.000004
2023-06-05 12:47:52,217 epoch 4 - iter 2618/3747 - loss 0.19336344 - samples/sec: 17.35 - lr: 0.000004
2023-06-05 12:49:17,492 epoch 4 - iter 2992/3747 - loss 0.19068712 - samples/sec: 17.55 - lr: 0.000003
2023-06-05 12:50:44,627 epoch 4 - iter 3366/3747 - loss 0.18999352 - samples/sec: 17.18 - lr: 0.000003
2023-06-05 12:52:11,748 epoch 4 - iter 3740/3747 - loss 0.18948652 - samples/sec: 17.18 - lr: 0.000003
2023-06-05 12:52:13,275 ----------------------------------------------------------------------------------------------------
2023-06-05 12:52:13,276 EPOCH 4 done: loss 0.1896 - lr 0.000003
2023-06-05 12:53:37,856 Evaluating as a multi-label problem: False
2023-06-05 12:53:37,924 DEV : loss 0.07302984595298767 - f1-score (micro avg)  0.9578
2023-06-05 12:53:38,040 BAD EPOCHS (no improvement): 4
2023-06-05 12:53:38,043 ----------------------------------------------------------------------------------------------------
2023-06-05 12:55:06,507 epoch 5 - iter 374/3747 - loss 0.18104393 - samples/sec: 16.92 - lr: 0.000003
2023-06-05 12:56:52,856 epoch 5 - iter 748/3747 - loss 0.18173068 - samples/sec: 14.07 - lr: 0.000003
2023-06-05 12:58:41,237 epoch 5 - iter 1122/3747 - loss 0.17943227 - samples/sec: 13.81 - lr: 0.000003
2023-06-05 13:00:35,042 epoch 5 - iter 1496/3747 - loss 0.17590402 - samples/sec: 13.15 - lr: 0.000003
2023-06-05 13:02:22,067 epoch 5 - iter 1870/3747 - loss 0.17649305 - samples/sec: 13.98 - lr: 0.000003
2023-06-05 13:03:56,632 epoch 5 - iter 2244/3747 - loss 0.17800948 - samples/sec: 15.83 - lr: 0.000003
2023-06-05 13:05:22,578 epoch 5 - iter 2618/3747 - loss 0.17832380 - samples/sec: 17.42 - lr: 0.000003
2023-06-05 13:06:51,336 epoch 5 - iter 2992/3747 - loss 0.17764424 - samples/sec: 16.86 - lr: 0.000003
2023-06-05 13:08:16,266 epoch 5 - iter 3366/3747 - loss 0.17668148 - samples/sec: 17.62 - lr: 0.000003
2023-06-05 13:09:41,282 epoch 5 - iter 3740/3747 - loss 0.17749547 - samples/sec: 17.60 - lr: 0.000003
2023-06-05 13:09:42,995 ----------------------------------------------------------------------------------------------------
2023-06-05 13:09:42,995 EPOCH 5 done: loss 0.1776 - lr 0.000003
2023-06-05 13:11:04,603 Evaluating as a multi-label problem: False
2023-06-05 13:11:04,678 DEV : loss 0.06178849935531616 - f1-score (micro avg)  0.9652
2023-06-05 13:11:04,785 BAD EPOCHS (no improvement): 4
2023-06-05 13:11:04,788 ----------------------------------------------------------------------------------------------------
2023-06-05 13:12:29,775 epoch 6 - iter 374/3747 - loss 0.17871222 - samples/sec: 17.61 - lr: 0.000003
2023-06-05 13:13:54,351 epoch 6 - iter 748/3747 - loss 0.16859269 - samples/sec: 17.70 - lr: 0.000003
2023-06-05 13:15:19,215 epoch 6 - iter 1122/3747 - loss 0.16791215 - samples/sec: 17.64 - lr: 0.000003
2023-06-05 13:16:46,619 epoch 6 - iter 1496/3747 - loss 0.16658410 - samples/sec: 17.12 - lr: 0.000003
2023-06-05 13:18:11,546 epoch 6 - iter 1870/3747 - loss 0.16778163 - samples/sec: 17.62 - lr: 0.000003
2023-06-05 13:19:38,040 epoch 6 - iter 2244/3747 - loss 0.16778086 - samples/sec: 17.30 - lr: 0.000002
2023-06-05 13:21:05,619 epoch 6 - iter 2618/3747 - loss 0.16778354 - samples/sec: 17.09 - lr: 0.000002
2023-06-05 13:22:35,419 epoch 6 - iter 2992/3747 - loss 0.16925786 - samples/sec: 16.67 - lr: 0.000002
2023-06-05 13:24:00,717 epoch 6 - iter 3366/3747 - loss 0.16889219 - samples/sec: 17.55 - lr: 0.000002
2023-06-05 13:25:25,813 epoch 6 - iter 3740/3747 - loss 0.16761185 - samples/sec: 17.59 - lr: 0.000002
2023-06-05 13:25:27,421 ----------------------------------------------------------------------------------------------------
2023-06-05 13:25:27,422 EPOCH 6 done: loss 0.1677 - lr 0.000002
2023-06-05 13:26:46,127 Evaluating as a multi-label problem: False
2023-06-05 13:26:46,200 DEV : loss 0.06355174630880356 - f1-score (micro avg)  0.9686
2023-06-05 13:26:46,343 BAD EPOCHS (no improvement): 4
2023-06-05 13:26:46,346 ----------------------------------------------------------------------------------------------------
2023-06-05 13:28:12,590 epoch 7 - iter 374/3747 - loss 0.15714123 - samples/sec: 17.36 - lr: 0.000002
2023-06-05 13:29:38,627 epoch 7 - iter 748/3747 - loss 0.16296050 - samples/sec: 17.40 - lr: 0.000002
2023-06-05 13:31:01,514 epoch 7 - iter 1122/3747 - loss 0.16537246 - samples/sec: 18.06 - lr: 0.000002
2023-06-05 13:32:26,594 epoch 7 - iter 1496/3747 - loss 0.16362817 - samples/sec: 17.59 - lr: 0.000002
2023-06-05 13:33:50,266 epoch 7 - iter 1870/3747 - loss 0.16421150 - samples/sec: 17.89 - lr: 0.000002
2023-06-05 13:35:18,109 epoch 7 - iter 2244/3747 - loss 0.16335978 - samples/sec: 17.04 - lr: 0.000002
2023-06-05 13:36:47,494 epoch 7 - iter 2618/3747 - loss 0.16335702 - samples/sec: 16.74 - lr: 0.000002
2023-06-05 13:38:14,109 epoch 7 - iter 2992/3747 - loss 0.16448508 - samples/sec: 17.28 - lr: 0.000002
2023-06-05 13:39:37,261 epoch 7 - iter 3366/3747 - loss 0.16380412 - samples/sec: 18.00 - lr: 0.000002
2023-06-05 13:40:59,882 epoch 7 - iter 3740/3747 - loss 0.16511996 - samples/sec: 18.12 - lr: 0.000002
2023-06-05 13:41:01,357 ----------------------------------------------------------------------------------------------------
2023-06-05 13:41:01,357 EPOCH 7 done: loss 0.1651 - lr 0.000002
2023-06-05 13:42:23,408 Evaluating as a multi-label problem: False
2023-06-05 13:42:23,473 DEV : loss 0.06313402205705643 - f1-score (micro avg)  0.9672
2023-06-05 13:42:23,590 BAD EPOCHS (no improvement): 4
2023-06-05 13:42:23,605 ----------------------------------------------------------------------------------------------------
2023-06-05 13:43:47,047 epoch 8 - iter 374/3747 - loss 0.15197911 - samples/sec: 17.94 - lr: 0.000002
2023-06-05 13:45:13,770 epoch 8 - iter 748/3747 - loss 0.15666374 - samples/sec: 17.26 - lr: 0.000002
2023-06-05 13:46:36,267 epoch 8 - iter 1122/3747 - loss 0.15696351 - samples/sec: 18.14 - lr: 0.000002
2023-06-05 13:48:02,593 epoch 8 - iter 1496/3747 - loss 0.15619238 - samples/sec: 17.34 - lr: 0.000001
2023-06-05 13:49:26,639 epoch 8 - iter 1870/3747 - loss 0.15669229 - samples/sec: 17.81 - lr: 0.000001
2023-06-05 13:50:54,826 epoch 8 - iter 2244/3747 - loss 0.15764897 - samples/sec: 16.97 - lr: 0.000001
2023-06-05 13:52:22,324 epoch 8 - iter 2618/3747 - loss 0.15722354 - samples/sec: 17.11 - lr: 0.000001
2023-06-05 13:53:45,171 epoch 8 - iter 2992/3747 - loss 0.15659764 - samples/sec: 18.07 - lr: 0.000001
2023-06-05 13:55:09,538 epoch 8 - iter 3366/3747 - loss 0.15648994 - samples/sec: 17.74 - lr: 0.000001
2023-06-05 13:56:35,184 epoch 8 - iter 3740/3747 - loss 0.15614123 - samples/sec: 17.48 - lr: 0.000001
2023-06-05 13:56:36,847 ----------------------------------------------------------------------------------------------------
2023-06-05 13:56:36,847 EPOCH 8 done: loss 0.1562 - lr 0.000001
2023-06-05 13:58:02,601 Evaluating as a multi-label problem: False
2023-06-05 13:58:02,669 DEV : loss 0.05905681848526001 - f1-score (micro avg)  0.9687
2023-06-05 13:58:02,784 BAD EPOCHS (no improvement): 4
2023-06-05 13:58:02,786 ----------------------------------------------------------------------------------------------------
2023-06-05 13:59:28,052 epoch 9 - iter 374/3747 - loss 0.16165558 - samples/sec: 17.55 - lr: 0.000001
2023-06-05 14:00:57,408 epoch 9 - iter 748/3747 - loss 0.16137474 - samples/sec: 16.75 - lr: 0.000001
2023-06-05 14:02:25,249 epoch 9 - iter 1122/3747 - loss 0.15552610 - samples/sec: 17.04 - lr: 0.000001
2023-06-05 14:03:51,568 epoch 9 - iter 1496/3747 - loss 0.15559704 - samples/sec: 17.34 - lr: 0.000001
2023-06-05 14:05:17,903 epoch 9 - iter 1870/3747 - loss 0.15530161 - samples/sec: 17.34 - lr: 0.000001
2023-06-05 14:06:47,142 epoch 9 - iter 2244/3747 - loss 0.15412817 - samples/sec: 16.77 - lr: 0.000001
2023-06-05 14:08:15,635 epoch 9 - iter 2618/3747 - loss 0.15480230 - samples/sec: 16.91 - lr: 0.000001
2023-06-05 14:09:41,010 epoch 9 - iter 2992/3747 - loss 0.15378019 - samples/sec: 17.53 - lr: 0.000001
2023-06-05 14:11:08,554 epoch 9 - iter 3366/3747 - loss 0.15420634 - samples/sec: 17.10 - lr: 0.000001
2023-06-05 14:12:37,100 epoch 9 - iter 3740/3747 - loss 0.15419087 - samples/sec: 16.90 - lr: 0.000001
2023-06-05 14:12:38,567 ----------------------------------------------------------------------------------------------------
2023-06-05 14:12:38,567 EPOCH 9 done: loss 0.1542 - lr 0.000001
2023-06-05 14:13:58,823 Evaluating as a multi-label problem: False
2023-06-05 14:13:58,876 DEV : loss 0.05918851122260094 - f1-score (micro avg)  0.9707
2023-06-05 14:13:58,973 BAD EPOCHS (no improvement): 4
2023-06-05 14:13:58,976 ----------------------------------------------------------------------------------------------------
2023-06-05 14:15:24,487 epoch 10 - iter 374/3747 - loss 0.15159693 - samples/sec: 17.50 - lr: 0.000001
2023-06-05 14:16:47,753 epoch 10 - iter 748/3747 - loss 0.15308365 - samples/sec: 17.98 - lr: 0.000000
2023-06-05 14:18:12,998 epoch 10 - iter 1122/3747 - loss 0.15405164 - samples/sec: 17.56 - lr: 0.000000
2023-06-05 14:19:38,075 epoch 10 - iter 1496/3747 - loss 0.15376956 - samples/sec: 17.59 - lr: 0.000000
2023-06-05 14:21:04,366 epoch 10 - iter 1870/3747 - loss 0.15335323 - samples/sec: 17.34 - lr: 0.000000
2023-06-05 14:22:29,737 epoch 10 - iter 2244/3747 - loss 0.15197585 - samples/sec: 17.53 - lr: 0.000000
2023-06-05 14:23:54,289 epoch 10 - iter 2618/3747 - loss 0.15338970 - samples/sec: 17.70 - lr: 0.000000
2023-06-05 14:25:20,768 epoch 10 - iter 2992/3747 - loss 0.15145536 - samples/sec: 17.31 - lr: 0.000000
2023-06-05 14:26:49,882 epoch 10 - iter 3366/3747 - loss 0.15038834 - samples/sec: 16.79 - lr: 0.000000
2023-06-05 14:28:16,814 epoch 10 - iter 3740/3747 - loss 0.15052978 - samples/sec: 17.22 - lr: 0.000000
2023-06-05 14:28:18,346 ----------------------------------------------------------------------------------------------------
2023-06-05 14:28:18,346 EPOCH 10 done: loss 0.1504 - lr 0.000000
2023-06-05 14:29:41,851 Evaluating as a multi-label problem: False
2023-06-05 14:29:41,915 DEV : loss 0.05979633331298828 - f1-score (micro avg)  0.9701
2023-06-05 14:29:42,028 BAD EPOCHS (no improvement): 4
2023-06-05 14:29:54,736 ----------------------------------------------------------------------------------------------------
2023-06-05 14:29:54,740 Testing using last state of model ...
2023-06-05 14:31:22,187 Evaluating as a multi-label problem: False
2023-06-05 14:31:22,256 0.931	0.9465	0.9387	0.907
2023-06-05 14:31:22,256 
Results:
- F-score (micro) 0.9387
- F-score (macro) 0.9266
- Accuracy 0.907

By class:
              precision    recall  f1-score   support

         ORG     0.9111    0.9446    0.9276      1661
         LOC     0.9497    0.9400    0.9449      1668
         PER     0.9857    0.9827    0.9842      1617
        MISC     0.8190    0.8832    0.8499       702

   micro avg     0.9310    0.9465    0.9387      5648
   macro avg     0.9164    0.9376    0.9266      5648
weighted avg     0.9324    0.9465    0.9392      5648

2023-06-05 14:31:22,256 ----------------------------------------------------------------------------------------------------
