2023-05-28 03:48:58,993 ----------------------------------------------------------------------------------------------------
2023-05-28 03:48:58,997 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-28 03:48:59,000 ----------------------------------------------------------------------------------------------------
2023-05-28 03:48:59,001 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-28 03:48:59,001 ----------------------------------------------------------------------------------------------------
2023-05-28 03:48:59,001 Parameters:
2023-05-28 03:48:59,001  - learning_rate: "0.000005"
2023-05-28 03:48:59,001  - mini_batch_size: "4"
2023-05-28 03:48:59,001  - patience: "3"
2023-05-28 03:48:59,001  - anneal_factor: "0.5"
2023-05-28 03:48:59,001  - max_epochs: "10"
2023-05-28 03:48:59,001  - shuffle: "True"
2023-05-28 03:48:59,001  - train_with_dev: "False"
2023-05-28 03:48:59,001  - batch_growth_annealing: "False"
2023-05-28 03:48:59,001 ----------------------------------------------------------------------------------------------------
2023-05-28 03:48:59,002 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_3"
2023-05-28 03:48:59,002 ----------------------------------------------------------------------------------------------------
2023-05-28 03:48:59,002 Device: cuda:0
2023-05-28 03:48:59,002 ----------------------------------------------------------------------------------------------------
2023-05-28 03:48:59,002 Embeddings storage mode: none
2023-05-28 03:48:59,002 ----------------------------------------------------------------------------------------------------
2023-05-28 03:52:05,344 epoch 1 - iter 777/7770 - loss 2.26475128 - samples/sec: 16.69 - lr: 0.000001
2023-05-28 03:55:19,029 epoch 1 - iter 1554/7770 - loss 1.26018546 - samples/sec: 16.05 - lr: 0.000001
2023-05-28 03:58:24,477 epoch 1 - iter 2331/7770 - loss 0.92596176 - samples/sec: 16.77 - lr: 0.000002
2023-05-28 04:01:26,596 epoch 1 - iter 3108/7770 - loss 0.72831144 - samples/sec: 17.07 - lr: 0.000002
2023-05-28 04:04:31,218 epoch 1 - iter 3885/7770 - loss 0.60445621 - samples/sec: 16.84 - lr: 0.000003
2023-05-28 04:07:28,248 epoch 1 - iter 4662/7770 - loss 0.52343030 - samples/sec: 17.56 - lr: 0.000003
2023-05-28 04:10:20,314 epoch 1 - iter 5439/7770 - loss 0.47463030 - samples/sec: 18.07 - lr: 0.000003
2023-05-28 04:13:16,303 epoch 1 - iter 6216/7770 - loss 0.43194334 - samples/sec: 17.67 - lr: 0.000004
2023-05-28 04:16:13,882 epoch 1 - iter 6993/7770 - loss 0.39766221 - samples/sec: 17.51 - lr: 0.000005
2023-05-28 04:19:09,875 epoch 1 - iter 7770/7770 - loss 0.36753395 - samples/sec: 17.67 - lr: 0.000005
2023-05-28 04:19:09,878 ----------------------------------------------------------------------------------------------------
2023-05-28 04:19:09,879 EPOCH 1 done: loss 0.3675 - lr 0.000005
2023-05-28 04:22:07,603 Evaluating as a multi-label problem: False
2023-05-28 04:22:07,705 DEV : loss 0.0900225043296814 - f1-score (micro avg)  0.9564
2023-05-28 04:22:07,950 BAD EPOCHS (no improvement): 4
2023-05-28 04:22:07,953 ----------------------------------------------------------------------------------------------------
2023-05-28 04:25:14,207 epoch 2 - iter 777/7770 - loss 0.14913853 - samples/sec: 16.70 - lr: 0.000005
2023-05-28 04:28:16,900 epoch 2 - iter 1554/7770 - loss 0.15003029 - samples/sec: 17.02 - lr: 0.000005
2023-05-28 04:31:16,335 epoch 2 - iter 2331/7770 - loss 0.15001162 - samples/sec: 17.33 - lr: 0.000005
2023-05-28 04:34:16,142 epoch 2 - iter 3108/7770 - loss 0.15049689 - samples/sec: 17.29 - lr: 0.000005
2023-05-28 04:37:16,722 epoch 2 - iter 3885/7770 - loss 0.14910611 - samples/sec: 17.22 - lr: 0.000005
2023-05-28 04:40:26,787 epoch 2 - iter 4662/7770 - loss 0.14885583 - samples/sec: 16.36 - lr: 0.000005
2023-05-28 04:43:28,645 epoch 2 - iter 5439/7770 - loss 0.14934554 - samples/sec: 17.10 - lr: 0.000005
2023-05-28 04:46:29,313 epoch 2 - iter 6216/7770 - loss 0.15018044 - samples/sec: 17.21 - lr: 0.000005
2023-05-28 04:49:28,455 epoch 2 - iter 6993/7770 - loss 0.14966308 - samples/sec: 17.36 - lr: 0.000005
2023-05-28 04:52:30,042 epoch 2 - iter 7770/7770 - loss 0.14941115 - samples/sec: 17.12 - lr: 0.000004
2023-05-28 04:52:30,047 ----------------------------------------------------------------------------------------------------
2023-05-28 04:52:30,047 EPOCH 2 done: loss 0.1494 - lr 0.000004
2023-05-28 04:55:15,937 Evaluating as a multi-label problem: False
2023-05-28 04:55:16,003 DEV : loss 0.09763479977846146 - f1-score (micro avg)  0.9557
2023-05-28 04:55:16,174 BAD EPOCHS (no improvement): 4
2023-05-28 04:55:16,177 ----------------------------------------------------------------------------------------------------
2023-05-28 04:58:19,557 epoch 3 - iter 777/7770 - loss 0.15087428 - samples/sec: 16.96 - lr: 0.000004
2023-05-28 05:01:24,229 epoch 3 - iter 1554/7770 - loss 0.15165296 - samples/sec: 16.84 - lr: 0.000004
2023-05-28 05:04:27,128 epoch 3 - iter 2331/7770 - loss 0.15008607 - samples/sec: 17.00 - lr: 0.000004
2023-05-28 05:07:30,178 epoch 3 - iter 3108/7770 - loss 0.14773715 - samples/sec: 16.99 - lr: 0.000004
2023-05-28 05:10:33,552 epoch 3 - iter 3885/7770 - loss 0.14680310 - samples/sec: 16.96 - lr: 0.000004
2023-05-28 05:13:34,338 epoch 3 - iter 4662/7770 - loss 0.14764598 - samples/sec: 17.20 - lr: 0.000004
2023-05-28 05:16:35,262 epoch 3 - iter 5439/7770 - loss 0.14690024 - samples/sec: 17.19 - lr: 0.000004
2023-05-28 05:19:34,872 epoch 3 - iter 6216/7770 - loss 0.14707130 - samples/sec: 17.31 - lr: 0.000004
2023-05-28 05:22:33,470 epoch 3 - iter 6993/7770 - loss 0.14740410 - samples/sec: 17.41 - lr: 0.000004
2023-05-28 05:25:50,045 epoch 3 - iter 7770/7770 - loss 0.14709275 - samples/sec: 15.82 - lr: 0.000004
2023-05-28 05:25:50,048 ----------------------------------------------------------------------------------------------------
2023-05-28 05:25:50,048 EPOCH 3 done: loss 0.1471 - lr 0.000004
2023-05-28 05:28:27,828 Evaluating as a multi-label problem: False
2023-05-28 05:28:27,924 DEV : loss 0.09356421232223511 - f1-score (micro avg)  0.9517
2023-05-28 05:28:28,181 BAD EPOCHS (no improvement): 4
2023-05-28 05:28:28,192 ----------------------------------------------------------------------------------------------------
2023-05-28 05:31:42,142 epoch 4 - iter 777/7770 - loss 0.14801793 - samples/sec: 16.03 - lr: 0.000004
2023-05-28 05:34:46,709 epoch 4 - iter 1554/7770 - loss 0.15018681 - samples/sec: 16.85 - lr: 0.000004
2023-05-28 05:37:49,414 epoch 4 - iter 2331/7770 - loss 0.14772956 - samples/sec: 17.02 - lr: 0.000004
2023-05-28 05:40:50,836 epoch 4 - iter 3108/7770 - loss 0.14597570 - samples/sec: 17.14 - lr: 0.000004
2023-05-28 05:43:52,557 epoch 4 - iter 3885/7770 - loss 0.14431596 - samples/sec: 17.11 - lr: 0.000004
2023-05-28 05:46:51,714 epoch 4 - iter 4662/7770 - loss 0.14394806 - samples/sec: 17.36 - lr: 0.000004
2023-05-28 05:49:51,262 epoch 4 - iter 5439/7770 - loss 0.14349921 - samples/sec: 17.32 - lr: 0.000004
2023-05-28 05:52:51,886 epoch 4 - iter 6216/7770 - loss 0.14379004 - samples/sec: 17.22 - lr: 0.000003
2023-05-28 05:55:53,868 epoch 4 - iter 6993/7770 - loss 0.14356848 - samples/sec: 17.09 - lr: 0.000003
2023-05-28 05:58:52,385 epoch 4 - iter 7770/7770 - loss 0.14338413 - samples/sec: 17.42 - lr: 0.000003
2023-05-28 05:58:52,389 ----------------------------------------------------------------------------------------------------
2023-05-28 05:58:52,389 EPOCH 4 done: loss 0.1434 - lr 0.000003
2023-05-28 06:01:48,058 Evaluating as a multi-label problem: False
2023-05-28 06:01:48,160 DEV : loss 0.08749207109212875 - f1-score (micro avg)  0.9636
2023-05-28 06:01:48,414 BAD EPOCHS (no improvement): 4
2023-05-28 06:01:48,418 ----------------------------------------------------------------------------------------------------
2023-05-28 06:04:55,412 epoch 5 - iter 777/7770 - loss 0.14679789 - samples/sec: 16.63 - lr: 0.000003
2023-05-28 06:07:58,018 epoch 5 - iter 1554/7770 - loss 0.14449177 - samples/sec: 17.03 - lr: 0.000003
2023-05-28 06:11:00,049 epoch 5 - iter 2331/7770 - loss 0.14451089 - samples/sec: 17.08 - lr: 0.000003
2023-05-28 06:14:01,027 epoch 5 - iter 3108/7770 - loss 0.14295853 - samples/sec: 17.18 - lr: 0.000003
2023-05-28 06:17:05,941 epoch 5 - iter 3885/7770 - loss 0.14337337 - samples/sec: 16.82 - lr: 0.000003
2023-05-28 06:20:08,582 epoch 5 - iter 4662/7770 - loss 0.14298812 - samples/sec: 17.03 - lr: 0.000003
2023-05-28 06:23:09,593 epoch 5 - iter 5439/7770 - loss 0.14216580 - samples/sec: 17.18 - lr: 0.000003
2023-05-28 06:26:13,238 epoch 5 - iter 6216/7770 - loss 0.14144095 - samples/sec: 16.93 - lr: 0.000003
2023-05-28 06:29:11,361 epoch 5 - iter 6993/7770 - loss 0.14078876 - samples/sec: 17.46 - lr: 0.000003
2023-05-28 06:32:12,333 epoch 5 - iter 7770/7770 - loss 0.14063002 - samples/sec: 17.18 - lr: 0.000003
2023-05-28 06:32:12,335 ----------------------------------------------------------------------------------------------------
2023-05-28 06:32:12,336 EPOCH 5 done: loss 0.1406 - lr 0.000003
2023-05-28 06:34:53,694 Evaluating as a multi-label problem: False
2023-05-28 06:34:53,794 DEV : loss 0.08601664751768112 - f1-score (micro avg)  0.961
2023-05-28 06:34:54,035 BAD EPOCHS (no improvement): 4
2023-05-28 06:34:54,038 ----------------------------------------------------------------------------------------------------
2023-05-28 06:37:56,664 epoch 6 - iter 777/7770 - loss 0.13175941 - samples/sec: 17.03 - lr: 0.000003
2023-05-28 06:40:58,196 epoch 6 - iter 1554/7770 - loss 0.13367224 - samples/sec: 17.13 - lr: 0.000003
2023-05-28 06:44:03,026 epoch 6 - iter 2331/7770 - loss 0.13389298 - samples/sec: 16.82 - lr: 0.000003
2023-05-28 06:47:02,797 epoch 6 - iter 3108/7770 - loss 0.13334993 - samples/sec: 17.30 - lr: 0.000003
2023-05-28 06:50:04,686 epoch 6 - iter 3885/7770 - loss 0.13331539 - samples/sec: 17.10 - lr: 0.000003
2023-05-28 06:53:06,321 epoch 6 - iter 4662/7770 - loss 0.13352644 - samples/sec: 17.12 - lr: 0.000002
2023-05-28 06:56:03,135 epoch 6 - iter 5439/7770 - loss 0.13561320 - samples/sec: 17.59 - lr: 0.000002
2023-05-28 06:59:00,482 epoch 6 - iter 6216/7770 - loss 0.13563307 - samples/sec: 17.53 - lr: 0.000002
2023-05-28 07:02:12,060 epoch 6 - iter 6993/7770 - loss 0.13607430 - samples/sec: 16.23 - lr: 0.000002
2023-05-28 07:05:17,492 epoch 6 - iter 7770/7770 - loss 0.13646062 - samples/sec: 16.77 - lr: 0.000002
2023-05-28 07:05:17,496 ----------------------------------------------------------------------------------------------------
2023-05-28 07:05:17,496 EPOCH 6 done: loss 0.1365 - lr 0.000002
2023-05-28 07:08:05,874 Evaluating as a multi-label problem: False
2023-05-28 07:08:05,966 DEV : loss 0.08598911762237549 - f1-score (micro avg)  0.9613
2023-05-28 07:08:06,226 BAD EPOCHS (no improvement): 4
2023-05-28 07:08:06,229 ----------------------------------------------------------------------------------------------------
2023-05-28 07:11:13,678 epoch 7 - iter 777/7770 - loss 0.14011087 - samples/sec: 16.59 - lr: 0.000002
2023-05-28 07:14:14,074 epoch 7 - iter 1554/7770 - loss 0.13735722 - samples/sec: 17.24 - lr: 0.000002
2023-05-28 07:17:15,426 epoch 7 - iter 2331/7770 - loss 0.13507405 - samples/sec: 17.15 - lr: 0.000002
2023-05-28 07:20:15,469 epoch 7 - iter 3108/7770 - loss 0.13518114 - samples/sec: 17.27 - lr: 0.000002
2023-05-28 07:23:15,694 epoch 7 - iter 3885/7770 - loss 0.13589124 - samples/sec: 17.25 - lr: 0.000002
2023-05-28 07:26:16,769 epoch 7 - iter 4662/7770 - loss 0.13548910 - samples/sec: 17.17 - lr: 0.000002
2023-05-28 07:29:12,990 epoch 7 - iter 5439/7770 - loss 0.13495704 - samples/sec: 17.65 - lr: 0.000002
2023-05-28 07:32:13,911 epoch 7 - iter 6216/7770 - loss 0.13498462 - samples/sec: 17.19 - lr: 0.000002
2023-05-28 07:35:15,048 epoch 7 - iter 6993/7770 - loss 0.13509567 - samples/sec: 17.17 - lr: 0.000002
2023-05-28 07:38:15,651 epoch 7 - iter 7770/7770 - loss 0.13505531 - samples/sec: 17.22 - lr: 0.000002
2023-05-28 07:38:15,655 ----------------------------------------------------------------------------------------------------
2023-05-28 07:38:15,655 EPOCH 7 done: loss 0.1351 - lr 0.000002
2023-05-28 07:41:11,224 Evaluating as a multi-label problem: False
2023-05-28 07:41:11,321 DEV : loss 0.08790789544582367 - f1-score (micro avg)  0.9622
2023-05-28 07:41:11,566 BAD EPOCHS (no improvement): 4
2023-05-28 07:41:11,569 ----------------------------------------------------------------------------------------------------
2023-05-28 07:44:17,225 epoch 8 - iter 777/7770 - loss 0.13455075 - samples/sec: 16.75 - lr: 0.000002
2023-05-28 07:47:20,216 epoch 8 - iter 1554/7770 - loss 0.13401689 - samples/sec: 16.99 - lr: 0.000002
2023-05-28 07:50:25,186 epoch 8 - iter 2331/7770 - loss 0.13294386 - samples/sec: 16.81 - lr: 0.000002
2023-05-28 07:53:27,485 epoch 8 - iter 3108/7770 - loss 0.13225626 - samples/sec: 17.06 - lr: 0.000001
2023-05-28 07:56:34,394 epoch 8 - iter 3885/7770 - loss 0.13316713 - samples/sec: 16.64 - lr: 0.000001
2023-05-28 07:59:37,824 epoch 8 - iter 4662/7770 - loss 0.13294585 - samples/sec: 16.95 - lr: 0.000001
2023-05-28 08:02:38,152 epoch 8 - iter 5439/7770 - loss 0.13382715 - samples/sec: 17.24 - lr: 0.000001
2023-05-28 08:05:36,074 epoch 8 - iter 6216/7770 - loss 0.13324705 - samples/sec: 17.48 - lr: 0.000001
2023-05-28 08:08:37,170 epoch 8 - iter 6993/7770 - loss 0.13320007 - samples/sec: 17.17 - lr: 0.000001
2023-05-28 08:11:36,327 epoch 8 - iter 7770/7770 - loss 0.13312783 - samples/sec: 17.36 - lr: 0.000001
2023-05-28 08:11:36,331 ----------------------------------------------------------------------------------------------------
2023-05-28 08:11:36,331 EPOCH 8 done: loss 0.1331 - lr 0.000001
2023-05-28 08:14:18,817 Evaluating as a multi-label problem: False
2023-05-28 08:14:18,876 DEV : loss 0.08711250871419907 - f1-score (micro avg)  0.9632
2023-05-28 08:14:19,053 BAD EPOCHS (no improvement): 4
2023-05-28 08:14:19,055 ----------------------------------------------------------------------------------------------------
2023-05-28 08:17:22,397 epoch 9 - iter 777/7770 - loss 0.13784613 - samples/sec: 16.96 - lr: 0.000001
2023-05-28 08:20:22,899 epoch 9 - iter 1554/7770 - loss 0.13614152 - samples/sec: 17.23 - lr: 0.000001
2023-05-28 08:23:26,594 epoch 9 - iter 2331/7770 - loss 0.13644897 - samples/sec: 16.93 - lr: 0.000001
2023-05-28 08:26:25,716 epoch 9 - iter 3108/7770 - loss 0.13595753 - samples/sec: 17.36 - lr: 0.000001
2023-05-28 08:29:25,607 epoch 9 - iter 3885/7770 - loss 0.13545258 - samples/sec: 17.29 - lr: 0.000001
2023-05-28 08:32:22,424 epoch 9 - iter 4662/7770 - loss 0.13446102 - samples/sec: 17.59 - lr: 0.000001
2023-05-28 08:35:19,761 epoch 9 - iter 5439/7770 - loss 0.13447594 - samples/sec: 17.53 - lr: 0.000001
2023-05-28 08:38:22,137 epoch 9 - iter 6216/7770 - loss 0.13419260 - samples/sec: 17.05 - lr: 0.000001
2023-05-28 08:41:32,914 epoch 9 - iter 6993/7770 - loss 0.13369993 - samples/sec: 16.30 - lr: 0.000001
2023-05-28 08:44:32,895 epoch 9 - iter 7770/7770 - loss 0.13354425 - samples/sec: 17.28 - lr: 0.000001
2023-05-28 08:44:32,898 ----------------------------------------------------------------------------------------------------
2023-05-28 08:44:32,898 EPOCH 9 done: loss 0.1335 - lr 0.000001
2023-05-28 08:47:18,323 Evaluating as a multi-label problem: False
2023-05-28 08:47:18,421 DEV : loss 0.08998166024684906 - f1-score (micro avg)  0.9642
2023-05-28 08:47:18,678 BAD EPOCHS (no improvement): 4
2023-05-28 08:47:18,681 ----------------------------------------------------------------------------------------------------
2023-05-28 08:50:27,502 epoch 10 - iter 777/7770 - loss 0.13119723 - samples/sec: 16.47 - lr: 0.000001
2023-05-28 08:53:29,456 epoch 10 - iter 1554/7770 - loss 0.13308139 - samples/sec: 17.09 - lr: 0.000000
2023-05-28 08:56:29,763 epoch 10 - iter 2331/7770 - loss 0.13146394 - samples/sec: 17.25 - lr: 0.000000
2023-05-28 08:59:30,765 epoch 10 - iter 3108/7770 - loss 0.13112571 - samples/sec: 17.18 - lr: 0.000000
2023-05-28 09:02:31,269 epoch 10 - iter 3885/7770 - loss 0.13017279 - samples/sec: 17.23 - lr: 0.000000
2023-05-28 09:05:27,812 epoch 10 - iter 4662/7770 - loss 0.13036249 - samples/sec: 17.61 - lr: 0.000000
2023-05-28 09:08:27,355 epoch 10 - iter 5439/7770 - loss 0.13004348 - samples/sec: 17.32 - lr: 0.000000
2023-05-28 09:11:24,231 epoch 10 - iter 6216/7770 - loss 0.12975948 - samples/sec: 17.58 - lr: 0.000000
2023-05-28 09:14:23,611 epoch 10 - iter 6993/7770 - loss 0.13010201 - samples/sec: 17.33 - lr: 0.000000
2023-05-28 09:17:23,874 epoch 10 - iter 7770/7770 - loss 0.12982064 - samples/sec: 17.25 - lr: 0.000000
2023-05-28 09:17:23,877 ----------------------------------------------------------------------------------------------------
2023-05-28 09:17:23,877 EPOCH 10 done: loss 0.1298 - lr 0.000000
2023-05-28 09:20:30,305 Evaluating as a multi-label problem: False
2023-05-28 09:20:30,407 DEV : loss 0.09175936877727509 - f1-score (micro avg)  0.9638
2023-05-28 09:20:30,668 BAD EPOCHS (no improvement): 4
2023-05-28 09:20:43,917 ----------------------------------------------------------------------------------------------------
2023-05-28 09:20:43,920 Testing using last state of model ...
2023-05-28 09:24:44,933 Evaluating as a multi-label problem: False
2023-05-28 09:24:45,002 0.9322	0.9424	0.9373	0.9102
2023-05-28 09:24:45,003 
Results:
- F-score (micro) 0.9373
- F-score (macro) 0.9343
- Accuracy 0.9102

By class:
              precision    recall  f1-score   support

         PER     0.9737    0.9820    0.9778      2715
         ORG     0.8989    0.9406    0.9193      2543
         LOC     0.9473    0.9345    0.9408      2442
        MISC     0.8998    0.8984    0.8991      1889

   micro avg     0.9322    0.9424    0.9373      9589
   macro avg     0.9299    0.9389    0.9343      9589
weighted avg     0.9326    0.9424    0.9374      9589

2023-05-28 09:24:45,003 ----------------------------------------------------------------------------------------------------
2023-05-28 09:24:45,003 ----------------------------------------------------------------------------------------------------
2023-05-28 09:27:05,922 Evaluating as a multi-label problem: False
2023-05-28 09:27:05,970 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-28 09:27:05,970 0.9403	0.9394	0.9398	0.9211
2023-05-28 09:27:05,971 ----------------------------------------------------------------------------------------------------
2023-05-28 09:28:32,493 Evaluating as a multi-label problem: False
2023-05-28 09:28:32,561 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-28 09:28:32,561 0.9269	0.9446	0.9356	0.9029
