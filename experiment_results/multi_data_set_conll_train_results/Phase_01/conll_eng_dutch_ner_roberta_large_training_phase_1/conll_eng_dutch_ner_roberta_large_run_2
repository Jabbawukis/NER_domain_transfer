2023-05-27 22:06:45,198 ----------------------------------------------------------------------------------------------------
2023-05-27 22:06:45,203 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-27 22:06:45,207 ----------------------------------------------------------------------------------------------------
2023-05-27 22:06:45,208 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-27 22:06:45,208 ----------------------------------------------------------------------------------------------------
2023-05-27 22:06:45,208 Parameters:
2023-05-27 22:06:45,208  - learning_rate: "0.000005"
2023-05-27 22:06:45,208  - mini_batch_size: "4"
2023-05-27 22:06:45,208  - patience: "3"
2023-05-27 22:06:45,208  - anneal_factor: "0.5"
2023-05-27 22:06:45,208  - max_epochs: "10"
2023-05-27 22:06:45,208  - shuffle: "True"
2023-05-27 22:06:45,208  - train_with_dev: "False"
2023-05-27 22:06:45,208  - batch_growth_annealing: "False"
2023-05-27 22:06:45,208 ----------------------------------------------------------------------------------------------------
2023-05-27 22:06:45,208 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_2"
2023-05-27 22:06:45,208 ----------------------------------------------------------------------------------------------------
2023-05-27 22:06:45,208 Device: cuda:0
2023-05-27 22:06:45,208 ----------------------------------------------------------------------------------------------------
2023-05-27 22:06:45,208 Embeddings storage mode: none
2023-05-27 22:06:45,209 ----------------------------------------------------------------------------------------------------
2023-05-27 22:09:52,041 epoch 1 - iter 777/7770 - loss 1.83719165 - samples/sec: 16.64 - lr: 0.000001
2023-05-27 22:12:58,910 epoch 1 - iter 1554/7770 - loss 1.04497931 - samples/sec: 16.64 - lr: 0.000001
2023-05-27 22:16:05,736 epoch 1 - iter 2331/7770 - loss 0.77976136 - samples/sec: 16.64 - lr: 0.000002
2023-05-27 22:19:11,408 epoch 1 - iter 3108/7770 - loss 0.62260650 - samples/sec: 16.75 - lr: 0.000002
2023-05-27 22:22:16,710 epoch 1 - iter 3885/7770 - loss 0.52386126 - samples/sec: 16.78 - lr: 0.000003
2023-05-27 22:25:15,663 epoch 1 - iter 4662/7770 - loss 0.45955567 - samples/sec: 17.38 - lr: 0.000003
2023-05-27 22:28:11,296 epoch 1 - iter 5439/7770 - loss 0.41919021 - samples/sec: 17.70 - lr: 0.000003
2023-05-27 22:31:09,364 epoch 1 - iter 6216/7770 - loss 0.38508005 - samples/sec: 17.46 - lr: 0.000004
2023-05-27 22:34:06,489 epoch 1 - iter 6993/7770 - loss 0.35986404 - samples/sec: 17.55 - lr: 0.000005
2023-05-27 22:37:04,812 epoch 1 - iter 7770/7770 - loss 0.33578348 - samples/sec: 17.44 - lr: 0.000005
2023-05-27 22:37:04,815 ----------------------------------------------------------------------------------------------------
2023-05-27 22:37:04,815 EPOCH 1 done: loss 0.3358 - lr 0.000005
2023-05-27 22:40:11,728 Evaluating as a multi-label problem: False
2023-05-27 22:40:11,827 DEV : loss 0.08752363175153732 - f1-score (micro avg)  0.9526
2023-05-27 22:40:12,052 BAD EPOCHS (no improvement): 4
2023-05-27 22:40:12,055 ----------------------------------------------------------------------------------------------------
2023-05-27 22:43:18,965 epoch 2 - iter 777/7770 - loss 0.17076229 - samples/sec: 16.64 - lr: 0.000005
2023-05-27 22:46:23,010 epoch 2 - iter 1554/7770 - loss 0.16348712 - samples/sec: 16.89 - lr: 0.000005
2023-05-27 22:49:27,193 epoch 2 - iter 2331/7770 - loss 0.16421683 - samples/sec: 16.88 - lr: 0.000005
2023-05-27 22:52:35,712 epoch 2 - iter 3108/7770 - loss 0.16354589 - samples/sec: 16.49 - lr: 0.000005
2023-05-27 22:55:40,755 epoch 2 - iter 3885/7770 - loss 0.16271755 - samples/sec: 16.80 - lr: 0.000005
2023-05-27 22:58:42,510 epoch 2 - iter 4662/7770 - loss 0.16302154 - samples/sec: 17.11 - lr: 0.000005
2023-05-27 23:01:45,828 epoch 2 - iter 5439/7770 - loss 0.16303095 - samples/sec: 16.96 - lr: 0.000005
2023-05-27 23:04:48,600 epoch 2 - iter 6216/7770 - loss 0.16271593 - samples/sec: 17.01 - lr: 0.000005
2023-05-27 23:07:47,957 epoch 2 - iter 6993/7770 - loss 0.16243783 - samples/sec: 17.34 - lr: 0.000005
2023-05-27 23:10:51,425 epoch 2 - iter 7770/7770 - loss 0.16194007 - samples/sec: 16.95 - lr: 0.000004
2023-05-27 23:10:51,429 ----------------------------------------------------------------------------------------------------
2023-05-27 23:10:51,429 EPOCH 2 done: loss 0.1619 - lr 0.000004
2023-05-27 23:13:39,016 Evaluating as a multi-label problem: False
2023-05-27 23:13:39,105 DEV : loss 0.07725759595632553 - f1-score (micro avg)  0.9562
2023-05-27 23:13:39,263 BAD EPOCHS (no improvement): 4
2023-05-27 23:13:39,265 ----------------------------------------------------------------------------------------------------
2023-05-27 23:16:44,759 epoch 3 - iter 777/7770 - loss 0.15478306 - samples/sec: 16.76 - lr: 0.000004
2023-05-27 23:19:49,438 epoch 3 - iter 1554/7770 - loss 0.15548809 - samples/sec: 16.84 - lr: 0.000004
2023-05-27 23:22:51,745 epoch 3 - iter 2331/7770 - loss 0.15609593 - samples/sec: 17.06 - lr: 0.000004
2023-05-27 23:25:55,636 epoch 3 - iter 3108/7770 - loss 0.15426447 - samples/sec: 16.91 - lr: 0.000004
2023-05-27 23:28:57,308 epoch 3 - iter 3885/7770 - loss 0.15482663 - samples/sec: 17.12 - lr: 0.000004
2023-05-27 23:31:58,633 epoch 3 - iter 4662/7770 - loss 0.15486331 - samples/sec: 17.15 - lr: 0.000004
2023-05-27 23:34:58,682 epoch 3 - iter 5439/7770 - loss 0.15508100 - samples/sec: 17.27 - lr: 0.000004
2023-05-27 23:38:10,501 epoch 3 - iter 6216/7770 - loss 0.15464194 - samples/sec: 16.21 - lr: 0.000004
2023-05-27 23:41:16,416 epoch 3 - iter 6993/7770 - loss 0.15476755 - samples/sec: 16.73 - lr: 0.000004
2023-05-27 23:44:22,143 epoch 3 - iter 7770/7770 - loss 0.15480193 - samples/sec: 16.74 - lr: 0.000004
2023-05-27 23:44:22,145 ----------------------------------------------------------------------------------------------------
2023-05-27 23:44:22,145 EPOCH 3 done: loss 0.1548 - lr 0.000004
2023-05-27 23:47:09,557 Evaluating as a multi-label problem: False
2023-05-27 23:47:09,659 DEV : loss 0.07672763615846634 - f1-score (micro avg)  0.9577
2023-05-27 23:47:09,848 BAD EPOCHS (no improvement): 4
2023-05-27 23:47:09,850 ----------------------------------------------------------------------------------------------------
2023-05-27 23:50:13,378 epoch 4 - iter 777/7770 - loss 0.15020575 - samples/sec: 16.94 - lr: 0.000004
2023-05-27 23:53:18,896 epoch 4 - iter 1554/7770 - loss 0.15343126 - samples/sec: 16.76 - lr: 0.000004
2023-05-27 23:56:24,297 epoch 4 - iter 2331/7770 - loss 0.15298408 - samples/sec: 16.77 - lr: 0.000004
2023-05-27 23:59:27,718 epoch 4 - iter 3108/7770 - loss 0.15201876 - samples/sec: 16.95 - lr: 0.000004
2023-05-28 00:02:31,506 epoch 4 - iter 3885/7770 - loss 0.15261357 - samples/sec: 16.92 - lr: 0.000004
2023-05-28 00:05:33,473 epoch 4 - iter 4662/7770 - loss 0.15243932 - samples/sec: 17.09 - lr: 0.000004
2023-05-28 00:08:35,670 epoch 4 - iter 5439/7770 - loss 0.15239071 - samples/sec: 17.07 - lr: 0.000004
2023-05-28 00:11:36,044 epoch 4 - iter 6216/7770 - loss 0.15152947 - samples/sec: 17.24 - lr: 0.000003
2023-05-28 00:14:32,832 epoch 4 - iter 6993/7770 - loss 0.15179077 - samples/sec: 17.59 - lr: 0.000003
2023-05-28 00:17:32,746 epoch 4 - iter 7770/7770 - loss 0.15105012 - samples/sec: 17.28 - lr: 0.000003
2023-05-28 00:17:32,750 ----------------------------------------------------------------------------------------------------
2023-05-28 00:17:32,750 EPOCH 4 done: loss 0.1511 - lr 0.000003
2023-05-28 00:20:33,031 Evaluating as a multi-label problem: False
2023-05-28 00:20:33,127 DEV : loss 0.07937391102313995 - f1-score (micro avg)  0.9615
2023-05-28 00:20:33,360 BAD EPOCHS (no improvement): 4
2023-05-28 00:20:33,374 ----------------------------------------------------------------------------------------------------
2023-05-28 00:23:37,234 epoch 5 - iter 777/7770 - loss 0.14530039 - samples/sec: 16.91 - lr: 0.000003
2023-05-28 00:26:39,315 epoch 5 - iter 1554/7770 - loss 0.14773129 - samples/sec: 17.08 - lr: 0.000003
2023-05-28 00:29:41,678 epoch 5 - iter 2331/7770 - loss 0.14905916 - samples/sec: 17.05 - lr: 0.000003
2023-05-28 00:32:50,149 epoch 5 - iter 3108/7770 - loss 0.15006955 - samples/sec: 16.50 - lr: 0.000003
2023-05-28 00:35:51,337 epoch 5 - iter 3885/7770 - loss 0.14993626 - samples/sec: 17.16 - lr: 0.000003
2023-05-28 00:38:51,347 epoch 5 - iter 4662/7770 - loss 0.14854791 - samples/sec: 17.27 - lr: 0.000003
2023-05-28 00:41:52,203 epoch 5 - iter 5439/7770 - loss 0.14826848 - samples/sec: 17.19 - lr: 0.000003
2023-05-28 00:44:53,303 epoch 5 - iter 6216/7770 - loss 0.14849859 - samples/sec: 17.17 - lr: 0.000003
2023-05-28 00:47:56,900 epoch 5 - iter 6993/7770 - loss 0.14806211 - samples/sec: 16.94 - lr: 0.000003
2023-05-28 00:50:59,497 epoch 5 - iter 7770/7770 - loss 0.14837145 - samples/sec: 17.03 - lr: 0.000003
2023-05-28 00:50:59,501 ----------------------------------------------------------------------------------------------------
2023-05-28 00:50:59,501 EPOCH 5 done: loss 0.1484 - lr 0.000003
2023-05-28 00:53:44,182 Evaluating as a multi-label problem: False
2023-05-28 00:53:44,271 DEV : loss 0.08010050654411316 - f1-score (micro avg)  0.962
2023-05-28 00:53:44,525 BAD EPOCHS (no improvement): 4
2023-05-28 00:53:44,528 ----------------------------------------------------------------------------------------------------
2023-05-28 00:56:48,217 epoch 6 - iter 777/7770 - loss 0.15066324 - samples/sec: 16.93 - lr: 0.000003
2023-05-28 00:59:49,824 epoch 6 - iter 1554/7770 - loss 0.14708484 - samples/sec: 17.12 - lr: 0.000003
2023-05-28 01:02:51,463 epoch 6 - iter 2331/7770 - loss 0.14727524 - samples/sec: 17.12 - lr: 0.000003
2023-05-28 01:05:54,604 epoch 6 - iter 3108/7770 - loss 0.14602395 - samples/sec: 16.98 - lr: 0.000003
2023-05-28 01:08:55,949 epoch 6 - iter 3885/7770 - loss 0.14450459 - samples/sec: 17.15 - lr: 0.000003
2023-05-28 01:11:53,467 epoch 6 - iter 4662/7770 - loss 0.14397688 - samples/sec: 17.52 - lr: 0.000002
2023-05-28 01:14:53,364 epoch 6 - iter 5439/7770 - loss 0.14387904 - samples/sec: 17.28 - lr: 0.000002
2023-05-28 01:18:02,880 epoch 6 - iter 6216/7770 - loss 0.14329261 - samples/sec: 16.41 - lr: 0.000002
2023-05-28 01:21:07,303 epoch 6 - iter 6993/7770 - loss 0.14357795 - samples/sec: 16.86 - lr: 0.000002
2023-05-28 01:24:10,122 epoch 6 - iter 7770/7770 - loss 0.14392406 - samples/sec: 17.01 - lr: 0.000002
2023-05-28 01:24:10,126 ----------------------------------------------------------------------------------------------------
2023-05-28 01:24:10,126 EPOCH 6 done: loss 0.1439 - lr 0.000002
2023-05-28 01:27:01,871 Evaluating as a multi-label problem: False
2023-05-28 01:27:01,949 DEV : loss 0.0811174213886261 - f1-score (micro avg)  0.9632
2023-05-28 01:27:02,103 BAD EPOCHS (no improvement): 4
2023-05-28 01:27:02,106 ----------------------------------------------------------------------------------------------------
2023-05-28 01:30:05,730 epoch 7 - iter 777/7770 - loss 0.13805696 - samples/sec: 16.93 - lr: 0.000002
2023-05-28 01:33:05,628 epoch 7 - iter 1554/7770 - loss 0.13945535 - samples/sec: 17.28 - lr: 0.000002
2023-05-28 01:36:07,523 epoch 7 - iter 2331/7770 - loss 0.14041858 - samples/sec: 17.09 - lr: 0.000002
2023-05-28 01:39:09,848 epoch 7 - iter 3108/7770 - loss 0.14044798 - samples/sec: 17.05 - lr: 0.000002
2023-05-28 01:42:12,160 epoch 7 - iter 3885/7770 - loss 0.14106884 - samples/sec: 17.06 - lr: 0.000002
2023-05-28 01:45:12,125 epoch 7 - iter 4662/7770 - loss 0.14070611 - samples/sec: 17.28 - lr: 0.000002
2023-05-28 01:48:10,674 epoch 7 - iter 5439/7770 - loss 0.14032712 - samples/sec: 17.42 - lr: 0.000002
2023-05-28 01:51:09,711 epoch 7 - iter 6216/7770 - loss 0.14052082 - samples/sec: 17.37 - lr: 0.000002
2023-05-28 01:54:08,751 epoch 7 - iter 6993/7770 - loss 0.13999348 - samples/sec: 17.37 - lr: 0.000002
2023-05-28 01:57:08,152 epoch 7 - iter 7770/7770 - loss 0.14031174 - samples/sec: 17.33 - lr: 0.000002
2023-05-28 01:57:08,155 ----------------------------------------------------------------------------------------------------
2023-05-28 01:57:08,155 EPOCH 7 done: loss 0.1403 - lr 0.000002
2023-05-28 02:00:11,569 Evaluating as a multi-label problem: False
2023-05-28 02:00:11,673 DEV : loss 0.07881350070238113 - f1-score (micro avg)  0.9636
2023-05-28 02:00:11,917 BAD EPOCHS (no improvement): 4
2023-05-28 02:00:11,920 ----------------------------------------------------------------------------------------------------
2023-05-28 02:03:17,397 epoch 8 - iter 777/7770 - loss 0.13287975 - samples/sec: 16.77 - lr: 0.000002
2023-05-28 02:06:20,790 epoch 8 - iter 1554/7770 - loss 0.13410331 - samples/sec: 16.96 - lr: 0.000002
2023-05-28 02:09:21,263 epoch 8 - iter 2331/7770 - loss 0.13752031 - samples/sec: 17.23 - lr: 0.000002
2023-05-28 02:12:23,153 epoch 8 - iter 3108/7770 - loss 0.13720559 - samples/sec: 17.10 - lr: 0.000001
2023-05-28 02:15:35,269 epoch 8 - iter 3885/7770 - loss 0.13604779 - samples/sec: 16.19 - lr: 0.000001
2023-05-28 02:18:38,810 epoch 8 - iter 4662/7770 - loss 0.13607101 - samples/sec: 16.94 - lr: 0.000001
2023-05-28 02:21:39,843 epoch 8 - iter 5439/7770 - loss 0.13591847 - samples/sec: 17.18 - lr: 0.000001
2023-05-28 02:24:41,575 epoch 8 - iter 6216/7770 - loss 0.13663550 - samples/sec: 17.11 - lr: 0.000001
2023-05-28 02:27:40,722 epoch 8 - iter 6993/7770 - loss 0.13645467 - samples/sec: 17.36 - lr: 0.000001
2023-05-28 02:30:41,376 epoch 8 - iter 7770/7770 - loss 0.13667737 - samples/sec: 17.21 - lr: 0.000001
2023-05-28 02:30:41,379 ----------------------------------------------------------------------------------------------------
2023-05-28 02:30:41,380 EPOCH 8 done: loss 0.1367 - lr 0.000001
2023-05-28 02:33:31,299 Evaluating as a multi-label problem: False
2023-05-28 02:33:31,393 DEV : loss 0.07754595577716827 - f1-score (micro avg)  0.9623
2023-05-28 02:33:31,628 BAD EPOCHS (no improvement): 4
2023-05-28 02:33:31,631 ----------------------------------------------------------------------------------------------------
2023-05-28 02:36:34,288 epoch 9 - iter 777/7770 - loss 0.13257599 - samples/sec: 17.02 - lr: 0.000001
2023-05-28 02:39:38,530 epoch 9 - iter 1554/7770 - loss 0.13296996 - samples/sec: 16.88 - lr: 0.000001
2023-05-28 02:42:42,144 epoch 9 - iter 2331/7770 - loss 0.13243526 - samples/sec: 16.93 - lr: 0.000001
2023-05-28 02:45:45,724 epoch 9 - iter 3108/7770 - loss 0.13263525 - samples/sec: 16.94 - lr: 0.000001
2023-05-28 02:48:46,475 epoch 9 - iter 3885/7770 - loss 0.13238398 - samples/sec: 17.20 - lr: 0.000001
2023-05-28 02:51:48,225 epoch 9 - iter 4662/7770 - loss 0.13254497 - samples/sec: 17.11 - lr: 0.000001
2023-05-28 02:54:46,403 epoch 9 - iter 5439/7770 - loss 0.13391636 - samples/sec: 17.45 - lr: 0.000001
2023-05-28 02:57:48,355 epoch 9 - iter 6216/7770 - loss 0.13419574 - samples/sec: 17.09 - lr: 0.000001
2023-05-28 03:01:00,446 epoch 9 - iter 6993/7770 - loss 0.13438387 - samples/sec: 16.19 - lr: 0.000001
2023-05-28 03:04:05,197 epoch 9 - iter 7770/7770 - loss 0.13416134 - samples/sec: 16.83 - lr: 0.000001
2023-05-28 03:04:05,202 ----------------------------------------------------------------------------------------------------
2023-05-28 03:04:05,202 EPOCH 9 done: loss 0.1342 - lr 0.000001
2023-05-28 03:06:50,531 Evaluating as a multi-label problem: False
2023-05-28 03:06:50,618 DEV : loss 0.08177895098924637 - f1-score (micro avg)  0.9639
2023-05-28 03:06:50,859 BAD EPOCHS (no improvement): 4
2023-05-28 03:06:50,863 ----------------------------------------------------------------------------------------------------
2023-05-28 03:10:02,288 epoch 10 - iter 777/7770 - loss 0.13060812 - samples/sec: 16.24 - lr: 0.000001
2023-05-28 03:13:03,667 epoch 10 - iter 1554/7770 - loss 0.13196414 - samples/sec: 17.14 - lr: 0.000000
2023-05-28 03:16:06,070 epoch 10 - iter 2331/7770 - loss 0.13122349 - samples/sec: 17.05 - lr: 0.000000
2023-05-28 03:19:03,298 epoch 10 - iter 3108/7770 - loss 0.13201661 - samples/sec: 17.54 - lr: 0.000000
2023-05-28 03:22:04,612 epoch 10 - iter 3885/7770 - loss 0.13299239 - samples/sec: 17.15 - lr: 0.000000
2023-05-28 03:25:06,001 epoch 10 - iter 4662/7770 - loss 0.13316445 - samples/sec: 17.14 - lr: 0.000000
2023-05-28 03:28:05,552 epoch 10 - iter 5439/7770 - loss 0.13340368 - samples/sec: 17.32 - lr: 0.000000
2023-05-28 03:31:06,606 epoch 10 - iter 6216/7770 - loss 0.13345290 - samples/sec: 17.17 - lr: 0.000000
2023-05-28 03:34:09,094 epoch 10 - iter 6993/7770 - loss 0.13428797 - samples/sec: 17.04 - lr: 0.000000
2023-05-28 03:37:11,601 epoch 10 - iter 7770/7770 - loss 0.13463872 - samples/sec: 17.04 - lr: 0.000000
2023-05-28 03:37:11,605 ----------------------------------------------------------------------------------------------------
2023-05-28 03:37:11,605 EPOCH 10 done: loss 0.1346 - lr 0.000000
2023-05-28 03:40:16,261 Evaluating as a multi-label problem: False
2023-05-28 03:40:16,352 DEV : loss 0.08218735456466675 - f1-score (micro avg)  0.9647
2023-05-28 03:40:16,593 BAD EPOCHS (no improvement): 4
2023-05-28 03:40:29,071 ----------------------------------------------------------------------------------------------------
2023-05-28 03:40:29,074 Testing using last state of model ...
2023-05-28 03:44:44,879 Evaluating as a multi-label problem: False
2023-05-28 03:44:44,988 0.932	0.9422	0.9371	0.9112
2023-05-28 03:44:44,989 
Results:
- F-score (micro) 0.9371
- F-score (macro) 0.9342
- Accuracy 0.9112

By class:
              precision    recall  f1-score   support

         PER     0.9750    0.9779    0.9765      2715
         ORG     0.8988    0.9398    0.9189      2543
         LOC     0.9493    0.9349    0.9420      2442
        MISC     0.8951    0.9037    0.8994      1889

   micro avg     0.9320    0.9422    0.9371      9589
   macro avg     0.9296    0.9391    0.9342      9589
weighted avg     0.9325    0.9422    0.9372      9589

2023-05-28 03:44:44,989 ----------------------------------------------------------------------------------------------------
2023-05-28 03:44:44,989 ----------------------------------------------------------------------------------------------------
2023-05-28 03:47:24,315 Evaluating as a multi-label problem: False
2023-05-28 03:47:24,369 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-28 03:47:24,369 0.9396	0.9391	0.9393	0.9216
2023-05-28 03:47:24,369 ----------------------------------------------------------------------------------------------------
2023-05-28 03:48:58,891 Evaluating as a multi-label problem: False
2023-05-28 03:48:58,956 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-28 03:48:58,957 0.9268	0.9444	0.9355	0.9041
