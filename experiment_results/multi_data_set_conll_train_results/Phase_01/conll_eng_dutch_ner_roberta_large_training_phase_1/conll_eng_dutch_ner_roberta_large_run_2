2023-06-05 17:38:41,972 ----------------------------------------------------------------------------------------------------
2023-06-05 17:38:41,977 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-05 17:38:41,982 ----------------------------------------------------------------------------------------------------
2023-06-05 17:38:41,983 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-05 17:38:41,983 ----------------------------------------------------------------------------------------------------
2023-06-05 17:38:41,983 Parameters:
2023-06-05 17:38:41,983  - learning_rate: "0.000005"
2023-06-05 17:38:41,983  - mini_batch_size: "4"
2023-06-05 17:38:41,983  - patience: "3"
2023-06-05 17:38:41,983  - anneal_factor: "0.5"
2023-06-05 17:38:41,983  - max_epochs: "10"
2023-06-05 17:38:41,983  - shuffle: "True"
2023-06-05 17:38:41,983  - train_with_dev: "False"
2023-06-05 17:38:41,984  - batch_growth_annealing: "False"
2023-06-05 17:38:41,984 ----------------------------------------------------------------------------------------------------
2023-06-05 17:38:41,984 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_2"
2023-06-05 17:38:41,984 ----------------------------------------------------------------------------------------------------
2023-06-05 17:38:41,984 Device: cuda:2
2023-06-05 17:38:41,984 ----------------------------------------------------------------------------------------------------
2023-06-05 17:38:41,984 Embeddings storage mode: none
2023-06-05 17:38:41,984 ----------------------------------------------------------------------------------------------------
2023-06-05 17:42:08,161 epoch 1 - iter 777/7770 - loss 2.02192000 - samples/sec: 15.08 - lr: 0.000001
2023-06-05 17:45:21,927 epoch 1 - iter 1554/7770 - loss 1.25211626 - samples/sec: 16.05 - lr: 0.000001
2023-06-05 17:48:37,057 epoch 1 - iter 2331/7770 - loss 1.00962858 - samples/sec: 15.93 - lr: 0.000002
2023-06-05 17:51:48,665 epoch 1 - iter 3108/7770 - loss 0.83626634 - samples/sec: 16.23 - lr: 0.000002
2023-06-05 17:55:02,684 epoch 1 - iter 3885/7770 - loss 0.72047970 - samples/sec: 16.03 - lr: 0.000003
2023-06-05 17:58:12,435 epoch 1 - iter 4662/7770 - loss 0.65101397 - samples/sec: 16.39 - lr: 0.000003
2023-06-05 18:01:15,708 epoch 1 - iter 5439/7770 - loss 0.60275239 - samples/sec: 16.97 - lr: 0.000003
2023-06-05 18:04:24,046 epoch 1 - iter 6216/7770 - loss 0.55678567 - samples/sec: 16.51 - lr: 0.000004
2023-06-05 18:07:32,768 epoch 1 - iter 6993/7770 - loss 0.52246343 - samples/sec: 16.48 - lr: 0.000005
2023-06-05 18:10:40,906 epoch 1 - iter 7770/7770 - loss 0.48911594 - samples/sec: 16.53 - lr: 0.000005
2023-06-05 18:10:40,909 ----------------------------------------------------------------------------------------------------
2023-06-05 18:10:40,909 EPOCH 1 done: loss 0.4891 - lr 0.000005
2023-06-05 18:13:57,186 Evaluating as a multi-label problem: False
2023-06-05 18:13:57,285 DEV : loss 0.13884174823760986 - f1-score (micro avg)  0.894
2023-06-05 18:13:57,536 BAD EPOCHS (no improvement): 4
2023-06-05 18:13:57,539 ----------------------------------------------------------------------------------------------------
2023-06-05 18:17:13,240 epoch 2 - iter 777/7770 - loss 0.24003818 - samples/sec: 15.89 - lr: 0.000005
2023-06-05 18:20:29,356 epoch 2 - iter 1554/7770 - loss 0.23651207 - samples/sec: 15.86 - lr: 0.000005
2023-06-05 18:23:43,800 epoch 2 - iter 2331/7770 - loss 0.22670182 - samples/sec: 15.99 - lr: 0.000005
2023-06-05 18:27:02,036 epoch 2 - iter 3108/7770 - loss 0.22615276 - samples/sec: 15.69 - lr: 0.000005
2023-06-05 18:30:16,226 epoch 2 - iter 3885/7770 - loss 0.22726820 - samples/sec: 16.01 - lr: 0.000005
2023-06-05 18:33:28,632 epoch 2 - iter 4662/7770 - loss 0.22678480 - samples/sec: 16.16 - lr: 0.000005
2023-06-05 18:36:43,325 epoch 2 - iter 5439/7770 - loss 0.22457290 - samples/sec: 15.97 - lr: 0.000005
2023-06-05 18:39:57,660 epoch 2 - iter 6216/7770 - loss 0.22367875 - samples/sec: 16.00 - lr: 0.000005
2023-06-05 18:43:12,421 epoch 2 - iter 6993/7770 - loss 0.22346265 - samples/sec: 15.97 - lr: 0.000005
2023-06-05 18:46:22,741 epoch 2 - iter 7770/7770 - loss 0.22190358 - samples/sec: 16.34 - lr: 0.000004
2023-06-05 18:46:22,747 ----------------------------------------------------------------------------------------------------
2023-06-05 18:46:22,747 EPOCH 2 done: loss 0.2219 - lr 0.000004
2023-06-05 18:49:19,132 Evaluating as a multi-label problem: False
2023-06-05 18:49:19,238 DEV : loss 0.07368189096450806 - f1-score (micro avg)  0.9437
2023-06-05 18:49:19,528 BAD EPOCHS (no improvement): 4
2023-06-05 18:49:19,532 ----------------------------------------------------------------------------------------------------
2023-06-05 18:52:32,640 epoch 3 - iter 777/7770 - loss 0.20026640 - samples/sec: 16.10 - lr: 0.000004
2023-06-05 18:55:44,751 epoch 3 - iter 1554/7770 - loss 0.19937379 - samples/sec: 16.19 - lr: 0.000004
2023-06-05 18:58:58,330 epoch 3 - iter 2331/7770 - loss 0.19419010 - samples/sec: 16.06 - lr: 0.000004
2023-06-05 19:02:08,032 epoch 3 - iter 3108/7770 - loss 0.19426543 - samples/sec: 16.39 - lr: 0.000004
2023-06-05 19:05:24,256 epoch 3 - iter 3885/7770 - loss 0.19400258 - samples/sec: 15.85 - lr: 0.000004
2023-06-05 19:08:34,976 epoch 3 - iter 4662/7770 - loss 0.19490428 - samples/sec: 16.30 - lr: 0.000004
2023-06-05 19:11:42,382 epoch 3 - iter 5439/7770 - loss 0.19413925 - samples/sec: 16.59 - lr: 0.000004
2023-06-05 19:15:02,312 epoch 3 - iter 6216/7770 - loss 0.19386391 - samples/sec: 15.55 - lr: 0.000004
2023-06-05 19:18:18,842 epoch 3 - iter 6993/7770 - loss 0.19230901 - samples/sec: 15.82 - lr: 0.000004
2023-06-05 19:21:33,175 epoch 3 - iter 7770/7770 - loss 0.19157458 - samples/sec: 16.00 - lr: 0.000004
2023-06-05 19:21:33,180 ----------------------------------------------------------------------------------------------------
2023-06-05 19:21:33,180 EPOCH 3 done: loss 0.1916 - lr 0.000004
2023-06-05 19:24:31,724 Evaluating as a multi-label problem: False
2023-06-05 19:24:31,815 DEV : loss 0.07441151887178421 - f1-score (micro avg)  0.949
2023-06-05 19:24:32,075 BAD EPOCHS (no improvement): 4
2023-06-05 19:24:32,080 ----------------------------------------------------------------------------------------------------
2023-06-05 19:27:50,395 epoch 4 - iter 777/7770 - loss 0.18088147 - samples/sec: 15.68 - lr: 0.000004
2023-06-05 19:31:07,258 epoch 4 - iter 1554/7770 - loss 0.17983236 - samples/sec: 15.80 - lr: 0.000004
2023-06-05 19:34:18,002 epoch 4 - iter 2331/7770 - loss 0.17679807 - samples/sec: 16.30 - lr: 0.000004
2023-06-05 19:37:28,837 epoch 4 - iter 3108/7770 - loss 0.17748534 - samples/sec: 16.29 - lr: 0.000004
2023-06-05 19:40:37,542 epoch 4 - iter 3885/7770 - loss 0.17837099 - samples/sec: 16.48 - lr: 0.000004
2023-06-05 19:43:46,301 epoch 4 - iter 4662/7770 - loss 0.17878253 - samples/sec: 16.47 - lr: 0.000004
2023-06-05 19:46:54,108 epoch 4 - iter 5439/7770 - loss 0.17831680 - samples/sec: 16.56 - lr: 0.000004
2023-06-05 19:50:02,109 epoch 4 - iter 6216/7770 - loss 0.17803162 - samples/sec: 16.54 - lr: 0.000003
2023-06-05 19:53:10,539 epoch 4 - iter 6993/7770 - loss 0.17689135 - samples/sec: 16.50 - lr: 0.000003
2023-06-05 19:56:20,884 epoch 4 - iter 7770/7770 - loss 0.17709884 - samples/sec: 16.34 - lr: 0.000003
2023-06-05 19:56:20,888 ----------------------------------------------------------------------------------------------------
2023-06-05 19:56:20,888 EPOCH 4 done: loss 0.1771 - lr 0.000003
2023-06-05 19:59:54,186 Evaluating as a multi-label problem: False
2023-06-05 19:59:54,281 DEV : loss 0.06613709777593613 - f1-score (micro avg)  0.9559
2023-06-05 19:59:54,526 BAD EPOCHS (no improvement): 4
2023-06-05 19:59:54,529 ----------------------------------------------------------------------------------------------------
2023-06-05 20:03:10,581 epoch 5 - iter 777/7770 - loss 0.15701359 - samples/sec: 15.86 - lr: 0.000003
2023-06-05 20:06:22,052 epoch 5 - iter 1554/7770 - loss 0.16133082 - samples/sec: 16.24 - lr: 0.000003
2023-06-05 20:09:30,742 epoch 5 - iter 2331/7770 - loss 0.16372296 - samples/sec: 16.48 - lr: 0.000003
2023-06-05 20:12:35,986 epoch 5 - iter 3108/7770 - loss 0.16256842 - samples/sec: 16.79 - lr: 0.000003
2023-06-05 20:15:45,818 epoch 5 - iter 3885/7770 - loss 0.16279854 - samples/sec: 16.38 - lr: 0.000003
2023-06-05 20:18:56,619 epoch 5 - iter 4662/7770 - loss 0.16321299 - samples/sec: 16.30 - lr: 0.000003
2023-06-05 20:22:08,016 epoch 5 - iter 5439/7770 - loss 0.16293830 - samples/sec: 16.25 - lr: 0.000003
2023-06-05 20:25:17,652 epoch 5 - iter 6216/7770 - loss 0.16444187 - samples/sec: 16.40 - lr: 0.000003
2023-06-05 20:28:24,199 epoch 5 - iter 6993/7770 - loss 0.16374618 - samples/sec: 16.67 - lr: 0.000003
2023-06-05 20:31:29,145 epoch 5 - iter 7770/7770 - loss 0.16383426 - samples/sec: 16.81 - lr: 0.000003
2023-06-05 20:31:29,150 ----------------------------------------------------------------------------------------------------
2023-06-05 20:31:29,150 EPOCH 5 done: loss 0.1638 - lr 0.000003
2023-06-05 20:34:25,683 Evaluating as a multi-label problem: False
2023-06-05 20:34:25,790 DEV : loss 0.06634555757045746 - f1-score (micro avg)  0.958
2023-06-05 20:34:26,020 BAD EPOCHS (no improvement): 4
2023-06-05 20:34:26,023 ----------------------------------------------------------------------------------------------------
2023-06-05 20:37:34,951 epoch 6 - iter 777/7770 - loss 0.15433963 - samples/sec: 16.46 - lr: 0.000003
2023-06-05 20:40:46,779 epoch 6 - iter 1554/7770 - loss 0.15756588 - samples/sec: 16.21 - lr: 0.000003
2023-06-05 20:43:56,655 epoch 6 - iter 2331/7770 - loss 0.15877298 - samples/sec: 16.38 - lr: 0.000003
2023-06-05 20:47:04,787 epoch 6 - iter 3108/7770 - loss 0.15853332 - samples/sec: 16.53 - lr: 0.000003
2023-06-05 20:50:07,063 epoch 6 - iter 3885/7770 - loss 0.15779514 - samples/sec: 17.06 - lr: 0.000003
2023-06-05 20:53:15,562 epoch 6 - iter 4662/7770 - loss 0.15804663 - samples/sec: 16.50 - lr: 0.000002
2023-06-05 20:56:25,056 epoch 6 - iter 5439/7770 - loss 0.15795286 - samples/sec: 16.41 - lr: 0.000002
2023-06-05 20:59:29,356 epoch 6 - iter 6216/7770 - loss 0.15799136 - samples/sec: 16.87 - lr: 0.000002
2023-06-05 21:02:46,439 epoch 6 - iter 6993/7770 - loss 0.15808315 - samples/sec: 15.78 - lr: 0.000002
2023-06-05 21:05:56,862 epoch 6 - iter 7770/7770 - loss 0.15823286 - samples/sec: 16.33 - lr: 0.000002
2023-06-05 21:05:56,867 ----------------------------------------------------------------------------------------------------
2023-06-05 21:05:56,867 EPOCH 6 done: loss 0.1582 - lr 0.000002
2023-06-05 21:08:39,751 Evaluating as a multi-label problem: False
2023-06-05 21:08:39,843 DEV : loss 0.06529022753238678 - f1-score (micro avg)  0.9579
2023-06-05 21:08:40,096 BAD EPOCHS (no improvement): 4
2023-06-05 21:08:40,099 ----------------------------------------------------------------------------------------------------
2023-06-05 21:11:51,671 epoch 7 - iter 777/7770 - loss 0.14584707 - samples/sec: 16.23 - lr: 0.000002
2023-06-05 21:15:00,429 epoch 7 - iter 1554/7770 - loss 0.15129904 - samples/sec: 16.47 - lr: 0.000002
2023-06-05 21:18:03,168 epoch 7 - iter 2331/7770 - loss 0.15042962 - samples/sec: 17.02 - lr: 0.000002
2023-06-05 21:21:09,341 epoch 7 - iter 3108/7770 - loss 0.15014668 - samples/sec: 16.70 - lr: 0.000002
2023-06-05 21:24:16,204 epoch 7 - iter 3885/7770 - loss 0.15098187 - samples/sec: 16.64 - lr: 0.000002
2023-06-05 21:27:19,946 epoch 7 - iter 4662/7770 - loss 0.15110230 - samples/sec: 16.92 - lr: 0.000002
2023-06-05 21:30:19,985 epoch 7 - iter 5439/7770 - loss 0.15076029 - samples/sec: 17.27 - lr: 0.000002
2023-06-05 21:33:27,793 epoch 7 - iter 6216/7770 - loss 0.15098075 - samples/sec: 16.56 - lr: 0.000002
2023-06-05 21:36:33,175 epoch 7 - iter 6993/7770 - loss 0.15136430 - samples/sec: 16.77 - lr: 0.000002
2023-06-05 21:39:33,183 epoch 7 - iter 7770/7770 - loss 0.15064025 - samples/sec: 17.27 - lr: 0.000002
2023-06-05 21:39:33,186 ----------------------------------------------------------------------------------------------------
2023-06-05 21:39:33,186 EPOCH 7 done: loss 0.1506 - lr 0.000002
2023-06-05 21:42:41,846 Evaluating as a multi-label problem: False
2023-06-05 21:42:41,944 DEV : loss 0.06964218616485596 - f1-score (micro avg)  0.9623
2023-06-05 21:42:42,160 BAD EPOCHS (no improvement): 4
2023-06-05 21:42:42,163 ----------------------------------------------------------------------------------------------------
2023-06-05 21:45:51,463 epoch 8 - iter 777/7770 - loss 0.14673674 - samples/sec: 16.43 - lr: 0.000002
2023-06-05 21:49:01,946 epoch 8 - iter 1554/7770 - loss 0.14828428 - samples/sec: 16.32 - lr: 0.000002
2023-06-05 21:52:10,081 epoch 8 - iter 2331/7770 - loss 0.15086161 - samples/sec: 16.53 - lr: 0.000002
2023-06-05 21:55:16,696 epoch 8 - iter 3108/7770 - loss 0.14801080 - samples/sec: 16.66 - lr: 0.000001
2023-06-05 21:58:36,500 epoch 8 - iter 3885/7770 - loss 0.14709563 - samples/sec: 15.56 - lr: 0.000001
2023-06-05 22:01:44,321 epoch 8 - iter 4662/7770 - loss 0.14701509 - samples/sec: 16.56 - lr: 0.000001
2023-06-05 22:04:50,424 epoch 8 - iter 5439/7770 - loss 0.14684040 - samples/sec: 16.71 - lr: 0.000001
2023-06-05 22:07:58,248 epoch 8 - iter 6216/7770 - loss 0.14652888 - samples/sec: 16.56 - lr: 0.000001
2023-06-05 22:11:02,990 epoch 8 - iter 6993/7770 - loss 0.14624190 - samples/sec: 16.83 - lr: 0.000001
2023-06-05 22:14:07,406 epoch 8 - iter 7770/7770 - loss 0.14575641 - samples/sec: 16.86 - lr: 0.000001
2023-06-05 22:14:07,410 ----------------------------------------------------------------------------------------------------
2023-06-05 22:14:07,411 EPOCH 8 done: loss 0.1458 - lr 0.000001
2023-06-05 22:17:05,737 Evaluating as a multi-label problem: False
2023-06-05 22:17:05,844 DEV : loss 0.06784354895353317 - f1-score (micro avg)  0.9619
2023-06-05 22:17:06,128 BAD EPOCHS (no improvement): 4
2023-06-05 22:17:06,131 ----------------------------------------------------------------------------------------------------
2023-06-05 22:20:17,387 epoch 9 - iter 777/7770 - loss 0.14347900 - samples/sec: 16.26 - lr: 0.000001
2023-06-05 22:23:28,482 epoch 9 - iter 1554/7770 - loss 0.14067161 - samples/sec: 16.27 - lr: 0.000001
2023-06-05 22:26:35,044 epoch 9 - iter 2331/7770 - loss 0.14208487 - samples/sec: 16.67 - lr: 0.000001
2023-06-05 22:29:46,107 epoch 9 - iter 3108/7770 - loss 0.14362607 - samples/sec: 16.27 - lr: 0.000001
2023-06-05 22:32:51,476 epoch 9 - iter 3885/7770 - loss 0.14383445 - samples/sec: 16.77 - lr: 0.000001
2023-06-05 22:35:54,904 epoch 9 - iter 4662/7770 - loss 0.14389599 - samples/sec: 16.95 - lr: 0.000001
2023-06-05 22:38:57,986 epoch 9 - iter 5439/7770 - loss 0.14281603 - samples/sec: 16.98 - lr: 0.000001
2023-06-05 22:42:03,879 epoch 9 - iter 6216/7770 - loss 0.14254676 - samples/sec: 16.73 - lr: 0.000001
2023-06-05 22:45:24,638 epoch 9 - iter 6993/7770 - loss 0.14265691 - samples/sec: 15.49 - lr: 0.000001
2023-06-05 22:48:33,759 epoch 9 - iter 7770/7770 - loss 0.14320372 - samples/sec: 16.44 - lr: 0.000001
2023-06-05 22:48:33,763 ----------------------------------------------------------------------------------------------------
2023-06-05 22:48:33,764 EPOCH 9 done: loss 0.1432 - lr 0.000001
2023-06-05 22:51:26,546 Evaluating as a multi-label problem: False
2023-06-05 22:51:26,646 DEV : loss 0.06803567707538605 - f1-score (micro avg)  0.961
2023-06-05 22:51:26,907 BAD EPOCHS (no improvement): 4
2023-06-05 22:51:26,910 ----------------------------------------------------------------------------------------------------
2023-06-05 22:54:42,829 epoch 10 - iter 777/7770 - loss 0.14339615 - samples/sec: 15.87 - lr: 0.000001
2023-06-05 22:57:50,594 epoch 10 - iter 1554/7770 - loss 0.14117237 - samples/sec: 16.56 - lr: 0.000000
2023-06-05 23:00:58,937 epoch 10 - iter 2331/7770 - loss 0.14073351 - samples/sec: 16.51 - lr: 0.000000
2023-06-05 23:04:07,196 epoch 10 - iter 3108/7770 - loss 0.13938005 - samples/sec: 16.52 - lr: 0.000000
2023-06-05 23:07:12,500 epoch 10 - iter 3885/7770 - loss 0.13950415 - samples/sec: 16.78 - lr: 0.000000
2023-06-05 23:10:18,272 epoch 10 - iter 4662/7770 - loss 0.13927691 - samples/sec: 16.74 - lr: 0.000000
2023-06-05 23:13:27,902 epoch 10 - iter 5439/7770 - loss 0.13988226 - samples/sec: 16.40 - lr: 0.000000
2023-06-05 23:16:31,778 epoch 10 - iter 6216/7770 - loss 0.13995334 - samples/sec: 16.91 - lr: 0.000000
2023-06-05 23:19:37,933 epoch 10 - iter 6993/7770 - loss 0.13980648 - samples/sec: 16.70 - lr: 0.000000
2023-06-05 23:22:42,952 epoch 10 - iter 7770/7770 - loss 0.13963650 - samples/sec: 16.81 - lr: 0.000000
2023-06-05 23:22:42,957 ----------------------------------------------------------------------------------------------------
2023-06-05 23:22:42,957 EPOCH 10 done: loss 0.1396 - lr 0.000000
2023-06-05 23:25:52,410 Evaluating as a multi-label problem: False
2023-06-05 23:25:52,502 DEV : loss 0.06752994656562805 - f1-score (micro avg)  0.9626
2023-06-05 23:25:52,767 BAD EPOCHS (no improvement): 4
2023-06-05 23:26:10,036 ----------------------------------------------------------------------------------------------------
2023-06-05 23:26:10,040 Testing using last state of model ...
2023-06-05 23:30:25,342 Evaluating as a multi-label problem: False
2023-06-05 23:30:25,454 0.9306	0.944	0.9373	0.9108
2023-06-05 23:30:25,455 
Results:
- F-score (micro) 0.9373
- F-score (macro) 0.9344
- Accuracy 0.9108

By class:
              precision    recall  f1-score   support

         PER     0.9758    0.9786    0.9772      2715
         ORG     0.8950    0.9449    0.9193      2543
         LOC     0.9423    0.9369    0.9396      2442
        MISC     0.9011    0.9021    0.9016      1889

   micro avg     0.9306    0.9440    0.9373      9589
   macro avg     0.9285    0.9406    0.9344      9589
weighted avg     0.9311    0.9440    0.9374      9589

2023-06-05 23:30:25,456 ----------------------------------------------------------------------------------------------------
2023-06-05 23:30:25,456 ----------------------------------------------------------------------------------------------------
2023-06-05 23:32:58,898 Evaluating as a multi-label problem: False
2023-06-05 23:32:58,953 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-05 23:32:58,954 0.9376	0.9419	0.9397	0.9204
2023-06-05 23:32:58,956 ----------------------------------------------------------------------------------------------------
2023-06-05 23:34:34,220 Evaluating as a multi-label problem: False
2023-06-05 23:34:34,286 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-05 23:34:34,286 0.9258	0.9455	0.9355	0.9042
