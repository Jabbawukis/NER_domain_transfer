2023-05-27 16:28:59,703 ----------------------------------------------------------------------------------------------------
2023-05-27 16:28:59,708 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-27 16:28:59,714 ----------------------------------------------------------------------------------------------------
2023-05-27 16:28:59,715 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-27 16:28:59,716 ----------------------------------------------------------------------------------------------------
2023-05-27 16:28:59,717 Parameters:
2023-05-27 16:28:59,717  - learning_rate: "0.000005"
2023-05-27 16:28:59,717  - mini_batch_size: "4"
2023-05-27 16:28:59,717  - patience: "3"
2023-05-27 16:28:59,719  - anneal_factor: "0.5"
2023-05-27 16:28:59,723  - max_epochs: "10"
2023-05-27 16:28:59,724  - shuffle: "True"
2023-05-27 16:28:59,726  - train_with_dev: "False"
2023-05-27 16:28:59,727  - batch_growth_annealing: "False"
2023-05-27 16:28:59,729 ----------------------------------------------------------------------------------------------------
2023-05-27 16:28:59,731 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_1"
2023-05-27 16:28:59,731 ----------------------------------------------------------------------------------------------------
2023-05-27 16:28:59,732 Device: cuda:0
2023-05-27 16:28:59,734 ----------------------------------------------------------------------------------------------------
2023-05-27 16:28:59,736 Embeddings storage mode: none
2023-05-27 16:28:59,737 ----------------------------------------------------------------------------------------------------
2023-05-27 16:32:01,587 epoch 1 - iter 777/7770 - loss 2.78465394 - samples/sec: 17.10 - lr: 0.000001
2023-05-27 16:35:05,272 epoch 1 - iter 1554/7770 - loss 1.64761387 - samples/sec: 16.93 - lr: 0.000001
2023-05-27 16:38:14,397 epoch 1 - iter 2331/7770 - loss 1.28867998 - samples/sec: 16.44 - lr: 0.000002
2023-05-27 16:41:21,414 epoch 1 - iter 3108/7770 - loss 1.04769748 - samples/sec: 16.63 - lr: 0.000002
2023-05-27 16:44:36,636 epoch 1 - iter 3885/7770 - loss 0.89085122 - samples/sec: 15.93 - lr: 0.000003
2023-05-27 16:47:38,550 epoch 1 - iter 4662/7770 - loss 0.79292981 - samples/sec: 17.09 - lr: 0.000003
2023-05-27 16:50:38,039 epoch 1 - iter 5439/7770 - loss 0.72755441 - samples/sec: 17.32 - lr: 0.000003
2023-05-27 16:53:40,872 epoch 1 - iter 6216/7770 - loss 0.66576509 - samples/sec: 17.01 - lr: 0.000004
2023-05-27 16:56:45,189 epoch 1 - iter 6993/7770 - loss 0.61814109 - samples/sec: 16.87 - lr: 0.000005
2023-05-27 16:59:46,551 epoch 1 - iter 7770/7770 - loss 0.57346024 - samples/sec: 17.14 - lr: 0.000005
2023-05-27 16:59:46,554 ----------------------------------------------------------------------------------------------------
2023-05-27 16:59:46,555 EPOCH 1 done: loss 0.5735 - lr 0.000005
2023-05-27 17:02:28,517 Evaluating as a multi-label problem: False
2023-05-27 17:02:28,620 DEV : loss 0.13579769432544708 - f1-score (micro avg)  0.8945
2023-05-27 17:02:28,819 BAD EPOCHS (no improvement): 4
2023-05-27 17:02:28,824 ----------------------------------------------------------------------------------------------------
2023-05-27 17:05:34,855 epoch 2 - iter 777/7770 - loss 0.24168254 - samples/sec: 16.72 - lr: 0.000005
2023-05-27 17:08:39,658 epoch 2 - iter 1554/7770 - loss 0.24039023 - samples/sec: 16.83 - lr: 0.000005
2023-05-27 17:11:46,126 epoch 2 - iter 2331/7770 - loss 0.23742766 - samples/sec: 16.68 - lr: 0.000005
2023-05-27 17:14:56,239 epoch 2 - iter 3108/7770 - loss 0.23455868 - samples/sec: 16.36 - lr: 0.000005
2023-05-27 17:18:01,936 epoch 2 - iter 3885/7770 - loss 0.23313102 - samples/sec: 16.75 - lr: 0.000005
2023-05-27 17:21:03,912 epoch 2 - iter 4662/7770 - loss 0.23213689 - samples/sec: 17.09 - lr: 0.000005
2023-05-27 17:24:06,910 epoch 2 - iter 5439/7770 - loss 0.22905617 - samples/sec: 16.99 - lr: 0.000005
2023-05-27 17:27:11,592 epoch 2 - iter 6216/7770 - loss 0.22722601 - samples/sec: 16.84 - lr: 0.000005
2023-05-27 17:30:14,929 epoch 2 - iter 6993/7770 - loss 0.22486145 - samples/sec: 16.96 - lr: 0.000005
2023-05-27 17:33:18,347 epoch 2 - iter 7770/7770 - loss 0.22345810 - samples/sec: 16.95 - lr: 0.000004
2023-05-27 17:33:18,351 ----------------------------------------------------------------------------------------------------
2023-05-27 17:33:18,351 EPOCH 2 done: loss 0.2235 - lr 0.000004
2023-05-27 17:36:01,474 Evaluating as a multi-label problem: False
2023-05-27 17:36:01,565 DEV : loss 0.08218852430582047 - f1-score (micro avg)  0.94
2023-05-27 17:36:01,750 BAD EPOCHS (no improvement): 4
2023-05-27 17:36:01,752 ----------------------------------------------------------------------------------------------------
2023-05-27 17:39:04,587 epoch 3 - iter 777/7770 - loss 0.20203729 - samples/sec: 17.01 - lr: 0.000004
2023-05-27 17:42:05,153 epoch 3 - iter 1554/7770 - loss 0.20059083 - samples/sec: 17.22 - lr: 0.000004
2023-05-27 17:45:06,757 epoch 3 - iter 2331/7770 - loss 0.20038554 - samples/sec: 17.12 - lr: 0.000004
2023-05-27 17:48:04,998 epoch 3 - iter 3108/7770 - loss 0.19827307 - samples/sec: 17.45 - lr: 0.000004
2023-05-27 17:51:13,237 epoch 3 - iter 3885/7770 - loss 0.20067476 - samples/sec: 16.52 - lr: 0.000004
2023-05-27 17:54:13,704 epoch 3 - iter 4662/7770 - loss 0.19974770 - samples/sec: 17.23 - lr: 0.000004
2023-05-27 17:57:12,551 epoch 3 - iter 5439/7770 - loss 0.19907931 - samples/sec: 17.39 - lr: 0.000004
2023-05-27 18:00:14,330 epoch 3 - iter 6216/7770 - loss 0.19883262 - samples/sec: 17.11 - lr: 0.000004
2023-05-27 18:03:15,165 epoch 3 - iter 6993/7770 - loss 0.19771499 - samples/sec: 17.20 - lr: 0.000004
2023-05-27 18:06:14,334 epoch 3 - iter 7770/7770 - loss 0.19657291 - samples/sec: 17.35 - lr: 0.000004
2023-05-27 18:06:14,338 ----------------------------------------------------------------------------------------------------
2023-05-27 18:06:14,338 EPOCH 3 done: loss 0.1966 - lr 0.000004
2023-05-27 18:08:48,815 Evaluating as a multi-label problem: False
2023-05-27 18:08:48,918 DEV : loss 0.07078798860311508 - f1-score (micro avg)  0.9521
2023-05-27 18:08:49,104 BAD EPOCHS (no improvement): 4
2023-05-27 18:08:49,108 ----------------------------------------------------------------------------------------------------
2023-05-27 18:11:48,082 epoch 4 - iter 777/7770 - loss 0.17231022 - samples/sec: 17.37 - lr: 0.000004
2023-05-27 18:14:48,135 epoch 4 - iter 1554/7770 - loss 0.17517955 - samples/sec: 17.27 - lr: 0.000004
2023-05-27 18:17:47,951 epoch 4 - iter 2331/7770 - loss 0.17321455 - samples/sec: 17.29 - lr: 0.000004
2023-05-27 18:20:49,547 epoch 4 - iter 3108/7770 - loss 0.17610056 - samples/sec: 17.12 - lr: 0.000004
2023-05-27 18:23:49,882 epoch 4 - iter 3885/7770 - loss 0.17622193 - samples/sec: 17.24 - lr: 0.000004
2023-05-27 18:26:56,902 epoch 4 - iter 4662/7770 - loss 0.17593289 - samples/sec: 16.63 - lr: 0.000004
2023-05-27 18:29:57,903 epoch 4 - iter 5439/7770 - loss 0.17564355 - samples/sec: 17.18 - lr: 0.000004
2023-05-27 18:32:57,701 epoch 4 - iter 6216/7770 - loss 0.17505292 - samples/sec: 17.29 - lr: 0.000003
2023-05-27 18:35:56,239 epoch 4 - iter 6993/7770 - loss 0.17527100 - samples/sec: 17.42 - lr: 0.000003
2023-05-27 18:38:58,417 epoch 4 - iter 7770/7770 - loss 0.17503577 - samples/sec: 17.07 - lr: 0.000003
2023-05-27 18:38:58,421 ----------------------------------------------------------------------------------------------------
2023-05-27 18:38:58,421 EPOCH 4 done: loss 0.1750 - lr 0.000003
2023-05-27 18:41:36,357 Evaluating as a multi-label problem: False
2023-05-27 18:41:36,452 DEV : loss 0.061331357806921005 - f1-score (micro avg)  0.956
2023-05-27 18:41:36,658 BAD EPOCHS (no improvement): 4
2023-05-27 18:41:36,660 ----------------------------------------------------------------------------------------------------
2023-05-27 18:44:35,132 epoch 5 - iter 777/7770 - loss 0.15849201 - samples/sec: 17.42 - lr: 0.000003
2023-05-27 18:47:35,025 epoch 5 - iter 1554/7770 - loss 0.16172966 - samples/sec: 17.29 - lr: 0.000003
2023-05-27 18:50:36,047 epoch 5 - iter 2331/7770 - loss 0.16669272 - samples/sec: 17.18 - lr: 0.000003
2023-05-27 18:53:35,616 epoch 5 - iter 3108/7770 - loss 0.16706826 - samples/sec: 17.32 - lr: 0.000003
2023-05-27 18:56:37,147 epoch 5 - iter 3885/7770 - loss 0.16740696 - samples/sec: 17.13 - lr: 0.000003
2023-05-27 18:59:37,961 epoch 5 - iter 4662/7770 - loss 0.16732034 - samples/sec: 17.20 - lr: 0.000003
2023-05-27 19:02:44,768 epoch 5 - iter 5439/7770 - loss 0.16671083 - samples/sec: 16.64 - lr: 0.000003
2023-05-27 19:05:48,971 epoch 5 - iter 6216/7770 - loss 0.16640807 - samples/sec: 16.88 - lr: 0.000003
2023-05-27 19:08:48,531 epoch 5 - iter 6993/7770 - loss 0.16555336 - samples/sec: 17.32 - lr: 0.000003
2023-05-27 19:11:49,538 epoch 5 - iter 7770/7770 - loss 0.16628975 - samples/sec: 17.18 - lr: 0.000003
2023-05-27 19:11:49,541 ----------------------------------------------------------------------------------------------------
2023-05-27 19:11:49,541 EPOCH 5 done: loss 0.1663 - lr 0.000003
2023-05-27 19:14:26,813 Evaluating as a multi-label problem: False
2023-05-27 19:14:26,908 DEV : loss 0.06823576241731644 - f1-score (micro avg)  0.9578
2023-05-27 19:14:27,114 BAD EPOCHS (no improvement): 4
2023-05-27 19:14:27,117 ----------------------------------------------------------------------------------------------------
2023-05-27 19:17:29,509 epoch 6 - iter 777/7770 - loss 0.15808819 - samples/sec: 17.05 - lr: 0.000003
2023-05-27 19:20:30,228 epoch 6 - iter 1554/7770 - loss 0.15907132 - samples/sec: 17.21 - lr: 0.000003
2023-05-27 19:23:28,094 epoch 6 - iter 2331/7770 - loss 0.15882374 - samples/sec: 17.48 - lr: 0.000003
2023-05-27 19:26:27,160 epoch 6 - iter 3108/7770 - loss 0.15745494 - samples/sec: 17.36 - lr: 0.000003
2023-05-27 19:29:26,505 epoch 6 - iter 3885/7770 - loss 0.15811816 - samples/sec: 17.34 - lr: 0.000003
2023-05-27 19:32:28,048 epoch 6 - iter 4662/7770 - loss 0.15810361 - samples/sec: 17.13 - lr: 0.000002
2023-05-27 19:35:30,290 epoch 6 - iter 5439/7770 - loss 0.15783351 - samples/sec: 17.06 - lr: 0.000002
2023-05-27 19:38:32,256 epoch 6 - iter 6216/7770 - loss 0.15759039 - samples/sec: 17.09 - lr: 0.000002
2023-05-27 19:41:42,190 epoch 6 - iter 6993/7770 - loss 0.15688005 - samples/sec: 16.37 - lr: 0.000002
2023-05-27 19:44:43,301 epoch 6 - iter 7770/7770 - loss 0.15767243 - samples/sec: 17.17 - lr: 0.000002
2023-05-27 19:44:43,304 ----------------------------------------------------------------------------------------------------
2023-05-27 19:44:43,304 EPOCH 6 done: loss 0.1577 - lr 0.000002
2023-05-27 19:47:15,199 Evaluating as a multi-label problem: False
2023-05-27 19:47:15,278 DEV : loss 0.06372325122356415 - f1-score (micro avg)  0.9618
2023-05-27 19:47:15,419 BAD EPOCHS (no improvement): 4
2023-05-27 19:47:15,421 ----------------------------------------------------------------------------------------------------
2023-05-27 19:50:17,817 epoch 7 - iter 777/7770 - loss 0.15343432 - samples/sec: 17.05 - lr: 0.000002
2023-05-27 19:53:19,098 epoch 7 - iter 1554/7770 - loss 0.14820468 - samples/sec: 17.15 - lr: 0.000002
2023-05-27 19:56:17,376 epoch 7 - iter 2331/7770 - loss 0.15042417 - samples/sec: 17.44 - lr: 0.000002
2023-05-27 19:59:17,881 epoch 7 - iter 3108/7770 - loss 0.15011860 - samples/sec: 17.23 - lr: 0.000002
2023-05-27 20:02:17,499 epoch 7 - iter 3885/7770 - loss 0.15071618 - samples/sec: 17.31 - lr: 0.000002
2023-05-27 20:05:15,014 epoch 7 - iter 4662/7770 - loss 0.15079420 - samples/sec: 17.52 - lr: 0.000002
2023-05-27 20:08:17,466 epoch 7 - iter 5439/7770 - loss 0.15074832 - samples/sec: 17.04 - lr: 0.000002
2023-05-27 20:11:16,488 epoch 7 - iter 6216/7770 - loss 0.15132408 - samples/sec: 17.37 - lr: 0.000002
2023-05-27 20:14:19,526 epoch 7 - iter 6993/7770 - loss 0.15083002 - samples/sec: 16.99 - lr: 0.000002
2023-05-27 20:17:29,171 epoch 7 - iter 7770/7770 - loss 0.15049916 - samples/sec: 16.40 - lr: 0.000002
2023-05-27 20:17:29,175 ----------------------------------------------------------------------------------------------------
2023-05-27 20:17:29,175 EPOCH 7 done: loss 0.1505 - lr 0.000002
2023-05-27 20:20:00,880 Evaluating as a multi-label problem: False
2023-05-27 20:20:00,960 DEV : loss 0.06268089264631271 - f1-score (micro avg)  0.9608
2023-05-27 20:20:01,145 BAD EPOCHS (no improvement): 4
2023-05-27 20:20:01,148 ----------------------------------------------------------------------------------------------------
2023-05-27 20:23:05,960 epoch 8 - iter 777/7770 - loss 0.15135157 - samples/sec: 16.83 - lr: 0.000002
2023-05-27 20:26:03,942 epoch 8 - iter 1554/7770 - loss 0.15034290 - samples/sec: 17.47 - lr: 0.000002
2023-05-27 20:29:04,233 epoch 8 - iter 2331/7770 - loss 0.14684189 - samples/sec: 17.25 - lr: 0.000002
2023-05-27 20:32:03,415 epoch 8 - iter 3108/7770 - loss 0.14562400 - samples/sec: 17.35 - lr: 0.000001
2023-05-27 20:35:02,902 epoch 8 - iter 3885/7770 - loss 0.14672408 - samples/sec: 17.32 - lr: 0.000001
2023-05-27 20:38:02,000 epoch 8 - iter 4662/7770 - loss 0.14597795 - samples/sec: 17.36 - lr: 0.000001
2023-05-27 20:41:05,041 epoch 8 - iter 5439/7770 - loss 0.14660240 - samples/sec: 16.99 - lr: 0.000001
2023-05-27 20:44:04,753 epoch 8 - iter 6216/7770 - loss 0.14611755 - samples/sec: 17.30 - lr: 0.000001
2023-05-27 20:47:07,597 epoch 8 - iter 6993/7770 - loss 0.14624484 - samples/sec: 17.01 - lr: 0.000001
2023-05-27 20:50:09,705 epoch 8 - iter 7770/7770 - loss 0.14683399 - samples/sec: 17.07 - lr: 0.000001
2023-05-27 20:50:09,709 ----------------------------------------------------------------------------------------------------
2023-05-27 20:50:09,709 EPOCH 8 done: loss 0.1468 - lr 0.000001
2023-05-27 20:52:52,081 Evaluating as a multi-label problem: False
2023-05-27 20:52:52,174 DEV : loss 0.06794171035289764 - f1-score (micro avg)  0.9622
2023-05-27 20:52:52,379 BAD EPOCHS (no improvement): 4
2023-05-27 20:52:52,382 ----------------------------------------------------------------------------------------------------
2023-05-27 20:55:54,006 epoch 9 - iter 777/7770 - loss 0.14441796 - samples/sec: 17.12 - lr: 0.000001
2023-05-27 20:59:00,290 epoch 9 - iter 1554/7770 - loss 0.14399904 - samples/sec: 16.69 - lr: 0.000001
2023-05-27 21:01:59,960 epoch 9 - iter 2331/7770 - loss 0.14549604 - samples/sec: 17.31 - lr: 0.000001
2023-05-27 21:04:59,203 epoch 9 - iter 3108/7770 - loss 0.14354837 - samples/sec: 17.35 - lr: 0.000001
2023-05-27 21:07:58,466 epoch 9 - iter 3885/7770 - loss 0.14417066 - samples/sec: 17.35 - lr: 0.000001
2023-05-27 21:10:59,591 epoch 9 - iter 4662/7770 - loss 0.14355780 - samples/sec: 17.17 - lr: 0.000001
2023-05-27 21:13:59,606 epoch 9 - iter 5439/7770 - loss 0.14334889 - samples/sec: 17.27 - lr: 0.000001
2023-05-27 21:17:01,900 epoch 9 - iter 6216/7770 - loss 0.14263797 - samples/sec: 17.06 - lr: 0.000001
2023-05-27 21:20:00,493 epoch 9 - iter 6993/7770 - loss 0.14274278 - samples/sec: 17.41 - lr: 0.000001
2023-05-27 21:22:59,844 epoch 9 - iter 7770/7770 - loss 0.14260748 - samples/sec: 17.34 - lr: 0.000001
2023-05-27 21:22:59,848 ----------------------------------------------------------------------------------------------------
2023-05-27 21:22:59,848 EPOCH 9 done: loss 0.1426 - lr 0.000001
2023-05-27 21:25:42,737 Evaluating as a multi-label problem: False
2023-05-27 21:25:42,815 DEV : loss 0.07164758443832397 - f1-score (micro avg)  0.9617
2023-05-27 21:25:42,984 BAD EPOCHS (no improvement): 4
2023-05-27 21:25:42,987 ----------------------------------------------------------------------------------------------------
2023-05-27 21:28:45,642 epoch 10 - iter 777/7770 - loss 0.13534048 - samples/sec: 17.02 - lr: 0.000001
2023-05-27 21:31:47,901 epoch 10 - iter 1554/7770 - loss 0.13765172 - samples/sec: 17.06 - lr: 0.000000
2023-05-27 21:34:49,956 epoch 10 - iter 2331/7770 - loss 0.13690856 - samples/sec: 17.08 - lr: 0.000000
2023-05-27 21:37:50,734 epoch 10 - iter 3108/7770 - loss 0.13755139 - samples/sec: 17.20 - lr: 0.000000
2023-05-27 21:40:50,789 epoch 10 - iter 3885/7770 - loss 0.13802160 - samples/sec: 17.27 - lr: 0.000000
2023-05-27 21:43:51,142 epoch 10 - iter 4662/7770 - loss 0.13844406 - samples/sec: 17.24 - lr: 0.000000
2023-05-27 21:46:54,893 epoch 10 - iter 5439/7770 - loss 0.13844927 - samples/sec: 16.92 - lr: 0.000000
2023-05-27 21:49:57,278 epoch 10 - iter 6216/7770 - loss 0.13898388 - samples/sec: 17.05 - lr: 0.000000
2023-05-27 21:52:57,851 epoch 10 - iter 6993/7770 - loss 0.13891026 - samples/sec: 17.22 - lr: 0.000000
2023-05-27 21:55:54,625 epoch 10 - iter 7770/7770 - loss 0.13944928 - samples/sec: 17.59 - lr: 0.000000
2023-05-27 21:55:54,629 ----------------------------------------------------------------------------------------------------
2023-05-27 21:55:54,629 EPOCH 10 done: loss 0.1394 - lr 0.000000
2023-05-27 21:58:32,423 Evaluating as a multi-label problem: False
2023-05-27 21:58:32,492 DEV : loss 0.06885910779237747 - f1-score (micro avg)  0.9626
2023-05-27 21:58:32,667 BAD EPOCHS (no improvement): 4
2023-05-27 21:58:44,089 ----------------------------------------------------------------------------------------------------
2023-05-27 21:58:44,092 Testing using last state of model ...
2023-05-27 22:02:42,494 Evaluating as a multi-label problem: False
2023-05-27 22:02:42,602 0.9325	0.9438	0.9381	0.9118
2023-05-27 22:02:42,603 
Results:
- F-score (micro) 0.9381
- F-score (macro) 0.9351
- Accuracy 0.9118

By class:
              precision    recall  f1-score   support

         PER     0.9765    0.9797    0.9781      2715
         ORG     0.8986    0.9406    0.9191      2543
         LOC     0.9522    0.9382    0.9451      2442
        MISC     0.8923    0.9037    0.8979      1889

   micro avg     0.9325    0.9438    0.9381      9589
   macro avg     0.9299    0.9405    0.9351      9589
weighted avg     0.9331    0.9438    0.9383      9589

2023-05-27 22:02:42,603 ----------------------------------------------------------------------------------------------------
2023-05-27 22:02:42,603 ----------------------------------------------------------------------------------------------------
2023-05-27 22:05:12,633 Evaluating as a multi-label problem: False
2023-05-27 22:05:12,682 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-27 22:05:12,683 0.9367	0.9419	0.9393	0.9202
2023-05-27 22:05:12,683 ----------------------------------------------------------------------------------------------------
2023-05-27 22:06:45,095 Evaluating as a multi-label problem: False
2023-05-27 22:06:45,161 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-27 22:06:45,162 0.9296	0.9451	0.9373	0.9061
