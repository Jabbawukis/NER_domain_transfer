2023-06-05 11:50:16,750 ----------------------------------------------------------------------------------------------------
2023-06-05 11:50:16,755 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-06-05 11:50:16,758 ----------------------------------------------------------------------------------------------------
2023-06-05 11:50:16,758 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-06-05 11:50:16,760 ----------------------------------------------------------------------------------------------------
2023-06-05 11:50:16,760 Parameters:
2023-06-05 11:50:16,760  - learning_rate: "0.000005"
2023-06-05 11:50:16,760  - mini_batch_size: "4"
2023-06-05 11:50:16,760  - patience: "3"
2023-06-05 11:50:16,760  - anneal_factor: "0.5"
2023-06-05 11:50:16,760  - max_epochs: "10"
2023-06-05 11:50:16,760  - shuffle: "True"
2023-06-05 11:50:16,760  - train_with_dev: "False"
2023-06-05 11:50:16,760  - batch_growth_annealing: "False"
2023-06-05 11:50:16,760 ----------------------------------------------------------------------------------------------------
2023-06-05 11:50:16,760 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_1"
2023-06-05 11:50:16,760 ----------------------------------------------------------------------------------------------------
2023-06-05 11:50:16,761 Device: cuda:2
2023-06-05 11:50:16,761 ----------------------------------------------------------------------------------------------------
2023-06-05 11:50:16,761 Embeddings storage mode: none
2023-06-05 11:50:16,761 ----------------------------------------------------------------------------------------------------
2023-06-05 11:53:23,158 epoch 1 - iter 777/7770 - loss 2.08566684 - samples/sec: 16.68 - lr: 0.000001
2023-06-05 11:56:30,940 epoch 1 - iter 1554/7770 - loss 1.29893903 - samples/sec: 16.56 - lr: 0.000001
2023-06-05 11:59:37,441 epoch 1 - iter 2331/7770 - loss 1.04661227 - samples/sec: 16.67 - lr: 0.000002
2023-06-05 12:02:45,124 epoch 1 - iter 3108/7770 - loss 0.86330645 - samples/sec: 16.57 - lr: 0.000002
2023-06-05 12:05:55,795 epoch 1 - iter 3885/7770 - loss 0.74213255 - samples/sec: 16.31 - lr: 0.000003
2023-06-05 12:09:03,490 epoch 1 - iter 4662/7770 - loss 0.67023754 - samples/sec: 16.57 - lr: 0.000003
2023-06-05 12:12:03,573 epoch 1 - iter 5439/7770 - loss 0.62062213 - samples/sec: 17.27 - lr: 0.000003
2023-06-05 12:15:06,371 epoch 1 - iter 6216/7770 - loss 0.57194208 - samples/sec: 17.01 - lr: 0.000004
2023-06-05 12:18:11,024 epoch 1 - iter 6993/7770 - loss 0.53561989 - samples/sec: 16.84 - lr: 0.000005
2023-06-05 12:21:16,228 epoch 1 - iter 7770/7770 - loss 0.50146453 - samples/sec: 16.79 - lr: 0.000005
2023-06-05 12:21:16,232 ----------------------------------------------------------------------------------------------------
2023-06-05 12:21:16,232 EPOCH 1 done: loss 0.5015 - lr 0.000005
2023-06-05 12:23:55,899 Evaluating as a multi-label problem: False
2023-06-05 12:23:56,008 DEV : loss 0.13381947576999664 - f1-score (micro avg)  0.8949
2023-06-05 12:23:56,201 BAD EPOCHS (no improvement): 4
2023-06-05 12:23:56,203 ----------------------------------------------------------------------------------------------------
2023-06-05 12:27:01,358 epoch 2 - iter 777/7770 - loss 0.25164461 - samples/sec: 16.80 - lr: 0.000005
2023-06-05 12:30:06,060 epoch 2 - iter 1554/7770 - loss 0.24083055 - samples/sec: 16.84 - lr: 0.000005
2023-06-05 12:33:11,615 epoch 2 - iter 2331/7770 - loss 0.23827171 - samples/sec: 16.76 - lr: 0.000005
2023-06-05 12:36:20,724 epoch 2 - iter 3108/7770 - loss 0.23843031 - samples/sec: 16.44 - lr: 0.000005
2023-06-05 12:39:34,022 epoch 2 - iter 3885/7770 - loss 0.23871159 - samples/sec: 16.09 - lr: 0.000005
2023-06-05 12:42:39,346 epoch 2 - iter 4662/7770 - loss 0.23761947 - samples/sec: 16.78 - lr: 0.000005
2023-06-05 12:45:44,760 epoch 2 - iter 5439/7770 - loss 0.23409787 - samples/sec: 16.77 - lr: 0.000005
2023-06-05 12:48:50,648 epoch 2 - iter 6216/7770 - loss 0.23401844 - samples/sec: 16.73 - lr: 0.000005
2023-06-05 12:51:56,788 epoch 2 - iter 6993/7770 - loss 0.23191000 - samples/sec: 16.71 - lr: 0.000005
2023-06-05 12:55:03,661 epoch 2 - iter 7770/7770 - loss 0.23010636 - samples/sec: 16.64 - lr: 0.000004
2023-06-05 12:55:03,665 ----------------------------------------------------------------------------------------------------
2023-06-05 12:55:03,665 EPOCH 2 done: loss 0.2301 - lr 0.000004
2023-06-05 12:58:41,959 Evaluating as a multi-label problem: False
2023-06-05 12:58:42,043 DEV : loss 0.08391650021076202 - f1-score (micro avg)  0.9478
2023-06-05 12:58:42,333 BAD EPOCHS (no improvement): 4
2023-06-05 12:58:42,340 ----------------------------------------------------------------------------------------------------
2023-06-05 13:02:31,386 epoch 3 - iter 777/7770 - loss 0.20849111 - samples/sec: 13.58 - lr: 0.000004
2023-06-05 13:05:43,821 epoch 3 - iter 1554/7770 - loss 0.20537679 - samples/sec: 16.16 - lr: 0.000004
2023-06-05 13:08:50,723 epoch 3 - iter 2331/7770 - loss 0.20200549 - samples/sec: 16.64 - lr: 0.000004
2023-06-05 13:11:52,565 epoch 3 - iter 3108/7770 - loss 0.19881302 - samples/sec: 17.10 - lr: 0.000004
2023-06-05 13:14:53,587 epoch 3 - iter 3885/7770 - loss 0.19886523 - samples/sec: 17.18 - lr: 0.000004
2023-06-05 13:18:05,707 epoch 3 - iter 4662/7770 - loss 0.19916197 - samples/sec: 16.19 - lr: 0.000004
2023-06-05 13:21:06,740 epoch 3 - iter 5439/7770 - loss 0.20082513 - samples/sec: 17.18 - lr: 0.000004
2023-06-05 13:24:11,595 epoch 3 - iter 6216/7770 - loss 0.20004020 - samples/sec: 16.82 - lr: 0.000004
2023-06-05 13:27:12,212 epoch 3 - iter 6993/7770 - loss 0.19893043 - samples/sec: 17.22 - lr: 0.000004
2023-06-05 13:30:13,894 epoch 3 - iter 7770/7770 - loss 0.19733142 - samples/sec: 17.12 - lr: 0.000004
2023-06-05 13:30:13,898 ----------------------------------------------------------------------------------------------------
2023-06-05 13:30:13,898 EPOCH 3 done: loss 0.1973 - lr 0.000004
2023-06-05 13:32:56,510 Evaluating as a multi-label problem: False
2023-06-05 13:32:56,604 DEV : loss 0.066886305809021 - f1-score (micro avg)  0.9529
2023-06-05 13:32:56,801 BAD EPOCHS (no improvement): 4
2023-06-05 13:32:56,804 ----------------------------------------------------------------------------------------------------
2023-06-05 13:36:02,450 epoch 4 - iter 777/7770 - loss 0.17658588 - samples/sec: 16.75 - lr: 0.000004
2023-06-05 13:39:03,024 epoch 4 - iter 1554/7770 - loss 0.17575552 - samples/sec: 17.22 - lr: 0.000004
2023-06-05 13:42:07,040 epoch 4 - iter 2331/7770 - loss 0.17631488 - samples/sec: 16.90 - lr: 0.000004
2023-06-05 13:45:05,478 epoch 4 - iter 3108/7770 - loss 0.17703124 - samples/sec: 17.43 - lr: 0.000004
2023-06-05 13:48:09,058 epoch 4 - iter 3885/7770 - loss 0.17806006 - samples/sec: 16.94 - lr: 0.000004
2023-06-05 13:51:13,552 epoch 4 - iter 4662/7770 - loss 0.17982682 - samples/sec: 16.85 - lr: 0.000004
2023-06-05 13:54:24,505 epoch 4 - iter 5439/7770 - loss 0.17956041 - samples/sec: 16.28 - lr: 0.000004
2023-06-05 13:57:28,020 epoch 4 - iter 6216/7770 - loss 0.18038364 - samples/sec: 16.94 - lr: 0.000003
2023-06-05 14:00:34,774 epoch 4 - iter 6993/7770 - loss 0.17965540 - samples/sec: 16.65 - lr: 0.000003
2023-06-05 14:03:40,168 epoch 4 - iter 7770/7770 - loss 0.17867672 - samples/sec: 16.77 - lr: 0.000003
2023-06-05 14:03:40,171 ----------------------------------------------------------------------------------------------------
2023-06-05 14:03:40,171 EPOCH 4 done: loss 0.1787 - lr 0.000003
2023-06-05 14:06:18,251 Evaluating as a multi-label problem: False
2023-06-05 14:06:18,309 DEV : loss 0.06661660224199295 - f1-score (micro avg)  0.9549
2023-06-05 14:06:18,446 BAD EPOCHS (no improvement): 4
2023-06-05 14:06:18,462 ----------------------------------------------------------------------------------------------------
2023-06-05 14:09:23,836 epoch 5 - iter 777/7770 - loss 0.16707477 - samples/sec: 16.78 - lr: 0.000003
2023-06-05 14:12:29,608 epoch 5 - iter 1554/7770 - loss 0.16689702 - samples/sec: 16.74 - lr: 0.000003
2023-06-05 14:15:34,267 epoch 5 - iter 2331/7770 - loss 0.16738895 - samples/sec: 16.84 - lr: 0.000003
2023-06-05 14:18:31,917 epoch 5 - iter 3108/7770 - loss 0.16833180 - samples/sec: 17.50 - lr: 0.000003
2023-06-05 14:21:33,090 epoch 5 - iter 3885/7770 - loss 0.16785007 - samples/sec: 17.16 - lr: 0.000003
2023-06-05 14:24:32,731 epoch 5 - iter 4662/7770 - loss 0.16790934 - samples/sec: 17.31 - lr: 0.000003
2023-06-05 14:27:37,715 epoch 5 - iter 5439/7770 - loss 0.16742023 - samples/sec: 16.81 - lr: 0.000003
2023-06-05 14:30:50,814 epoch 5 - iter 6216/7770 - loss 0.16687388 - samples/sec: 16.10 - lr: 0.000003
2023-06-05 14:33:59,761 epoch 5 - iter 6993/7770 - loss 0.16612608 - samples/sec: 16.46 - lr: 0.000003
2023-06-05 14:37:08,421 epoch 5 - iter 7770/7770 - loss 0.16607285 - samples/sec: 16.48 - lr: 0.000003
2023-06-05 14:37:08,425 ----------------------------------------------------------------------------------------------------
2023-06-05 14:37:08,425 EPOCH 5 done: loss 0.1661 - lr 0.000003
2023-06-05 14:40:00,327 Evaluating as a multi-label problem: False
2023-06-05 14:40:00,426 DEV : loss 0.06526952981948853 - f1-score (micro avg)  0.9591
2023-06-05 14:40:00,608 BAD EPOCHS (no improvement): 4
2023-06-05 14:40:00,612 ----------------------------------------------------------------------------------------------------
2023-06-05 14:43:12,825 epoch 6 - iter 777/7770 - loss 0.15923516 - samples/sec: 16.18 - lr: 0.000003
2023-06-05 14:46:23,810 epoch 6 - iter 1554/7770 - loss 0.16177315 - samples/sec: 16.28 - lr: 0.000003
2023-06-05 14:49:34,104 epoch 6 - iter 2331/7770 - loss 0.16099057 - samples/sec: 16.34 - lr: 0.000003
2023-06-05 14:52:41,795 epoch 6 - iter 3108/7770 - loss 0.16050454 - samples/sec: 16.57 - lr: 0.000003
2023-06-05 14:55:44,421 epoch 6 - iter 3885/7770 - loss 0.16066072 - samples/sec: 17.03 - lr: 0.000003
2023-06-05 14:58:52,433 epoch 6 - iter 4662/7770 - loss 0.15959122 - samples/sec: 16.54 - lr: 0.000002
2023-06-05 15:02:02,863 epoch 6 - iter 5439/7770 - loss 0.16006732 - samples/sec: 16.33 - lr: 0.000002
2023-06-05 15:05:08,097 epoch 6 - iter 6216/7770 - loss 0.15922371 - samples/sec: 16.79 - lr: 0.000002
2023-06-05 15:08:23,753 epoch 6 - iter 6993/7770 - loss 0.15896540 - samples/sec: 15.89 - lr: 0.000002
2023-06-05 15:11:33,100 epoch 6 - iter 7770/7770 - loss 0.15909014 - samples/sec: 16.42 - lr: 0.000002
2023-06-05 15:11:33,105 ----------------------------------------------------------------------------------------------------
2023-06-05 15:11:33,105 EPOCH 6 done: loss 0.1591 - lr 0.000002
2023-06-05 15:14:09,884 Evaluating as a multi-label problem: False
2023-06-05 15:14:09,960 DEV : loss 0.06416631489992142 - f1-score (micro avg)  0.9602
2023-06-05 15:14:10,103 BAD EPOCHS (no improvement): 4
2023-06-05 15:14:10,113 ----------------------------------------------------------------------------------------------------
2023-06-05 15:17:17,933 epoch 7 - iter 777/7770 - loss 0.15147887 - samples/sec: 16.56 - lr: 0.000002
2023-06-05 15:20:24,762 epoch 7 - iter 1554/7770 - loss 0.15311168 - samples/sec: 16.64 - lr: 0.000002
2023-06-05 15:23:29,504 epoch 7 - iter 2331/7770 - loss 0.15466469 - samples/sec: 16.83 - lr: 0.000002
2023-06-05 15:26:33,990 epoch 7 - iter 3108/7770 - loss 0.15327255 - samples/sec: 16.86 - lr: 0.000002
2023-06-05 15:29:37,159 epoch 7 - iter 3885/7770 - loss 0.15358351 - samples/sec: 16.98 - lr: 0.000002
2023-06-05 15:32:43,641 epoch 7 - iter 4662/7770 - loss 0.15313362 - samples/sec: 16.67 - lr: 0.000002
2023-06-05 15:35:49,524 epoch 7 - iter 5439/7770 - loss 0.15234590 - samples/sec: 16.73 - lr: 0.000002
2023-06-05 15:38:55,169 epoch 7 - iter 6216/7770 - loss 0.15241215 - samples/sec: 16.75 - lr: 0.000002
2023-06-05 15:42:05,181 epoch 7 - iter 6993/7770 - loss 0.15278558 - samples/sec: 16.37 - lr: 0.000002
2023-06-05 15:45:21,443 epoch 7 - iter 7770/7770 - loss 0.15295090 - samples/sec: 15.84 - lr: 0.000002
2023-06-05 15:45:21,446 ----------------------------------------------------------------------------------------------------
2023-06-05 15:45:21,446 EPOCH 7 done: loss 0.1530 - lr 0.000002
2023-06-05 15:47:58,174 Evaluating as a multi-label problem: False
2023-06-05 15:47:58,270 DEV : loss 0.0672793835401535 - f1-score (micro avg)  0.9626
2023-06-05 15:47:58,499 BAD EPOCHS (no improvement): 4
2023-06-05 15:47:58,502 ----------------------------------------------------------------------------------------------------
2023-06-05 15:51:08,032 epoch 8 - iter 777/7770 - loss 0.14495779 - samples/sec: 16.41 - lr: 0.000002
2023-06-05 15:54:11,802 epoch 8 - iter 1554/7770 - loss 0.14418942 - samples/sec: 16.92 - lr: 0.000002
2023-06-05 15:57:13,268 epoch 8 - iter 2331/7770 - loss 0.14451310 - samples/sec: 17.14 - lr: 0.000002
2023-06-05 16:00:17,788 epoch 8 - iter 3108/7770 - loss 0.14578210 - samples/sec: 16.85 - lr: 0.000001
2023-06-05 16:03:23,792 epoch 8 - iter 3885/7770 - loss 0.14619799 - samples/sec: 16.72 - lr: 0.000001
2023-06-05 16:06:34,432 epoch 8 - iter 4662/7770 - loss 0.14697712 - samples/sec: 16.31 - lr: 0.000001
2023-06-05 16:09:40,521 epoch 8 - iter 5439/7770 - loss 0.14712880 - samples/sec: 16.71 - lr: 0.000001
2023-06-05 16:12:46,239 epoch 8 - iter 6216/7770 - loss 0.14687616 - samples/sec: 16.74 - lr: 0.000001
2023-06-05 16:15:49,803 epoch 8 - iter 6993/7770 - loss 0.14661659 - samples/sec: 16.94 - lr: 0.000001
2023-06-05 16:18:57,588 epoch 8 - iter 7770/7770 - loss 0.14650360 - samples/sec: 16.56 - lr: 0.000001
2023-06-05 16:18:57,592 ----------------------------------------------------------------------------------------------------
2023-06-05 16:18:57,592 EPOCH 8 done: loss 0.1465 - lr 0.000001
2023-06-05 16:21:48,587 Evaluating as a multi-label problem: False
2023-06-05 16:21:48,691 DEV : loss 0.06595209240913391 - f1-score (micro avg)  0.9619
2023-06-05 16:21:48,914 BAD EPOCHS (no improvement): 4
2023-06-05 16:21:48,926 ----------------------------------------------------------------------------------------------------
2023-06-05 16:24:56,860 epoch 9 - iter 777/7770 - loss 0.14820908 - samples/sec: 16.55 - lr: 0.000001
2023-06-05 16:28:07,251 epoch 9 - iter 1554/7770 - loss 0.14497699 - samples/sec: 16.33 - lr: 0.000001
2023-06-05 16:31:13,047 epoch 9 - iter 2331/7770 - loss 0.14618578 - samples/sec: 16.74 - lr: 0.000001
2023-06-05 16:34:17,057 epoch 9 - iter 3108/7770 - loss 0.14491482 - samples/sec: 16.90 - lr: 0.000001
2023-06-05 16:37:20,975 epoch 9 - iter 3885/7770 - loss 0.14526818 - samples/sec: 16.91 - lr: 0.000001
2023-06-05 16:40:26,128 epoch 9 - iter 4662/7770 - loss 0.14539437 - samples/sec: 16.80 - lr: 0.000001
2023-06-05 16:43:29,785 epoch 9 - iter 5439/7770 - loss 0.14534117 - samples/sec: 16.93 - lr: 0.000001
2023-06-05 16:46:35,215 epoch 9 - iter 6216/7770 - loss 0.14462985 - samples/sec: 16.77 - lr: 0.000001
2023-06-05 16:49:38,585 epoch 9 - iter 6993/7770 - loss 0.14459339 - samples/sec: 16.96 - lr: 0.000001
2023-06-05 16:52:45,907 epoch 9 - iter 7770/7770 - loss 0.14496906 - samples/sec: 16.60 - lr: 0.000001
2023-06-05 16:52:45,911 ----------------------------------------------------------------------------------------------------
2023-06-05 16:52:45,911 EPOCH 9 done: loss 0.1450 - lr 0.000001
2023-06-05 16:55:35,325 Evaluating as a multi-label problem: False
2023-06-05 16:55:35,397 DEV : loss 0.06645257771015167 - f1-score (micro avg)  0.9634
2023-06-05 16:55:35,573 BAD EPOCHS (no improvement): 4
2023-06-05 16:55:35,576 ----------------------------------------------------------------------------------------------------
2023-06-05 16:58:42,439 epoch 10 - iter 777/7770 - loss 0.13820236 - samples/sec: 16.64 - lr: 0.000001
2023-06-05 17:01:47,103 epoch 10 - iter 1554/7770 - loss 0.13904973 - samples/sec: 16.84 - lr: 0.000000
2023-06-05 17:04:51,756 epoch 10 - iter 2331/7770 - loss 0.13944439 - samples/sec: 16.84 - lr: 0.000000
2023-06-05 17:08:03,794 epoch 10 - iter 3108/7770 - loss 0.14008189 - samples/sec: 16.19 - lr: 0.000000
2023-06-05 17:11:07,597 epoch 10 - iter 3885/7770 - loss 0.13969191 - samples/sec: 16.92 - lr: 0.000000
2023-06-05 17:14:14,553 epoch 10 - iter 4662/7770 - loss 0.14002410 - samples/sec: 16.63 - lr: 0.000000
2023-06-05 17:17:24,304 epoch 10 - iter 5439/7770 - loss 0.13989734 - samples/sec: 16.39 - lr: 0.000000
2023-06-05 17:20:28,921 epoch 10 - iter 6216/7770 - loss 0.14015810 - samples/sec: 16.84 - lr: 0.000000
2023-06-05 17:23:31,595 epoch 10 - iter 6993/7770 - loss 0.14055279 - samples/sec: 17.02 - lr: 0.000000
2023-06-05 17:26:39,738 epoch 10 - iter 7770/7770 - loss 0.14057284 - samples/sec: 16.53 - lr: 0.000000
2023-06-05 17:26:39,742 ----------------------------------------------------------------------------------------------------
2023-06-05 17:26:39,742 EPOCH 10 done: loss 0.1406 - lr 0.000000
2023-06-05 17:29:30,409 Evaluating as a multi-label problem: False
2023-06-05 17:29:30,508 DEV : loss 0.06693931668996811 - f1-score (micro avg)  0.9632
2023-06-05 17:29:30,731 BAD EPOCHS (no improvement): 4
2023-06-05 17:29:47,170 ----------------------------------------------------------------------------------------------------
2023-06-05 17:29:47,179 Testing using last state of model ...
2023-06-05 17:33:53,349 Evaluating as a multi-label problem: False
2023-06-05 17:33:53,475 0.9352	0.9458	0.9405	0.9154
2023-06-05 17:33:53,476 
Results:
- F-score (micro) 0.9405
- F-score (macro) 0.9373
- Accuracy 0.9154

By class:
              precision    recall  f1-score   support

         PER     0.9823    0.9790    0.9806      2715
         ORG     0.9025    0.9501    0.9257      2543
         LOC     0.9483    0.9394    0.9438      2442
        MISC     0.8976    0.9005    0.8990      1889

   micro avg     0.9352    0.9458    0.9405      9589
   macro avg     0.9327    0.9422    0.9373      9589
weighted avg     0.9358    0.9458    0.9406      9589

2023-06-05 17:33:53,476 ----------------------------------------------------------------------------------------------------
2023-06-05 17:33:53,476 ----------------------------------------------------------------------------------------------------
2023-06-05 17:36:42,453 Evaluating as a multi-label problem: False
2023-06-05 17:36:42,503 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-06-05 17:36:42,504 0.9416	0.9411	0.9414	0.9238
2023-06-05 17:36:42,504 ----------------------------------------------------------------------------------------------------
2023-06-05 17:38:23,493 Evaluating as a multi-label problem: False
2023-06-05 17:38:23,558 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-06-05 17:38:23,559 0.9309	0.949	0.9399	0.9097
