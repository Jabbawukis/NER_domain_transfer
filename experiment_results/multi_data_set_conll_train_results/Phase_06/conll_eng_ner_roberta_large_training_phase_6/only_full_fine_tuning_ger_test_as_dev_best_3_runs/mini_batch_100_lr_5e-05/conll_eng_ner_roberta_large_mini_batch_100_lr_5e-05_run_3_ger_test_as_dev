2023-07-12 22:13:03,978 ----------------------------------------------------------------------------------------------------
2023-07-12 22:13:03,980 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): XLMRobertaEmbeddings(
        (word_embeddings): Embedding(250003, 1024)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): XLMRobertaEncoder(
        (layer): ModuleList(
          (0-23): 24 x XLMRobertaLayer(
            (attention): XLMRobertaAttention(
              (self): XLMRobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): XLMRobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): XLMRobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): XLMRobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): XLMRobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-07-12 22:13:03,980 ----------------------------------------------------------------------------------------------------
2023-07-12 22:13:03,980 Corpus: 14987 train + 3160 dev + 3684 test sentences
2023-07-12 22:13:03,980 ----------------------------------------------------------------------------------------------------
2023-07-12 22:13:03,980 Train:  14987 sentences
2023-07-12 22:13:03,980         (train_with_dev=False, train_with_test=False)
2023-07-12 22:13:03,980 ----------------------------------------------------------------------------------------------------
2023-07-12 22:13:03,980 Training Params:
2023-07-12 22:13:03,980  - learning_rate: "5e-05" 
2023-07-12 22:13:03,980  - mini_batch_size: "100"
2023-07-12 22:13:03,980  - max_epochs: "10"
2023-07-12 22:13:03,980  - shuffle: "True"
2023-07-12 22:13:03,980 ----------------------------------------------------------------------------------------------------
2023-07-12 22:13:03,980 Plugins:
2023-07-12 22:13:03,980  - LinearScheduler | warmup_fraction: '0.1'
2023-07-12 22:13:03,980 ----------------------------------------------------------------------------------------------------
2023-07-12 22:13:03,980 Final evaluation on model after last epoch (final-model.pt)
2023-07-12 22:13:03,980  - metric: "('micro avg', 'f1-score')"
2023-07-12 22:13:03,980 ----------------------------------------------------------------------------------------------------
2023-07-12 22:13:03,980 Computation:
2023-07-12 22:13:03,980  - compute on device: cuda:3
2023-07-12 22:13:03,980  - embedding storage: none
2023-07-12 22:13:03,980 ----------------------------------------------------------------------------------------------------
2023-07-12 22:13:03,980 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_mini_batch_100_lr_5e-05_run_3_ger_test_as_dev"
2023-07-12 22:13:03,980 ----------------------------------------------------------------------------------------------------
2023-07-12 22:13:03,980 Removed gradient clipping
2023-07-12 22:13:03,980 ----------------------------------------------------------------------------------------------------
2023-07-12 22:13:03,980 ----------------------------------------------------------------------------------------------------
2023-07-12 22:14:04,215 epoch 1 - iter 15/150 - loss 3.46864313 - time (sec): 60.23 - samples/sec: 339.43 - lr: 0.000005 - momentum: 0.000000
2023-07-12 22:14:51,680 epoch 1 - iter 30/150 - loss 2.69299214 - time (sec): 107.70 - samples/sec: 374.93 - lr: 0.000010 - momentum: 0.000000
2023-07-12 22:15:39,068 epoch 1 - iter 45/150 - loss 2.10669859 - time (sec): 155.09 - samples/sec: 393.36 - lr: 0.000015 - momentum: 0.000000
2023-07-12 22:16:25,421 epoch 1 - iter 60/150 - loss 1.74112669 - time (sec): 201.44 - samples/sec: 404.61 - lr: 0.000020 - momentum: 0.000000
2023-07-12 22:17:11,519 epoch 1 - iter 75/150 - loss 1.49487091 - time (sec): 247.54 - samples/sec: 411.04 - lr: 0.000025 - momentum: 0.000000
2023-07-12 22:17:58,065 epoch 1 - iter 90/150 - loss 1.29921318 - time (sec): 294.08 - samples/sec: 417.65 - lr: 0.000030 - momentum: 0.000000
2023-07-12 22:18:44,111 epoch 1 - iter 105/150 - loss 1.15442955 - time (sec): 340.13 - samples/sec: 420.45 - lr: 0.000035 - momentum: 0.000000
2023-07-12 22:19:28,053 epoch 1 - iter 120/150 - loss 1.03296358 - time (sec): 384.07 - samples/sec: 425.47 - lr: 0.000040 - momentum: 0.000000
2023-07-12 22:20:14,332 epoch 1 - iter 135/150 - loss 0.93427602 - time (sec): 430.35 - samples/sec: 427.47 - lr: 0.000045 - momentum: 0.000000
2023-07-12 22:20:59,852 epoch 1 - iter 150/150 - loss 0.85235353 - time (sec): 475.87 - samples/sec: 429.88 - lr: 0.000050 - momentum: 0.000000
2023-07-12 22:20:59,852 ----------------------------------------------------------------------------------------------------
2023-07-12 22:20:59,852 EPOCH 1 done: loss 0.8524 - lr: 0.000050
2023-07-12 22:21:43,424 DEV : loss 0.1707373410463333 - f1-score (micro avg)  0.7042
2023-07-12 22:22:24,207 TEST : loss 0.11594711244106293 - f1-score (micro avg)  0.8762
2023-07-12 22:22:24,257 ----------------------------------------------------------------------------------------------------
2023-07-12 22:23:09,282 epoch 2 - iter 15/150 - loss 0.10958404 - time (sec): 45.02 - samples/sec: 468.36 - lr: 0.000049 - momentum: 0.000000
2023-07-12 22:24:10,375 epoch 2 - iter 30/150 - loss 0.10860258 - time (sec): 106.12 - samples/sec: 386.93 - lr: 0.000049 - momentum: 0.000000
2023-07-12 22:25:24,252 epoch 2 - iter 45/150 - loss 0.10581267 - time (sec): 179.99 - samples/sec: 345.80 - lr: 0.000048 - momentum: 0.000000
2023-07-12 22:26:15,715 epoch 2 - iter 60/150 - loss 0.10163416 - time (sec): 231.46 - samples/sec: 353.91 - lr: 0.000048 - momentum: 0.000000
2023-07-12 22:27:01,768 epoch 2 - iter 75/150 - loss 0.09769899 - time (sec): 277.51 - samples/sec: 368.49 - lr: 0.000047 - momentum: 0.000000
2023-07-12 22:27:48,708 epoch 2 - iter 90/150 - loss 0.09571652 - time (sec): 324.45 - samples/sec: 377.97 - lr: 0.000047 - momentum: 0.000000
2023-07-12 22:28:34,986 epoch 2 - iter 105/150 - loss 0.09164195 - time (sec): 370.73 - samples/sec: 387.25 - lr: 0.000046 - momentum: 0.000000
2023-07-12 22:29:20,481 epoch 2 - iter 120/150 - loss 0.08894749 - time (sec): 416.22 - samples/sec: 393.02 - lr: 0.000046 - momentum: 0.000000
2023-07-12 22:30:06,641 epoch 2 - iter 135/150 - loss 0.08778512 - time (sec): 462.38 - samples/sec: 397.84 - lr: 0.000045 - momentum: 0.000000
2023-07-12 22:30:51,830 epoch 2 - iter 150/150 - loss 0.08670953 - time (sec): 507.57 - samples/sec: 403.03 - lr: 0.000045 - momentum: 0.000000
2023-07-12 22:30:51,831 ----------------------------------------------------------------------------------------------------
2023-07-12 22:30:51,831 EPOCH 2 done: loss 0.0867 - lr: 0.000045
2023-07-12 22:31:36,617 DEV : loss 0.16627465188503265 - f1-score (micro avg)  0.7066
2023-07-12 22:32:19,131 TEST : loss 0.08773187547922134 - f1-score (micro avg)  0.9158
2023-07-12 22:32:19,181 ----------------------------------------------------------------------------------------------------
2023-07-12 22:33:05,861 epoch 3 - iter 15/150 - loss 0.06169203 - time (sec): 46.68 - samples/sec: 434.53 - lr: 0.000044 - momentum: 0.000000
2023-07-12 22:33:52,564 epoch 3 - iter 30/150 - loss 0.06014861 - time (sec): 93.38 - samples/sec: 448.51 - lr: 0.000043 - momentum: 0.000000
2023-07-12 22:34:38,209 epoch 3 - iter 45/150 - loss 0.05722092 - time (sec): 139.03 - samples/sec: 446.08 - lr: 0.000043 - momentum: 0.000000
2023-07-12 22:35:23,824 epoch 3 - iter 60/150 - loss 0.05587891 - time (sec): 184.64 - samples/sec: 449.84 - lr: 0.000042 - momentum: 0.000000
2023-07-12 22:36:29,144 epoch 3 - iter 75/150 - loss 0.05436781 - time (sec): 249.96 - samples/sec: 410.46 - lr: 0.000042 - momentum: 0.000000
2023-07-12 22:37:44,831 epoch 3 - iter 90/150 - loss 0.05381774 - time (sec): 325.65 - samples/sec: 379.07 - lr: 0.000041 - momentum: 0.000000
2023-07-12 22:38:33,024 epoch 3 - iter 105/150 - loss 0.05198729 - time (sec): 373.84 - samples/sec: 383.79 - lr: 0.000041 - momentum: 0.000000
2023-07-12 22:39:19,725 epoch 3 - iter 120/150 - loss 0.05131381 - time (sec): 420.54 - samples/sec: 389.66 - lr: 0.000040 - momentum: 0.000000
2023-07-12 22:40:05,980 epoch 3 - iter 135/150 - loss 0.05029065 - time (sec): 466.80 - samples/sec: 395.31 - lr: 0.000040 - momentum: 0.000000
2023-07-12 22:40:51,940 epoch 3 - iter 150/150 - loss 0.04889157 - time (sec): 512.76 - samples/sec: 398.95 - lr: 0.000039 - momentum: 0.000000
2023-07-12 22:40:51,941 ----------------------------------------------------------------------------------------------------
2023-07-12 22:40:51,941 EPOCH 3 done: loss 0.0489 - lr: 0.000039
2023-07-12 22:41:34,360 DEV : loss 0.15123215317726135 - f1-score (micro avg)  0.7466
2023-07-12 22:42:16,286 TEST : loss 0.0755314752459526 - f1-score (micro avg)  0.9349
2023-07-12 22:42:16,335 ----------------------------------------------------------------------------------------------------
2023-07-12 22:43:00,557 epoch 4 - iter 15/150 - loss 0.03306866 - time (sec): 44.22 - samples/sec: 450.29 - lr: 0.000038 - momentum: 0.000000
2023-07-12 22:43:44,602 epoch 4 - iter 30/150 - loss 0.02948198 - time (sec): 88.26 - samples/sec: 457.98 - lr: 0.000038 - momentum: 0.000000
2023-07-12 22:44:31,652 epoch 4 - iter 45/150 - loss 0.02915298 - time (sec): 135.31 - samples/sec: 444.85 - lr: 0.000037 - momentum: 0.000000
2023-07-12 22:45:17,592 epoch 4 - iter 60/150 - loss 0.03095022 - time (sec): 181.26 - samples/sec: 445.54 - lr: 0.000037 - momentum: 0.000000
2023-07-12 22:46:03,685 epoch 4 - iter 75/150 - loss 0.03105885 - time (sec): 227.35 - samples/sec: 444.84 - lr: 0.000036 - momentum: 0.000000
2023-07-12 22:46:50,408 epoch 4 - iter 90/150 - loss 0.03070822 - time (sec): 274.07 - samples/sec: 445.24 - lr: 0.000036 - momentum: 0.000000
2023-07-12 22:47:34,795 epoch 4 - iter 105/150 - loss 0.03023518 - time (sec): 318.46 - samples/sec: 447.17 - lr: 0.000035 - momentum: 0.000000
2023-07-12 22:48:44,559 epoch 4 - iter 120/150 - loss 0.03051325 - time (sec): 388.22 - samples/sec: 419.34 - lr: 0.000035 - momentum: 0.000000
2023-07-12 22:49:56,316 epoch 4 - iter 135/150 - loss 0.03027042 - time (sec): 459.98 - samples/sec: 400.08 - lr: 0.000034 - momentum: 0.000000
2023-07-12 22:50:43,822 epoch 4 - iter 150/150 - loss 0.02960420 - time (sec): 507.48 - samples/sec: 403.10 - lr: 0.000033 - momentum: 0.000000
2023-07-12 22:50:43,823 ----------------------------------------------------------------------------------------------------
2023-07-12 22:50:43,823 EPOCH 4 done: loss 0.0296 - lr: 0.000033
2023-07-12 22:51:26,479 DEV : loss 0.15140600502490997 - f1-score (micro avg)  0.7411
2023-07-12 22:52:09,004 TEST : loss 0.08161169290542603 - f1-score (micro avg)  0.9294
2023-07-12 22:52:09,255 ----------------------------------------------------------------------------------------------------
2023-07-12 22:52:55,258 epoch 5 - iter 15/150 - loss 0.01966466 - time (sec): 46.00 - samples/sec: 448.87 - lr: 0.000033 - momentum: 0.000000
2023-07-12 22:53:42,033 epoch 5 - iter 30/150 - loss 0.02287997 - time (sec): 92.77 - samples/sec: 448.83 - lr: 0.000032 - momentum: 0.000000
2023-07-12 22:54:27,601 epoch 5 - iter 45/150 - loss 0.02582126 - time (sec): 138.34 - samples/sec: 448.86 - lr: 0.000032 - momentum: 0.000000
2023-07-12 22:55:11,727 epoch 5 - iter 60/150 - loss 0.02569425 - time (sec): 182.47 - samples/sec: 453.71 - lr: 0.000031 - momentum: 0.000000
2023-07-12 22:55:55,974 epoch 5 - iter 75/150 - loss 0.02667744 - time (sec): 226.71 - samples/sec: 453.57 - lr: 0.000031 - momentum: 0.000000
2023-07-12 22:56:43,158 epoch 5 - iter 90/150 - loss 0.02658105 - time (sec): 273.90 - samples/sec: 450.75 - lr: 0.000030 - momentum: 0.000000
2023-07-12 22:57:28,952 epoch 5 - iter 105/150 - loss 0.02617714 - time (sec): 319.69 - samples/sec: 451.80 - lr: 0.000030 - momentum: 0.000000
2023-07-12 22:58:14,982 epoch 5 - iter 120/150 - loss 0.02566595 - time (sec): 365.72 - samples/sec: 448.95 - lr: 0.000029 - momentum: 0.000000
2023-07-12 22:59:00,975 epoch 5 - iter 135/150 - loss 0.02514864 - time (sec): 411.72 - samples/sec: 447.17 - lr: 0.000028 - momentum: 0.000000
2023-07-12 22:59:44,918 epoch 5 - iter 150/150 - loss 0.02518374 - time (sec): 455.66 - samples/sec: 448.95 - lr: 0.000028 - momentum: 0.000000
2023-07-12 22:59:44,919 ----------------------------------------------------------------------------------------------------
2023-07-12 22:59:44,919 EPOCH 5 done: loss 0.0252 - lr: 0.000028
2023-07-12 23:00:51,408 DEV : loss 0.19090859591960907 - f1-score (micro avg)  0.7069
2023-07-12 23:02:00,819 TEST : loss 0.09652471542358398 - f1-score (micro avg)  0.9309
2023-07-12 23:02:00,869 ----------------------------------------------------------------------------------------------------
2023-07-12 23:02:51,126 epoch 6 - iter 15/150 - loss 0.02317174 - time (sec): 50.26 - samples/sec: 408.76 - lr: 0.000027 - momentum: 0.000000
2023-07-12 23:03:37,930 epoch 6 - iter 30/150 - loss 0.01910198 - time (sec): 97.06 - samples/sec: 425.77 - lr: 0.000027 - momentum: 0.000000
2023-07-12 23:04:24,001 epoch 6 - iter 45/150 - loss 0.01936994 - time (sec): 143.13 - samples/sec: 431.96 - lr: 0.000026 - momentum: 0.000000
2023-07-12 23:05:11,466 epoch 6 - iter 60/150 - loss 0.01979542 - time (sec): 190.60 - samples/sec: 429.78 - lr: 0.000026 - momentum: 0.000000
2023-07-12 23:05:57,433 epoch 6 - iter 75/150 - loss 0.02101628 - time (sec): 236.56 - samples/sec: 434.22 - lr: 0.000025 - momentum: 0.000000
2023-07-12 23:06:43,102 epoch 6 - iter 90/150 - loss 0.02133135 - time (sec): 282.23 - samples/sec: 437.69 - lr: 0.000025 - momentum: 0.000000
2023-07-12 23:07:28,045 epoch 6 - iter 105/150 - loss 0.02178670 - time (sec): 327.18 - samples/sec: 440.57 - lr: 0.000024 - momentum: 0.000000
2023-07-12 23:08:12,657 epoch 6 - iter 120/150 - loss 0.02206151 - time (sec): 371.79 - samples/sec: 441.93 - lr: 0.000024 - momentum: 0.000000
2023-07-12 23:08:58,823 epoch 6 - iter 135/150 - loss 0.02196612 - time (sec): 417.95 - samples/sec: 441.53 - lr: 0.000023 - momentum: 0.000000
2023-07-12 23:09:45,253 epoch 6 - iter 150/150 - loss 0.02174950 - time (sec): 464.38 - samples/sec: 440.51 - lr: 0.000022 - momentum: 0.000000
2023-07-12 23:09:45,254 ----------------------------------------------------------------------------------------------------
2023-07-12 23:09:45,254 EPOCH 6 done: loss 0.0217 - lr: 0.000022
2023-07-12 23:10:27,486 DEV : loss 0.17656798660755157 - f1-score (micro avg)  0.7288
2023-07-12 23:11:09,391 TEST : loss 0.09550215303897858 - f1-score (micro avg)  0.9286
2023-07-12 23:11:09,456 ----------------------------------------------------------------------------------------------------
2023-07-12 23:11:59,399 epoch 7 - iter 15/150 - loss 0.01665803 - time (sec): 49.94 - samples/sec: 409.63 - lr: 0.000022 - momentum: 0.000000
2023-07-12 23:13:12,787 epoch 7 - iter 30/150 - loss 0.01712795 - time (sec): 123.33 - samples/sec: 328.38 - lr: 0.000021 - momentum: 0.000000
2023-07-12 23:14:15,550 epoch 7 - iter 45/150 - loss 0.01746750 - time (sec): 186.09 - samples/sec: 325.95 - lr: 0.000021 - momentum: 0.000000
2023-07-12 23:15:02,064 epoch 7 - iter 60/150 - loss 0.01807680 - time (sec): 232.61 - samples/sec: 349.16 - lr: 0.000020 - momentum: 0.000000
2023-07-12 23:15:48,706 epoch 7 - iter 75/150 - loss 0.01767436 - time (sec): 279.25 - samples/sec: 364.83 - lr: 0.000020 - momentum: 0.000000
2023-07-12 23:16:34,439 epoch 7 - iter 90/150 - loss 0.01727389 - time (sec): 324.98 - samples/sec: 376.97 - lr: 0.000019 - momentum: 0.000000
2023-07-12 23:17:20,625 epoch 7 - iter 105/150 - loss 0.01719381 - time (sec): 371.17 - samples/sec: 385.37 - lr: 0.000019 - momentum: 0.000000
2023-07-12 23:18:06,286 epoch 7 - iter 120/150 - loss 0.01670433 - time (sec): 416.83 - samples/sec: 392.63 - lr: 0.000018 - momentum: 0.000000
2023-07-12 23:18:51,438 epoch 7 - iter 135/150 - loss 0.01642380 - time (sec): 461.98 - samples/sec: 399.86 - lr: 0.000017 - momentum: 0.000000
2023-07-12 23:19:35,662 epoch 7 - iter 150/150 - loss 0.01615458 - time (sec): 506.20 - samples/sec: 404.12 - lr: 0.000017 - momentum: 0.000000
2023-07-12 23:19:35,663 ----------------------------------------------------------------------------------------------------
2023-07-12 23:19:35,663 EPOCH 7 done: loss 0.0162 - lr: 0.000017
2023-07-12 23:20:20,684 DEV : loss 0.1744520664215088 - f1-score (micro avg)  0.729
2023-07-12 23:21:01,836 TEST : loss 0.09301865845918655 - f1-score (micro avg)  0.9353
2023-07-12 23:21:01,893 ----------------------------------------------------------------------------------------------------
2023-07-12 23:21:48,912 epoch 8 - iter 15/150 - loss 0.01141943 - time (sec): 47.02 - samples/sec: 443.69 - lr: 0.000016 - momentum: 0.000000
2023-07-12 23:22:35,235 epoch 8 - iter 30/150 - loss 0.01272623 - time (sec): 93.34 - samples/sec: 444.55 - lr: 0.000016 - momentum: 0.000000
2023-07-12 23:23:20,689 epoch 8 - iter 45/150 - loss 0.01254840 - time (sec): 138.79 - samples/sec: 443.66 - lr: 0.000015 - momentum: 0.000000
2023-07-12 23:24:16,250 epoch 8 - iter 60/150 - loss 0.01234758 - time (sec): 194.35 - samples/sec: 421.83 - lr: 0.000015 - momentum: 0.000000
2023-07-12 23:25:33,170 epoch 8 - iter 75/150 - loss 0.01217844 - time (sec): 271.28 - samples/sec: 377.24 - lr: 0.000014 - momentum: 0.000000
2023-07-12 23:26:28,864 epoch 8 - iter 90/150 - loss 0.01230492 - time (sec): 326.97 - samples/sec: 372.82 - lr: 0.000014 - momentum: 0.000000
2023-07-12 23:27:15,273 epoch 8 - iter 105/150 - loss 0.01173660 - time (sec): 373.38 - samples/sec: 381.49 - lr: 0.000013 - momentum: 0.000000
2023-07-12 23:28:02,000 epoch 8 - iter 120/150 - loss 0.01180788 - time (sec): 420.10 - samples/sec: 387.44 - lr: 0.000012 - momentum: 0.000000
2023-07-12 23:28:48,278 epoch 8 - iter 135/150 - loss 0.01164827 - time (sec): 466.38 - samples/sec: 393.81 - lr: 0.000012 - momentum: 0.000000
2023-07-12 23:29:33,891 epoch 8 - iter 150/150 - loss 0.01144999 - time (sec): 512.00 - samples/sec: 399.55 - lr: 0.000011 - momentum: 0.000000
2023-07-12 23:29:33,891 ----------------------------------------------------------------------------------------------------
2023-07-12 23:29:33,892 EPOCH 8 done: loss 0.0114 - lr: 0.000011
2023-07-12 23:30:17,751 DEV : loss 0.17031919956207275 - f1-score (micro avg)  0.7395
2023-07-12 23:30:59,236 TEST : loss 0.09352923184633255 - f1-score (micro avg)  0.9355
2023-07-12 23:30:59,304 ----------------------------------------------------------------------------------------------------
2023-07-12 23:31:43,082 epoch 9 - iter 15/150 - loss 0.00677133 - time (sec): 43.78 - samples/sec: 451.53 - lr: 0.000011 - momentum: 0.000000
2023-07-12 23:32:30,367 epoch 9 - iter 30/150 - loss 0.00791792 - time (sec): 91.06 - samples/sec: 439.75 - lr: 0.000010 - momentum: 0.000000
2023-07-12 23:33:16,510 epoch 9 - iter 45/150 - loss 0.00856723 - time (sec): 137.20 - samples/sec: 442.39 - lr: 0.000010 - momentum: 0.000000
2023-07-12 23:34:02,465 epoch 9 - iter 60/150 - loss 0.00829069 - time (sec): 183.16 - samples/sec: 447.54 - lr: 0.000009 - momentum: 0.000000
2023-07-12 23:34:48,252 epoch 9 - iter 75/150 - loss 0.00887601 - time (sec): 228.95 - samples/sec: 444.86 - lr: 0.000009 - momentum: 0.000000
2023-07-12 23:35:32,242 epoch 9 - iter 90/150 - loss 0.00927238 - time (sec): 272.94 - samples/sec: 448.48 - lr: 0.000008 - momentum: 0.000000
2023-07-12 23:36:37,549 epoch 9 - iter 105/150 - loss 0.00892474 - time (sec): 338.24 - samples/sec: 424.79 - lr: 0.000008 - momentum: 0.000000
2023-07-12 23:37:52,140 epoch 9 - iter 120/150 - loss 0.00903737 - time (sec): 412.83 - samples/sec: 397.56 - lr: 0.000007 - momentum: 0.000000
2023-07-12 23:38:41,218 epoch 9 - iter 135/150 - loss 0.00939683 - time (sec): 461.91 - samples/sec: 399.51 - lr: 0.000006 - momentum: 0.000000
2023-07-12 23:39:28,188 epoch 9 - iter 150/150 - loss 0.00910968 - time (sec): 508.88 - samples/sec: 401.99 - lr: 0.000006 - momentum: 0.000000
2023-07-12 23:39:28,189 ----------------------------------------------------------------------------------------------------
2023-07-12 23:39:28,189 EPOCH 9 done: loss 0.0091 - lr: 0.000006
2023-07-12 23:40:10,790 DEV : loss 0.16838519275188446 - f1-score (micro avg)  0.7452
2023-07-12 23:40:53,231 TEST : loss 0.09449398517608643 - f1-score (micro avg)  0.9362
2023-07-12 23:40:53,282 ----------------------------------------------------------------------------------------------------
2023-07-12 23:41:39,548 epoch 10 - iter 15/150 - loss 0.00750169 - time (sec): 46.26 - samples/sec: 441.52 - lr: 0.000005 - momentum: 0.000000
2023-07-12 23:42:26,312 epoch 10 - iter 30/150 - loss 0.00793958 - time (sec): 93.03 - samples/sec: 443.69 - lr: 0.000005 - momentum: 0.000000
2023-07-12 23:43:11,923 epoch 10 - iter 45/150 - loss 0.00726879 - time (sec): 138.64 - samples/sec: 442.96 - lr: 0.000004 - momentum: 0.000000
2023-07-12 23:43:56,524 epoch 10 - iter 60/150 - loss 0.00783716 - time (sec): 183.24 - samples/sec: 443.50 - lr: 0.000004 - momentum: 0.000000
2023-07-12 23:44:42,302 epoch 10 - iter 75/150 - loss 0.00752377 - time (sec): 229.02 - samples/sec: 446.64 - lr: 0.000003 - momentum: 0.000000
2023-07-12 23:45:27,946 epoch 10 - iter 90/150 - loss 0.00720180 - time (sec): 274.66 - samples/sec: 448.73 - lr: 0.000003 - momentum: 0.000000
2023-07-12 23:46:14,682 epoch 10 - iter 105/150 - loss 0.00699611 - time (sec): 321.40 - samples/sec: 448.13 - lr: 0.000002 - momentum: 0.000000
2023-07-12 23:47:00,478 epoch 10 - iter 120/150 - loss 0.00750603 - time (sec): 367.19 - samples/sec: 447.65 - lr: 0.000001 - momentum: 0.000000
2023-07-12 23:47:44,260 epoch 10 - iter 135/150 - loss 0.00732749 - time (sec): 410.98 - samples/sec: 450.26 - lr: 0.000001 - momentum: 0.000000
2023-07-12 23:48:58,817 epoch 10 - iter 150/150 - loss 0.00752174 - time (sec): 485.53 - samples/sec: 421.32 - lr: 0.000000 - momentum: 0.000000
2023-07-12 23:48:58,818 ----------------------------------------------------------------------------------------------------
2023-07-12 23:48:58,818 EPOCH 10 done: loss 0.0075 - lr: 0.000000
2023-07-12 23:50:03,922 DEV : loss 0.17697963118553162 - f1-score (micro avg)  0.7455
2023-07-12 23:50:47,324 TEST : loss 0.09700893610715866 - f1-score (micro avg)  0.937
2023-07-12 23:50:58,597 ----------------------------------------------------------------------------------------------------
2023-07-12 23:50:58,602 Testing using last state of model ...
2023-07-12 23:51:39,370 
Results:
- F-score (micro) 0.937
- F-score (macro) 0.923
- Accuracy 0.9068

By class:
              precision    recall  f1-score   support

         ORG     0.9166    0.9458    0.9310      1661
         LOC     0.9473    0.9371    0.9421      1668
         PER     0.9851    0.9808    0.9830      1617
        MISC     0.8166    0.8561    0.8359       702

   micro avg     0.9319    0.9421    0.9370      5648
   macro avg     0.9164    0.9300    0.9230      5648
weighted avg     0.9328    0.9421    0.9373      5648

2023-07-12 23:51:39,370 ----------------------------------------------------------------------------------------------------
