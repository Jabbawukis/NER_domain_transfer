2023-07-07 16:52:03,683 ----------------------------------------------------------------------------------------------------
2023-07-07 16:52:03,688 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): XLMRobertaEmbeddings(
        (word_embeddings): Embedding(250003, 1024)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): XLMRobertaEncoder(
        (layer): ModuleList(
          (0-23): 24 x XLMRobertaLayer(
            (attention): XLMRobertaAttention(
              (self): XLMRobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): XLMRobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): XLMRobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): XLMRobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): XLMRobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-07-07 16:52:03,688 ----------------------------------------------------------------------------------------------------
2023-07-07 16:52:03,689 Corpus: 14987 train + 3160 dev + 3684 test sentences
2023-07-07 16:52:03,689 ----------------------------------------------------------------------------------------------------
2023-07-07 16:52:03,689 Train:  14987 sentences
2023-07-07 16:52:03,689         (train_with_dev=False, train_with_test=False)
2023-07-07 16:52:03,689 ----------------------------------------------------------------------------------------------------
2023-07-07 16:52:03,689 Training Params:
2023-07-07 16:52:03,689  - learning_rate: "9e-05" 
2023-07-07 16:52:03,689  - mini_batch_size: "40"
2023-07-07 16:52:03,689  - max_epochs: "10"
2023-07-07 16:52:03,689  - shuffle: "True"
2023-07-07 16:52:03,689 ----------------------------------------------------------------------------------------------------
2023-07-07 16:52:03,690 Plugins:
2023-07-07 16:52:03,690  - LinearScheduler | warmup_fraction: '0.1'
2023-07-07 16:52:03,690 ----------------------------------------------------------------------------------------------------
2023-07-07 16:52:03,690 Final evaluation on model after last epoch (final-model.pt)
2023-07-07 16:52:03,690  - metric: "('micro avg', 'f1-score')"
2023-07-07 16:52:03,690 ----------------------------------------------------------------------------------------------------
2023-07-07 16:52:03,690 Computation:
2023-07-07 16:52:03,690  - compute on device: cuda:1
2023-07-07 16:52:03,690  - embedding storage: none
2023-07-07 16:52:03,690 ----------------------------------------------------------------------------------------------------
2023-07-07 16:52:03,690 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_mini_batch_40_lr_9e-05_ger_test_as_dev"
2023-07-07 16:52:03,690 ----------------------------------------------------------------------------------------------------
2023-07-07 16:52:03,690 Removed gradient clipping
2023-07-07 16:52:03,690 ----------------------------------------------------------------------------------------------------
2023-07-07 16:52:03,690 ----------------------------------------------------------------------------------------------------
2023-07-07 16:52:52,178 epoch 1 - iter 37/375 - loss 2.13628092 - time (sec): 48.48 - samples/sec: 430.89 - lr: 0.000009 - momentum: 0.000000
2023-07-07 16:53:38,760 epoch 1 - iter 74/375 - loss 1.41473390 - time (sec): 95.07 - samples/sec: 431.87 - lr: 0.000018 - momentum: 0.000000
2023-07-07 16:54:24,390 epoch 1 - iter 111/375 - loss 1.10128261 - time (sec): 140.70 - samples/sec: 435.87 - lr: 0.000026 - momentum: 0.000000
2023-07-07 16:55:10,159 epoch 1 - iter 148/375 - loss 0.90344421 - time (sec): 186.47 - samples/sec: 433.47 - lr: 0.000035 - momentum: 0.000000
2023-07-07 16:55:55,704 epoch 1 - iter 185/375 - loss 0.76166859 - time (sec): 232.01 - samples/sec: 435.41 - lr: 0.000044 - momentum: 0.000000
2023-07-07 16:56:40,653 epoch 1 - iter 222/375 - loss 0.65775530 - time (sec): 276.96 - samples/sec: 438.01 - lr: 0.000053 - momentum: 0.000000
2023-07-07 16:57:25,784 epoch 1 - iter 259/375 - loss 0.58005996 - time (sec): 322.09 - samples/sec: 440.14 - lr: 0.000062 - momentum: 0.000000
2023-07-07 16:58:12,229 epoch 1 - iter 296/375 - loss 0.52558354 - time (sec): 368.54 - samples/sec: 439.55 - lr: 0.000071 - momentum: 0.000000
2023-07-07 16:58:58,785 epoch 1 - iter 333/375 - loss 0.48118128 - time (sec): 415.09 - samples/sec: 437.09 - lr: 0.000080 - momentum: 0.000000
2023-07-07 16:59:44,255 epoch 1 - iter 370/375 - loss 0.44318623 - time (sec): 460.56 - samples/sec: 438.55 - lr: 0.000089 - momentum: 0.000000
2023-07-07 16:59:50,227 ----------------------------------------------------------------------------------------------------
2023-07-07 16:59:50,227 EPOCH 1 done: loss 0.4393 - lr: 0.000089
2023-07-07 17:00:33,674 DEV : loss 0.17220626771450043 - f1-score (micro avg)  0.6988
2023-07-07 17:01:17,225 TEST : loss 0.10400237143039703 - f1-score (micro avg)  0.8923
2023-07-07 17:01:17,274 ----------------------------------------------------------------------------------------------------
2023-07-07 17:02:04,211 epoch 2 - iter 37/375 - loss 0.09833622 - time (sec): 46.93 - samples/sec: 437.20 - lr: 0.000089 - momentum: 0.000000
2023-07-07 17:02:50,174 epoch 2 - iter 74/375 - loss 0.09127984 - time (sec): 92.90 - samples/sec: 437.41 - lr: 0.000088 - momentum: 0.000000
2023-07-07 17:03:36,307 epoch 2 - iter 111/375 - loss 0.08381421 - time (sec): 139.03 - samples/sec: 439.29 - lr: 0.000087 - momentum: 0.000000
2023-07-07 17:04:23,295 epoch 2 - iter 148/375 - loss 0.08495382 - time (sec): 186.02 - samples/sec: 435.57 - lr: 0.000086 - momentum: 0.000000
2023-07-07 17:05:08,747 epoch 2 - iter 185/375 - loss 0.08116132 - time (sec): 231.47 - samples/sec: 436.35 - lr: 0.000085 - momentum: 0.000000
2023-07-07 17:05:53,206 epoch 2 - iter 222/375 - loss 0.07962428 - time (sec): 275.93 - samples/sec: 441.88 - lr: 0.000084 - momentum: 0.000000
2023-07-07 17:06:38,915 epoch 2 - iter 259/375 - loss 0.07792168 - time (sec): 321.64 - samples/sec: 439.69 - lr: 0.000083 - momentum: 0.000000
2023-07-07 17:07:25,358 epoch 2 - iter 296/375 - loss 0.07800841 - time (sec): 368.08 - samples/sec: 438.49 - lr: 0.000082 - momentum: 0.000000
2023-07-07 17:08:11,705 epoch 2 - iter 333/375 - loss 0.07808641 - time (sec): 414.43 - samples/sec: 439.16 - lr: 0.000081 - momentum: 0.000000
2023-07-07 17:08:58,686 epoch 2 - iter 370/375 - loss 0.07724154 - time (sec): 461.41 - samples/sec: 437.76 - lr: 0.000080 - momentum: 0.000000
2023-07-07 17:09:04,489 ----------------------------------------------------------------------------------------------------
2023-07-07 17:09:04,490 EPOCH 2 done: loss 0.0773 - lr: 0.000080
2023-07-07 17:09:47,950 DEV : loss 0.15114645659923553 - f1-score (micro avg)  0.6771
2023-07-07 17:10:28,579 TEST : loss 0.09516479820013046 - f1-score (micro avg)  0.9004
2023-07-07 17:10:28,625 ----------------------------------------------------------------------------------------------------
2023-07-07 17:11:14,779 epoch 3 - iter 37/375 - loss 0.07081754 - time (sec): 46.15 - samples/sec: 421.02 - lr: 0.000079 - momentum: 0.000000
2023-07-07 17:12:01,594 epoch 3 - iter 74/375 - loss 0.06260077 - time (sec): 92.97 - samples/sec: 427.30 - lr: 0.000078 - momentum: 0.000000
2023-07-07 17:12:47,813 epoch 3 - iter 111/375 - loss 0.05878475 - time (sec): 139.19 - samples/sec: 427.68 - lr: 0.000077 - momentum: 0.000000
2023-07-07 17:13:34,002 epoch 3 - iter 148/375 - loss 0.06088529 - time (sec): 185.38 - samples/sec: 432.30 - lr: 0.000076 - momentum: 0.000000
2023-07-07 17:14:20,846 epoch 3 - iter 185/375 - loss 0.06339071 - time (sec): 232.22 - samples/sec: 432.64 - lr: 0.000075 - momentum: 0.000000
2023-07-07 17:15:08,626 epoch 3 - iter 222/375 - loss 0.06339735 - time (sec): 280.00 - samples/sec: 430.59 - lr: 0.000074 - momentum: 0.000000
2023-07-07 17:15:56,415 epoch 3 - iter 259/375 - loss 0.06581208 - time (sec): 327.79 - samples/sec: 430.50 - lr: 0.000073 - momentum: 0.000000
2023-07-07 17:16:43,251 epoch 3 - iter 296/375 - loss 0.06513704 - time (sec): 374.63 - samples/sec: 430.63 - lr: 0.000072 - momentum: 0.000000
2023-07-07 17:17:30,437 epoch 3 - iter 333/375 - loss 0.06511390 - time (sec): 421.81 - samples/sec: 430.64 - lr: 0.000071 - momentum: 0.000000
2023-07-07 17:18:17,484 epoch 3 - iter 370/375 - loss 0.06383275 - time (sec): 468.86 - samples/sec: 431.05 - lr: 0.000070 - momentum: 0.000000
2023-07-07 17:18:23,555 ----------------------------------------------------------------------------------------------------
2023-07-07 17:18:23,555 EPOCH 3 done: loss 0.0637 - lr: 0.000070
2023-07-07 17:19:07,363 DEV : loss 0.2176160365343094 - f1-score (micro avg)  0.6046
2023-07-07 17:19:48,225 TEST : loss 0.09446591883897781 - f1-score (micro avg)  0.9053
2023-07-07 17:19:48,271 ----------------------------------------------------------------------------------------------------
2023-07-07 17:20:33,855 epoch 4 - iter 37/375 - loss 0.04186430 - time (sec): 45.58 - samples/sec: 421.74 - lr: 0.000069 - momentum: 0.000000
2023-07-07 17:21:19,104 epoch 4 - iter 74/375 - loss 0.05783651 - time (sec): 90.83 - samples/sec: 431.30 - lr: 0.000068 - momentum: 0.000000
2023-07-07 17:22:04,359 epoch 4 - iter 111/375 - loss 0.05980384 - time (sec): 136.09 - samples/sec: 436.68 - lr: 0.000067 - momentum: 0.000000
2023-07-07 17:22:50,819 epoch 4 - iter 148/375 - loss 0.07015495 - time (sec): 182.55 - samples/sec: 438.46 - lr: 0.000066 - momentum: 0.000000
2023-07-07 17:23:35,153 epoch 4 - iter 185/375 - loss 0.07064903 - time (sec): 226.88 - samples/sec: 442.32 - lr: 0.000065 - momentum: 0.000000
2023-07-07 17:24:19,778 epoch 4 - iter 222/375 - loss 0.06952680 - time (sec): 271.51 - samples/sec: 444.21 - lr: 0.000064 - momentum: 0.000000
2023-07-07 17:25:04,307 epoch 4 - iter 259/375 - loss 0.06691692 - time (sec): 316.03 - samples/sec: 446.86 - lr: 0.000063 - momentum: 0.000000
2023-07-07 17:25:50,148 epoch 4 - iter 296/375 - loss 0.06413872 - time (sec): 361.88 - samples/sec: 448.45 - lr: 0.000062 - momentum: 0.000000
2023-07-07 17:26:36,082 epoch 4 - iter 333/375 - loss 0.06139724 - time (sec): 407.81 - samples/sec: 446.52 - lr: 0.000061 - momentum: 0.000000
2023-07-07 17:27:22,226 epoch 4 - iter 370/375 - loss 0.05874791 - time (sec): 453.95 - samples/sec: 444.49 - lr: 0.000060 - momentum: 0.000000
2023-07-07 17:27:28,383 ----------------------------------------------------------------------------------------------------
2023-07-07 17:27:28,384 EPOCH 4 done: loss 0.0584 - lr: 0.000060
2023-07-07 17:28:10,905 DEV : loss 0.2162938266992569 - f1-score (micro avg)  0.6132
2023-07-07 17:28:52,866 TEST : loss 0.08623258024454117 - f1-score (micro avg)  0.9231
2023-07-07 17:28:52,919 ----------------------------------------------------------------------------------------------------
2023-07-07 17:29:39,085 epoch 5 - iter 37/375 - loss 0.03678137 - time (sec): 46.16 - samples/sec: 430.29 - lr: 0.000059 - momentum: 0.000000
2023-07-07 17:30:25,196 epoch 5 - iter 74/375 - loss 0.03548128 - time (sec): 92.27 - samples/sec: 437.32 - lr: 0.000058 - momentum: 0.000000
2023-07-07 17:31:11,567 epoch 5 - iter 111/375 - loss 0.03535023 - time (sec): 138.65 - samples/sec: 431.64 - lr: 0.000057 - momentum: 0.000000
2023-07-07 17:31:55,321 epoch 5 - iter 148/375 - loss 0.03418774 - time (sec): 182.40 - samples/sec: 438.91 - lr: 0.000056 - momentum: 0.000000
2023-07-07 17:32:40,279 epoch 5 - iter 185/375 - loss 0.03392998 - time (sec): 227.36 - samples/sec: 443.32 - lr: 0.000055 - momentum: 0.000000
2023-07-07 17:33:26,886 epoch 5 - iter 222/375 - loss 0.03405513 - time (sec): 273.97 - samples/sec: 441.49 - lr: 0.000054 - momentum: 0.000000
2023-07-07 17:34:12,049 epoch 5 - iter 259/375 - loss 0.03311804 - time (sec): 319.13 - samples/sec: 443.75 - lr: 0.000053 - momentum: 0.000000
2023-07-07 17:34:56,839 epoch 5 - iter 296/375 - loss 0.03331347 - time (sec): 363.92 - samples/sec: 443.76 - lr: 0.000052 - momentum: 0.000000
2023-07-07 17:35:42,469 epoch 5 - iter 333/375 - loss 0.03510596 - time (sec): 409.55 - samples/sec: 442.88 - lr: 0.000051 - momentum: 0.000000
2023-07-07 17:36:27,395 epoch 5 - iter 370/375 - loss 0.03506525 - time (sec): 454.47 - samples/sec: 444.49 - lr: 0.000050 - momentum: 0.000000
2023-07-07 17:36:33,120 ----------------------------------------------------------------------------------------------------
2023-07-07 17:36:33,120 EPOCH 5 done: loss 0.0352 - lr: 0.000050
2023-07-07 17:37:15,274 DEV : loss 0.26521581411361694 - f1-score (micro avg)  0.6395
2023-07-07 17:37:57,120 TEST : loss 0.10344550013542175 - f1-score (micro avg)  0.9176
2023-07-07 17:37:57,166 ----------------------------------------------------------------------------------------------------
2023-07-07 17:38:43,391 epoch 6 - iter 37/375 - loss 0.03683069 - time (sec): 46.22 - samples/sec: 431.51 - lr: 0.000049 - momentum: 0.000000
2023-07-07 17:39:29,949 epoch 6 - iter 74/375 - loss 0.03248315 - time (sec): 92.78 - samples/sec: 432.84 - lr: 0.000048 - momentum: 0.000000
2023-07-07 17:40:15,446 epoch 6 - iter 111/375 - loss 0.04200119 - time (sec): 138.28 - samples/sec: 441.45 - lr: 0.000047 - momentum: 0.000000
2023-07-07 17:41:02,418 epoch 6 - iter 148/375 - loss 0.03957000 - time (sec): 185.25 - samples/sec: 437.80 - lr: 0.000046 - momentum: 0.000000
2023-07-07 17:41:48,663 epoch 6 - iter 185/375 - loss 0.03723787 - time (sec): 231.50 - samples/sec: 436.81 - lr: 0.000045 - momentum: 0.000000
2023-07-07 17:42:34,801 epoch 6 - iter 222/375 - loss 0.03542708 - time (sec): 277.63 - samples/sec: 437.56 - lr: 0.000044 - momentum: 0.000000
2023-07-07 17:43:21,078 epoch 6 - iter 259/375 - loss 0.03450882 - time (sec): 323.91 - samples/sec: 437.54 - lr: 0.000043 - momentum: 0.000000
2023-07-07 17:44:06,946 epoch 6 - iter 296/375 - loss 0.03284466 - time (sec): 369.78 - samples/sec: 437.70 - lr: 0.000042 - momentum: 0.000000
2023-07-07 17:44:53,347 epoch 6 - iter 333/375 - loss 0.03159073 - time (sec): 416.18 - samples/sec: 436.71 - lr: 0.000041 - momentum: 0.000000
2023-07-07 17:45:40,349 epoch 6 - iter 370/375 - loss 0.03091739 - time (sec): 463.18 - samples/sec: 435.93 - lr: 0.000040 - momentum: 0.000000
2023-07-07 17:45:46,212 ----------------------------------------------------------------------------------------------------
2023-07-07 17:45:46,213 EPOCH 6 done: loss 0.0307 - lr: 0.000040
2023-07-07 17:46:28,799 DEV : loss 0.19436992704868317 - f1-score (micro avg)  0.6631
2023-07-07 17:47:09,475 TEST : loss 0.08831588923931122 - f1-score (micro avg)  0.9232
2023-07-07 17:47:09,520 ----------------------------------------------------------------------------------------------------
2023-07-07 17:47:56,644 epoch 7 - iter 37/375 - loss 0.01433593 - time (sec): 47.12 - samples/sec: 408.07 - lr: 0.000039 - momentum: 0.000000
2023-07-07 17:48:43,087 epoch 7 - iter 74/375 - loss 0.01512791 - time (sec): 93.57 - samples/sec: 421.98 - lr: 0.000038 - momentum: 0.000000
2023-07-07 17:49:29,171 epoch 7 - iter 111/375 - loss 0.01568831 - time (sec): 139.65 - samples/sec: 428.00 - lr: 0.000037 - momentum: 0.000000
2023-07-07 17:50:14,553 epoch 7 - iter 148/375 - loss 0.01641534 - time (sec): 185.03 - samples/sec: 428.69 - lr: 0.000036 - momentum: 0.000000
2023-07-07 17:51:01,362 epoch 7 - iter 185/375 - loss 0.01616388 - time (sec): 231.84 - samples/sec: 428.46 - lr: 0.000035 - momentum: 0.000000
2023-07-07 17:51:47,739 epoch 7 - iter 222/375 - loss 0.01602763 - time (sec): 278.22 - samples/sec: 430.84 - lr: 0.000034 - momentum: 0.000000
2023-07-07 17:52:34,266 epoch 7 - iter 259/375 - loss 0.01575410 - time (sec): 324.74 - samples/sec: 432.62 - lr: 0.000033 - momentum: 0.000000
2023-07-07 17:53:21,313 epoch 7 - iter 296/375 - loss 0.01646643 - time (sec): 371.79 - samples/sec: 433.71 - lr: 0.000032 - momentum: 0.000000
2023-07-07 17:54:09,579 epoch 7 - iter 333/375 - loss 0.01668062 - time (sec): 420.06 - samples/sec: 432.93 - lr: 0.000031 - momentum: 0.000000
2023-07-07 17:54:58,021 epoch 7 - iter 370/375 - loss 0.01658749 - time (sec): 468.50 - samples/sec: 431.37 - lr: 0.000030 - momentum: 0.000000
2023-07-07 17:55:04,234 ----------------------------------------------------------------------------------------------------
2023-07-07 17:55:04,235 EPOCH 7 done: loss 0.0164 - lr: 0.000030
2023-07-07 17:55:47,265 DEV : loss 0.24350179731845856 - f1-score (micro avg)  0.6261
2023-07-07 17:56:28,228 TEST : loss 0.09599793702363968 - f1-score (micro avg)  0.9212
2023-07-07 17:56:28,273 ----------------------------------------------------------------------------------------------------
2023-07-07 17:57:15,624 epoch 8 - iter 37/375 - loss 0.01065228 - time (sec): 47.35 - samples/sec: 429.32 - lr: 0.000029 - momentum: 0.000000
2023-07-07 17:58:02,121 epoch 8 - iter 74/375 - loss 0.01076373 - time (sec): 93.85 - samples/sec: 426.65 - lr: 0.000028 - momentum: 0.000000
2023-07-07 17:58:49,466 epoch 8 - iter 111/375 - loss 0.01093381 - time (sec): 141.19 - samples/sec: 424.40 - lr: 0.000027 - momentum: 0.000000
2023-07-07 17:59:37,508 epoch 8 - iter 148/375 - loss 0.01083392 - time (sec): 189.23 - samples/sec: 421.76 - lr: 0.000026 - momentum: 0.000000
2023-07-07 18:00:25,676 epoch 8 - iter 185/375 - loss 0.01135352 - time (sec): 237.40 - samples/sec: 424.94 - lr: 0.000025 - momentum: 0.000000
2023-07-07 18:01:13,629 epoch 8 - iter 222/375 - loss 0.01142676 - time (sec): 285.35 - samples/sec: 424.33 - lr: 0.000024 - momentum: 0.000000
2023-07-07 18:02:04,290 epoch 8 - iter 259/375 - loss 0.01232363 - time (sec): 336.02 - samples/sec: 421.48 - lr: 0.000023 - momentum: 0.000000
2023-07-07 18:02:54,758 epoch 8 - iter 296/375 - loss 0.01199285 - time (sec): 386.48 - samples/sec: 417.29 - lr: 0.000022 - momentum: 0.000000
2023-07-07 18:03:43,595 epoch 8 - iter 333/375 - loss 0.01252541 - time (sec): 435.32 - samples/sec: 417.29 - lr: 0.000021 - momentum: 0.000000
2023-07-07 18:04:32,874 epoch 8 - iter 370/375 - loss 0.01250734 - time (sec): 484.60 - samples/sec: 416.76 - lr: 0.000020 - momentum: 0.000000
2023-07-07 18:04:39,050 ----------------------------------------------------------------------------------------------------
2023-07-07 18:04:39,050 EPOCH 8 done: loss 0.0126 - lr: 0.000020
2023-07-07 18:05:29,319 DEV : loss 0.23803994059562683 - f1-score (micro avg)  0.6451
2023-07-07 18:06:17,058 TEST : loss 0.09343534708023071 - f1-score (micro avg)  0.9274
2023-07-07 18:06:17,504 ----------------------------------------------------------------------------------------------------
2023-07-07 18:07:05,296 epoch 9 - iter 37/375 - loss 0.00469424 - time (sec): 47.79 - samples/sec: 431.26 - lr: 0.000019 - momentum: 0.000000
2023-07-07 18:07:51,452 epoch 9 - iter 74/375 - loss 0.00748699 - time (sec): 93.94 - samples/sec: 435.32 - lr: 0.000018 - momentum: 0.000000
2023-07-07 18:08:38,278 epoch 9 - iter 111/375 - loss 0.00845514 - time (sec): 140.77 - samples/sec: 431.33 - lr: 0.000017 - momentum: 0.000000
2023-07-07 18:09:25,290 epoch 9 - iter 148/375 - loss 0.00803004 - time (sec): 187.78 - samples/sec: 428.81 - lr: 0.000016 - momentum: 0.000000
2023-07-07 18:10:12,102 epoch 9 - iter 185/375 - loss 0.00797421 - time (sec): 234.59 - samples/sec: 428.92 - lr: 0.000015 - momentum: 0.000000
2023-07-07 18:10:58,844 epoch 9 - iter 222/375 - loss 0.00810395 - time (sec): 281.34 - samples/sec: 426.76 - lr: 0.000014 - momentum: 0.000000
2023-07-07 18:11:46,020 epoch 9 - iter 259/375 - loss 0.00829335 - time (sec): 328.51 - samples/sec: 426.25 - lr: 0.000013 - momentum: 0.000000
2023-07-07 18:12:32,901 epoch 9 - iter 296/375 - loss 0.00833485 - time (sec): 375.39 - samples/sec: 428.38 - lr: 0.000012 - momentum: 0.000000
2023-07-07 18:13:18,646 epoch 9 - iter 333/375 - loss 0.00837167 - time (sec): 421.14 - samples/sec: 430.50 - lr: 0.000011 - momentum: 0.000000
2023-07-07 18:14:04,282 epoch 9 - iter 370/375 - loss 0.00819925 - time (sec): 466.77 - samples/sec: 433.05 - lr: 0.000010 - momentum: 0.000000
2023-07-07 18:14:10,627 ----------------------------------------------------------------------------------------------------
2023-07-07 18:14:10,628 EPOCH 9 done: loss 0.0083 - lr: 0.000010
2023-07-07 18:15:00,154 DEV : loss 0.2400532215833664 - f1-score (micro avg)  0.6384
2023-07-07 18:15:47,306 TEST : loss 0.09194628149271011 - f1-score (micro avg)  0.9307
2023-07-07 18:15:47,554 ----------------------------------------------------------------------------------------------------
2023-07-07 18:16:37,371 epoch 10 - iter 37/375 - loss 0.00733731 - time (sec): 49.81 - samples/sec: 416.22 - lr: 0.000009 - momentum: 0.000000
2023-07-07 18:17:26,211 epoch 10 - iter 74/375 - loss 0.00617818 - time (sec): 98.65 - samples/sec: 414.19 - lr: 0.000008 - momentum: 0.000000
2023-07-07 18:18:14,428 epoch 10 - iter 111/375 - loss 0.00617514 - time (sec): 146.87 - samples/sec: 413.04 - lr: 0.000007 - momentum: 0.000000
2023-07-07 18:19:04,581 epoch 10 - iter 148/375 - loss 0.00585614 - time (sec): 197.02 - samples/sec: 408.07 - lr: 0.000006 - momentum: 0.000000
2023-07-07 18:19:51,120 epoch 10 - iter 185/375 - loss 0.00557161 - time (sec): 243.56 - samples/sec: 412.18 - lr: 0.000005 - momentum: 0.000000
2023-07-07 18:20:37,407 epoch 10 - iter 222/375 - loss 0.00606265 - time (sec): 289.85 - samples/sec: 416.55 - lr: 0.000004 - momentum: 0.000000
2023-07-07 18:21:23,523 epoch 10 - iter 259/375 - loss 0.00591398 - time (sec): 335.96 - samples/sec: 420.41 - lr: 0.000003 - momentum: 0.000000
2023-07-07 18:22:09,811 epoch 10 - iter 296/375 - loss 0.00631734 - time (sec): 382.25 - samples/sec: 423.30 - lr: 0.000002 - momentum: 0.000000
2023-07-07 18:22:56,487 epoch 10 - iter 333/375 - loss 0.00620772 - time (sec): 428.93 - samples/sec: 424.62 - lr: 0.000001 - momentum: 0.000000
2023-07-07 18:23:41,459 epoch 10 - iter 370/375 - loss 0.00617130 - time (sec): 473.90 - samples/sec: 426.15 - lr: 0.000000 - momentum: 0.000000
2023-07-07 18:23:46,974 ----------------------------------------------------------------------------------------------------
2023-07-07 18:23:46,974 EPOCH 10 done: loss 0.0062 - lr: 0.000000
2023-07-07 18:24:31,982 DEV : loss 0.2531154155731201 - f1-score (micro avg)  0.6521
2023-07-07 18:25:13,773 TEST : loss 0.09819452464580536 - f1-score (micro avg)  0.9312
2023-07-07 18:25:27,393 ----------------------------------------------------------------------------------------------------
2023-07-07 18:25:27,400 Testing using last state of model ...
2023-07-07 18:26:08,835 
Results:
- F-score (micro) 0.9312
- F-score (macro) 0.9184
- Accuracy 0.8983

By class:
              precision    recall  f1-score   support

         ORG     0.9012    0.9278    0.9143      1661
         LOC     0.9397    0.9436    0.9417      1668
         PER     0.9784    0.9808    0.9796      1617
        MISC     0.8145    0.8632    0.8382       702

   micro avg     0.9230    0.9396    0.9312      5648
   macro avg     0.9084    0.9289    0.9184      5648
weighted avg     0.9239    0.9396    0.9316      5648

2023-07-07 18:26:08,836 ----------------------------------------------------------------------------------------------------
