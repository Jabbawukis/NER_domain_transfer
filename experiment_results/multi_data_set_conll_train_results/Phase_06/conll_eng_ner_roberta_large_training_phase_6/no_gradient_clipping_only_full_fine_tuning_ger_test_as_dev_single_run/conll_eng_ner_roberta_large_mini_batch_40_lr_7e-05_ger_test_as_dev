2023-07-07 13:46:11,667 ----------------------------------------------------------------------------------------------------
2023-07-07 13:46:11,669 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): XLMRobertaEmbeddings(
        (word_embeddings): Embedding(250003, 1024)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): XLMRobertaEncoder(
        (layer): ModuleList(
          (0-23): 24 x XLMRobertaLayer(
            (attention): XLMRobertaAttention(
              (self): XLMRobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): XLMRobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): XLMRobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): XLMRobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): XLMRobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-07-07 13:46:11,669 ----------------------------------------------------------------------------------------------------
2023-07-07 13:46:11,669 Corpus: 14987 train + 3160 dev + 3684 test sentences
2023-07-07 13:46:11,669 ----------------------------------------------------------------------------------------------------
2023-07-07 13:46:11,669 Train:  14987 sentences
2023-07-07 13:46:11,669         (train_with_dev=False, train_with_test=False)
2023-07-07 13:46:11,669 ----------------------------------------------------------------------------------------------------
2023-07-07 13:46:11,669 Training Params:
2023-07-07 13:46:11,669  - learning_rate: "7e-05" 
2023-07-07 13:46:11,669  - mini_batch_size: "40"
2023-07-07 13:46:11,669  - max_epochs: "10"
2023-07-07 13:46:11,669  - shuffle: "True"
2023-07-07 13:46:11,669 ----------------------------------------------------------------------------------------------------
2023-07-07 13:46:11,669 Plugins:
2023-07-07 13:46:11,669  - LinearScheduler | warmup_fraction: '0.1'
2023-07-07 13:46:11,669 ----------------------------------------------------------------------------------------------------
2023-07-07 13:46:11,669 Final evaluation on model after last epoch (final-model.pt)
2023-07-07 13:46:11,669  - metric: "('micro avg', 'f1-score')"
2023-07-07 13:46:11,669 ----------------------------------------------------------------------------------------------------
2023-07-07 13:46:11,669 Computation:
2023-07-07 13:46:11,669  - compute on device: cuda:1
2023-07-07 13:46:11,670  - embedding storage: none
2023-07-07 13:46:11,670 ----------------------------------------------------------------------------------------------------
2023-07-07 13:46:11,670 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_mini_batch_40_lr_7e-05_ger_test_as_dev"
2023-07-07 13:46:11,670 ----------------------------------------------------------------------------------------------------
2023-07-07 13:46:11,670 Removed gradient clipping
2023-07-07 13:46:11,670 ----------------------------------------------------------------------------------------------------
2023-07-07 13:46:11,670 ----------------------------------------------------------------------------------------------------
2023-07-07 13:46:56,799 epoch 1 - iter 37/375 - loss 3.31013363 - time (sec): 45.13 - samples/sec: 432.73 - lr: 0.000007 - momentum: 0.000000
2023-07-07 13:47:44,077 epoch 1 - iter 74/375 - loss 2.06357147 - time (sec): 92.41 - samples/sec: 432.00 - lr: 0.000014 - momentum: 0.000000
2023-07-07 13:48:29,941 epoch 1 - iter 111/375 - loss 1.56136392 - time (sec): 138.27 - samples/sec: 429.80 - lr: 0.000021 - momentum: 0.000000
2023-07-07 13:49:15,459 epoch 1 - iter 148/375 - loss 1.24496542 - time (sec): 183.79 - samples/sec: 432.68 - lr: 0.000027 - momentum: 0.000000
2023-07-07 13:50:00,293 epoch 1 - iter 185/375 - loss 1.03442194 - time (sec): 228.62 - samples/sec: 436.38 - lr: 0.000034 - momentum: 0.000000
2023-07-07 13:50:45,700 epoch 1 - iter 222/375 - loss 0.88105689 - time (sec): 274.03 - samples/sec: 439.87 - lr: 0.000041 - momentum: 0.000000
2023-07-07 13:51:33,025 epoch 1 - iter 259/375 - loss 0.77066929 - time (sec): 321.35 - samples/sec: 440.01 - lr: 0.000048 - momentum: 0.000000
2023-07-07 13:52:19,938 epoch 1 - iter 296/375 - loss 0.70509785 - time (sec): 368.27 - samples/sec: 438.09 - lr: 0.000055 - momentum: 0.000000
2023-07-07 13:53:05,369 epoch 1 - iter 333/375 - loss 0.64677384 - time (sec): 413.70 - samples/sec: 439.36 - lr: 0.000062 - momentum: 0.000000
2023-07-07 13:53:50,937 epoch 1 - iter 370/375 - loss 0.59438551 - time (sec): 459.27 - samples/sec: 440.45 - lr: 0.000069 - momentum: 0.000000
2023-07-07 13:53:56,892 ----------------------------------------------------------------------------------------------------
2023-07-07 13:53:56,892 EPOCH 1 done: loss 0.5894 - lr: 0.000069
2023-07-07 13:54:39,459 DEV : loss 0.21549728512763977 - f1-score (micro avg)  0.6353
2023-07-07 13:55:22,692 TEST : loss 0.14515544474124908 - f1-score (micro avg)  0.8647
2023-07-07 13:55:22,743 ----------------------------------------------------------------------------------------------------
2023-07-07 13:56:09,303 epoch 2 - iter 37/375 - loss 0.11540002 - time (sec): 46.56 - samples/sec: 433.18 - lr: 0.000069 - momentum: 0.000000
2023-07-07 13:56:55,812 epoch 2 - iter 74/375 - loss 0.10586987 - time (sec): 93.07 - samples/sec: 428.91 - lr: 0.000068 - momentum: 0.000000
2023-07-07 13:57:41,137 epoch 2 - iter 111/375 - loss 0.10081615 - time (sec): 138.39 - samples/sec: 436.68 - lr: 0.000068 - momentum: 0.000000
2023-07-07 13:58:27,195 epoch 2 - iter 148/375 - loss 0.09833771 - time (sec): 184.45 - samples/sec: 436.11 - lr: 0.000067 - momentum: 0.000000
2023-07-07 13:59:13,538 epoch 2 - iter 185/375 - loss 0.09673253 - time (sec): 230.79 - samples/sec: 438.89 - lr: 0.000066 - momentum: 0.000000
2023-07-07 13:59:58,166 epoch 2 - iter 222/375 - loss 0.09288097 - time (sec): 275.42 - samples/sec: 441.23 - lr: 0.000065 - momentum: 0.000000
2023-07-07 14:00:44,338 epoch 2 - iter 259/375 - loss 0.09010251 - time (sec): 321.59 - samples/sec: 439.06 - lr: 0.000065 - momentum: 0.000000
2023-07-07 14:01:30,565 epoch 2 - iter 296/375 - loss 0.08752298 - time (sec): 367.82 - samples/sec: 440.44 - lr: 0.000064 - momentum: 0.000000
2023-07-07 14:02:16,680 epoch 2 - iter 333/375 - loss 0.08488372 - time (sec): 413.94 - samples/sec: 440.03 - lr: 0.000063 - momentum: 0.000000
2023-07-07 14:03:03,197 epoch 2 - iter 370/375 - loss 0.08322716 - time (sec): 460.45 - samples/sec: 439.14 - lr: 0.000062 - momentum: 0.000000
2023-07-07 14:03:09,272 ----------------------------------------------------------------------------------------------------
2023-07-07 14:03:09,272 EPOCH 2 done: loss 0.0829 - lr: 0.000062
2023-07-07 14:03:55,396 DEV : loss 0.17363949120044708 - f1-score (micro avg)  0.7037
2023-07-07 14:04:37,108 TEST : loss 0.09545768797397614 - f1-score (micro avg)  0.9101
2023-07-07 14:04:37,152 ----------------------------------------------------------------------------------------------------
2023-07-07 14:05:23,195 epoch 3 - iter 37/375 - loss 0.04375772 - time (sec): 46.04 - samples/sec: 430.01 - lr: 0.000061 - momentum: 0.000000
2023-07-07 14:06:09,821 epoch 3 - iter 74/375 - loss 0.04432225 - time (sec): 92.67 - samples/sec: 437.31 - lr: 0.000061 - momentum: 0.000000
2023-07-07 14:06:54,929 epoch 3 - iter 111/375 - loss 0.04537247 - time (sec): 137.77 - samples/sec: 437.88 - lr: 0.000060 - momentum: 0.000000
2023-07-07 14:07:41,228 epoch 3 - iter 148/375 - loss 0.04720287 - time (sec): 184.07 - samples/sec: 441.90 - lr: 0.000059 - momentum: 0.000000
2023-07-07 14:08:28,521 epoch 3 - iter 185/375 - loss 0.05090216 - time (sec): 231.37 - samples/sec: 437.56 - lr: 0.000058 - momentum: 0.000000
2023-07-07 14:09:14,635 epoch 3 - iter 222/375 - loss 0.05340400 - time (sec): 277.48 - samples/sec: 438.22 - lr: 0.000058 - momentum: 0.000000
2023-07-07 14:09:59,554 epoch 3 - iter 259/375 - loss 0.05403794 - time (sec): 322.40 - samples/sec: 441.53 - lr: 0.000057 - momentum: 0.000000
2023-07-07 14:10:45,738 epoch 3 - iter 296/375 - loss 0.05370805 - time (sec): 368.58 - samples/sec: 441.57 - lr: 0.000056 - momentum: 0.000000
2023-07-07 14:11:31,669 epoch 3 - iter 333/375 - loss 0.05468448 - time (sec): 414.52 - samples/sec: 439.26 - lr: 0.000055 - momentum: 0.000000
2023-07-07 14:12:17,789 epoch 3 - iter 370/375 - loss 0.05423138 - time (sec): 460.64 - samples/sec: 438.19 - lr: 0.000055 - momentum: 0.000000
2023-07-07 14:12:23,465 ----------------------------------------------------------------------------------------------------
2023-07-07 14:12:23,466 EPOCH 3 done: loss 0.0540 - lr: 0.000055
2023-07-07 14:13:07,346 DEV : loss 0.1692422777414322 - f1-score (micro avg)  0.7103
2023-07-07 14:13:49,959 TEST : loss 0.10012013465166092 - f1-score (micro avg)  0.9048
2023-07-07 14:13:50,004 ----------------------------------------------------------------------------------------------------
2023-07-07 14:14:36,380 epoch 4 - iter 37/375 - loss 0.03663839 - time (sec): 46.37 - samples/sec: 439.42 - lr: 0.000054 - momentum: 0.000000
2023-07-07 14:15:22,594 epoch 4 - iter 74/375 - loss 0.03816390 - time (sec): 92.59 - samples/sec: 437.94 - lr: 0.000053 - momentum: 0.000000
2023-07-07 14:16:07,462 epoch 4 - iter 111/375 - loss 0.03769353 - time (sec): 137.46 - samples/sec: 439.56 - lr: 0.000052 - momentum: 0.000000
2023-07-07 14:16:54,349 epoch 4 - iter 148/375 - loss 0.03806996 - time (sec): 184.34 - samples/sec: 438.31 - lr: 0.000051 - momentum: 0.000000
2023-07-07 14:17:40,675 epoch 4 - iter 185/375 - loss 0.03777590 - time (sec): 230.67 - samples/sec: 439.02 - lr: 0.000051 - momentum: 0.000000
2023-07-07 14:18:25,707 epoch 4 - iter 222/375 - loss 0.03897186 - time (sec): 275.70 - samples/sec: 439.48 - lr: 0.000050 - momentum: 0.000000
2023-07-07 14:19:11,852 epoch 4 - iter 259/375 - loss 0.03786725 - time (sec): 321.85 - samples/sec: 439.71 - lr: 0.000049 - momentum: 0.000000
2023-07-07 14:19:56,362 epoch 4 - iter 296/375 - loss 0.03722997 - time (sec): 366.36 - samples/sec: 443.03 - lr: 0.000048 - momentum: 0.000000
2023-07-07 14:20:40,939 epoch 4 - iter 333/375 - loss 0.03751084 - time (sec): 410.93 - samples/sec: 444.49 - lr: 0.000048 - momentum: 0.000000
2023-07-07 14:21:26,982 epoch 4 - iter 370/375 - loss 0.03810408 - time (sec): 456.98 - samples/sec: 442.76 - lr: 0.000047 - momentum: 0.000000
2023-07-07 14:21:32,806 ----------------------------------------------------------------------------------------------------
2023-07-07 14:21:32,807 EPOCH 4 done: loss 0.0380 - lr: 0.000047
2023-07-07 14:22:16,835 DEV : loss 0.1923653781414032 - f1-score (micro avg)  0.6999
2023-07-07 14:22:57,774 TEST : loss 0.11539715528488159 - f1-score (micro avg)  0.9022
2023-07-07 14:22:57,829 ----------------------------------------------------------------------------------------------------
2023-07-07 14:23:42,456 epoch 5 - iter 37/375 - loss 0.03357384 - time (sec): 44.63 - samples/sec: 455.77 - lr: 0.000046 - momentum: 0.000000
2023-07-07 14:24:28,440 epoch 5 - iter 74/375 - loss 0.03383956 - time (sec): 90.61 - samples/sec: 440.00 - lr: 0.000045 - momentum: 0.000000
2023-07-07 14:25:14,555 epoch 5 - iter 111/375 - loss 0.03442367 - time (sec): 136.72 - samples/sec: 440.08 - lr: 0.000044 - momentum: 0.000000
2023-07-07 14:25:59,206 epoch 5 - iter 148/375 - loss 0.03299998 - time (sec): 181.37 - samples/sec: 447.18 - lr: 0.000044 - momentum: 0.000000
2023-07-07 14:26:45,224 epoch 5 - iter 185/375 - loss 0.03408681 - time (sec): 227.39 - samples/sec: 443.74 - lr: 0.000043 - momentum: 0.000000
2023-07-07 14:27:31,040 epoch 5 - iter 222/375 - loss 0.03469224 - time (sec): 273.21 - samples/sec: 443.72 - lr: 0.000042 - momentum: 0.000000
2023-07-07 14:28:16,117 epoch 5 - iter 259/375 - loss 0.03414546 - time (sec): 318.29 - samples/sec: 443.07 - lr: 0.000041 - momentum: 0.000000
2023-07-07 14:29:01,240 epoch 5 - iter 296/375 - loss 0.03375347 - time (sec): 363.41 - samples/sec: 442.80 - lr: 0.000041 - momentum: 0.000000
2023-07-07 14:29:47,569 epoch 5 - iter 333/375 - loss 0.03387111 - time (sec): 409.74 - samples/sec: 443.02 - lr: 0.000040 - momentum: 0.000000
2023-07-07 14:30:33,810 epoch 5 - iter 370/375 - loss 0.03367610 - time (sec): 455.98 - samples/sec: 443.03 - lr: 0.000039 - momentum: 0.000000
2023-07-07 14:30:39,834 ----------------------------------------------------------------------------------------------------
2023-07-07 14:30:39,834 EPOCH 5 done: loss 0.0335 - lr: 0.000039
2023-07-07 14:31:24,399 DEV : loss 0.17677944898605347 - f1-score (micro avg)  0.7222
2023-07-07 14:32:09,448 TEST : loss 0.0985705628991127 - f1-score (micro avg)  0.9136
2023-07-07 14:32:09,500 ----------------------------------------------------------------------------------------------------
2023-07-07 14:32:56,175 epoch 6 - iter 37/375 - loss 0.01943877 - time (sec): 46.67 - samples/sec: 438.60 - lr: 0.000038 - momentum: 0.000000
2023-07-07 14:33:42,322 epoch 6 - iter 74/375 - loss 0.02065806 - time (sec): 92.82 - samples/sec: 437.33 - lr: 0.000037 - momentum: 0.000000
2023-07-07 14:34:29,727 epoch 6 - iter 111/375 - loss 0.02189842 - time (sec): 140.23 - samples/sec: 431.48 - lr: 0.000037 - momentum: 0.000000
2023-07-07 14:35:15,799 epoch 6 - iter 148/375 - loss 0.02418428 - time (sec): 186.30 - samples/sec: 431.99 - lr: 0.000036 - momentum: 0.000000
2023-07-07 14:36:02,113 epoch 6 - iter 185/375 - loss 0.02561345 - time (sec): 232.61 - samples/sec: 434.12 - lr: 0.000035 - momentum: 0.000000
2023-07-07 14:36:48,409 epoch 6 - iter 222/375 - loss 0.03341691 - time (sec): 278.91 - samples/sec: 434.21 - lr: 0.000034 - momentum: 0.000000
2023-07-07 14:37:35,524 epoch 6 - iter 259/375 - loss 0.03522382 - time (sec): 326.02 - samples/sec: 432.80 - lr: 0.000034 - momentum: 0.000000
2023-07-07 14:38:23,099 epoch 6 - iter 296/375 - loss 0.03749253 - time (sec): 373.60 - samples/sec: 431.70 - lr: 0.000033 - momentum: 0.000000
2023-07-07 14:39:09,598 epoch 6 - iter 333/375 - loss 0.03821651 - time (sec): 420.10 - samples/sec: 431.66 - lr: 0.000032 - momentum: 0.000000
2023-07-07 14:39:55,977 epoch 6 - iter 370/375 - loss 0.03725971 - time (sec): 466.48 - samples/sec: 432.77 - lr: 0.000031 - momentum: 0.000000
2023-07-07 14:40:01,786 ----------------------------------------------------------------------------------------------------
2023-07-07 14:40:01,786 EPOCH 6 done: loss 0.0374 - lr: 0.000031
2023-07-07 14:40:44,771 DEV : loss 0.22272135317325592 - f1-score (micro avg)  0.6748
2023-07-07 14:41:27,616 TEST : loss 0.10076052695512772 - f1-score (micro avg)  0.921
2023-07-07 14:41:27,662 ----------------------------------------------------------------------------------------------------
2023-07-07 14:42:13,891 epoch 7 - iter 37/375 - loss 0.02229182 - time (sec): 46.23 - samples/sec: 451.86 - lr: 0.000030 - momentum: 0.000000
2023-07-07 14:42:59,738 epoch 7 - iter 74/375 - loss 0.02299417 - time (sec): 92.07 - samples/sec: 443.00 - lr: 0.000030 - momentum: 0.000000
2023-07-07 14:43:44,397 epoch 7 - iter 111/375 - loss 0.02304900 - time (sec): 136.73 - samples/sec: 442.21 - lr: 0.000029 - momentum: 0.000000
2023-07-07 14:44:29,794 epoch 7 - iter 148/375 - loss 0.02267023 - time (sec): 182.13 - samples/sec: 439.78 - lr: 0.000028 - momentum: 0.000000
2023-07-07 14:45:15,123 epoch 7 - iter 185/375 - loss 0.02379620 - time (sec): 227.46 - samples/sec: 441.21 - lr: 0.000027 - momentum: 0.000000
2023-07-07 14:45:59,028 epoch 7 - iter 222/375 - loss 0.02495488 - time (sec): 271.36 - samples/sec: 445.31 - lr: 0.000027 - momentum: 0.000000
2023-07-07 14:46:44,965 epoch 7 - iter 259/375 - loss 0.02511301 - time (sec): 317.30 - samples/sec: 444.75 - lr: 0.000026 - momentum: 0.000000
2023-07-07 14:47:29,588 epoch 7 - iter 296/375 - loss 0.02449127 - time (sec): 361.92 - samples/sec: 446.35 - lr: 0.000025 - momentum: 0.000000
2023-07-07 14:48:15,781 epoch 7 - iter 333/375 - loss 0.02388551 - time (sec): 408.12 - samples/sec: 444.35 - lr: 0.000024 - momentum: 0.000000
2023-07-07 14:49:02,173 epoch 7 - iter 370/375 - loss 0.02315564 - time (sec): 454.51 - samples/sec: 444.48 - lr: 0.000024 - momentum: 0.000000
2023-07-07 14:49:08,184 ----------------------------------------------------------------------------------------------------
2023-07-07 14:49:08,185 EPOCH 7 done: loss 0.0231 - lr: 0.000024
2023-07-07 14:49:50,993 DEV : loss 0.17698846757411957 - f1-score (micro avg)  0.7072
2023-07-07 14:50:33,729 TEST : loss 0.08954071998596191 - f1-score (micro avg)  0.9277
2023-07-07 14:50:33,774 ----------------------------------------------------------------------------------------------------
2023-07-07 14:51:19,909 epoch 8 - iter 37/375 - loss 0.01331538 - time (sec): 46.13 - samples/sec: 426.30 - lr: 0.000023 - momentum: 0.000000
2023-07-07 14:52:05,145 epoch 8 - iter 74/375 - loss 0.01371131 - time (sec): 91.37 - samples/sec: 429.89 - lr: 0.000022 - momentum: 0.000000
2023-07-07 14:52:49,760 epoch 8 - iter 111/375 - loss 0.01452876 - time (sec): 135.98 - samples/sec: 441.39 - lr: 0.000021 - momentum: 0.000000
2023-07-07 14:53:33,908 epoch 8 - iter 148/375 - loss 0.01439503 - time (sec): 180.13 - samples/sec: 449.14 - lr: 0.000020 - momentum: 0.000000
2023-07-07 14:54:17,096 epoch 8 - iter 185/375 - loss 0.01495367 - time (sec): 223.32 - samples/sec: 455.71 - lr: 0.000020 - momentum: 0.000000
2023-07-07 14:55:01,352 epoch 8 - iter 222/375 - loss 0.01507049 - time (sec): 267.58 - samples/sec: 454.85 - lr: 0.000019 - momentum: 0.000000
2023-07-07 14:55:46,012 epoch 8 - iter 259/375 - loss 0.01493177 - time (sec): 312.24 - samples/sec: 455.38 - lr: 0.000018 - momentum: 0.000000
2023-07-07 14:56:30,305 epoch 8 - iter 296/375 - loss 0.01442720 - time (sec): 356.53 - samples/sec: 452.91 - lr: 0.000017 - momentum: 0.000000
2023-07-07 14:57:14,876 epoch 8 - iter 333/375 - loss 0.01399819 - time (sec): 401.10 - samples/sec: 454.17 - lr: 0.000017 - momentum: 0.000000
2023-07-07 14:57:59,519 epoch 8 - iter 370/375 - loss 0.01389206 - time (sec): 445.74 - samples/sec: 453.23 - lr: 0.000016 - momentum: 0.000000
2023-07-07 14:58:05,194 ----------------------------------------------------------------------------------------------------
2023-07-07 14:58:05,195 EPOCH 8 done: loss 0.0138 - lr: 0.000016
2023-07-07 14:58:49,469 DEV : loss 0.17890845239162445 - f1-score (micro avg)  0.7276
2023-07-07 14:59:33,728 TEST : loss 0.0965430811047554 - f1-score (micro avg)  0.9319
2023-07-07 14:59:33,842 ----------------------------------------------------------------------------------------------------
2023-07-07 15:00:18,853 epoch 9 - iter 37/375 - loss 0.01107260 - time (sec): 45.01 - samples/sec: 455.89 - lr: 0.000015 - momentum: 0.000000
2023-07-07 15:01:03,273 epoch 9 - iter 74/375 - loss 0.00901610 - time (sec): 89.43 - samples/sec: 459.96 - lr: 0.000014 - momentum: 0.000000
2023-07-07 15:01:48,040 epoch 9 - iter 111/375 - loss 0.00921092 - time (sec): 134.19 - samples/sec: 458.01 - lr: 0.000013 - momentum: 0.000000
2023-07-07 15:02:33,249 epoch 9 - iter 148/375 - loss 0.00986642 - time (sec): 179.40 - samples/sec: 454.62 - lr: 0.000013 - momentum: 0.000000
2023-07-07 15:03:18,367 epoch 9 - iter 185/375 - loss 0.01001261 - time (sec): 224.52 - samples/sec: 453.65 - lr: 0.000012 - momentum: 0.000000
2023-07-07 15:04:04,579 epoch 9 - iter 222/375 - loss 0.00982109 - time (sec): 270.73 - samples/sec: 452.92 - lr: 0.000011 - momentum: 0.000000
2023-07-07 15:04:50,904 epoch 9 - iter 259/375 - loss 0.00968873 - time (sec): 317.06 - samples/sec: 449.61 - lr: 0.000010 - momentum: 0.000000
2023-07-07 15:05:36,692 epoch 9 - iter 296/375 - loss 0.00964742 - time (sec): 362.85 - samples/sec: 448.58 - lr: 0.000010 - momentum: 0.000000
2023-07-07 15:06:23,684 epoch 9 - iter 333/375 - loss 0.00955362 - time (sec): 409.84 - samples/sec: 444.34 - lr: 0.000009 - momentum: 0.000000
2023-07-07 15:07:10,262 epoch 9 - iter 370/375 - loss 0.00932200 - time (sec): 456.42 - samples/sec: 441.98 - lr: 0.000008 - momentum: 0.000000
2023-07-07 15:07:15,981 ----------------------------------------------------------------------------------------------------
2023-07-07 15:07:15,981 EPOCH 9 done: loss 0.0095 - lr: 0.000008
2023-07-07 15:07:59,732 DEV : loss 0.16901545226573944 - f1-score (micro avg)  0.7465
2023-07-07 15:08:41,159 TEST : loss 0.09519054740667343 - f1-score (micro avg)  0.9371
2023-07-07 15:08:41,204 ----------------------------------------------------------------------------------------------------
2023-07-07 15:09:27,619 epoch 10 - iter 37/375 - loss 0.00827411 - time (sec): 46.41 - samples/sec: 437.68 - lr: 0.000007 - momentum: 0.000000
2023-07-07 15:10:13,295 epoch 10 - iter 74/375 - loss 0.00807936 - time (sec): 92.09 - samples/sec: 433.03 - lr: 0.000006 - momentum: 0.000000
2023-07-07 15:10:59,983 epoch 10 - iter 111/375 - loss 0.00775974 - time (sec): 138.78 - samples/sec: 434.48 - lr: 0.000006 - momentum: 0.000000
2023-07-07 15:11:44,604 epoch 10 - iter 148/375 - loss 0.00743471 - time (sec): 183.40 - samples/sec: 441.35 - lr: 0.000005 - momentum: 0.000000
2023-07-07 15:12:29,588 epoch 10 - iter 185/375 - loss 0.00766399 - time (sec): 228.38 - samples/sec: 443.52 - lr: 0.000004 - momentum: 0.000000
2023-07-07 15:13:15,051 epoch 10 - iter 222/375 - loss 0.00710841 - time (sec): 273.85 - samples/sec: 443.25 - lr: 0.000003 - momentum: 0.000000
2023-07-07 15:13:59,857 epoch 10 - iter 259/375 - loss 0.00693546 - time (sec): 318.65 - samples/sec: 444.40 - lr: 0.000003 - momentum: 0.000000
2023-07-07 15:14:44,356 epoch 10 - iter 296/375 - loss 0.00700613 - time (sec): 363.15 - samples/sec: 446.28 - lr: 0.000002 - momentum: 0.000000
2023-07-07 15:15:29,529 epoch 10 - iter 333/375 - loss 0.00700210 - time (sec): 408.32 - samples/sec: 446.15 - lr: 0.000001 - momentum: 0.000000
2023-07-07 15:16:15,410 epoch 10 - iter 370/375 - loss 0.00704871 - time (sec): 454.20 - samples/sec: 444.52 - lr: 0.000000 - momentum: 0.000000
2023-07-07 15:16:21,239 ----------------------------------------------------------------------------------------------------
2023-07-07 15:16:21,239 EPOCH 10 done: loss 0.0070 - lr: 0.000000
2023-07-07 15:17:05,358 DEV : loss 0.18972334265708923 - f1-score (micro avg)  0.7334
2023-07-07 15:17:46,670 TEST : loss 0.09988513588905334 - f1-score (micro avg)  0.9358
2023-07-07 15:17:58,480 ----------------------------------------------------------------------------------------------------
2023-07-07 15:17:58,487 Testing using last state of model ...
2023-07-07 15:18:39,454 
Results:
- F-score (micro) 0.9358
- F-score (macro) 0.9215
- Accuracy 0.9039

By class:
              precision    recall  f1-score   support

         ORG     0.9144    0.9386    0.9263      1661
         LOC     0.9460    0.9460    0.9460      1668
         PER     0.9814    0.9808    0.9811      1617
        MISC     0.8128    0.8533    0.8325       702

   micro avg     0.9294    0.9423    0.9358      5648
   macro avg     0.9137    0.9297    0.9215      5648
weighted avg     0.9303    0.9423    0.9362      5648

2023-07-07 15:18:39,454 ----------------------------------------------------------------------------------------------------
