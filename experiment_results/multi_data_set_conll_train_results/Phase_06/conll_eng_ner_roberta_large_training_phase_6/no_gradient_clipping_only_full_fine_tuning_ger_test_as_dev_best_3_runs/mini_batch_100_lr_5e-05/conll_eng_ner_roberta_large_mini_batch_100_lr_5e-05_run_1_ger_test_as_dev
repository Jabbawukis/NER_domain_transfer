2023-07-12 18:55:52,682 ----------------------------------------------------------------------------------------------------
2023-07-12 18:55:52,683 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): XLMRobertaEmbeddings(
        (word_embeddings): Embedding(250003, 1024)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): XLMRobertaEncoder(
        (layer): ModuleList(
          (0-23): 24 x XLMRobertaLayer(
            (attention): XLMRobertaAttention(
              (self): XLMRobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): XLMRobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): XLMRobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): XLMRobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): XLMRobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-07-12 18:55:52,683 ----------------------------------------------------------------------------------------------------
2023-07-12 18:55:52,683 Corpus: 14987 train + 3160 dev + 3684 test sentences
2023-07-12 18:55:52,683 ----------------------------------------------------------------------------------------------------
2023-07-12 18:55:52,683 Train:  14987 sentences
2023-07-12 18:55:52,683         (train_with_dev=False, train_with_test=False)
2023-07-12 18:55:52,683 ----------------------------------------------------------------------------------------------------
2023-07-12 18:55:52,683 Training Params:
2023-07-12 18:55:52,683  - learning_rate: "5e-05" 
2023-07-12 18:55:52,684  - mini_batch_size: "100"
2023-07-12 18:55:52,684  - max_epochs: "10"
2023-07-12 18:55:52,684  - shuffle: "True"
2023-07-12 18:55:52,684 ----------------------------------------------------------------------------------------------------
2023-07-12 18:55:52,684 Plugins:
2023-07-12 18:55:52,684  - LinearScheduler | warmup_fraction: '0.1'
2023-07-12 18:55:52,684 ----------------------------------------------------------------------------------------------------
2023-07-12 18:55:52,684 Final evaluation on model after last epoch (final-model.pt)
2023-07-12 18:55:52,684  - metric: "('micro avg', 'f1-score')"
2023-07-12 18:55:52,684 ----------------------------------------------------------------------------------------------------
2023-07-12 18:55:52,684 Computation:
2023-07-12 18:55:52,684  - compute on device: cuda:3
2023-07-12 18:55:52,684  - embedding storage: none
2023-07-12 18:55:52,684 ----------------------------------------------------------------------------------------------------
2023-07-12 18:55:52,684 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_mini_batch_100_lr_5e-05_run_1_ger_test_as_dev"
2023-07-12 18:55:52,684 ----------------------------------------------------------------------------------------------------
2023-07-12 18:55:52,684 Removed gradient clipping
2023-07-12 18:55:52,684 ----------------------------------------------------------------------------------------------------
2023-07-12 18:55:52,684 ----------------------------------------------------------------------------------------------------
2023-07-12 18:57:06,503 epoch 1 - iter 15/150 - loss 2.77266417 - time (sec): 73.82 - samples/sec: 284.76 - lr: 0.000005 - momentum: 0.000000
2023-07-12 18:57:54,394 epoch 1 - iter 30/150 - loss 2.12220588 - time (sec): 121.71 - samples/sec: 346.98 - lr: 0.000010 - momentum: 0.000000
2023-07-12 18:58:40,494 epoch 1 - iter 45/150 - loss 1.68825475 - time (sec): 167.81 - samples/sec: 372.85 - lr: 0.000015 - momentum: 0.000000
2023-07-12 18:59:26,522 epoch 1 - iter 60/150 - loss 1.42054186 - time (sec): 213.84 - samples/sec: 389.06 - lr: 0.000020 - momentum: 0.000000
2023-07-12 19:00:12,368 epoch 1 - iter 75/150 - loss 1.23295068 - time (sec): 259.68 - samples/sec: 402.49 - lr: 0.000025 - momentum: 0.000000
2023-07-12 19:00:58,486 epoch 1 - iter 90/150 - loss 1.10249815 - time (sec): 305.80 - samples/sec: 407.70 - lr: 0.000030 - momentum: 0.000000
2023-07-12 19:01:44,782 epoch 1 - iter 105/150 - loss 0.99518022 - time (sec): 352.10 - samples/sec: 409.97 - lr: 0.000035 - momentum: 0.000000
2023-07-12 19:02:29,388 epoch 1 - iter 120/150 - loss 0.89833544 - time (sec): 396.70 - samples/sec: 415.73 - lr: 0.000040 - momentum: 0.000000
2023-07-12 19:03:12,592 epoch 1 - iter 135/150 - loss 0.81755435 - time (sec): 439.91 - samples/sec: 420.21 - lr: 0.000045 - momentum: 0.000000
2023-07-12 19:03:58,134 epoch 1 - iter 150/150 - loss 0.75245917 - time (sec): 485.45 - samples/sec: 421.40 - lr: 0.000050 - momentum: 0.000000
2023-07-12 19:03:58,135 ----------------------------------------------------------------------------------------------------
2023-07-12 19:03:58,135 EPOCH 1 done: loss 0.7525 - lr: 0.000050
2023-07-12 19:04:40,390 DEV : loss 0.1882641613483429 - f1-score (micro avg)  0.6846
2023-07-12 19:05:22,411 TEST : loss 0.11810769885778427 - f1-score (micro avg)  0.8886
2023-07-12 19:05:22,462 ----------------------------------------------------------------------------------------------------
2023-07-12 19:06:08,526 epoch 2 - iter 15/150 - loss 0.10350344 - time (sec): 46.06 - samples/sec: 438.81 - lr: 0.000049 - momentum: 0.000000
2023-07-12 19:06:52,534 epoch 2 - iter 30/150 - loss 0.09682016 - time (sec): 90.07 - samples/sec: 457.83 - lr: 0.000049 - momentum: 0.000000
2023-07-12 19:07:57,782 epoch 2 - iter 45/150 - loss 0.09718792 - time (sec): 155.32 - samples/sec: 396.19 - lr: 0.000048 - momentum: 0.000000
2023-07-12 19:09:15,073 epoch 2 - iter 60/150 - loss 0.09370943 - time (sec): 232.61 - samples/sec: 354.70 - lr: 0.000048 - momentum: 0.000000
2023-07-12 19:10:03,743 epoch 2 - iter 75/150 - loss 0.09482551 - time (sec): 281.28 - samples/sec: 364.08 - lr: 0.000047 - momentum: 0.000000
2023-07-12 19:10:51,143 epoch 2 - iter 90/150 - loss 0.09365632 - time (sec): 328.68 - samples/sec: 375.37 - lr: 0.000047 - momentum: 0.000000
2023-07-12 19:11:37,489 epoch 2 - iter 105/150 - loss 0.09287746 - time (sec): 375.03 - samples/sec: 382.00 - lr: 0.000046 - momentum: 0.000000
2023-07-12 19:12:24,503 epoch 2 - iter 120/150 - loss 0.09640795 - time (sec): 422.04 - samples/sec: 387.55 - lr: 0.000046 - momentum: 0.000000
2023-07-12 19:13:11,086 epoch 2 - iter 135/150 - loss 0.09570921 - time (sec): 468.62 - samples/sec: 391.68 - lr: 0.000045 - momentum: 0.000000
2023-07-12 19:13:57,579 epoch 2 - iter 150/150 - loss 0.09764917 - time (sec): 515.12 - samples/sec: 397.13 - lr: 0.000045 - momentum: 0.000000
2023-07-12 19:13:57,580 ----------------------------------------------------------------------------------------------------
2023-07-12 19:13:57,580 EPOCH 2 done: loss 0.0976 - lr: 0.000045
2023-07-12 19:14:41,067 DEV : loss 0.16358622908592224 - f1-score (micro avg)  0.7128
2023-07-12 19:15:24,197 TEST : loss 0.11569102853536606 - f1-score (micro avg)  0.8847
2023-07-12 19:15:24,247 ----------------------------------------------------------------------------------------------------
2023-07-12 19:16:10,369 epoch 3 - iter 15/150 - loss 0.08872827 - time (sec): 46.12 - samples/sec: 445.16 - lr: 0.000044 - momentum: 0.000000
2023-07-12 19:16:56,631 epoch 3 - iter 30/150 - loss 0.07717255 - time (sec): 92.38 - samples/sec: 443.94 - lr: 0.000043 - momentum: 0.000000
2023-07-12 19:17:43,682 epoch 3 - iter 45/150 - loss 0.07058907 - time (sec): 139.43 - samples/sec: 444.49 - lr: 0.000043 - momentum: 0.000000
2023-07-12 19:18:29,077 epoch 3 - iter 60/150 - loss 0.06717508 - time (sec): 184.83 - samples/sec: 447.64 - lr: 0.000042 - momentum: 0.000000
2023-07-12 19:19:13,084 epoch 3 - iter 75/150 - loss 0.06260169 - time (sec): 228.84 - samples/sec: 448.76 - lr: 0.000042 - momentum: 0.000000
2023-07-12 19:20:18,605 epoch 3 - iter 90/150 - loss 0.06047452 - time (sec): 294.36 - samples/sec: 418.52 - lr: 0.000041 - momentum: 0.000000
2023-07-12 19:21:32,033 epoch 3 - iter 105/150 - loss 0.05815131 - time (sec): 367.78 - samples/sec: 389.58 - lr: 0.000041 - momentum: 0.000000
2023-07-12 19:22:21,572 epoch 3 - iter 120/150 - loss 0.05652794 - time (sec): 417.32 - samples/sec: 391.90 - lr: 0.000040 - momentum: 0.000000
2023-07-12 19:23:08,572 epoch 3 - iter 135/150 - loss 0.05455950 - time (sec): 464.32 - samples/sec: 396.42 - lr: 0.000040 - momentum: 0.000000
2023-07-12 19:23:54,812 epoch 3 - iter 150/150 - loss 0.05233198 - time (sec): 510.56 - samples/sec: 400.67 - lr: 0.000039 - momentum: 0.000000
2023-07-12 19:23:54,812 ----------------------------------------------------------------------------------------------------
2023-07-12 19:23:54,812 EPOCH 3 done: loss 0.0523 - lr: 0.000039
2023-07-12 19:24:38,284 DEV : loss 0.17640994489192963 - f1-score (micro avg)  0.7168
2023-07-12 19:25:19,522 TEST : loss 0.08646062761545181 - f1-score (micro avg)  0.9287
2023-07-12 19:25:19,571 ----------------------------------------------------------------------------------------------------
2023-07-12 19:26:05,603 epoch 4 - iter 15/150 - loss 0.03647916 - time (sec): 46.03 - samples/sec: 413.75 - lr: 0.000038 - momentum: 0.000000
2023-07-12 19:26:51,225 epoch 4 - iter 30/150 - loss 0.03600641 - time (sec): 91.65 - samples/sec: 435.92 - lr: 0.000038 - momentum: 0.000000
2023-07-12 19:27:36,328 epoch 4 - iter 45/150 - loss 0.03534428 - time (sec): 136.75 - samples/sec: 440.96 - lr: 0.000037 - momentum: 0.000000
2023-07-12 19:28:20,823 epoch 4 - iter 60/150 - loss 0.03483171 - time (sec): 181.25 - samples/sec: 443.86 - lr: 0.000037 - momentum: 0.000000
2023-07-12 19:29:06,904 epoch 4 - iter 75/150 - loss 0.03280589 - time (sec): 227.33 - samples/sec: 443.72 - lr: 0.000036 - momentum: 0.000000
2023-07-12 19:29:53,256 epoch 4 - iter 90/150 - loss 0.03224395 - time (sec): 273.68 - samples/sec: 442.93 - lr: 0.000036 - momentum: 0.000000
2023-07-12 19:30:39,502 epoch 4 - iter 105/150 - loss 0.03135957 - time (sec): 319.93 - samples/sec: 444.79 - lr: 0.000035 - momentum: 0.000000
2023-07-12 19:31:25,310 epoch 4 - iter 120/150 - loss 0.03101377 - time (sec): 365.74 - samples/sec: 444.78 - lr: 0.000035 - momentum: 0.000000
2023-07-12 19:32:08,197 epoch 4 - iter 135/150 - loss 0.03055203 - time (sec): 408.62 - samples/sec: 450.08 - lr: 0.000034 - momentum: 0.000000
2023-07-12 19:33:15,278 epoch 4 - iter 150/150 - loss 0.03033525 - time (sec): 475.70 - samples/sec: 430.03 - lr: 0.000033 - momentum: 0.000000
2023-07-12 19:33:15,278 ----------------------------------------------------------------------------------------------------
2023-07-12 19:33:15,279 EPOCH 4 done: loss 0.0303 - lr: 0.000033
2023-07-12 19:34:27,770 DEV : loss 0.1551263928413391 - f1-score (micro avg)  0.7356
2023-07-12 19:35:11,410 TEST : loss 0.08469536900520325 - f1-score (micro avg)  0.9315
2023-07-12 19:35:11,459 ----------------------------------------------------------------------------------------------------
2023-07-12 19:35:58,940 epoch 5 - iter 15/150 - loss 0.02154571 - time (sec): 47.48 - samples/sec: 424.16 - lr: 0.000033 - momentum: 0.000000
2023-07-12 19:36:45,644 epoch 5 - iter 30/150 - loss 0.02233035 - time (sec): 94.18 - samples/sec: 432.84 - lr: 0.000032 - momentum: 0.000000
2023-07-12 19:37:32,670 epoch 5 - iter 45/150 - loss 0.02325862 - time (sec): 141.21 - samples/sec: 437.64 - lr: 0.000032 - momentum: 0.000000
2023-07-12 19:38:18,801 epoch 5 - iter 60/150 - loss 0.02175256 - time (sec): 187.34 - samples/sec: 436.36 - lr: 0.000031 - momentum: 0.000000
2023-07-12 19:39:04,888 epoch 5 - iter 75/150 - loss 0.02135134 - time (sec): 233.43 - samples/sec: 437.52 - lr: 0.000031 - momentum: 0.000000
2023-07-12 19:39:49,941 epoch 5 - iter 90/150 - loss 0.02232900 - time (sec): 278.48 - samples/sec: 440.78 - lr: 0.000030 - momentum: 0.000000
2023-07-12 19:40:33,209 epoch 5 - iter 105/150 - loss 0.02339752 - time (sec): 321.75 - samples/sec: 444.71 - lr: 0.000030 - momentum: 0.000000
2023-07-12 19:41:20,149 epoch 5 - iter 120/150 - loss 0.02331990 - time (sec): 368.69 - samples/sec: 444.65 - lr: 0.000029 - momentum: 0.000000
2023-07-12 19:42:06,553 epoch 5 - iter 135/150 - loss 0.02303217 - time (sec): 415.09 - samples/sec: 443.79 - lr: 0.000028 - momentum: 0.000000
2023-07-12 19:42:52,699 epoch 5 - iter 150/150 - loss 0.02362583 - time (sec): 461.24 - samples/sec: 443.52 - lr: 0.000028 - momentum: 0.000000
2023-07-12 19:42:52,700 ----------------------------------------------------------------------------------------------------
2023-07-12 19:42:52,700 EPOCH 5 done: loss 0.0236 - lr: 0.000028
2023-07-12 19:43:34,962 DEV : loss 0.15010899305343628 - f1-score (micro avg)  0.7594
2023-07-12 19:44:20,983 TEST : loss 0.08505614101886749 - f1-score (micro avg)  0.9333
2023-07-12 19:44:21,042 ----------------------------------------------------------------------------------------------------
2023-07-12 19:45:31,646 epoch 6 - iter 15/150 - loss 0.01918323 - time (sec): 70.60 - samples/sec: 291.08 - lr: 0.000027 - momentum: 0.000000
2023-07-12 19:46:44,595 epoch 6 - iter 30/150 - loss 0.01910861 - time (sec): 143.55 - samples/sec: 289.68 - lr: 0.000027 - momentum: 0.000000
2023-07-12 19:47:31,582 epoch 6 - iter 45/150 - loss 0.01882637 - time (sec): 190.54 - samples/sec: 325.74 - lr: 0.000026 - momentum: 0.000000
2023-07-12 19:48:18,638 epoch 6 - iter 60/150 - loss 0.01918545 - time (sec): 237.59 - samples/sec: 345.93 - lr: 0.000026 - momentum: 0.000000
2023-07-12 19:49:05,526 epoch 6 - iter 75/150 - loss 0.01948547 - time (sec): 284.48 - samples/sec: 360.02 - lr: 0.000025 - momentum: 0.000000
2023-07-12 19:49:52,597 epoch 6 - iter 90/150 - loss 0.01876848 - time (sec): 331.55 - samples/sec: 369.83 - lr: 0.000025 - momentum: 0.000000
2023-07-12 19:50:39,636 epoch 6 - iter 105/150 - loss 0.01960619 - time (sec): 378.59 - samples/sec: 379.45 - lr: 0.000024 - momentum: 0.000000
2023-07-12 19:51:25,909 epoch 6 - iter 120/150 - loss 0.01997765 - time (sec): 424.86 - samples/sec: 386.44 - lr: 0.000024 - momentum: 0.000000
2023-07-12 19:52:11,401 epoch 6 - iter 135/150 - loss 0.02017537 - time (sec): 470.36 - samples/sec: 393.78 - lr: 0.000023 - momentum: 0.000000
2023-07-12 19:52:56,374 epoch 6 - iter 150/150 - loss 0.02019085 - time (sec): 515.33 - samples/sec: 396.96 - lr: 0.000022 - momentum: 0.000000
2023-07-12 19:52:56,375 ----------------------------------------------------------------------------------------------------
2023-07-12 19:52:56,375 EPOCH 6 done: loss 0.0202 - lr: 0.000022
2023-07-12 19:53:39,123 DEV : loss 0.17116737365722656 - f1-score (micro avg)  0.7458
2023-07-12 19:54:21,384 TEST : loss 0.08644009381532669 - f1-score (micro avg)  0.9348
2023-07-12 19:54:21,435 ----------------------------------------------------------------------------------------------------
2023-07-12 19:55:07,175 epoch 7 - iter 15/150 - loss 0.01609250 - time (sec): 45.74 - samples/sec: 456.03 - lr: 0.000022 - momentum: 0.000000
2023-07-12 19:55:52,702 epoch 7 - iter 30/150 - loss 0.01650288 - time (sec): 91.26 - samples/sec: 455.68 - lr: 0.000021 - momentum: 0.000000
2023-07-12 19:56:38,034 epoch 7 - iter 45/150 - loss 0.01598024 - time (sec): 136.60 - samples/sec: 459.69 - lr: 0.000021 - momentum: 0.000000
2023-07-12 19:57:37,403 epoch 7 - iter 60/150 - loss 0.01582289 - time (sec): 195.97 - samples/sec: 424.30 - lr: 0.000020 - momentum: 0.000000
2023-07-12 19:58:52,457 epoch 7 - iter 75/150 - loss 0.01611415 - time (sec): 271.02 - samples/sec: 383.37 - lr: 0.000020 - momentum: 0.000000
2023-07-12 19:59:46,430 epoch 7 - iter 90/150 - loss 0.01645007 - time (sec): 324.99 - samples/sec: 380.33 - lr: 0.000019 - momentum: 0.000000
2023-07-12 20:00:33,337 epoch 7 - iter 105/150 - loss 0.01581473 - time (sec): 371.90 - samples/sec: 388.23 - lr: 0.000019 - momentum: 0.000000
2023-07-12 20:01:20,001 epoch 7 - iter 120/150 - loss 0.01524629 - time (sec): 418.56 - samples/sec: 394.23 - lr: 0.000018 - momentum: 0.000000
2023-07-12 20:02:06,439 epoch 7 - iter 135/150 - loss 0.01532269 - time (sec): 465.00 - samples/sec: 397.09 - lr: 0.000017 - momentum: 0.000000
2023-07-12 20:02:52,421 epoch 7 - iter 150/150 - loss 0.01542511 - time (sec): 510.98 - samples/sec: 400.34 - lr: 0.000017 - momentum: 0.000000
2023-07-12 20:02:52,422 ----------------------------------------------------------------------------------------------------
2023-07-12 20:02:52,422 EPOCH 7 done: loss 0.0154 - lr: 0.000017
2023-07-12 20:03:34,511 DEV : loss 0.16920511424541473 - f1-score (micro avg)  0.7403
2023-07-12 20:04:15,511 TEST : loss 0.08995787054300308 - f1-score (micro avg)  0.9308
2023-07-12 20:04:15,562 ----------------------------------------------------------------------------------------------------
2023-07-12 20:05:00,094 epoch 8 - iter 15/150 - loss 0.02243423 - time (sec): 44.53 - samples/sec: 466.28 - lr: 0.000016 - momentum: 0.000000
2023-07-12 20:05:46,470 epoch 8 - iter 30/150 - loss 0.02091874 - time (sec): 90.91 - samples/sec: 449.08 - lr: 0.000016 - momentum: 0.000000
2023-07-12 20:06:32,914 epoch 8 - iter 45/150 - loss 0.02037674 - time (sec): 137.35 - samples/sec: 442.13 - lr: 0.000015 - momentum: 0.000000
2023-07-12 20:07:19,301 epoch 8 - iter 60/150 - loss 0.02442492 - time (sec): 183.74 - samples/sec: 445.10 - lr: 0.000015 - momentum: 0.000000
2023-07-12 20:08:05,198 epoch 8 - iter 75/150 - loss 0.02508692 - time (sec): 229.63 - samples/sec: 442.26 - lr: 0.000014 - momentum: 0.000000
2023-07-12 20:08:51,030 epoch 8 - iter 90/150 - loss 0.02441109 - time (sec): 275.47 - samples/sec: 443.27 - lr: 0.000014 - momentum: 0.000000
2023-07-12 20:09:45,737 epoch 8 - iter 105/150 - loss 0.02327391 - time (sec): 330.17 - samples/sec: 432.35 - lr: 0.000013 - momentum: 0.000000
2023-07-12 20:11:00,496 epoch 8 - iter 120/150 - loss 0.02305003 - time (sec): 404.93 - samples/sec: 404.81 - lr: 0.000012 - momentum: 0.000000
2023-07-12 20:11:57,385 epoch 8 - iter 135/150 - loss 0.02232121 - time (sec): 461.82 - samples/sec: 398.51 - lr: 0.000012 - momentum: 0.000000
2023-07-12 20:12:43,536 epoch 8 - iter 150/150 - loss 0.02154464 - time (sec): 507.97 - samples/sec: 402.71 - lr: 0.000011 - momentum: 0.000000
2023-07-12 20:12:43,537 ----------------------------------------------------------------------------------------------------
2023-07-12 20:12:43,537 EPOCH 8 done: loss 0.0215 - lr: 0.000011
2023-07-12 20:13:25,831 DEV : loss 0.17508898675441742 - f1-score (micro avg)  0.7459
2023-07-12 20:14:06,793 TEST : loss 0.09547045081853867 - f1-score (micro avg)  0.9303
2023-07-12 20:14:06,843 ----------------------------------------------------------------------------------------------------
2023-07-12 20:14:53,365 epoch 9 - iter 15/150 - loss 0.01735900 - time (sec): 46.52 - samples/sec: 438.00 - lr: 0.000011 - momentum: 0.000000
2023-07-12 20:15:38,577 epoch 9 - iter 30/150 - loss 0.01428323 - time (sec): 91.73 - samples/sec: 438.29 - lr: 0.000010 - momentum: 0.000000
2023-07-12 20:16:23,858 epoch 9 - iter 45/150 - loss 0.01425172 - time (sec): 137.01 - samples/sec: 438.65 - lr: 0.000010 - momentum: 0.000000
2023-07-12 20:17:08,319 epoch 9 - iter 60/150 - loss 0.01395615 - time (sec): 181.47 - samples/sec: 446.61 - lr: 0.000009 - momentum: 0.000000
2023-07-12 20:17:54,056 epoch 9 - iter 75/150 - loss 0.01316098 - time (sec): 227.21 - samples/sec: 448.04 - lr: 0.000009 - momentum: 0.000000
2023-07-12 20:18:40,982 epoch 9 - iter 90/150 - loss 0.01344559 - time (sec): 274.14 - samples/sec: 445.30 - lr: 0.000008 - momentum: 0.000000
2023-07-12 20:19:27,171 epoch 9 - iter 105/150 - loss 0.01306386 - time (sec): 320.33 - samples/sec: 445.70 - lr: 0.000008 - momentum: 0.000000
2023-07-12 20:20:12,946 epoch 9 - iter 120/150 - loss 0.01305080 - time (sec): 366.10 - samples/sec: 445.71 - lr: 0.000007 - momentum: 0.000000
2023-07-12 20:20:58,200 epoch 9 - iter 135/150 - loss 0.01283169 - time (sec): 411.36 - samples/sec: 446.86 - lr: 0.000006 - momentum: 0.000000
2023-07-12 20:21:50,342 epoch 9 - iter 150/150 - loss 0.01255188 - time (sec): 463.50 - samples/sec: 441.36 - lr: 0.000006 - momentum: 0.000000
2023-07-12 20:21:50,342 ----------------------------------------------------------------------------------------------------
2023-07-12 20:21:50,342 EPOCH 9 done: loss 0.0126 - lr: 0.000006
2023-07-12 20:23:01,153 DEV : loss 0.16467557847499847 - f1-score (micro avg)  0.7597
2023-07-12 20:24:01,481 TEST : loss 0.09482766687870026 - f1-score (micro avg)  0.9349
2023-07-12 20:24:01,531 ----------------------------------------------------------------------------------------------------
2023-07-12 20:24:48,813 epoch 10 - iter 15/150 - loss 0.00895718 - time (sec): 47.28 - samples/sec: 438.96 - lr: 0.000005 - momentum: 0.000000
2023-07-12 20:25:35,316 epoch 10 - iter 30/150 - loss 0.01024295 - time (sec): 93.78 - samples/sec: 443.16 - lr: 0.000005 - momentum: 0.000000
2023-07-12 20:26:21,180 epoch 10 - iter 45/150 - loss 0.00975582 - time (sec): 139.65 - samples/sec: 448.27 - lr: 0.000004 - momentum: 0.000000
2023-07-12 20:27:07,575 epoch 10 - iter 60/150 - loss 0.00946159 - time (sec): 186.04 - samples/sec: 445.91 - lr: 0.000004 - momentum: 0.000000
2023-07-12 20:27:53,570 epoch 10 - iter 75/150 - loss 0.00951683 - time (sec): 232.04 - samples/sec: 445.13 - lr: 0.000003 - momentum: 0.000000
2023-07-12 20:28:39,863 epoch 10 - iter 90/150 - loss 0.00954130 - time (sec): 278.33 - samples/sec: 445.64 - lr: 0.000003 - momentum: 0.000000
2023-07-12 20:29:23,576 epoch 10 - iter 105/150 - loss 0.00990959 - time (sec): 322.04 - samples/sec: 449.15 - lr: 0.000002 - momentum: 0.000000
2023-07-12 20:30:07,646 epoch 10 - iter 120/150 - loss 0.00975290 - time (sec): 366.11 - samples/sec: 450.14 - lr: 0.000001 - momentum: 0.000000
2023-07-12 20:30:52,588 epoch 10 - iter 135/150 - loss 0.00991438 - time (sec): 411.06 - samples/sec: 449.31 - lr: 0.000001 - momentum: 0.000000
2023-07-12 20:31:38,994 epoch 10 - iter 150/150 - loss 0.00975396 - time (sec): 457.46 - samples/sec: 447.18 - lr: 0.000000 - momentum: 0.000000
2023-07-12 20:31:38,994 ----------------------------------------------------------------------------------------------------
2023-07-12 20:31:38,994 EPOCH 10 done: loss 0.0098 - lr: 0.000000
2023-07-12 20:32:22,819 DEV : loss 0.16846109926700592 - f1-score (micro avg)  0.7555
2023-07-12 20:33:04,048 TEST : loss 0.09543007612228394 - f1-score (micro avg)  0.934
2023-07-12 20:33:15,888 ----------------------------------------------------------------------------------------------------
2023-07-12 20:33:15,890 Testing using last state of model ...
2023-07-12 20:34:02,829 
Results:
- F-score (micro) 0.934
- F-score (macro) 0.9189
- Accuracy 0.9053

By class:
              precision    recall  f1-score   support

         ORG     0.9171    0.9326    0.9248      1661
         LOC     0.9430    0.9430    0.9430      1668
         PER     0.9857    0.9808    0.9833      1617
        MISC     0.8003    0.8504    0.8246       702

   micro avg     0.9287    0.9393    0.9340      5648
   macro avg     0.9115    0.9267    0.9189      5648
weighted avg     0.9299    0.9393    0.9345      5648

2023-07-12 20:34:02,829 ----------------------------------------------------------------------------------------------------
