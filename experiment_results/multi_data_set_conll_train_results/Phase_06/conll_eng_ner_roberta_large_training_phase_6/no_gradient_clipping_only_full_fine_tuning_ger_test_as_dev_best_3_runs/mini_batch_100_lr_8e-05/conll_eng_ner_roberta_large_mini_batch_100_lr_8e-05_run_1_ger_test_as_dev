2023-07-12 04:34:04,664 ----------------------------------------------------------------------------------------------------
2023-07-12 04:34:04,666 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): XLMRobertaEmbeddings(
        (word_embeddings): Embedding(250003, 1024)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): XLMRobertaEncoder(
        (layer): ModuleList(
          (0-23): 24 x XLMRobertaLayer(
            (attention): XLMRobertaAttention(
              (self): XLMRobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): XLMRobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): XLMRobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): XLMRobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): XLMRobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-07-12 04:34:04,666 ----------------------------------------------------------------------------------------------------
2023-07-12 04:34:04,666 Corpus: 14987 train + 3160 dev + 3684 test sentences
2023-07-12 04:34:04,666 ----------------------------------------------------------------------------------------------------
2023-07-12 04:34:04,666 Train:  14987 sentences
2023-07-12 04:34:04,666         (train_with_dev=False, train_with_test=False)
2023-07-12 04:34:04,666 ----------------------------------------------------------------------------------------------------
2023-07-12 04:34:04,666 Training Params:
2023-07-12 04:34:04,666  - learning_rate: "8e-05" 
2023-07-12 04:34:04,666  - mini_batch_size: "100"
2023-07-12 04:34:04,666  - max_epochs: "10"
2023-07-12 04:34:04,666  - shuffle: "True"
2023-07-12 04:34:04,666 ----------------------------------------------------------------------------------------------------
2023-07-12 04:34:04,666 Plugins:
2023-07-12 04:34:04,666  - LinearScheduler | warmup_fraction: '0.1'
2023-07-12 04:34:04,666 ----------------------------------------------------------------------------------------------------
2023-07-12 04:34:04,666 Final evaluation on model after last epoch (final-model.pt)
2023-07-12 04:34:04,666  - metric: "('micro avg', 'f1-score')"
2023-07-12 04:34:04,666 ----------------------------------------------------------------------------------------------------
2023-07-12 04:34:04,666 Computation:
2023-07-12 04:34:04,666  - compute on device: cuda:3
2023-07-12 04:34:04,666  - embedding storage: none
2023-07-12 04:34:04,666 ----------------------------------------------------------------------------------------------------
2023-07-12 04:34:04,666 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_mini_batch_100_lr_8e-05_run_1_ger_test_as_dev"
2023-07-12 04:34:04,666 ----------------------------------------------------------------------------------------------------
2023-07-12 04:34:04,667 Removed gradient clipping
2023-07-12 04:34:04,667 ----------------------------------------------------------------------------------------------------
2023-07-12 04:34:04,667 ----------------------------------------------------------------------------------------------------
2023-07-12 04:34:48,920 epoch 1 - iter 15/150 - loss 3.54581626 - time (sec): 44.25 - samples/sec: 465.58 - lr: 0.000007 - momentum: 0.000000
2023-07-12 04:35:34,260 epoch 1 - iter 30/150 - loss 2.52413341 - time (sec): 89.59 - samples/sec: 464.28 - lr: 0.000015 - momentum: 0.000000
2023-07-12 04:36:19,093 epoch 1 - iter 45/150 - loss 1.92519414 - time (sec): 134.43 - samples/sec: 460.95 - lr: 0.000023 - momentum: 0.000000
2023-07-12 04:37:02,969 epoch 1 - iter 60/150 - loss 1.58273889 - time (sec): 178.30 - samples/sec: 462.34 - lr: 0.000031 - momentum: 0.000000
2023-07-12 04:37:48,215 epoch 1 - iter 75/150 - loss 1.35057353 - time (sec): 223.55 - samples/sec: 463.74 - lr: 0.000039 - momentum: 0.000000
2023-07-12 04:38:33,042 epoch 1 - iter 90/150 - loss 1.18422103 - time (sec): 268.37 - samples/sec: 460.43 - lr: 0.000047 - momentum: 0.000000
2023-07-12 04:39:17,578 epoch 1 - iter 105/150 - loss 1.04164669 - time (sec): 312.91 - samples/sec: 460.45 - lr: 0.000055 - momentum: 0.000000
2023-07-12 04:40:02,527 epoch 1 - iter 120/150 - loss 0.93524910 - time (sec): 357.86 - samples/sec: 458.64 - lr: 0.000063 - momentum: 0.000000
2023-07-12 04:40:47,686 epoch 1 - iter 135/150 - loss 0.84674722 - time (sec): 403.02 - samples/sec: 457.13 - lr: 0.000071 - momentum: 0.000000
2023-07-12 04:41:32,531 epoch 1 - iter 150/150 - loss 0.77312574 - time (sec): 447.86 - samples/sec: 456.76 - lr: 0.000079 - momentum: 0.000000
2023-07-12 04:41:32,532 ----------------------------------------------------------------------------------------------------
2023-07-12 04:41:32,532 EPOCH 1 done: loss 0.7731 - lr: 0.000079
2023-07-12 04:42:13,877 DEV : loss 0.1773422211408615 - f1-score (micro avg)  0.691
2023-07-12 04:42:55,124 TEST : loss 0.10286872833967209 - f1-score (micro avg)  0.8967
2023-07-12 04:42:55,170 ----------------------------------------------------------------------------------------------------
2023-07-12 04:43:40,313 epoch 2 - iter 15/150 - loss 0.08817686 - time (sec): 45.14 - samples/sec: 453.25 - lr: 0.000079 - momentum: 0.000000
2023-07-12 04:44:25,642 epoch 2 - iter 30/150 - loss 0.08717969 - time (sec): 90.47 - samples/sec: 457.50 - lr: 0.000078 - momentum: 0.000000
2023-07-12 04:45:10,093 epoch 2 - iter 45/150 - loss 0.08252553 - time (sec): 134.92 - samples/sec: 459.05 - lr: 0.000077 - momentum: 0.000000
2023-07-12 04:45:54,784 epoch 2 - iter 60/150 - loss 0.08021475 - time (sec): 179.61 - samples/sec: 458.77 - lr: 0.000077 - momentum: 0.000000
2023-07-12 04:46:39,343 epoch 2 - iter 75/150 - loss 0.07769721 - time (sec): 224.17 - samples/sec: 460.58 - lr: 0.000076 - momentum: 0.000000
2023-07-12 04:47:25,263 epoch 2 - iter 90/150 - loss 0.07715967 - time (sec): 270.09 - samples/sec: 458.92 - lr: 0.000075 - momentum: 0.000000
2023-07-12 04:48:09,749 epoch 2 - iter 105/150 - loss 0.07554520 - time (sec): 314.58 - samples/sec: 458.48 - lr: 0.000074 - momentum: 0.000000
2023-07-12 04:48:54,670 epoch 2 - iter 120/150 - loss 0.07418170 - time (sec): 359.50 - samples/sec: 457.27 - lr: 0.000073 - momentum: 0.000000
2023-07-12 04:49:39,506 epoch 2 - iter 135/150 - loss 0.07276752 - time (sec): 404.33 - samples/sec: 457.19 - lr: 0.000072 - momentum: 0.000000
2023-07-12 04:50:22,985 epoch 2 - iter 150/150 - loss 0.07081817 - time (sec): 447.81 - samples/sec: 456.81 - lr: 0.000071 - momentum: 0.000000
2023-07-12 04:50:22,985 ----------------------------------------------------------------------------------------------------
2023-07-12 04:50:22,986 EPOCH 2 done: loss 0.0708 - lr: 0.000071
2023-07-12 04:51:04,348 DEV : loss 0.179571732878685 - f1-score (micro avg)  0.6999
2023-07-12 04:51:45,446 TEST : loss 0.08692383021116257 - f1-score (micro avg)  0.9227
2023-07-12 04:51:45,494 ----------------------------------------------------------------------------------------------------
2023-07-12 04:52:30,263 epoch 3 - iter 15/150 - loss 0.04533805 - time (sec): 44.77 - samples/sec: 447.96 - lr: 0.000070 - momentum: 0.000000
2023-07-12 04:53:14,534 epoch 3 - iter 30/150 - loss 0.04249141 - time (sec): 89.04 - samples/sec: 454.40 - lr: 0.000069 - momentum: 0.000000
2023-07-12 04:53:59,530 epoch 3 - iter 45/150 - loss 0.04225131 - time (sec): 134.03 - samples/sec: 453.03 - lr: 0.000069 - momentum: 0.000000
2023-07-12 04:54:44,240 epoch 3 - iter 60/150 - loss 0.04175144 - time (sec): 178.74 - samples/sec: 451.56 - lr: 0.000068 - momentum: 0.000000
2023-07-12 04:55:28,587 epoch 3 - iter 75/150 - loss 0.04231520 - time (sec): 223.09 - samples/sec: 455.13 - lr: 0.000067 - momentum: 0.000000
2023-07-12 04:56:13,024 epoch 3 - iter 90/150 - loss 0.04929572 - time (sec): 267.53 - samples/sec: 457.78 - lr: 0.000066 - momentum: 0.000000
2023-07-12 04:56:58,027 epoch 3 - iter 105/150 - loss 0.05362764 - time (sec): 312.53 - samples/sec: 457.17 - lr: 0.000065 - momentum: 0.000000
2023-07-12 04:57:42,237 epoch 3 - iter 120/150 - loss 0.05555946 - time (sec): 356.74 - samples/sec: 458.13 - lr: 0.000064 - momentum: 0.000000
2023-07-12 04:58:26,826 epoch 3 - iter 135/150 - loss 0.19514003 - time (sec): 401.33 - samples/sec: 459.20 - lr: 0.000063 - momentum: 0.000000
2023-07-12 04:59:11,974 epoch 3 - iter 150/150 - loss 0.29014942 - time (sec): 446.48 - samples/sec: 458.18 - lr: 0.000062 - momentum: 0.000000
2023-07-12 04:59:11,974 ----------------------------------------------------------------------------------------------------
2023-07-12 04:59:11,974 EPOCH 3 done: loss 0.2901 - lr: 0.000062
2023-07-12 04:59:52,903 DEV : loss 0.5311646461486816 - f1-score (micro avg)  0.0
2023-07-12 05:00:33,566 TEST : loss 1.0640311241149902 - f1-score (micro avg)  0.0
2023-07-12 05:00:33,612 ----------------------------------------------------------------------------------------------------
2023-07-12 05:01:18,314 epoch 4 - iter 15/150 - loss 1.02170712 - time (sec): 44.70 - samples/sec: 461.85 - lr: 0.000062 - momentum: 0.000000
2023-07-12 05:02:03,153 epoch 4 - iter 30/150 - loss 0.99486323 - time (sec): 89.54 - samples/sec: 466.10 - lr: 0.000061 - momentum: 0.000000
2023-07-12 05:02:48,197 epoch 4 - iter 45/150 - loss 0.99209773 - time (sec): 134.58 - samples/sec: 460.89 - lr: 0.000060 - momentum: 0.000000
2023-07-12 05:03:32,603 epoch 4 - iter 60/150 - loss 0.97281462 - time (sec): 178.99 - samples/sec: 462.69 - lr: 0.000059 - momentum: 0.000000
2023-07-12 05:04:18,577 epoch 4 - iter 75/150 - loss 0.97264224 - time (sec): 224.96 - samples/sec: 455.41 - lr: 0.000058 - momentum: 0.000000
2023-07-12 05:05:03,232 epoch 4 - iter 90/150 - loss 0.97038191 - time (sec): 269.62 - samples/sec: 454.26 - lr: 0.000057 - momentum: 0.000000
2023-07-12 05:05:47,936 epoch 4 - iter 105/150 - loss 0.96829975 - time (sec): 314.32 - samples/sec: 455.50 - lr: 0.000056 - momentum: 0.000000
2023-07-12 05:06:32,444 epoch 4 - iter 120/150 - loss 0.96772382 - time (sec): 358.83 - samples/sec: 456.28 - lr: 0.000055 - momentum: 0.000000
2023-07-12 05:07:16,817 epoch 4 - iter 135/150 - loss 0.96263207 - time (sec): 403.20 - samples/sec: 456.56 - lr: 0.000054 - momentum: 0.000000
2023-07-12 05:08:01,524 epoch 4 - iter 150/150 - loss 0.95714058 - time (sec): 447.91 - samples/sec: 456.71 - lr: 0.000054 - momentum: 0.000000
2023-07-12 05:08:01,524 ----------------------------------------------------------------------------------------------------
2023-07-12 05:08:01,525 EPOCH 4 done: loss 0.9571 - lr: 0.000054
2023-07-12 05:08:42,541 DEV : loss 0.5028687715530396 - f1-score (micro avg)  0.0
2023-07-12 05:09:21,823 TEST : loss 0.8985929489135742 - f1-score (micro avg)  0.0
2023-07-12 05:09:21,886 ----------------------------------------------------------------------------------------------------
2023-07-12 05:10:07,084 epoch 5 - iter 15/150 - loss 0.89901491 - time (sec): 45.20 - samples/sec: 455.49 - lr: 0.000053 - momentum: 0.000000
2023-07-12 05:10:51,937 epoch 5 - iter 30/150 - loss 0.92178838 - time (sec): 90.05 - samples/sec: 454.96 - lr: 0.000052 - momentum: 0.000000
2023-07-12 05:11:36,903 epoch 5 - iter 45/150 - loss 0.92233713 - time (sec): 135.01 - samples/sec: 456.60 - lr: 0.000051 - momentum: 0.000000
2023-07-12 05:12:21,449 epoch 5 - iter 60/150 - loss 0.91827891 - time (sec): 179.56 - samples/sec: 451.33 - lr: 0.000050 - momentum: 0.000000
2023-07-12 05:13:06,401 epoch 5 - iter 75/150 - loss 0.92295367 - time (sec): 224.51 - samples/sec: 451.64 - lr: 0.000049 - momentum: 0.000000
2023-07-12 05:13:51,525 epoch 5 - iter 90/150 - loss 0.92090535 - time (sec): 269.64 - samples/sec: 453.45 - lr: 0.000048 - momentum: 0.000000
2023-07-12 05:14:36,637 epoch 5 - iter 105/150 - loss 0.91852587 - time (sec): 314.75 - samples/sec: 452.20 - lr: 0.000047 - momentum: 0.000000
2023-07-12 05:15:21,216 epoch 5 - iter 120/150 - loss 0.91612944 - time (sec): 359.33 - samples/sec: 452.96 - lr: 0.000046 - momentum: 0.000000
2023-07-12 05:16:05,598 epoch 5 - iter 135/150 - loss 0.91122810 - time (sec): 403.71 - samples/sec: 455.38 - lr: 0.000046 - momentum: 0.000000
2023-07-12 05:16:50,543 epoch 5 - iter 150/150 - loss 0.91057211 - time (sec): 448.65 - samples/sec: 455.96 - lr: 0.000045 - momentum: 0.000000
2023-07-12 05:16:50,544 ----------------------------------------------------------------------------------------------------
2023-07-12 05:16:50,544 EPOCH 5 done: loss 0.9106 - lr: 0.000045
2023-07-12 05:17:32,543 DEV : loss 0.5063435435295105 - f1-score (micro avg)  0.0
2023-07-12 05:18:11,863 TEST : loss 0.8910421133041382 - f1-score (micro avg)  0.0
2023-07-12 05:18:11,910 ----------------------------------------------------------------------------------------------------
2023-07-12 05:18:57,474 epoch 6 - iter 15/150 - loss 0.86213144 - time (sec): 45.56 - samples/sec: 454.96 - lr: 0.000044 - momentum: 0.000000
2023-07-12 05:19:42,664 epoch 6 - iter 30/150 - loss 0.86286435 - time (sec): 90.75 - samples/sec: 456.36 - lr: 0.000043 - momentum: 0.000000
2023-07-12 05:20:27,566 epoch 6 - iter 45/150 - loss 0.88133486 - time (sec): 135.65 - samples/sec: 455.03 - lr: 0.000042 - momentum: 0.000000
2023-07-12 05:21:12,977 epoch 6 - iter 60/150 - loss 0.88429539 - time (sec): 181.07 - samples/sec: 452.44 - lr: 0.000041 - momentum: 0.000000
2023-07-12 05:21:57,828 epoch 6 - iter 75/150 - loss 0.88504648 - time (sec): 225.92 - samples/sec: 454.33 - lr: 0.000040 - momentum: 0.000000
2023-07-12 05:22:42,232 epoch 6 - iter 90/150 - loss 0.88977797 - time (sec): 270.32 - samples/sec: 453.27 - lr: 0.000039 - momentum: 0.000000
2023-07-12 05:23:27,153 epoch 6 - iter 105/150 - loss 0.89138807 - time (sec): 315.24 - samples/sec: 453.60 - lr: 0.000039 - momentum: 0.000000
2023-07-12 05:24:12,053 epoch 6 - iter 120/150 - loss 0.89553101 - time (sec): 360.14 - samples/sec: 453.19 - lr: 0.000038 - momentum: 0.000000
2023-07-12 05:24:57,086 epoch 6 - iter 135/150 - loss 0.89264086 - time (sec): 405.17 - samples/sec: 454.26 - lr: 0.000037 - momentum: 0.000000
2023-07-12 05:25:42,277 epoch 6 - iter 150/150 - loss 0.89572664 - time (sec): 450.37 - samples/sec: 454.22 - lr: 0.000036 - momentum: 0.000000
2023-07-12 05:25:42,278 ----------------------------------------------------------------------------------------------------
2023-07-12 05:25:42,278 EPOCH 6 done: loss 0.8957 - lr: 0.000036
2023-07-12 05:26:23,230 DEV : loss 0.5072622895240784 - f1-score (micro avg)  0.0
2023-07-12 05:27:03,872 TEST : loss 0.8891918659210205 - f1-score (micro avg)  0.0
2023-07-12 05:27:03,919 ----------------------------------------------------------------------------------------------------
2023-07-12 05:27:48,390 epoch 7 - iter 15/150 - loss 0.87493894 - time (sec): 44.47 - samples/sec: 442.99 - lr: 0.000035 - momentum: 0.000000
2023-07-12 05:28:33,127 epoch 7 - iter 30/150 - loss 0.87663845 - time (sec): 89.21 - samples/sec: 450.47 - lr: 0.000034 - momentum: 0.000000
2023-07-12 05:29:18,262 epoch 7 - iter 45/150 - loss 0.88450248 - time (sec): 134.34 - samples/sec: 451.31 - lr: 0.000033 - momentum: 0.000000
2023-07-12 05:30:02,813 epoch 7 - iter 60/150 - loss 0.87880685 - time (sec): 178.89 - samples/sec: 454.56 - lr: 0.000032 - momentum: 0.000000
2023-07-12 05:30:47,320 epoch 7 - iter 75/150 - loss 0.87649780 - time (sec): 223.40 - samples/sec: 455.48 - lr: 0.000031 - momentum: 0.000000
2023-07-12 05:31:32,397 epoch 7 - iter 90/150 - loss 0.87970738 - time (sec): 268.48 - samples/sec: 455.41 - lr: 0.000031 - momentum: 0.000000
2023-07-12 05:32:16,386 epoch 7 - iter 105/150 - loss 0.88069218 - time (sec): 312.47 - samples/sec: 459.43 - lr: 0.000030 - momentum: 0.000000
2023-07-12 05:33:02,164 epoch 7 - iter 120/150 - loss 0.88365021 - time (sec): 358.24 - samples/sec: 456.99 - lr: 0.000029 - momentum: 0.000000
2023-07-12 05:33:47,029 epoch 7 - iter 135/150 - loss 0.88280324 - time (sec): 403.11 - samples/sec: 457.95 - lr: 0.000028 - momentum: 0.000000
2023-07-12 05:34:31,152 epoch 7 - iter 150/150 - loss 0.88832686 - time (sec): 447.23 - samples/sec: 457.41 - lr: 0.000027 - momentum: 0.000000
2023-07-12 05:34:31,153 ----------------------------------------------------------------------------------------------------
2023-07-12 05:34:31,153 EPOCH 7 done: loss 0.8883 - lr: 0.000027
2023-07-12 05:35:12,008 DEV : loss 0.5130792260169983 - f1-score (micro avg)  0.0
2023-07-12 05:35:51,107 TEST : loss 0.8856226205825806 - f1-score (micro avg)  0.0
2023-07-12 05:35:51,169 ----------------------------------------------------------------------------------------------------
2023-07-12 05:36:33,286 epoch 8 - iter 15/150 - loss 0.88646007 - time (sec): 42.12 - samples/sec: 469.40 - lr: 0.000026 - momentum: 0.000000
2023-07-12 05:37:16,336 epoch 8 - iter 30/150 - loss 0.90316929 - time (sec): 85.17 - samples/sec: 468.52 - lr: 0.000025 - momentum: 0.000000
2023-07-12 05:38:00,066 epoch 8 - iter 45/150 - loss 0.90221908 - time (sec): 128.90 - samples/sec: 475.10 - lr: 0.000024 - momentum: 0.000000
2023-07-12 05:38:42,317 epoch 8 - iter 60/150 - loss 0.89414744 - time (sec): 171.15 - samples/sec: 476.34 - lr: 0.000024 - momentum: 0.000000
2023-07-12 05:39:25,727 epoch 8 - iter 75/150 - loss 0.88921004 - time (sec): 214.56 - samples/sec: 474.16 - lr: 0.000023 - momentum: 0.000000
2023-07-12 05:40:08,741 epoch 8 - iter 90/150 - loss 0.88059671 - time (sec): 257.57 - samples/sec: 478.42 - lr: 0.000022 - momentum: 0.000000
2023-07-12 05:40:53,165 epoch 8 - iter 105/150 - loss 0.88381190 - time (sec): 301.99 - samples/sec: 474.19 - lr: 0.000021 - momentum: 0.000000
2023-07-12 05:41:35,180 epoch 8 - iter 120/150 - loss 0.88441595 - time (sec): 344.01 - samples/sec: 477.05 - lr: 0.000020 - momentum: 0.000000
2023-07-12 05:42:17,070 epoch 8 - iter 135/150 - loss 0.88490900 - time (sec): 385.90 - samples/sec: 478.21 - lr: 0.000019 - momentum: 0.000000
2023-07-12 05:42:59,104 epoch 8 - iter 150/150 - loss 0.88427063 - time (sec): 427.93 - samples/sec: 478.04 - lr: 0.000018 - momentum: 0.000000
2023-07-12 05:42:59,104 ----------------------------------------------------------------------------------------------------
2023-07-12 05:42:59,104 EPOCH 8 done: loss 0.8843 - lr: 0.000018
2023-07-12 05:43:40,133 DEV : loss 0.5101965665817261 - f1-score (micro avg)  0.0
2023-07-12 05:44:19,648 TEST : loss 0.887251615524292 - f1-score (micro avg)  0.0
2023-07-12 05:44:19,700 ----------------------------------------------------------------------------------------------------
2023-07-12 05:45:04,624 epoch 9 - iter 15/150 - loss 0.87929963 - time (sec): 44.92 - samples/sec: 459.35 - lr: 0.000017 - momentum: 0.000000
2023-07-12 05:45:48,792 epoch 9 - iter 30/150 - loss 0.88098013 - time (sec): 89.09 - samples/sec: 459.60 - lr: 0.000016 - momentum: 0.000000
2023-07-12 05:46:33,497 epoch 9 - iter 45/150 - loss 0.88173761 - time (sec): 133.80 - samples/sec: 459.86 - lr: 0.000016 - momentum: 0.000000
2023-07-12 05:47:18,502 epoch 9 - iter 60/150 - loss 0.88483774 - time (sec): 178.80 - samples/sec: 460.70 - lr: 0.000015 - momentum: 0.000000
2023-07-12 05:48:02,944 epoch 9 - iter 75/150 - loss 0.88706773 - time (sec): 223.24 - samples/sec: 460.80 - lr: 0.000014 - momentum: 0.000000
2023-07-12 05:48:49,185 epoch 9 - iter 90/150 - loss 0.88465744 - time (sec): 269.48 - samples/sec: 458.44 - lr: 0.000013 - momentum: 0.000000
2023-07-12 05:49:34,191 epoch 9 - iter 105/150 - loss 0.88423668 - time (sec): 314.49 - samples/sec: 456.22 - lr: 0.000012 - momentum: 0.000000
2023-07-12 05:50:19,917 epoch 9 - iter 120/150 - loss 0.88085407 - time (sec): 360.22 - samples/sec: 454.99 - lr: 0.000011 - momentum: 0.000000
2023-07-12 05:51:05,135 epoch 9 - iter 135/150 - loss 0.88328943 - time (sec): 405.43 - samples/sec: 454.05 - lr: 0.000010 - momentum: 0.000000
2023-07-12 05:51:50,075 epoch 9 - iter 150/150 - loss 0.88196859 - time (sec): 450.37 - samples/sec: 454.22 - lr: 0.000009 - momentum: 0.000000
2023-07-12 05:51:50,076 ----------------------------------------------------------------------------------------------------
2023-07-12 05:51:50,076 EPOCH 9 done: loss 0.8820 - lr: 0.000009
2023-07-12 05:52:32,393 DEV : loss 0.512724757194519 - f1-score (micro avg)  0.0
2023-07-12 05:53:11,793 TEST : loss 0.8853849768638611 - f1-score (micro avg)  0.0
2023-07-12 05:53:11,840 ----------------------------------------------------------------------------------------------------
2023-07-12 05:53:56,662 epoch 10 - iter 15/150 - loss 0.86977054 - time (sec): 44.82 - samples/sec: 445.63 - lr: 0.000008 - momentum: 0.000000
2023-07-12 05:54:42,111 epoch 10 - iter 30/150 - loss 0.86140805 - time (sec): 90.27 - samples/sec: 454.55 - lr: 0.000008 - momentum: 0.000000
2023-07-12 05:55:26,071 epoch 10 - iter 45/150 - loss 0.87019363 - time (sec): 134.23 - samples/sec: 454.79 - lr: 0.000007 - momentum: 0.000000
2023-07-12 05:56:10,465 epoch 10 - iter 60/150 - loss 0.87741680 - time (sec): 178.62 - samples/sec: 455.99 - lr: 0.000006 - momentum: 0.000000
2023-07-12 05:56:55,585 epoch 10 - iter 75/150 - loss 0.87670871 - time (sec): 223.74 - samples/sec: 456.73 - lr: 0.000005 - momentum: 0.000000
2023-07-12 05:57:40,607 epoch 10 - iter 90/150 - loss 0.87665162 - time (sec): 268.77 - samples/sec: 455.26 - lr: 0.000004 - momentum: 0.000000
2023-07-12 05:58:25,830 epoch 10 - iter 105/150 - loss 0.87982265 - time (sec): 313.99 - samples/sec: 454.65 - lr: 0.000003 - momentum: 0.000000
2023-07-12 05:59:11,337 epoch 10 - iter 120/150 - loss 0.88162998 - time (sec): 359.49 - samples/sec: 453.24 - lr: 0.000002 - momentum: 0.000000
2023-07-12 05:59:55,911 epoch 10 - iter 135/150 - loss 0.88119140 - time (sec): 404.07 - samples/sec: 455.51 - lr: 0.000001 - momentum: 0.000000
2023-07-12 06:00:40,015 epoch 10 - iter 150/150 - loss 0.88045524 - time (sec): 448.17 - samples/sec: 456.45 - lr: 0.000001 - momentum: 0.000000
2023-07-12 06:00:40,015 ----------------------------------------------------------------------------------------------------
2023-07-12 06:00:40,015 EPOCH 10 done: loss 0.8805 - lr: 0.000001
2023-07-12 06:01:22,325 DEV : loss 0.5116875767707825 - f1-score (micro avg)  0.0
2023-07-12 06:02:01,739 TEST : loss 0.8861101865768433 - f1-score (micro avg)  0.0
2023-07-12 06:02:14,406 ----------------------------------------------------------------------------------------------------
2023-07-12 06:02:14,410 Testing using last state of model ...
2023-07-12 06:02:53,985 
Results:
- F-score (micro) 0.0
- F-score (macro) 0.0
- Accuracy 0.0

By class:
              precision    recall  f1-score   support

         LOC     0.0000    0.0000    0.0000    1668.0
         ORG     0.0000    0.0000    0.0000    1661.0
         PER     0.0000    0.0000    0.0000    1617.0
        MISC     0.0000    0.0000    0.0000     702.0

   micro avg     0.0000    0.0000    0.0000    5648.0
   macro avg     0.0000    0.0000    0.0000    5648.0
weighted avg     0.0000    0.0000    0.0000    5648.0

2023-07-12 06:02:53,985 ----------------------------------------------------------------------------------------------------
