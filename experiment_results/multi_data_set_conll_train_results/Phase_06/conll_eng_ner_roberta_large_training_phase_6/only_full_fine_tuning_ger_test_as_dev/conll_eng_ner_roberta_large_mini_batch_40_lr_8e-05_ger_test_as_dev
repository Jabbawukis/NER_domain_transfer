2023-07-07 15:18:45,859 ----------------------------------------------------------------------------------------------------
2023-07-07 15:18:45,864 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): XLMRobertaEmbeddings(
        (word_embeddings): Embedding(250003, 1024)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): XLMRobertaEncoder(
        (layer): ModuleList(
          (0-23): 24 x XLMRobertaLayer(
            (attention): XLMRobertaAttention(
              (self): XLMRobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): XLMRobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): XLMRobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): XLMRobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): XLMRobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-07-07 15:18:45,864 ----------------------------------------------------------------------------------------------------
2023-07-07 15:18:45,864 Corpus: 14987 train + 3160 dev + 3684 test sentences
2023-07-07 15:18:45,864 ----------------------------------------------------------------------------------------------------
2023-07-07 15:18:45,864 Train:  14987 sentences
2023-07-07 15:18:45,865         (train_with_dev=False, train_with_test=False)
2023-07-07 15:18:45,865 ----------------------------------------------------------------------------------------------------
2023-07-07 15:18:45,865 Training Params:
2023-07-07 15:18:45,865  - learning_rate: "8e-05" 
2023-07-07 15:18:45,865  - mini_batch_size: "40"
2023-07-07 15:18:45,865  - max_epochs: "10"
2023-07-07 15:18:45,865  - shuffle: "True"
2023-07-07 15:18:45,865 ----------------------------------------------------------------------------------------------------
2023-07-07 15:18:45,865 Plugins:
2023-07-07 15:18:45,865  - LinearScheduler | warmup_fraction: '0.1'
2023-07-07 15:18:45,865 ----------------------------------------------------------------------------------------------------
2023-07-07 15:18:45,865 Final evaluation on model after last epoch (final-model.pt)
2023-07-07 15:18:45,866  - metric: "('micro avg', 'f1-score')"
2023-07-07 15:18:45,866 ----------------------------------------------------------------------------------------------------
2023-07-07 15:18:45,866 Computation:
2023-07-07 15:18:45,866  - compute on device: cuda:1
2023-07-07 15:18:45,866  - embedding storage: none
2023-07-07 15:18:45,866 ----------------------------------------------------------------------------------------------------
2023-07-07 15:18:45,866 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_mini_batch_40_lr_8e-05_ger_test_as_dev"
2023-07-07 15:18:45,866 ----------------------------------------------------------------------------------------------------
2023-07-07 15:18:45,866 Removed gradient clipping
2023-07-07 15:18:45,866 ----------------------------------------------------------------------------------------------------
2023-07-07 15:18:45,866 ----------------------------------------------------------------------------------------------------
2023-07-07 15:19:34,099 epoch 1 - iter 37/375 - loss 2.95493890 - time (sec): 48.23 - samples/sec: 419.13 - lr: 0.000008 - momentum: 0.000000
2023-07-07 15:20:20,337 epoch 1 - iter 74/375 - loss 1.90232069 - time (sec): 94.47 - samples/sec: 423.39 - lr: 0.000016 - momentum: 0.000000
2023-07-07 15:21:06,053 epoch 1 - iter 111/375 - loss 1.41746909 - time (sec): 140.18 - samples/sec: 427.35 - lr: 0.000023 - momentum: 0.000000
2023-07-07 15:21:51,416 epoch 1 - iter 148/375 - loss 1.12805664 - time (sec): 185.55 - samples/sec: 433.82 - lr: 0.000031 - momentum: 0.000000
2023-07-07 15:22:38,101 epoch 1 - iter 185/375 - loss 0.93687339 - time (sec): 232.23 - samples/sec: 436.98 - lr: 0.000039 - momentum: 0.000000
2023-07-07 15:23:24,856 epoch 1 - iter 222/375 - loss 0.80847369 - time (sec): 278.99 - samples/sec: 437.69 - lr: 0.000047 - momentum: 0.000000
2023-07-07 15:24:11,242 epoch 1 - iter 259/375 - loss 0.71356132 - time (sec): 325.37 - samples/sec: 436.26 - lr: 0.000055 - momentum: 0.000000
2023-07-07 15:24:58,064 epoch 1 - iter 296/375 - loss 0.65255504 - time (sec): 372.19 - samples/sec: 434.31 - lr: 0.000063 - momentum: 0.000000
2023-07-07 15:25:43,418 epoch 1 - iter 333/375 - loss 0.59734029 - time (sec): 417.55 - samples/sec: 436.57 - lr: 0.000071 - momentum: 0.000000
2023-07-07 15:26:29,313 epoch 1 - iter 370/375 - loss 0.55194433 - time (sec): 463.44 - samples/sec: 436.35 - lr: 0.000079 - momentum: 0.000000
2023-07-07 15:26:34,972 ----------------------------------------------------------------------------------------------------
2023-07-07 15:26:34,972 EPOCH 1 done: loss 0.5479 - lr: 0.000079
2023-07-07 15:27:19,626 DEV : loss 0.28160372376441956 - f1-score (micro avg)  0.5742
2023-07-07 15:28:03,364 TEST : loss 0.3635847568511963 - f1-score (micro avg)  0.6333
2023-07-07 15:28:03,418 ----------------------------------------------------------------------------------------------------
2023-07-07 15:28:49,260 epoch 2 - iter 37/375 - loss 0.15037391 - time (sec): 45.84 - samples/sec: 437.29 - lr: 0.000079 - momentum: 0.000000
2023-07-07 15:29:35,238 epoch 2 - iter 74/375 - loss 0.11815624 - time (sec): 91.82 - samples/sec: 434.33 - lr: 0.000078 - momentum: 0.000000
2023-07-07 15:30:21,287 epoch 2 - iter 111/375 - loss 0.10396238 - time (sec): 137.87 - samples/sec: 432.45 - lr: 0.000077 - momentum: 0.000000
2023-07-07 15:31:07,221 epoch 2 - iter 148/375 - loss 0.09612381 - time (sec): 183.80 - samples/sec: 435.84 - lr: 0.000077 - momentum: 0.000000
2023-07-07 15:31:53,383 epoch 2 - iter 185/375 - loss 0.09210815 - time (sec): 229.96 - samples/sec: 435.54 - lr: 0.000076 - momentum: 0.000000
2023-07-07 15:32:39,808 epoch 2 - iter 222/375 - loss 0.09255608 - time (sec): 276.39 - samples/sec: 434.82 - lr: 0.000075 - momentum: 0.000000
2023-07-07 15:33:26,719 epoch 2 - iter 259/375 - loss 0.08883070 - time (sec): 323.30 - samples/sec: 434.70 - lr: 0.000074 - momentum: 0.000000
2023-07-07 15:34:13,572 epoch 2 - iter 296/375 - loss 0.08663780 - time (sec): 370.15 - samples/sec: 435.75 - lr: 0.000073 - momentum: 0.000000
2023-07-07 15:35:01,248 epoch 2 - iter 333/375 - loss 0.08395816 - time (sec): 417.83 - samples/sec: 436.00 - lr: 0.000072 - momentum: 0.000000
2023-07-07 15:35:47,508 epoch 2 - iter 370/375 - loss 0.08289489 - time (sec): 464.09 - samples/sec: 434.97 - lr: 0.000071 - momentum: 0.000000
2023-07-07 15:35:53,635 ----------------------------------------------------------------------------------------------------
2023-07-07 15:35:53,635 EPOCH 2 done: loss 0.0829 - lr: 0.000071
2023-07-07 15:36:37,865 DEV : loss 0.18313723802566528 - f1-score (micro avg)  0.694
2023-07-07 15:37:19,039 TEST : loss 0.08821691572666168 - f1-score (micro avg)  0.9198
2023-07-07 15:37:19,084 ----------------------------------------------------------------------------------------------------
2023-07-07 15:38:05,851 epoch 3 - iter 37/375 - loss 0.04578039 - time (sec): 46.77 - samples/sec: 434.27 - lr: 0.000070 - momentum: 0.000000
2023-07-07 15:38:52,844 epoch 3 - iter 74/375 - loss 0.04757256 - time (sec): 93.76 - samples/sec: 434.97 - lr: 0.000069 - momentum: 0.000000
2023-07-07 15:39:39,289 epoch 3 - iter 111/375 - loss 0.04578102 - time (sec): 140.20 - samples/sec: 436.78 - lr: 0.000069 - momentum: 0.000000
2023-07-07 15:40:24,366 epoch 3 - iter 148/375 - loss 0.04575587 - time (sec): 185.28 - samples/sec: 436.94 - lr: 0.000068 - momentum: 0.000000
2023-07-07 15:41:09,968 epoch 3 - iter 185/375 - loss 0.04556859 - time (sec): 230.88 - samples/sec: 438.37 - lr: 0.000067 - momentum: 0.000000
2023-07-07 15:41:56,463 epoch 3 - iter 222/375 - loss 0.04517154 - time (sec): 277.38 - samples/sec: 440.66 - lr: 0.000066 - momentum: 0.000000
2023-07-07 15:42:41,619 epoch 3 - iter 259/375 - loss 0.04639961 - time (sec): 322.53 - samples/sec: 440.39 - lr: 0.000065 - momentum: 0.000000
2023-07-07 15:43:28,022 epoch 3 - iter 296/375 - loss 0.04846805 - time (sec): 368.94 - samples/sec: 440.71 - lr: 0.000064 - momentum: 0.000000
2023-07-07 15:44:12,745 epoch 3 - iter 333/375 - loss 0.04999411 - time (sec): 413.66 - samples/sec: 440.51 - lr: 0.000063 - momentum: 0.000000
2023-07-07 15:44:58,421 epoch 3 - iter 370/375 - loss 0.04942637 - time (sec): 459.34 - samples/sec: 439.31 - lr: 0.000062 - momentum: 0.000000
2023-07-07 15:45:04,125 ----------------------------------------------------------------------------------------------------
2023-07-07 15:45:04,125 EPOCH 3 done: loss 0.0491 - lr: 0.000062
2023-07-07 15:45:48,284 DEV : loss 0.2008168250322342 - f1-score (micro avg)  0.6912
2023-07-07 15:46:30,197 TEST : loss 0.09597063809633255 - f1-score (micro avg)  0.9246
2023-07-07 15:46:30,250 ----------------------------------------------------------------------------------------------------
2023-07-07 15:47:16,463 epoch 4 - iter 37/375 - loss 0.03650665 - time (sec): 46.21 - samples/sec: 437.50 - lr: 0.000061 - momentum: 0.000000
2023-07-07 15:48:01,342 epoch 4 - iter 74/375 - loss 0.03712319 - time (sec): 91.09 - samples/sec: 446.59 - lr: 0.000061 - momentum: 0.000000
2023-07-07 15:48:47,758 epoch 4 - iter 111/375 - loss 0.03574786 - time (sec): 137.51 - samples/sec: 447.21 - lr: 0.000060 - momentum: 0.000000
2023-07-07 15:49:33,000 epoch 4 - iter 148/375 - loss 0.03793308 - time (sec): 182.75 - samples/sec: 445.19 - lr: 0.000059 - momentum: 0.000000
2023-07-07 15:50:16,923 epoch 4 - iter 185/375 - loss 0.03621538 - time (sec): 226.67 - samples/sec: 448.82 - lr: 0.000058 - momentum: 0.000000
2023-07-07 15:51:03,721 epoch 4 - iter 222/375 - loss 0.03662196 - time (sec): 273.47 - samples/sec: 445.35 - lr: 0.000057 - momentum: 0.000000
2023-07-07 15:51:49,699 epoch 4 - iter 259/375 - loss 0.03585921 - time (sec): 319.45 - samples/sec: 445.07 - lr: 0.000056 - momentum: 0.000000
2023-07-07 15:52:35,392 epoch 4 - iter 296/375 - loss 0.03543830 - time (sec): 365.14 - samples/sec: 442.70 - lr: 0.000055 - momentum: 0.000000
2023-07-07 15:53:20,662 epoch 4 - iter 333/375 - loss 0.03524142 - time (sec): 410.41 - samples/sec: 443.31 - lr: 0.000054 - momentum: 0.000000
2023-07-07 15:54:04,484 epoch 4 - iter 370/375 - loss 0.06450051 - time (sec): 454.23 - samples/sec: 444.44 - lr: 0.000054 - momentum: 0.000000
2023-07-07 15:54:09,990 ----------------------------------------------------------------------------------------------------
2023-07-07 15:54:09,990 EPOCH 4 done: loss 0.0675 - lr: 0.000054
2023-07-07 15:54:56,383 DEV : loss 0.26348423957824707 - f1-score (micro avg)  0.568
2023-07-07 15:55:40,031 TEST : loss 0.10199049860239029 - f1-score (micro avg)  0.8954
2023-07-07 15:55:40,084 ----------------------------------------------------------------------------------------------------
2023-07-07 15:56:26,203 epoch 5 - iter 37/375 - loss 0.12409540 - time (sec): 46.12 - samples/sec: 430.53 - lr: 0.000053 - momentum: 0.000000
2023-07-07 15:57:11,119 epoch 5 - iter 74/375 - loss 0.09559465 - time (sec): 91.03 - samples/sec: 435.39 - lr: 0.000052 - momentum: 0.000000
2023-07-07 15:57:55,347 epoch 5 - iter 111/375 - loss 0.08037466 - time (sec): 135.26 - samples/sec: 443.14 - lr: 0.000051 - momentum: 0.000000
2023-07-07 15:58:39,578 epoch 5 - iter 148/375 - loss 0.07418876 - time (sec): 179.49 - samples/sec: 445.67 - lr: 0.000050 - momentum: 0.000000
2023-07-07 15:59:25,224 epoch 5 - iter 185/375 - loss 0.06953888 - time (sec): 225.14 - samples/sec: 441.96 - lr: 0.000049 - momentum: 0.000000
2023-07-07 16:00:12,518 epoch 5 - iter 222/375 - loss 0.06461446 - time (sec): 272.43 - samples/sec: 440.21 - lr: 0.000048 - momentum: 0.000000
2023-07-07 16:00:58,951 epoch 5 - iter 259/375 - loss 0.06107499 - time (sec): 318.86 - samples/sec: 439.71 - lr: 0.000047 - momentum: 0.000000
2023-07-07 16:01:45,702 epoch 5 - iter 296/375 - loss 0.05654877 - time (sec): 365.62 - samples/sec: 441.51 - lr: 0.000046 - momentum: 0.000000
2023-07-07 16:02:31,946 epoch 5 - iter 333/375 - loss 0.05444738 - time (sec): 411.86 - samples/sec: 441.63 - lr: 0.000046 - momentum: 0.000000
2023-07-07 16:03:18,457 epoch 5 - iter 370/375 - loss 0.05299054 - time (sec): 458.37 - samples/sec: 440.85 - lr: 0.000045 - momentum: 0.000000
2023-07-07 16:03:24,373 ----------------------------------------------------------------------------------------------------
2023-07-07 16:03:24,373 EPOCH 5 done: loss 0.0526 - lr: 0.000045
2023-07-07 16:04:07,741 DEV : loss 0.17869040369987488 - f1-score (micro avg)  0.7253
2023-07-07 16:04:50,827 TEST : loss 0.10712406039237976 - f1-score (micro avg)  0.9134
2023-07-07 16:04:50,879 ----------------------------------------------------------------------------------------------------
2023-07-07 16:05:37,219 epoch 6 - iter 37/375 - loss 0.02693166 - time (sec): 46.34 - samples/sec: 435.40 - lr: 0.000044 - momentum: 0.000000
2023-07-07 16:06:24,216 epoch 6 - iter 74/375 - loss 0.02822517 - time (sec): 93.33 - samples/sec: 430.33 - lr: 0.000043 - momentum: 0.000000
2023-07-07 16:07:10,946 epoch 6 - iter 111/375 - loss 0.02877794 - time (sec): 140.06 - samples/sec: 432.42 - lr: 0.000042 - momentum: 0.000000
2023-07-07 16:07:54,765 epoch 6 - iter 148/375 - loss 0.03029353 - time (sec): 183.88 - samples/sec: 439.04 - lr: 0.000041 - momentum: 0.000000
2023-07-07 16:08:39,016 epoch 6 - iter 185/375 - loss 0.03024087 - time (sec): 228.13 - samples/sec: 443.93 - lr: 0.000040 - momentum: 0.000000
2023-07-07 16:09:25,074 epoch 6 - iter 222/375 - loss 0.02961106 - time (sec): 274.19 - samples/sec: 443.88 - lr: 0.000039 - momentum: 0.000000
2023-07-07 16:10:11,188 epoch 6 - iter 259/375 - loss 0.02857456 - time (sec): 320.31 - samples/sec: 445.10 - lr: 0.000038 - momentum: 0.000000
2023-07-07 16:10:56,961 epoch 6 - iter 296/375 - loss 0.02803183 - time (sec): 366.08 - samples/sec: 443.47 - lr: 0.000038 - momentum: 0.000000
2023-07-07 16:11:43,597 epoch 6 - iter 333/375 - loss 0.02744856 - time (sec): 412.72 - samples/sec: 440.45 - lr: 0.000037 - momentum: 0.000000
2023-07-07 16:12:29,767 epoch 6 - iter 370/375 - loss 0.02711450 - time (sec): 458.89 - samples/sec: 440.48 - lr: 0.000036 - momentum: 0.000000
2023-07-07 16:12:35,659 ----------------------------------------------------------------------------------------------------
2023-07-07 16:12:35,660 EPOCH 6 done: loss 0.0270 - lr: 0.000036
2023-07-07 16:13:18,730 DEV : loss 0.17568247020244598 - f1-score (micro avg)  0.7112
2023-07-07 16:14:01,673 TEST : loss 0.09627046436071396 - f1-score (micro avg)  0.9254
2023-07-07 16:14:01,731 ----------------------------------------------------------------------------------------------------
2023-07-07 16:14:47,980 epoch 7 - iter 37/375 - loss 0.01920197 - time (sec): 46.25 - samples/sec: 446.90 - lr: 0.000035 - momentum: 0.000000
2023-07-07 16:15:34,868 epoch 7 - iter 74/375 - loss 0.01819618 - time (sec): 93.14 - samples/sec: 444.64 - lr: 0.000034 - momentum: 0.000000
2023-07-07 16:16:21,330 epoch 7 - iter 111/375 - loss 0.01933683 - time (sec): 139.60 - samples/sec: 442.34 - lr: 0.000033 - momentum: 0.000000
2023-07-07 16:17:08,230 epoch 7 - iter 148/375 - loss 0.01909993 - time (sec): 186.50 - samples/sec: 439.30 - lr: 0.000032 - momentum: 0.000000
2023-07-07 16:17:54,355 epoch 7 - iter 185/375 - loss 0.01818495 - time (sec): 232.62 - samples/sec: 441.03 - lr: 0.000031 - momentum: 0.000000
2023-07-07 16:18:40,176 epoch 7 - iter 222/375 - loss 0.01795947 - time (sec): 278.44 - samples/sec: 439.52 - lr: 0.000030 - momentum: 0.000000
2023-07-07 16:19:25,723 epoch 7 - iter 259/375 - loss 0.01768957 - time (sec): 323.99 - samples/sec: 437.40 - lr: 0.000030 - momentum: 0.000000
2023-07-07 16:20:12,220 epoch 7 - iter 296/375 - loss 0.01777220 - time (sec): 370.49 - samples/sec: 437.90 - lr: 0.000029 - momentum: 0.000000
2023-07-07 16:20:59,846 epoch 7 - iter 333/375 - loss 0.01773803 - time (sec): 418.11 - samples/sec: 436.19 - lr: 0.000028 - momentum: 0.000000
2023-07-07 16:21:46,058 epoch 7 - iter 370/375 - loss 0.01778480 - time (sec): 464.33 - samples/sec: 435.02 - lr: 0.000027 - momentum: 0.000000
2023-07-07 16:21:51,900 ----------------------------------------------------------------------------------------------------
2023-07-07 16:21:51,901 EPOCH 7 done: loss 0.0177 - lr: 0.000027
2023-07-07 16:22:36,250 DEV : loss 0.1750209629535675 - f1-score (micro avg)  0.7202
2023-07-07 16:23:20,726 TEST : loss 0.09596218913793564 - f1-score (micro avg)  0.9261
2023-07-07 16:23:20,779 ----------------------------------------------------------------------------------------------------
2023-07-07 16:24:05,760 epoch 8 - iter 37/375 - loss 0.01351759 - time (sec): 44.98 - samples/sec: 451.85 - lr: 0.000026 - momentum: 0.000000
2023-07-07 16:24:50,519 epoch 8 - iter 74/375 - loss 0.01496747 - time (sec): 89.74 - samples/sec: 449.13 - lr: 0.000025 - momentum: 0.000000
2023-07-07 16:25:36,337 epoch 8 - iter 111/375 - loss 0.01501988 - time (sec): 135.56 - samples/sec: 447.24 - lr: 0.000024 - momentum: 0.000000
2023-07-07 16:26:20,963 epoch 8 - iter 148/375 - loss 0.01439798 - time (sec): 180.18 - samples/sec: 445.76 - lr: 0.000023 - momentum: 0.000000
2023-07-07 16:27:05,513 epoch 8 - iter 185/375 - loss 0.01395794 - time (sec): 224.73 - samples/sec: 450.06 - lr: 0.000022 - momentum: 0.000000
2023-07-07 16:27:50,383 epoch 8 - iter 222/375 - loss 0.01305756 - time (sec): 269.60 - samples/sec: 450.65 - lr: 0.000022 - momentum: 0.000000
2023-07-07 16:28:35,074 epoch 8 - iter 259/375 - loss 0.01255146 - time (sec): 314.29 - samples/sec: 451.34 - lr: 0.000021 - momentum: 0.000000
2023-07-07 16:29:21,222 epoch 8 - iter 296/375 - loss 0.01286887 - time (sec): 360.44 - samples/sec: 449.65 - lr: 0.000020 - momentum: 0.000000
2023-07-07 16:30:06,923 epoch 8 - iter 333/375 - loss 0.01312789 - time (sec): 406.14 - samples/sec: 447.98 - lr: 0.000019 - momentum: 0.000000
2023-07-07 16:30:53,853 epoch 8 - iter 370/375 - loss 0.01265486 - time (sec): 453.07 - samples/sec: 445.78 - lr: 0.000018 - momentum: 0.000000
2023-07-07 16:30:59,467 ----------------------------------------------------------------------------------------------------
2023-07-07 16:30:59,467 EPOCH 8 done: loss 0.0127 - lr: 0.000018
2023-07-07 16:31:43,334 DEV : loss 0.16727536916732788 - f1-score (micro avg)  0.7307
2023-07-07 16:32:26,774 TEST : loss 0.09151647984981537 - f1-score (micro avg)  0.933
2023-07-07 16:32:26,852 ----------------------------------------------------------------------------------------------------
2023-07-07 16:33:15,106 epoch 9 - iter 37/375 - loss 0.00933974 - time (sec): 48.25 - samples/sec: 430.94 - lr: 0.000017 - momentum: 0.000000
2023-07-07 16:34:01,360 epoch 9 - iter 74/375 - loss 0.00850775 - time (sec): 94.51 - samples/sec: 423.72 - lr: 0.000016 - momentum: 0.000000
2023-07-07 16:34:47,190 epoch 9 - iter 111/375 - loss 0.00830584 - time (sec): 140.34 - samples/sec: 431.15 - lr: 0.000015 - momentum: 0.000000
2023-07-07 16:35:33,452 epoch 9 - iter 148/375 - loss 0.00950654 - time (sec): 186.60 - samples/sec: 432.38 - lr: 0.000014 - momentum: 0.000000
2023-07-07 16:36:19,210 epoch 9 - iter 185/375 - loss 0.00926678 - time (sec): 232.36 - samples/sec: 435.17 - lr: 0.000014 - momentum: 0.000000
2023-07-07 16:37:04,596 epoch 9 - iter 222/375 - loss 0.00894492 - time (sec): 277.74 - samples/sec: 436.53 - lr: 0.000013 - momentum: 0.000000
2023-07-07 16:37:49,413 epoch 9 - iter 259/375 - loss 0.00925106 - time (sec): 322.56 - samples/sec: 436.92 - lr: 0.000012 - momentum: 0.000000
2023-07-07 16:38:34,662 epoch 9 - iter 296/375 - loss 0.00943259 - time (sec): 367.81 - samples/sec: 437.73 - lr: 0.000011 - momentum: 0.000000
2023-07-07 16:39:20,437 epoch 9 - iter 333/375 - loss 0.00922152 - time (sec): 413.58 - samples/sec: 437.80 - lr: 0.000010 - momentum: 0.000000
2023-07-07 16:40:06,717 epoch 9 - iter 370/375 - loss 0.00911650 - time (sec): 459.86 - samples/sec: 438.97 - lr: 0.000009 - momentum: 0.000000
2023-07-07 16:40:12,441 ----------------------------------------------------------------------------------------------------
2023-07-07 16:40:12,441 EPOCH 9 done: loss 0.0090 - lr: 0.000009
2023-07-07 16:40:57,592 DEV : loss 0.18101869523525238 - f1-score (micro avg)  0.7352
2023-07-07 16:41:39,316 TEST : loss 0.09458708763122559 - f1-score (micro avg)  0.9331
2023-07-07 16:41:39,363 ----------------------------------------------------------------------------------------------------
2023-07-07 16:42:26,172 epoch 10 - iter 37/375 - loss 0.00631720 - time (sec): 46.81 - samples/sec: 441.98 - lr: 0.000008 - momentum: 0.000000
2023-07-07 16:43:12,905 epoch 10 - iter 74/375 - loss 0.00638322 - time (sec): 93.54 - samples/sec: 436.55 - lr: 0.000007 - momentum: 0.000000
2023-07-07 16:43:59,342 epoch 10 - iter 111/375 - loss 0.00657536 - time (sec): 139.98 - samples/sec: 430.85 - lr: 0.000006 - momentum: 0.000000
2023-07-07 16:44:45,109 epoch 10 - iter 148/375 - loss 0.00649410 - time (sec): 185.74 - samples/sec: 434.47 - lr: 0.000006 - momentum: 0.000000
2023-07-07 16:45:31,586 epoch 10 - iter 185/375 - loss 0.00601081 - time (sec): 232.22 - samples/sec: 436.79 - lr: 0.000005 - momentum: 0.000000
2023-07-07 16:46:17,467 epoch 10 - iter 222/375 - loss 0.00606912 - time (sec): 278.10 - samples/sec: 436.84 - lr: 0.000004 - momentum: 0.000000
2023-07-07 16:47:03,001 epoch 10 - iter 259/375 - loss 0.00600214 - time (sec): 323.64 - samples/sec: 437.83 - lr: 0.000003 - momentum: 0.000000
2023-07-07 16:47:49,505 epoch 10 - iter 296/375 - loss 0.00580250 - time (sec): 370.14 - samples/sec: 439.15 - lr: 0.000002 - momentum: 0.000000
2023-07-07 16:48:35,739 epoch 10 - iter 333/375 - loss 0.00614720 - time (sec): 416.37 - samples/sec: 437.93 - lr: 0.000001 - momentum: 0.000000
2023-07-07 16:49:22,881 epoch 10 - iter 370/375 - loss 0.00635695 - time (sec): 463.52 - samples/sec: 436.28 - lr: 0.000000 - momentum: 0.000000
2023-07-07 16:49:28,846 ----------------------------------------------------------------------------------------------------
2023-07-07 16:49:28,847 EPOCH 10 done: loss 0.0064 - lr: 0.000000
2023-07-07 16:50:14,257 DEV : loss 0.19491666555404663 - f1-score (micro avg)  0.7221
2023-07-07 16:50:57,876 TEST : loss 0.09856219589710236 - f1-score (micro avg)  0.9327
2023-07-07 16:51:12,571 ----------------------------------------------------------------------------------------------------
2023-07-07 16:51:12,576 Testing using last state of model ...
2023-07-07 16:51:56,708 
Results:
- F-score (micro) 0.9327
- F-score (macro) 0.9192
- Accuracy 0.8999

By class:
              precision    recall  f1-score   support

         ORG     0.8980    0.9380    0.9176      1661
         LOC     0.9514    0.9394    0.9454      1668
         PER     0.9832    0.9759    0.9795      1617
        MISC     0.8124    0.8575    0.8344       702

   micro avg     0.9262    0.9393    0.9327      5648
   macro avg     0.9113    0.9277    0.9192      5648
weighted avg     0.9275    0.9393    0.9332      5648

2023-07-07 16:51:56,708 ----------------------------------------------------------------------------------------------------
