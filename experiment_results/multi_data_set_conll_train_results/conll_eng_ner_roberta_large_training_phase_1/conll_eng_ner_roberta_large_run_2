2023-05-26 23:22:28,484 ----------------------------------------------------------------------------------------------------
2023-05-26 23:22:28,492 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-26 23:22:28,495 ----------------------------------------------------------------------------------------------------
2023-05-26 23:22:28,496 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-05-26 23:22:28,496 ----------------------------------------------------------------------------------------------------
2023-05-26 23:22:28,496 Parameters:
2023-05-26 23:22:28,496  - learning_rate: "0.000005"
2023-05-26 23:22:28,496  - mini_batch_size: "4"
2023-05-26 23:22:28,496  - patience: "3"
2023-05-26 23:22:28,497  - anneal_factor: "0.5"
2023-05-26 23:22:28,497  - max_epochs: "10"
2023-05-26 23:22:28,497  - shuffle: "True"
2023-05-26 23:22:28,497  - train_with_dev: "False"
2023-05-26 23:22:28,497  - batch_growth_annealing: "False"
2023-05-26 23:22:28,497 ----------------------------------------------------------------------------------------------------
2023-05-26 23:22:28,498 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_2"
2023-05-26 23:22:28,498 ----------------------------------------------------------------------------------------------------
2023-05-26 23:22:28,498 Device: cuda:0
2023-05-26 23:22:28,498 ----------------------------------------------------------------------------------------------------
2023-05-26 23:22:28,498 Embeddings storage mode: none
2023-05-26 23:22:28,498 ----------------------------------------------------------------------------------------------------
2023-05-26 23:23:54,925 epoch 1 - iter 374/3747 - loss 2.31664062 - samples/sec: 17.32 - lr: 0.000000
2023-05-26 23:25:22,052 epoch 1 - iter 748/3747 - loss 1.42339970 - samples/sec: 17.18 - lr: 0.000001
2023-05-26 23:26:48,230 epoch 1 - iter 1122/3747 - loss 1.05346432 - samples/sec: 17.37 - lr: 0.000001
2023-05-26 23:28:12,223 epoch 1 - iter 1496/3747 - loss 0.86394488 - samples/sec: 17.82 - lr: 0.000002
2023-05-26 23:29:38,382 epoch 1 - iter 1870/3747 - loss 0.72107019 - samples/sec: 17.37 - lr: 0.000002
2023-05-26 23:31:03,044 epoch 1 - iter 2244/3747 - loss 0.61679097 - samples/sec: 17.68 - lr: 0.000003
2023-05-26 23:32:29,871 epoch 1 - iter 2618/3747 - loss 0.54297716 - samples/sec: 17.24 - lr: 0.000003
2023-05-26 23:33:57,767 epoch 1 - iter 2992/3747 - loss 0.49406752 - samples/sec: 17.03 - lr: 0.000004
2023-05-26 23:35:24,045 epoch 1 - iter 3366/3747 - loss 0.45235418 - samples/sec: 17.35 - lr: 0.000004
2023-05-26 23:36:48,700 epoch 1 - iter 3740/3747 - loss 0.41735399 - samples/sec: 17.68 - lr: 0.000005
2023-05-26 23:36:50,212 ----------------------------------------------------------------------------------------------------
2023-05-26 23:36:50,213 EPOCH 1 done: loss 0.4173 - lr 0.000005
2023-05-26 23:38:13,082 Evaluating as a multi-label problem: False
2023-05-26 23:38:13,147 DEV : loss 0.07794229686260223 - f1-score (micro avg)  0.9602
2023-05-26 23:38:13,270 BAD EPOCHS (no improvement): 4
2023-05-26 23:38:13,273 ----------------------------------------------------------------------------------------------------
2023-05-26 23:39:40,709 epoch 2 - iter 374/3747 - loss 0.17621567 - samples/sec: 17.12 - lr: 0.000005
2023-05-26 23:41:05,962 epoch 2 - iter 748/3747 - loss 0.17251253 - samples/sec: 17.56 - lr: 0.000005
2023-05-26 23:42:30,887 epoch 2 - iter 1122/3747 - loss 0.16817550 - samples/sec: 17.62 - lr: 0.000005
2023-05-26 23:43:56,023 epoch 2 - iter 1496/3747 - loss 0.16872528 - samples/sec: 17.58 - lr: 0.000005
2023-05-26 23:45:21,212 epoch 2 - iter 1870/3747 - loss 0.16838577 - samples/sec: 17.57 - lr: 0.000005
2023-05-26 23:46:50,148 epoch 2 - iter 2244/3747 - loss 0.17094241 - samples/sec: 16.83 - lr: 0.000005
2023-05-26 23:48:17,544 epoch 2 - iter 2618/3747 - loss 0.17114014 - samples/sec: 17.13 - lr: 0.000005
2023-05-26 23:49:43,748 epoch 2 - iter 2992/3747 - loss 0.17184676 - samples/sec: 17.36 - lr: 0.000005
2023-05-26 23:51:08,251 epoch 2 - iter 3366/3747 - loss 0.17215968 - samples/sec: 17.71 - lr: 0.000005
2023-05-26 23:52:33,904 epoch 2 - iter 3740/3747 - loss 0.17194188 - samples/sec: 17.47 - lr: 0.000004
2023-05-26 23:52:35,364 ----------------------------------------------------------------------------------------------------
2023-05-26 23:52:35,364 EPOCH 2 done: loss 0.1719 - lr 0.000004
2023-05-26 23:53:53,530 Evaluating as a multi-label problem: False
2023-05-26 23:53:53,601 DEV : loss 0.0730292871594429 - f1-score (micro avg)  0.9672
2023-05-26 23:53:53,732 BAD EPOCHS (no improvement): 4
2023-05-26 23:53:53,734 ----------------------------------------------------------------------------------------------------
2023-05-26 23:55:19,446 epoch 3 - iter 374/3747 - loss 0.15215969 - samples/sec: 17.46 - lr: 0.000004
2023-05-26 23:56:43,259 epoch 3 - iter 748/3747 - loss 0.15878470 - samples/sec: 17.86 - lr: 0.000004
2023-05-26 23:58:08,969 epoch 3 - iter 1122/3747 - loss 0.16205973 - samples/sec: 17.46 - lr: 0.000004
2023-05-26 23:59:34,350 epoch 3 - iter 1496/3747 - loss 0.16510741 - samples/sec: 17.53 - lr: 0.000004
2023-05-27 00:01:01,133 epoch 3 - iter 1870/3747 - loss 0.16645102 - samples/sec: 17.25 - lr: 0.000004
2023-05-27 00:02:27,707 epoch 3 - iter 2244/3747 - loss 0.16730397 - samples/sec: 17.29 - lr: 0.000004
2023-05-27 00:03:52,736 epoch 3 - iter 2618/3747 - loss 0.16812138 - samples/sec: 17.60 - lr: 0.000004
2023-05-27 00:05:16,580 epoch 3 - iter 2992/3747 - loss 0.16897058 - samples/sec: 17.85 - lr: 0.000004
2023-05-27 00:06:41,601 epoch 3 - iter 3366/3747 - loss 0.16833249 - samples/sec: 17.60 - lr: 0.000004
2023-05-27 00:08:06,613 epoch 3 - iter 3740/3747 - loss 0.16720128 - samples/sec: 17.61 - lr: 0.000004
2023-05-27 00:08:08,251 ----------------------------------------------------------------------------------------------------
2023-05-27 00:08:08,252 EPOCH 3 done: loss 0.1671 - lr 0.000004
2023-05-27 00:09:33,428 Evaluating as a multi-label problem: False
2023-05-27 00:09:33,493 DEV : loss 0.07347309589385986 - f1-score (micro avg)  0.9678
2023-05-27 00:09:33,618 BAD EPOCHS (no improvement): 4
2023-05-27 00:09:33,621 ----------------------------------------------------------------------------------------------------
2023-05-27 00:10:59,336 epoch 4 - iter 374/3747 - loss 0.15602762 - samples/sec: 17.46 - lr: 0.000004
2023-05-27 00:12:24,927 epoch 4 - iter 748/3747 - loss 0.16221993 - samples/sec: 17.49 - lr: 0.000004
2023-05-27 00:13:49,559 epoch 4 - iter 1122/3747 - loss 0.16016659 - samples/sec: 17.69 - lr: 0.000004
2023-05-27 00:15:15,633 epoch 4 - iter 1496/3747 - loss 0.16136143 - samples/sec: 17.39 - lr: 0.000004
2023-05-27 00:16:43,706 epoch 4 - iter 1870/3747 - loss 0.16319503 - samples/sec: 16.99 - lr: 0.000004
2023-05-27 00:18:09,731 epoch 4 - iter 2244/3747 - loss 0.16183379 - samples/sec: 17.40 - lr: 0.000004
2023-05-27 00:19:33,865 epoch 4 - iter 2618/3747 - loss 0.16015582 - samples/sec: 17.79 - lr: 0.000004
2023-05-27 00:20:56,744 epoch 4 - iter 2992/3747 - loss 0.15977658 - samples/sec: 18.06 - lr: 0.000003
2023-05-27 00:22:21,617 epoch 4 - iter 3366/3747 - loss 0.16086477 - samples/sec: 17.63 - lr: 0.000003
2023-05-27 00:23:47,826 epoch 4 - iter 3740/3747 - loss 0.16066424 - samples/sec: 17.36 - lr: 0.000003
2023-05-27 00:23:49,356 ----------------------------------------------------------------------------------------------------
2023-05-27 00:23:49,357 EPOCH 4 done: loss 0.1606 - lr 0.000003
2023-05-27 00:25:09,223 Evaluating as a multi-label problem: False
2023-05-27 00:25:09,281 DEV : loss 0.07103917002677917 - f1-score (micro avg)  0.9694
2023-05-27 00:25:09,374 BAD EPOCHS (no improvement): 4
2023-05-27 00:25:09,377 ----------------------------------------------------------------------------------------------------
2023-05-27 00:26:36,399 epoch 5 - iter 374/3747 - loss 0.15012424 - samples/sec: 17.20 - lr: 0.000003
2023-05-27 00:28:00,089 epoch 5 - iter 748/3747 - loss 0.15450242 - samples/sec: 17.88 - lr: 0.000003
2023-05-27 00:29:26,709 epoch 5 - iter 1122/3747 - loss 0.15462853 - samples/sec: 17.28 - lr: 0.000003
2023-05-27 00:30:53,760 epoch 5 - iter 1496/3747 - loss 0.15359397 - samples/sec: 17.19 - lr: 0.000003
2023-05-27 00:32:20,514 epoch 5 - iter 1870/3747 - loss 0.15450895 - samples/sec: 17.25 - lr: 0.000003
2023-05-27 00:33:45,773 epoch 5 - iter 2244/3747 - loss 0.15601495 - samples/sec: 17.55 - lr: 0.000003
2023-05-27 00:35:11,472 epoch 5 - iter 2618/3747 - loss 0.15600649 - samples/sec: 17.46 - lr: 0.000003
2023-05-27 00:36:35,988 epoch 5 - iter 2992/3747 - loss 0.15662713 - samples/sec: 17.71 - lr: 0.000003
2023-05-27 00:38:00,899 epoch 5 - iter 3366/3747 - loss 0.15629812 - samples/sec: 17.63 - lr: 0.000003
2023-05-27 00:39:26,481 epoch 5 - iter 3740/3747 - loss 0.15500337 - samples/sec: 17.49 - lr: 0.000003
2023-05-27 00:39:28,062 ----------------------------------------------------------------------------------------------------
2023-05-27 00:39:28,063 EPOCH 5 done: loss 0.1549 - lr 0.000003
2023-05-27 00:40:55,874 Evaluating as a multi-label problem: False
2023-05-27 00:40:55,934 DEV : loss 0.08256169408559799 - f1-score (micro avg)  0.967
2023-05-27 00:40:56,031 BAD EPOCHS (no improvement): 4
2023-05-27 00:40:56,034 ----------------------------------------------------------------------------------------------------
2023-05-27 00:42:23,811 epoch 6 - iter 374/3747 - loss 0.15314618 - samples/sec: 17.05 - lr: 0.000003
2023-05-27 00:43:50,609 epoch 6 - iter 748/3747 - loss 0.15136390 - samples/sec: 17.24 - lr: 0.000003
2023-05-27 00:45:19,001 epoch 6 - iter 1122/3747 - loss 0.15288004 - samples/sec: 16.93 - lr: 0.000003
2023-05-27 00:46:43,616 epoch 6 - iter 1496/3747 - loss 0.15396309 - samples/sec: 17.69 - lr: 0.000003
2023-05-27 00:48:08,443 epoch 6 - iter 1870/3747 - loss 0.15292065 - samples/sec: 17.64 - lr: 0.000003
2023-05-27 00:49:33,481 epoch 6 - iter 2244/3747 - loss 0.15131235 - samples/sec: 17.60 - lr: 0.000002
2023-05-27 00:51:00,583 epoch 6 - iter 2618/3747 - loss 0.15121721 - samples/sec: 17.18 - lr: 0.000002
2023-05-27 00:52:26,171 epoch 6 - iter 2992/3747 - loss 0.15003420 - samples/sec: 17.49 - lr: 0.000002
2023-05-27 00:53:51,209 epoch 6 - iter 3366/3747 - loss 0.15084455 - samples/sec: 17.60 - lr: 0.000002
2023-05-27 00:55:17,193 epoch 6 - iter 3740/3747 - loss 0.15111415 - samples/sec: 17.41 - lr: 0.000002
2023-05-27 00:55:18,880 ----------------------------------------------------------------------------------------------------
2023-05-27 00:55:18,880 EPOCH 6 done: loss 0.1511 - lr 0.000002
2023-05-27 00:56:42,232 Evaluating as a multi-label problem: False
2023-05-27 00:56:42,295 DEV : loss 0.08050725609064102 - f1-score (micro avg)  0.9663
2023-05-27 00:56:42,391 BAD EPOCHS (no improvement): 4
2023-05-27 00:56:42,394 ----------------------------------------------------------------------------------------------------
2023-05-27 00:58:10,536 epoch 7 - iter 374/3747 - loss 0.15031776 - samples/sec: 16.98 - lr: 0.000002
2023-05-27 00:59:35,680 epoch 7 - iter 748/3747 - loss 0.14834151 - samples/sec: 17.58 - lr: 0.000002
2023-05-27 01:01:01,839 epoch 7 - iter 1122/3747 - loss 0.14754350 - samples/sec: 17.37 - lr: 0.000002
2023-05-27 01:02:28,906 epoch 7 - iter 1496/3747 - loss 0.14723937 - samples/sec: 17.19 - lr: 0.000002
2023-05-27 01:03:55,661 epoch 7 - iter 1870/3747 - loss 0.14853583 - samples/sec: 17.25 - lr: 0.000002
2023-05-27 01:05:20,408 epoch 7 - iter 2244/3747 - loss 0.14789740 - samples/sec: 17.66 - lr: 0.000002
2023-05-27 01:06:46,028 epoch 7 - iter 2618/3747 - loss 0.14913737 - samples/sec: 17.48 - lr: 0.000002
2023-05-27 01:08:09,890 epoch 7 - iter 2992/3747 - loss 0.14790373 - samples/sec: 17.85 - lr: 0.000002
2023-05-27 01:09:39,791 epoch 7 - iter 3366/3747 - loss 0.14783044 - samples/sec: 16.65 - lr: 0.000002
2023-05-27 01:11:07,534 epoch 7 - iter 3740/3747 - loss 0.14843681 - samples/sec: 17.06 - lr: 0.000002
2023-05-27 01:11:09,229 ----------------------------------------------------------------------------------------------------
2023-05-27 01:11:09,229 EPOCH 7 done: loss 0.1484 - lr 0.000002
2023-05-27 01:12:28,705 Evaluating as a multi-label problem: False
2023-05-27 01:12:28,765 DEV : loss 0.08309949189424515 - f1-score (micro avg)  0.9668
2023-05-27 01:12:28,873 BAD EPOCHS (no improvement): 4
2023-05-27 01:12:28,876 ----------------------------------------------------------------------------------------------------
2023-05-27 01:13:56,977 epoch 8 - iter 374/3747 - loss 0.14986940 - samples/sec: 16.99 - lr: 0.000002
2023-05-27 01:15:25,483 epoch 8 - iter 748/3747 - loss 0.14447200 - samples/sec: 16.91 - lr: 0.000002
2023-05-27 01:16:52,254 epoch 8 - iter 1122/3747 - loss 0.14831567 - samples/sec: 17.25 - lr: 0.000002
2023-05-27 01:18:18,657 epoch 8 - iter 1496/3747 - loss 0.14623996 - samples/sec: 17.32 - lr: 0.000001
2023-05-27 01:19:44,451 epoch 8 - iter 1870/3747 - loss 0.14599067 - samples/sec: 17.44 - lr: 0.000001
2023-05-27 01:21:09,941 epoch 8 - iter 2244/3747 - loss 0.14591058 - samples/sec: 17.51 - lr: 0.000001
2023-05-27 01:22:35,762 epoch 8 - iter 2618/3747 - loss 0.14677421 - samples/sec: 17.44 - lr: 0.000001
2023-05-27 01:24:03,652 epoch 8 - iter 2992/3747 - loss 0.14704400 - samples/sec: 17.03 - lr: 0.000001
2023-05-27 01:25:29,618 epoch 8 - iter 3366/3747 - loss 0.14700958 - samples/sec: 17.41 - lr: 0.000001
2023-05-27 01:26:55,415 epoch 8 - iter 3740/3747 - loss 0.14699233 - samples/sec: 17.44 - lr: 0.000001
2023-05-27 01:26:57,052 ----------------------------------------------------------------------------------------------------
2023-05-27 01:26:57,052 EPOCH 8 done: loss 0.1470 - lr 0.000001
2023-05-27 01:28:23,700 Evaluating as a multi-label problem: False
2023-05-27 01:28:23,774 DEV : loss 0.08514127880334854 - f1-score (micro avg)  0.9651
2023-05-27 01:28:23,895 BAD EPOCHS (no improvement): 4
2023-05-27 01:28:23,898 ----------------------------------------------------------------------------------------------------
2023-05-27 01:29:52,105 epoch 9 - iter 374/3747 - loss 0.14599329 - samples/sec: 16.97 - lr: 0.000001
2023-05-27 01:31:18,389 epoch 9 - iter 748/3747 - loss 0.14065657 - samples/sec: 17.34 - lr: 0.000001
2023-05-27 01:32:43,496 epoch 9 - iter 1122/3747 - loss 0.14094608 - samples/sec: 17.58 - lr: 0.000001
2023-05-27 01:34:10,269 epoch 9 - iter 1496/3747 - loss 0.13983000 - samples/sec: 17.25 - lr: 0.000001
2023-05-27 01:35:37,075 epoch 9 - iter 1870/3747 - loss 0.14059439 - samples/sec: 17.24 - lr: 0.000001
2023-05-27 01:37:02,633 epoch 9 - iter 2244/3747 - loss 0.14280248 - samples/sec: 17.49 - lr: 0.000001
2023-05-27 01:38:31,155 epoch 9 - iter 2618/3747 - loss 0.14275492 - samples/sec: 16.91 - lr: 0.000001
2023-05-27 01:39:58,256 epoch 9 - iter 2992/3747 - loss 0.14262172 - samples/sec: 17.18 - lr: 0.000001
2023-05-27 01:41:25,103 epoch 9 - iter 3366/3747 - loss 0.14287036 - samples/sec: 17.23 - lr: 0.000001
2023-05-27 01:42:51,536 epoch 9 - iter 3740/3747 - loss 0.14296792 - samples/sec: 17.32 - lr: 0.000001
2023-05-27 01:42:53,169 ----------------------------------------------------------------------------------------------------
2023-05-27 01:42:53,169 EPOCH 9 done: loss 0.1430 - lr 0.000001
2023-05-27 01:44:14,981 Evaluating as a multi-label problem: False
2023-05-27 01:44:15,057 DEV : loss 0.08661068975925446 - f1-score (micro avg)  0.967
2023-05-27 01:44:15,163 BAD EPOCHS (no improvement): 4
2023-05-27 01:44:15,166 ----------------------------------------------------------------------------------------------------
2023-05-27 01:45:41,938 epoch 10 - iter 374/3747 - loss 0.14313826 - samples/sec: 17.25 - lr: 0.000001
2023-05-27 01:47:08,893 epoch 10 - iter 748/3747 - loss 0.14223357 - samples/sec: 17.21 - lr: 0.000000
2023-05-27 01:48:34,812 epoch 10 - iter 1122/3747 - loss 0.14352227 - samples/sec: 17.42 - lr: 0.000000
2023-05-27 01:50:00,660 epoch 10 - iter 1496/3747 - loss 0.14282430 - samples/sec: 17.43 - lr: 0.000000
2023-05-27 01:51:26,416 epoch 10 - iter 1870/3747 - loss 0.14164037 - samples/sec: 17.45 - lr: 0.000000
2023-05-27 01:52:51,843 epoch 10 - iter 2244/3747 - loss 0.14146265 - samples/sec: 17.52 - lr: 0.000000
2023-05-27 01:54:16,239 epoch 10 - iter 2618/3747 - loss 0.14166089 - samples/sec: 17.73 - lr: 0.000000
2023-05-27 01:55:41,348 epoch 10 - iter 2992/3747 - loss 0.14183158 - samples/sec: 17.58 - lr: 0.000000
2023-05-27 01:57:05,882 epoch 10 - iter 3366/3747 - loss 0.14142950 - samples/sec: 17.70 - lr: 0.000000
2023-05-27 01:58:31,428 epoch 10 - iter 3740/3747 - loss 0.14168270 - samples/sec: 17.49 - lr: 0.000000
2023-05-27 01:58:33,086 ----------------------------------------------------------------------------------------------------
2023-05-27 01:58:33,086 EPOCH 10 done: loss 0.1417 - lr 0.000000
2023-05-27 01:59:57,603 Evaluating as a multi-label problem: False
2023-05-27 01:59:57,666 DEV : loss 0.08561569452285767 - f1-score (micro avg)  0.9667
2023-05-27 01:59:57,766 BAD EPOCHS (no improvement): 4
2023-05-27 02:00:09,686 ----------------------------------------------------------------------------------------------------
2023-05-27 02:00:09,693 Testing using last state of model ...
2023-05-27 02:01:34,223 Evaluating as a multi-label problem: False
2023-05-27 02:01:34,289 0.9318	0.9441	0.9379	0.9068
2023-05-27 02:01:34,289 
Results:
- F-score (micro) 0.9379
- F-score (macro) 0.926
- Accuracy 0.9068

By class:
              precision    recall  f1-score   support

         ORG     0.9153    0.9368    0.9259      1661
         LOC     0.9497    0.9400    0.9449      1668
         PER     0.9815    0.9833    0.9824      1617
        MISC     0.8229    0.8803    0.8507       702

   micro avg     0.9318    0.9441    0.9379      5648
   macro avg     0.9174    0.9351    0.9260      5648
weighted avg     0.9329    0.9441    0.9383      5648

2023-05-27 02:01:34,289 ----------------------------------------------------------------------------------------------------
