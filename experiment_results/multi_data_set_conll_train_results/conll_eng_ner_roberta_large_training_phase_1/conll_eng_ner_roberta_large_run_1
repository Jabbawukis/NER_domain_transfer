2023-05-26 20:40:39,820 ----------------------------------------------------------------------------------------------------
2023-05-26 20:40:39,825 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-26 20:40:39,828 ----------------------------------------------------------------------------------------------------
2023-05-26 20:40:39,831 Corpus: "Corpus: 14987 train + 3466 dev + 3684 test sentences"
2023-05-26 20:40:39,832 ----------------------------------------------------------------------------------------------------
2023-05-26 20:40:39,832 Parameters:
2023-05-26 20:40:39,834  - learning_rate: "0.000005"
2023-05-26 20:40:39,836  - mini_batch_size: "4"
2023-05-26 20:40:39,836  - patience: "3"
2023-05-26 20:40:39,837  - anneal_factor: "0.5"
2023-05-26 20:40:39,838  - max_epochs: "10"
2023-05-26 20:40:39,838  - shuffle: "True"
2023-05-26 20:40:39,838  - train_with_dev: "False"
2023-05-26 20:40:39,838  - batch_growth_annealing: "False"
2023-05-26 20:40:39,838 ----------------------------------------------------------------------------------------------------
2023-05-26 20:40:39,838 Model training base path: "resources/taggers/conll_eng_ner_roberta_large_run_1"
2023-05-26 20:40:39,839 ----------------------------------------------------------------------------------------------------
2023-05-26 20:40:39,839 Device: cuda:0
2023-05-26 20:40:39,839 ----------------------------------------------------------------------------------------------------
2023-05-26 20:40:39,839 Embeddings storage mode: none
2023-05-26 20:40:39,840 ----------------------------------------------------------------------------------------------------
2023-05-26 20:42:06,712 epoch 1 - iter 374/3747 - loss 3.49243091 - samples/sec: 17.23 - lr: 0.000000
2023-05-26 20:43:37,328 epoch 1 - iter 748/3747 - loss 2.53669263 - samples/sec: 16.52 - lr: 0.000001
2023-05-26 20:45:05,053 epoch 1 - iter 1122/3747 - loss 1.97851140 - samples/sec: 17.06 - lr: 0.000001
2023-05-26 20:46:33,050 epoch 1 - iter 1496/3747 - loss 1.69536937 - samples/sec: 17.01 - lr: 0.000002
2023-05-26 20:48:00,920 epoch 1 - iter 1870/3747 - loss 1.44378228 - samples/sec: 17.03 - lr: 0.000002
2023-05-26 20:49:33,524 epoch 1 - iter 2244/3747 - loss 1.24212081 - samples/sec: 16.16 - lr: 0.000003
2023-05-26 20:51:02,799 epoch 1 - iter 2618/3747 - loss 1.08941346 - samples/sec: 16.77 - lr: 0.000003
2023-05-26 20:52:30,226 epoch 1 - iter 2992/3747 - loss 0.98504500 - samples/sec: 17.12 - lr: 0.000004
2023-05-26 20:53:57,801 epoch 1 - iter 3366/3747 - loss 0.89748811 - samples/sec: 17.09 - lr: 0.000004
2023-05-26 20:55:26,693 epoch 1 - iter 3740/3747 - loss 0.82002333 - samples/sec: 16.84 - lr: 0.000005
2023-05-26 20:55:28,158 ----------------------------------------------------------------------------------------------------
2023-05-26 20:55:28,158 EPOCH 1 done: loss 0.8192 - lr 0.000005
2023-05-26 20:56:52,769 Evaluating as a multi-label problem: False
2023-05-26 20:56:52,842 DEV : loss 0.11473648995161057 - f1-score (micro avg)  0.9047
2023-05-26 20:56:52,947 BAD EPOCHS (no improvement): 4
2023-05-26 20:56:52,950 ----------------------------------------------------------------------------------------------------
2023-05-26 20:58:20,130 epoch 2 - iter 374/3747 - loss 0.24882927 - samples/sec: 17.17 - lr: 0.000005
2023-05-26 20:59:48,680 epoch 2 - iter 748/3747 - loss 0.24607056 - samples/sec: 16.90 - lr: 0.000005
2023-05-26 21:01:16,732 epoch 2 - iter 1122/3747 - loss 0.24533995 - samples/sec: 17.00 - lr: 0.000005
2023-05-26 21:02:44,126 epoch 2 - iter 1496/3747 - loss 0.24445793 - samples/sec: 17.13 - lr: 0.000005
2023-05-26 21:04:12,577 epoch 2 - iter 1870/3747 - loss 0.24428539 - samples/sec: 16.92 - lr: 0.000005
2023-05-26 21:05:42,220 epoch 2 - iter 2244/3747 - loss 0.24376775 - samples/sec: 16.70 - lr: 0.000005
2023-05-26 21:07:14,135 epoch 2 - iter 2618/3747 - loss 0.24243126 - samples/sec: 16.28 - lr: 0.000005
2023-05-26 21:08:41,406 epoch 2 - iter 2992/3747 - loss 0.24142664 - samples/sec: 17.15 - lr: 0.000005
2023-05-26 21:10:08,728 epoch 2 - iter 3366/3747 - loss 0.23969475 - samples/sec: 17.14 - lr: 0.000005
2023-05-26 21:11:36,154 epoch 2 - iter 3740/3747 - loss 0.23804097 - samples/sec: 17.12 - lr: 0.000004
2023-05-26 21:11:37,643 ----------------------------------------------------------------------------------------------------
2023-05-26 21:11:37,643 EPOCH 2 done: loss 0.2381 - lr 0.000004
2023-05-26 21:13:01,567 Evaluating as a multi-label problem: False
2023-05-26 21:13:01,631 DEV : loss 0.08310405164957047 - f1-score (micro avg)  0.9491
2023-05-26 21:13:01,717 BAD EPOCHS (no improvement): 4
2023-05-26 21:13:01,722 ----------------------------------------------------------------------------------------------------
2023-05-26 21:14:29,736 epoch 3 - iter 374/3747 - loss 0.20333110 - samples/sec: 17.01 - lr: 0.000004
2023-05-26 21:15:57,367 epoch 3 - iter 748/3747 - loss 0.20261507 - samples/sec: 17.08 - lr: 0.000004
2023-05-26 21:17:25,294 epoch 3 - iter 1122/3747 - loss 0.20772769 - samples/sec: 17.02 - lr: 0.000004
2023-05-26 21:18:51,974 epoch 3 - iter 1496/3747 - loss 0.20785397 - samples/sec: 17.27 - lr: 0.000004
2023-05-26 21:20:19,037 epoch 3 - iter 1870/3747 - loss 0.20668113 - samples/sec: 17.19 - lr: 0.000004
2023-05-26 21:21:46,437 epoch 3 - iter 2244/3747 - loss 0.20650935 - samples/sec: 17.13 - lr: 0.000004
2023-05-26 21:23:15,248 epoch 3 - iter 2618/3747 - loss 0.20638383 - samples/sec: 16.85 - lr: 0.000004
2023-05-26 21:24:43,763 epoch 3 - iter 2992/3747 - loss 0.20619862 - samples/sec: 16.91 - lr: 0.000004
2023-05-26 21:26:12,039 epoch 3 - iter 3366/3747 - loss 0.20637943 - samples/sec: 16.96 - lr: 0.000004
2023-05-26 21:27:43,798 epoch 3 - iter 3740/3747 - loss 0.20666917 - samples/sec: 16.31 - lr: 0.000004
2023-05-26 21:27:45,411 ----------------------------------------------------------------------------------------------------
2023-05-26 21:27:45,412 EPOCH 3 done: loss 0.2064 - lr 0.000004
2023-05-26 21:29:05,256 Evaluating as a multi-label problem: False
2023-05-26 21:29:05,324 DEV : loss 0.0671711415052414 - f1-score (micro avg)  0.9592
2023-05-26 21:29:05,429 BAD EPOCHS (no improvement): 4
2023-05-26 21:29:05,433 ----------------------------------------------------------------------------------------------------
2023-05-26 21:30:34,237 epoch 4 - iter 374/3747 - loss 0.18449060 - samples/sec: 16.85 - lr: 0.000004
2023-05-26 21:32:01,878 epoch 4 - iter 748/3747 - loss 0.18642707 - samples/sec: 17.08 - lr: 0.000004
2023-05-26 21:33:29,350 epoch 4 - iter 1122/3747 - loss 0.18840436 - samples/sec: 17.11 - lr: 0.000004
2023-05-26 21:34:56,850 epoch 4 - iter 1496/3747 - loss 0.18894295 - samples/sec: 17.11 - lr: 0.000004
2023-05-26 21:36:24,647 epoch 4 - iter 1870/3747 - loss 0.18927997 - samples/sec: 17.05 - lr: 0.000004
2023-05-26 21:37:52,387 epoch 4 - iter 2244/3747 - loss 0.18900262 - samples/sec: 17.06 - lr: 0.000004
2023-05-26 21:39:19,826 epoch 4 - iter 2618/3747 - loss 0.18838689 - samples/sec: 17.12 - lr: 0.000004
2023-05-26 21:40:47,828 epoch 4 - iter 2992/3747 - loss 0.18826913 - samples/sec: 17.01 - lr: 0.000003
2023-05-26 21:42:16,866 epoch 4 - iter 3366/3747 - loss 0.18772519 - samples/sec: 16.81 - lr: 0.000003
2023-05-26 21:43:45,414 epoch 4 - iter 3740/3747 - loss 0.18818240 - samples/sec: 16.90 - lr: 0.000003
2023-05-26 21:43:46,937 ----------------------------------------------------------------------------------------------------
2023-05-26 21:43:46,937 EPOCH 4 done: loss 0.1882 - lr 0.000003
2023-05-26 21:45:08,760 Evaluating as a multi-label problem: False
2023-05-26 21:45:08,814 DEV : loss 0.05970513075590134 - f1-score (micro avg)  0.9615
2023-05-26 21:45:08,901 BAD EPOCHS (no improvement): 4
2023-05-26 21:45:08,905 ----------------------------------------------------------------------------------------------------
2023-05-26 21:46:36,340 epoch 5 - iter 374/3747 - loss 0.18843072 - samples/sec: 17.12 - lr: 0.000003
2023-05-26 21:48:04,757 epoch 5 - iter 748/3747 - loss 0.18293219 - samples/sec: 16.93 - lr: 0.000003
2023-05-26 21:49:33,140 epoch 5 - iter 1122/3747 - loss 0.17983941 - samples/sec: 16.93 - lr: 0.000003
2023-05-26 21:51:03,594 epoch 5 - iter 1496/3747 - loss 0.17864172 - samples/sec: 16.55 - lr: 0.000003
2023-05-26 21:52:29,633 epoch 5 - iter 1870/3747 - loss 0.18022668 - samples/sec: 17.40 - lr: 0.000003
2023-05-26 21:53:56,939 epoch 5 - iter 2244/3747 - loss 0.17891657 - samples/sec: 17.14 - lr: 0.000003
2023-05-26 21:55:25,899 epoch 5 - iter 2618/3747 - loss 0.17799726 - samples/sec: 16.82 - lr: 0.000003
2023-05-26 21:56:54,448 epoch 5 - iter 2992/3747 - loss 0.17797124 - samples/sec: 16.90 - lr: 0.000003
2023-05-26 21:58:21,913 epoch 5 - iter 3366/3747 - loss 0.17608430 - samples/sec: 17.11 - lr: 0.000003
2023-05-26 21:59:49,555 epoch 5 - iter 3740/3747 - loss 0.17684861 - samples/sec: 17.08 - lr: 0.000003
2023-05-26 21:59:51,066 ----------------------------------------------------------------------------------------------------
2023-05-26 21:59:51,066 EPOCH 5 done: loss 0.1767 - lr 0.000003
2023-05-26 22:01:14,504 Evaluating as a multi-label problem: False
2023-05-26 22:01:14,582 DEV : loss 0.05643339455127716 - f1-score (micro avg)  0.9634
2023-05-26 22:01:14,709 BAD EPOCHS (no improvement): 4
2023-05-26 22:01:14,713 ----------------------------------------------------------------------------------------------------
2023-05-26 22:02:43,796 epoch 6 - iter 374/3747 - loss 0.17160865 - samples/sec: 16.80 - lr: 0.000003
2023-05-26 22:04:12,943 epoch 6 - iter 748/3747 - loss 0.16867581 - samples/sec: 16.79 - lr: 0.000003
2023-05-26 22:05:40,399 epoch 6 - iter 1122/3747 - loss 0.16605864 - samples/sec: 17.11 - lr: 0.000003
2023-05-26 22:07:07,400 epoch 6 - iter 1496/3747 - loss 0.16602414 - samples/sec: 17.20 - lr: 0.000003
2023-05-26 22:08:33,894 epoch 6 - iter 1870/3747 - loss 0.16669095 - samples/sec: 17.30 - lr: 0.000003
2023-05-26 22:10:03,088 epoch 6 - iter 2244/3747 - loss 0.16730209 - samples/sec: 16.78 - lr: 0.000002
2023-05-26 22:11:29,309 epoch 6 - iter 2618/3747 - loss 0.16571848 - samples/sec: 17.36 - lr: 0.000002
2023-05-26 22:12:59,001 epoch 6 - iter 2992/3747 - loss 0.16692703 - samples/sec: 16.69 - lr: 0.000002
2023-05-26 22:14:26,326 epoch 6 - iter 3366/3747 - loss 0.16657134 - samples/sec: 17.14 - lr: 0.000002
2023-05-26 22:15:54,321 epoch 6 - iter 3740/3747 - loss 0.16635817 - samples/sec: 17.01 - lr: 0.000002
2023-05-26 22:15:55,916 ----------------------------------------------------------------------------------------------------
2023-05-26 22:15:55,916 EPOCH 6 done: loss 0.1663 - lr 0.000002
2023-05-26 22:17:18,706 Evaluating as a multi-label problem: False
2023-05-26 22:17:18,784 DEV : loss 0.05709008499979973 - f1-score (micro avg)  0.9685
2023-05-26 22:17:18,884 BAD EPOCHS (no improvement): 4
2023-05-26 22:17:18,887 ----------------------------------------------------------------------------------------------------
2023-05-26 22:18:47,955 epoch 7 - iter 374/3747 - loss 0.15568508 - samples/sec: 16.81 - lr: 0.000002
2023-05-26 22:20:13,718 epoch 7 - iter 748/3747 - loss 0.15889495 - samples/sec: 17.45 - lr: 0.000002
2023-05-26 22:21:40,721 epoch 7 - iter 1122/3747 - loss 0.16261395 - samples/sec: 17.20 - lr: 0.000002
2023-05-26 22:23:07,934 epoch 7 - iter 1496/3747 - loss 0.16277219 - samples/sec: 17.16 - lr: 0.000002
2023-05-26 22:24:35,856 epoch 7 - iter 1870/3747 - loss 0.16248096 - samples/sec: 17.02 - lr: 0.000002
2023-05-26 22:26:03,014 epoch 7 - iter 2244/3747 - loss 0.16216832 - samples/sec: 17.17 - lr: 0.000002
2023-05-26 22:27:30,154 epoch 7 - iter 2618/3747 - loss 0.16253630 - samples/sec: 17.18 - lr: 0.000002
2023-05-26 22:28:56,741 epoch 7 - iter 2992/3747 - loss 0.16368629 - samples/sec: 17.29 - lr: 0.000002
2023-05-26 22:30:25,370 epoch 7 - iter 3366/3747 - loss 0.16263066 - samples/sec: 16.89 - lr: 0.000002
2023-05-26 22:31:53,921 epoch 7 - iter 3740/3747 - loss 0.16301060 - samples/sec: 16.90 - lr: 0.000002
2023-05-26 22:31:55,440 ----------------------------------------------------------------------------------------------------
2023-05-26 22:31:55,440 EPOCH 7 done: loss 0.1629 - lr 0.000002
2023-05-26 22:33:18,445 Evaluating as a multi-label problem: False
2023-05-26 22:33:18,513 DEV : loss 0.06260348111391068 - f1-score (micro avg)  0.969
2023-05-26 22:33:18,626 BAD EPOCHS (no improvement): 4
2023-05-26 22:33:18,629 ----------------------------------------------------------------------------------------------------
2023-05-26 22:34:45,483 epoch 8 - iter 374/3747 - loss 0.15557970 - samples/sec: 17.23 - lr: 0.000002
2023-05-26 22:36:15,944 epoch 8 - iter 748/3747 - loss 0.15863460 - samples/sec: 16.55 - lr: 0.000002
2023-05-26 22:37:43,696 epoch 8 - iter 1122/3747 - loss 0.15996590 - samples/sec: 17.06 - lr: 0.000002
2023-05-26 22:39:10,669 epoch 8 - iter 1496/3747 - loss 0.16083444 - samples/sec: 17.21 - lr: 0.000001
2023-05-26 22:40:35,357 epoch 8 - iter 1870/3747 - loss 0.16253953 - samples/sec: 17.67 - lr: 0.000001
2023-05-26 22:42:01,986 epoch 8 - iter 2244/3747 - loss 0.16213291 - samples/sec: 17.28 - lr: 0.000001
2023-05-26 22:43:28,880 epoch 8 - iter 2618/3747 - loss 0.16121178 - samples/sec: 17.22 - lr: 0.000001
2023-05-26 22:44:56,614 epoch 8 - iter 2992/3747 - loss 0.16115691 - samples/sec: 17.06 - lr: 0.000001
2023-05-26 22:46:22,053 epoch 8 - iter 3366/3747 - loss 0.16095566 - samples/sec: 17.52 - lr: 0.000001
2023-05-26 22:47:48,248 epoch 8 - iter 3740/3747 - loss 0.15956541 - samples/sec: 17.36 - lr: 0.000001
2023-05-26 22:47:49,845 ----------------------------------------------------------------------------------------------------
2023-05-26 22:47:49,845 EPOCH 8 done: loss 0.1594 - lr 0.000001
2023-05-26 22:49:11,573 Evaluating as a multi-label problem: False
2023-05-26 22:49:11,637 DEV : loss 0.06351898610591888 - f1-score (micro avg)  0.9675
2023-05-26 22:49:11,749 BAD EPOCHS (no improvement): 4
2023-05-26 22:49:11,752 ----------------------------------------------------------------------------------------------------
2023-05-26 22:50:39,535 epoch 9 - iter 374/3747 - loss 0.15254607 - samples/sec: 17.05 - lr: 0.000001
2023-05-26 22:52:08,471 epoch 9 - iter 748/3747 - loss 0.15296681 - samples/sec: 16.83 - lr: 0.000001
2023-05-26 22:53:34,038 epoch 9 - iter 1122/3747 - loss 0.15247086 - samples/sec: 17.49 - lr: 0.000001
2023-05-26 22:54:59,198 epoch 9 - iter 1496/3747 - loss 0.15417607 - samples/sec: 17.58 - lr: 0.000001
2023-05-26 22:56:25,465 epoch 9 - iter 1870/3747 - loss 0.15332070 - samples/sec: 17.35 - lr: 0.000001
2023-05-26 22:57:56,156 epoch 9 - iter 2244/3747 - loss 0.15362930 - samples/sec: 16.50 - lr: 0.000001
2023-05-26 22:59:24,509 epoch 9 - iter 2618/3747 - loss 0.15239960 - samples/sec: 16.94 - lr: 0.000001
2023-05-26 23:00:51,254 epoch 9 - iter 2992/3747 - loss 0.15223393 - samples/sec: 17.25 - lr: 0.000001
2023-05-26 23:02:18,721 epoch 9 - iter 3366/3747 - loss 0.15257421 - samples/sec: 17.11 - lr: 0.000001
2023-05-26 23:03:44,403 epoch 9 - iter 3740/3747 - loss 0.15238505 - samples/sec: 17.47 - lr: 0.000001
2023-05-26 23:03:45,908 ----------------------------------------------------------------------------------------------------
2023-05-26 23:03:45,908 EPOCH 9 done: loss 0.1526 - lr 0.000001
2023-05-26 23:05:06,316 Evaluating as a multi-label problem: False
2023-05-26 23:05:06,372 DEV : loss 0.06582600623369217 - f1-score (micro avg)  0.9692
2023-05-26 23:05:06,468 BAD EPOCHS (no improvement): 4
2023-05-26 23:05:06,471 ----------------------------------------------------------------------------------------------------
2023-05-26 23:06:34,041 epoch 10 - iter 374/3747 - loss 0.16285128 - samples/sec: 17.09 - lr: 0.000001
2023-05-26 23:07:57,829 epoch 10 - iter 748/3747 - loss 0.15942198 - samples/sec: 17.86 - lr: 0.000000
2023-05-26 23:09:24,519 epoch 10 - iter 1122/3747 - loss 0.15707104 - samples/sec: 17.27 - lr: 0.000000
2023-05-26 23:10:49,886 epoch 10 - iter 1496/3747 - loss 0.15395050 - samples/sec: 17.53 - lr: 0.000000
2023-05-26 23:12:14,430 epoch 10 - iter 1870/3747 - loss 0.15390730 - samples/sec: 17.70 - lr: 0.000000
2023-05-26 23:13:41,142 epoch 10 - iter 2244/3747 - loss 0.15193734 - samples/sec: 17.26 - lr: 0.000000
2023-05-26 23:15:07,815 epoch 10 - iter 2618/3747 - loss 0.15149678 - samples/sec: 17.27 - lr: 0.000000
2023-05-26 23:16:32,371 epoch 10 - iter 2992/3747 - loss 0.15116989 - samples/sec: 17.70 - lr: 0.000000
2023-05-26 23:18:01,706 epoch 10 - iter 3366/3747 - loss 0.15068436 - samples/sec: 16.75 - lr: 0.000000
2023-05-26 23:19:26,658 epoch 10 - iter 3740/3747 - loss 0.15051217 - samples/sec: 17.62 - lr: 0.000000
2023-05-26 23:19:28,129 ----------------------------------------------------------------------------------------------------
2023-05-26 23:19:28,129 EPOCH 10 done: loss 0.1505 - lr 0.000000
2023-05-26 23:20:46,282 Evaluating as a multi-label problem: False
2023-05-26 23:20:46,348 DEV : loss 0.06317576766014099 - f1-score (micro avg)  0.9701
2023-05-26 23:20:46,473 BAD EPOCHS (no improvement): 4
2023-05-26 23:21:00,337 ----------------------------------------------------------------------------------------------------
2023-05-26 23:21:00,341 Testing using last state of model ...
2023-05-26 23:22:28,386 Evaluating as a multi-label problem: False
2023-05-26 23:22:28,450 0.9296	0.9439	0.9367	0.9051
2023-05-26 23:22:28,450 
Results:
- F-score (micro) 0.9367
- F-score (macro) 0.9224
- Accuracy 0.9051

By class:
              precision    recall  f1-score   support

         ORG     0.9119    0.9410    0.9262      1661
         LOC     0.9549    0.9400    0.9474      1668
         PER     0.9857    0.9827    0.9842      1617
        MISC     0.7966    0.8704    0.8319       702

   micro avg     0.9296    0.9439    0.9367      5648
   macro avg     0.9123    0.9335    0.9224      5648
weighted avg     0.9314    0.9439    0.9374      5648

2023-05-26 23:22:28,450 ----------------------------------------------------------------------------------------------------
