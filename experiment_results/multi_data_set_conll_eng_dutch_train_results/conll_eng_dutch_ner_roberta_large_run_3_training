2023-05-23 22:57:19,221 ----------------------------------------------------------------------------------------------------
2023-05-23 22:57:19,233 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-23 22:57:19,241 ----------------------------------------------------------------------------------------------------
2023-05-23 22:57:19,242 Corpus: "MultiCorpus: 3108 train + 644 dev + 900 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-23 22:57:19,242 ----------------------------------------------------------------------------------------------------
2023-05-23 22:57:19,242 Parameters:
2023-05-23 22:57:19,242  - learning_rate: "0.000005"
2023-05-23 22:57:19,242  - mini_batch_size: "4"
2023-05-23 22:57:19,242  - patience: "3"
2023-05-23 22:57:19,242  - anneal_factor: "0.5"
2023-05-23 22:57:19,242  - max_epochs: "10"
2023-05-23 22:57:19,243  - shuffle: "True"
2023-05-23 22:57:19,243  - train_with_dev: "False"
2023-05-23 22:57:19,243  - batch_growth_annealing: "False"
2023-05-23 22:57:19,243 ----------------------------------------------------------------------------------------------------
2023-05-23 22:57:19,243 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_3"
2023-05-23 22:57:19,243 ----------------------------------------------------------------------------------------------------
2023-05-23 22:57:19,243 Device: cuda:0
2023-05-23 22:57:19,243 ----------------------------------------------------------------------------------------------------
2023-05-23 22:57:19,243 Embeddings storage mode: none
2023-05-23 22:57:19,243 ----------------------------------------------------------------------------------------------------
2023-05-23 22:57:39,234 epoch 1 - iter 77/777 - loss 2.64144311 - samples/sec: 15.42 - lr: 0.000000
2023-05-23 22:57:59,044 epoch 1 - iter 154/777 - loss 2.01331860 - samples/sec: 15.56 - lr: 0.000001
2023-05-23 22:58:18,439 epoch 1 - iter 231/777 - loss 1.56169578 - samples/sec: 15.89 - lr: 0.000001
2023-05-23 22:58:38,095 epoch 1 - iter 308/777 - loss 1.23776876 - samples/sec: 15.68 - lr: 0.000002
2023-05-23 22:58:57,765 epoch 1 - iter 385/777 - loss 1.00868898 - samples/sec: 15.67 - lr: 0.000002
2023-05-23 22:59:16,821 epoch 1 - iter 462/777 - loss 0.88691470 - samples/sec: 16.17 - lr: 0.000003
2023-05-23 22:59:35,173 epoch 1 - iter 539/777 - loss 0.79062879 - samples/sec: 16.79 - lr: 0.000003
2023-05-23 22:59:54,102 epoch 1 - iter 616/777 - loss 0.71139922 - samples/sec: 16.28 - lr: 0.000004
2023-05-23 23:00:13,271 epoch 1 - iter 693/777 - loss 0.64162416 - samples/sec: 16.08 - lr: 0.000004
2023-05-23 23:00:32,397 epoch 1 - iter 770/777 - loss 0.58323330 - samples/sec: 16.11 - lr: 0.000005
2023-05-23 23:00:34,311 ----------------------------------------------------------------------------------------------------
2023-05-23 23:00:34,311 EPOCH 1 done: loss 0.5784 - lr 0.000005
2023-05-23 23:00:51,442 Evaluating as a multi-label problem: False
2023-05-23 23:00:51,456 DEV : loss 0.08823920786380768 - f1-score (micro avg)  0.9327
2023-05-23 23:00:51,480 BAD EPOCHS (no improvement): 4
2023-05-23 23:00:51,483 ----------------------------------------------------------------------------------------------------
2023-05-23 23:01:10,558 epoch 2 - iter 77/777 - loss 0.18845251 - samples/sec: 16.16 - lr: 0.000005
2023-05-23 23:01:30,119 epoch 2 - iter 154/777 - loss 0.17590758 - samples/sec: 15.76 - lr: 0.000005
2023-05-23 23:01:49,151 epoch 2 - iter 231/777 - loss 0.17661293 - samples/sec: 16.19 - lr: 0.000005
2023-05-23 23:02:08,438 epoch 2 - iter 308/777 - loss 0.18139912 - samples/sec: 15.98 - lr: 0.000005
2023-05-23 23:02:27,496 epoch 2 - iter 385/777 - loss 0.18104867 - samples/sec: 16.17 - lr: 0.000005
2023-05-23 23:02:46,671 epoch 2 - iter 462/777 - loss 0.18014694 - samples/sec: 16.07 - lr: 0.000005
2023-05-23 23:03:05,891 epoch 2 - iter 539/777 - loss 0.17724269 - samples/sec: 16.04 - lr: 0.000005
2023-05-23 23:03:24,699 epoch 2 - iter 616/777 - loss 0.17863700 - samples/sec: 16.39 - lr: 0.000005
2023-05-23 23:03:43,711 epoch 2 - iter 693/777 - loss 0.17882296 - samples/sec: 16.21 - lr: 0.000005
2023-05-23 23:04:02,900 epoch 2 - iter 770/777 - loss 0.17607358 - samples/sec: 16.06 - lr: 0.000004
2023-05-23 23:04:04,550 ----------------------------------------------------------------------------------------------------
2023-05-23 23:04:04,550 EPOCH 2 done: loss 0.1766 - lr 0.000004
2023-05-23 23:04:21,742 Evaluating as a multi-label problem: False
2023-05-23 23:04:21,757 DEV : loss 0.09996628016233444 - f1-score (micro avg)  0.9385
2023-05-23 23:04:21,784 BAD EPOCHS (no improvement): 4
2023-05-23 23:04:21,789 ----------------------------------------------------------------------------------------------------
2023-05-23 23:04:40,902 epoch 3 - iter 77/777 - loss 0.17325768 - samples/sec: 16.13 - lr: 0.000004
2023-05-23 23:04:59,609 epoch 3 - iter 154/777 - loss 0.17888089 - samples/sec: 16.47 - lr: 0.000004
2023-05-23 23:05:17,766 epoch 3 - iter 231/777 - loss 0.17726130 - samples/sec: 16.97 - lr: 0.000004
2023-05-23 23:05:36,927 epoch 3 - iter 308/777 - loss 0.17094521 - samples/sec: 16.08 - lr: 0.000004
2023-05-23 23:05:55,608 epoch 3 - iter 385/777 - loss 0.17390492 - samples/sec: 16.50 - lr: 0.000004
2023-05-23 23:06:14,525 epoch 3 - iter 462/777 - loss 0.17237208 - samples/sec: 16.29 - lr: 0.000004
2023-05-23 23:06:33,528 epoch 3 - iter 539/777 - loss 0.17367871 - samples/sec: 16.22 - lr: 0.000004
2023-05-23 23:06:52,207 epoch 3 - iter 616/777 - loss 0.17469127 - samples/sec: 16.50 - lr: 0.000004
2023-05-23 23:07:11,180 epoch 3 - iter 693/777 - loss 0.17362917 - samples/sec: 16.24 - lr: 0.000004
2023-05-23 23:07:29,791 epoch 3 - iter 770/777 - loss 0.17330872 - samples/sec: 16.56 - lr: 0.000004
2023-05-23 23:07:31,452 ----------------------------------------------------------------------------------------------------
2023-05-23 23:07:31,452 EPOCH 3 done: loss 0.1744 - lr 0.000004
2023-05-23 23:07:48,257 Evaluating as a multi-label problem: False
2023-05-23 23:07:48,272 DEV : loss 0.11090816557407379 - f1-score (micro avg)  0.9288
2023-05-23 23:07:48,298 BAD EPOCHS (no improvement): 4
2023-05-23 23:07:48,303 ----------------------------------------------------------------------------------------------------
2023-05-23 23:08:07,575 epoch 4 - iter 77/777 - loss 0.17365429 - samples/sec: 15.99 - lr: 0.000004
2023-05-23 23:08:26,572 epoch 4 - iter 154/777 - loss 0.15958042 - samples/sec: 16.22 - lr: 0.000004
2023-05-23 23:08:45,421 epoch 4 - iter 231/777 - loss 0.15763198 - samples/sec: 16.35 - lr: 0.000004
2023-05-23 23:09:04,640 epoch 4 - iter 308/777 - loss 0.16288357 - samples/sec: 16.04 - lr: 0.000004
2023-05-23 23:09:24,038 epoch 4 - iter 385/777 - loss 0.16323101 - samples/sec: 15.89 - lr: 0.000004
2023-05-23 23:09:42,846 epoch 4 - iter 462/777 - loss 0.16653384 - samples/sec: 16.39 - lr: 0.000004
2023-05-23 23:10:02,092 epoch 4 - iter 539/777 - loss 0.16735435 - samples/sec: 16.01 - lr: 0.000004
2023-05-23 23:10:20,353 epoch 4 - iter 616/777 - loss 0.16631436 - samples/sec: 16.88 - lr: 0.000003
2023-05-23 23:10:39,193 epoch 4 - iter 693/777 - loss 0.16932510 - samples/sec: 16.36 - lr: 0.000003
2023-05-23 23:10:57,988 epoch 4 - iter 770/777 - loss 0.16661463 - samples/sec: 16.40 - lr: 0.000003
2023-05-23 23:10:59,645 ----------------------------------------------------------------------------------------------------
2023-05-23 23:10:59,646 EPOCH 4 done: loss 0.1672 - lr 0.000003
2023-05-23 23:11:16,335 Evaluating as a multi-label problem: False
2023-05-23 23:11:16,352 DEV : loss 0.101168192923069 - f1-score (micro avg)  0.9368
2023-05-23 23:11:16,382 BAD EPOCHS (no improvement): 4
2023-05-23 23:11:16,390 ----------------------------------------------------------------------------------------------------
2023-05-23 23:11:35,637 epoch 5 - iter 77/777 - loss 0.17566172 - samples/sec: 16.02 - lr: 0.000003
2023-05-23 23:11:55,730 epoch 5 - iter 154/777 - loss 0.16948559 - samples/sec: 15.34 - lr: 0.000003
2023-05-23 23:12:15,864 epoch 5 - iter 231/777 - loss 0.16206725 - samples/sec: 15.31 - lr: 0.000003
2023-05-23 23:12:35,160 epoch 5 - iter 308/777 - loss 0.16183639 - samples/sec: 15.98 - lr: 0.000003
2023-05-23 23:12:54,032 epoch 5 - iter 385/777 - loss 0.16321932 - samples/sec: 16.33 - lr: 0.000003
2023-05-23 23:13:13,333 epoch 5 - iter 462/777 - loss 0.16236009 - samples/sec: 15.97 - lr: 0.000003
2023-05-23 23:13:32,311 epoch 5 - iter 539/777 - loss 0.16394868 - samples/sec: 16.24 - lr: 0.000003
2023-05-23 23:13:50,727 epoch 5 - iter 616/777 - loss 0.16452802 - samples/sec: 16.74 - lr: 0.000003
2023-05-23 23:14:09,559 epoch 5 - iter 693/777 - loss 0.16260121 - samples/sec: 16.37 - lr: 0.000003
2023-05-23 23:14:28,178 epoch 5 - iter 770/777 - loss 0.16328636 - samples/sec: 16.55 - lr: 0.000003
2023-05-23 23:14:29,921 ----------------------------------------------------------------------------------------------------
2023-05-23 23:14:29,921 EPOCH 5 done: loss 0.1630 - lr 0.000003
2023-05-23 23:14:54,034 Evaluating as a multi-label problem: False
2023-05-23 23:14:54,051 DEV : loss 0.08843720704317093 - f1-score (micro avg)  0.9501
2023-05-23 23:14:54,074 BAD EPOCHS (no improvement): 4
2023-05-23 23:14:54,076 ----------------------------------------------------------------------------------------------------
2023-05-23 23:15:13,517 epoch 6 - iter 77/777 - loss 0.16123239 - samples/sec: 15.85 - lr: 0.000003
2023-05-23 23:15:32,801 epoch 6 - iter 154/777 - loss 0.15958217 - samples/sec: 15.98 - lr: 0.000003
2023-05-23 23:15:52,163 epoch 6 - iter 231/777 - loss 0.16163202 - samples/sec: 15.92 - lr: 0.000003
2023-05-23 23:16:11,344 epoch 6 - iter 308/777 - loss 0.16365019 - samples/sec: 16.07 - lr: 0.000003
2023-05-23 23:16:30,604 epoch 6 - iter 385/777 - loss 0.16717801 - samples/sec: 16.00 - lr: 0.000003
2023-05-23 23:16:49,123 epoch 6 - iter 462/777 - loss 0.16901040 - samples/sec: 16.64 - lr: 0.000002
2023-05-23 23:17:07,732 epoch 6 - iter 539/777 - loss 0.16883139 - samples/sec: 16.56 - lr: 0.000002
2023-05-23 23:17:26,689 epoch 6 - iter 616/777 - loss 0.16356885 - samples/sec: 16.26 - lr: 0.000002
2023-05-23 23:17:45,116 epoch 6 - iter 693/777 - loss 0.16345881 - samples/sec: 16.73 - lr: 0.000002
2023-05-23 23:18:04,222 epoch 6 - iter 770/777 - loss 0.16236905 - samples/sec: 16.13 - lr: 0.000002
2023-05-23 23:18:05,912 ----------------------------------------------------------------------------------------------------
2023-05-23 23:18:05,913 EPOCH 6 done: loss 0.1623 - lr 0.000002
2023-05-23 23:18:23,287 Evaluating as a multi-label problem: False
2023-05-23 23:18:23,303 DEV : loss 0.09331826865673065 - f1-score (micro avg)  0.9456
2023-05-23 23:18:23,334 BAD EPOCHS (no improvement): 4
2023-05-23 23:18:23,337 ----------------------------------------------------------------------------------------------------
2023-05-23 23:18:42,024 epoch 7 - iter 77/777 - loss 0.16326417 - samples/sec: 16.49 - lr: 0.000002
2023-05-23 23:19:01,092 epoch 7 - iter 154/777 - loss 0.16279822 - samples/sec: 16.16 - lr: 0.000002
2023-05-23 23:19:19,871 epoch 7 - iter 231/777 - loss 0.16537407 - samples/sec: 16.41 - lr: 0.000002
2023-05-23 23:19:38,911 epoch 7 - iter 308/777 - loss 0.16374319 - samples/sec: 16.19 - lr: 0.000002
2023-05-23 23:19:58,401 epoch 7 - iter 385/777 - loss 0.16324830 - samples/sec: 15.81 - lr: 0.000002
2023-05-23 23:20:17,771 epoch 7 - iter 462/777 - loss 0.16189632 - samples/sec: 15.91 - lr: 0.000002
2023-05-23 23:20:36,739 epoch 7 - iter 539/777 - loss 0.16162048 - samples/sec: 16.25 - lr: 0.000002
2023-05-23 23:20:55,403 epoch 7 - iter 616/777 - loss 0.16360685 - samples/sec: 16.51 - lr: 0.000002
2023-05-23 23:21:14,340 epoch 7 - iter 693/777 - loss 0.16374935 - samples/sec: 16.27 - lr: 0.000002
2023-05-23 23:21:33,317 epoch 7 - iter 770/777 - loss 0.16181218 - samples/sec: 16.24 - lr: 0.000002
2023-05-23 23:21:34,890 ----------------------------------------------------------------------------------------------------
2023-05-23 23:21:34,890 EPOCH 7 done: loss 0.1615 - lr 0.000002
2023-05-23 23:21:52,210 Evaluating as a multi-label problem: False
2023-05-23 23:21:52,226 DEV : loss 0.08087900280952454 - f1-score (micro avg)  0.9522
2023-05-23 23:21:52,252 BAD EPOCHS (no improvement): 4
2023-05-23 23:21:52,255 ----------------------------------------------------------------------------------------------------
2023-05-23 23:22:11,191 epoch 8 - iter 77/777 - loss 0.14632138 - samples/sec: 16.28 - lr: 0.000002
2023-05-23 23:22:30,047 epoch 8 - iter 154/777 - loss 0.16807247 - samples/sec: 16.34 - lr: 0.000002
2023-05-23 23:22:48,749 epoch 8 - iter 231/777 - loss 0.16724474 - samples/sec: 16.48 - lr: 0.000002
2023-05-23 23:23:07,677 epoch 8 - iter 308/777 - loss 0.16630037 - samples/sec: 16.28 - lr: 0.000001
2023-05-23 23:23:26,529 epoch 8 - iter 385/777 - loss 0.16396919 - samples/sec: 16.35 - lr: 0.000001
2023-05-23 23:23:45,287 epoch 8 - iter 462/777 - loss 0.16367994 - samples/sec: 16.43 - lr: 0.000001
2023-05-23 23:24:03,808 epoch 8 - iter 539/777 - loss 0.15987644 - samples/sec: 16.64 - lr: 0.000001
2023-05-23 23:24:22,183 epoch 8 - iter 616/777 - loss 0.15831211 - samples/sec: 16.77 - lr: 0.000001
2023-05-23 23:24:40,850 epoch 8 - iter 693/777 - loss 0.15858258 - samples/sec: 16.51 - lr: 0.000001
2023-05-23 23:24:59,333 epoch 8 - iter 770/777 - loss 0.15667113 - samples/sec: 16.67 - lr: 0.000001
2023-05-23 23:25:01,014 ----------------------------------------------------------------------------------------------------
2023-05-23 23:25:01,014 EPOCH 8 done: loss 0.1574 - lr 0.000001
2023-05-23 23:25:17,706 Evaluating as a multi-label problem: False
2023-05-23 23:25:17,721 DEV : loss 0.0795927420258522 - f1-score (micro avg)  0.9514
2023-05-23 23:25:17,744 BAD EPOCHS (no improvement): 4
2023-05-23 23:25:17,746 ----------------------------------------------------------------------------------------------------
2023-05-23 23:25:36,547 epoch 9 - iter 77/777 - loss 0.14189342 - samples/sec: 16.39 - lr: 0.000001
2023-05-23 23:25:54,980 epoch 9 - iter 154/777 - loss 0.14119784 - samples/sec: 16.72 - lr: 0.000001
2023-05-23 23:26:13,524 epoch 9 - iter 231/777 - loss 0.14010777 - samples/sec: 16.62 - lr: 0.000001
2023-05-23 23:26:32,640 epoch 9 - iter 308/777 - loss 0.14452880 - samples/sec: 16.12 - lr: 0.000001
2023-05-23 23:26:51,762 epoch 9 - iter 385/777 - loss 0.14616816 - samples/sec: 16.12 - lr: 0.000001
2023-05-23 23:27:10,533 epoch 9 - iter 462/777 - loss 0.14780768 - samples/sec: 16.42 - lr: 0.000001
2023-05-23 23:27:29,895 epoch 9 - iter 539/777 - loss 0.14829287 - samples/sec: 15.92 - lr: 0.000001
2023-05-23 23:27:49,054 epoch 9 - iter 616/777 - loss 0.14759079 - samples/sec: 16.09 - lr: 0.000001
2023-05-23 23:28:08,133 epoch 9 - iter 693/777 - loss 0.14714080 - samples/sec: 16.15 - lr: 0.000001
2023-05-23 23:28:26,762 epoch 9 - iter 770/777 - loss 0.14653611 - samples/sec: 16.54 - lr: 0.000001
2023-05-23 23:28:28,365 ----------------------------------------------------------------------------------------------------
2023-05-23 23:28:28,365 EPOCH 9 done: loss 0.1461 - lr 0.000001
2023-05-23 23:28:44,979 Evaluating as a multi-label problem: False
2023-05-23 23:28:44,995 DEV : loss 0.08335167914628983 - f1-score (micro avg)  0.9507
2023-05-23 23:28:45,022 BAD EPOCHS (no improvement): 4
2023-05-23 23:28:45,025 ----------------------------------------------------------------------------------------------------
2023-05-23 23:29:03,677 epoch 10 - iter 77/777 - loss 0.15895901 - samples/sec: 16.53 - lr: 0.000001
2023-05-23 23:29:22,292 epoch 10 - iter 154/777 - loss 0.15767922 - samples/sec: 16.56 - lr: 0.000000
2023-05-23 23:29:41,474 epoch 10 - iter 231/777 - loss 0.15317967 - samples/sec: 16.07 - lr: 0.000000
2023-05-23 23:30:00,435 epoch 10 - iter 308/777 - loss 0.14927706 - samples/sec: 16.25 - lr: 0.000000
2023-05-23 23:30:18,744 epoch 10 - iter 385/777 - loss 0.14822683 - samples/sec: 16.83 - lr: 0.000000
2023-05-23 23:30:37,828 epoch 10 - iter 462/777 - loss 0.14766612 - samples/sec: 16.15 - lr: 0.000000
2023-05-23 23:30:56,678 epoch 10 - iter 539/777 - loss 0.14825291 - samples/sec: 16.35 - lr: 0.000000
2023-05-23 23:31:15,294 epoch 10 - iter 616/777 - loss 0.14670740 - samples/sec: 16.56 - lr: 0.000000
2023-05-23 23:31:34,021 epoch 10 - iter 693/777 - loss 0.14717195 - samples/sec: 16.46 - lr: 0.000000
2023-05-23 23:31:52,920 epoch 10 - iter 770/777 - loss 0.14741988 - samples/sec: 16.31 - lr: 0.000000
2023-05-23 23:31:54,560 ----------------------------------------------------------------------------------------------------
2023-05-23 23:31:54,560 EPOCH 10 done: loss 0.1471 - lr 0.000000
2023-05-23 23:32:11,003 Evaluating as a multi-label problem: False
2023-05-23 23:32:11,018 DEV : loss 0.08235803246498108 - f1-score (micro avg)  0.9526
2023-05-23 23:32:11,055 BAD EPOCHS (no improvement): 4
2023-05-23 23:32:24,342 ----------------------------------------------------------------------------------------------------
2023-05-23 23:32:24,345 Testing using last state of model ...
2023-05-23 23:32:56,425 Evaluating as a multi-label problem: False
2023-05-23 23:32:56,451 0.9208	0.9172	0.919	0.8794
2023-05-23 23:32:56,452 
Results:
- F-score (micro) 0.919
- F-score (macro) 0.9092
- Accuracy 0.8794

By class:
              precision    recall  f1-score   support

         PER     0.9647    0.9837    0.9741       306
         LOC     0.9496    0.9423    0.9459       260
         ORG     0.9170    0.8701    0.8929       254
        MISC     0.8128    0.8352    0.8238       182

   micro avg     0.9208    0.9172    0.9190      1002
   macro avg     0.9111    0.9078    0.9092      1002
weighted avg     0.9211    0.9172    0.9189      1002

2023-05-23 23:32:56,452 ----------------------------------------------------------------------------------------------------
2023-05-23 23:32:56,452 ----------------------------------------------------------------------------------------------------
2023-05-23 23:35:24,667 Evaluating as a multi-label problem: False
2023-05-23 23:35:24,718 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-23 23:35:24,718 0.9058	0.8881	0.8969	0.8651
2023-05-23 23:35:24,718 ----------------------------------------------------------------------------------------------------
2023-05-23 23:37:03,684 Evaluating as a multi-label problem: False
2023-05-23 23:37:03,751 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-23 23:37:03,751 0.9094	0.9262	0.9177	0.8775
