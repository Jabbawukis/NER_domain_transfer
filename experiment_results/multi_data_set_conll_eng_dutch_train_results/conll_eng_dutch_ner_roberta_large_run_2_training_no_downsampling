2023-05-25 15:28:26,728 ----------------------------------------------------------------------------------------------------
2023-05-25 15:28:26,734 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-25 15:28:26,738 ----------------------------------------------------------------------------------------------------
2023-05-25 15:28:26,739 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-25 15:28:26,739 ----------------------------------------------------------------------------------------------------
2023-05-25 15:28:26,739 Parameters:
2023-05-25 15:28:26,739  - learning_rate: "0.000005"
2023-05-25 15:28:26,739  - mini_batch_size: "4"
2023-05-25 15:28:26,739  - patience: "3"
2023-05-25 15:28:26,739  - anneal_factor: "0.5"
2023-05-25 15:28:26,739  - max_epochs: "10"
2023-05-25 15:28:26,739  - shuffle: "True"
2023-05-25 15:28:26,739  - train_with_dev: "False"
2023-05-25 15:28:26,739  - batch_growth_annealing: "False"
2023-05-25 15:28:26,739 ----------------------------------------------------------------------------------------------------
2023-05-25 15:28:26,740 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_2"
2023-05-25 15:28:26,740 ----------------------------------------------------------------------------------------------------
2023-05-25 15:28:26,740 Device: cuda:0
2023-05-25 15:28:26,740 ----------------------------------------------------------------------------------------------------
2023-05-25 15:28:26,740 Embeddings storage mode: none
2023-05-25 15:28:26,740 ----------------------------------------------------------------------------------------------------
2023-05-25 15:31:50,782 epoch 1 - iter 777/7770 - loss 1.97307426 - samples/sec: 15.24 - lr: 0.000001
2023-05-25 15:35:20,791 epoch 1 - iter 1554/7770 - loss 1.12230270 - samples/sec: 14.81 - lr: 0.000001
2023-05-25 15:38:41,606 epoch 1 - iter 2331/7770 - loss 0.83334008 - samples/sec: 15.48 - lr: 0.000002
2023-05-25 15:42:02,800 epoch 1 - iter 3108/7770 - loss 0.66149689 - samples/sec: 15.45 - lr: 0.000002
2023-05-25 15:45:21,340 epoch 1 - iter 3885/7770 - loss 0.55289508 - samples/sec: 15.66 - lr: 0.000003
2023-05-25 15:48:34,353 epoch 1 - iter 4662/7770 - loss 0.48316420 - samples/sec: 16.11 - lr: 0.000003
2023-05-25 15:51:41,226 epoch 1 - iter 5439/7770 - loss 0.44057999 - samples/sec: 16.64 - lr: 0.000003
2023-05-25 15:54:50,323 epoch 1 - iter 6216/7770 - loss 0.40359559 - samples/sec: 16.44 - lr: 0.000004
2023-05-25 15:58:01,377 epoch 1 - iter 6993/7770 - loss 0.37495894 - samples/sec: 16.28 - lr: 0.000005
2023-05-25 16:01:13,931 epoch 1 - iter 7770/7770 - loss 0.34937051 - samples/sec: 16.15 - lr: 0.000005
2023-05-25 16:01:13,935 ----------------------------------------------------------------------------------------------------
2023-05-25 16:01:13,935 EPOCH 1 done: loss 0.3494 - lr 0.000005
2023-05-25 16:04:24,543 Evaluating as a multi-label problem: False
2023-05-25 16:04:24,652 DEV : loss 0.07500580698251724 - f1-score (micro avg)  0.9607
2023-05-25 16:04:24,908 BAD EPOCHS (no improvement): 4
2023-05-25 16:04:24,914 ----------------------------------------------------------------------------------------------------
2023-05-25 16:07:44,594 epoch 2 - iter 777/7770 - loss 0.15598562 - samples/sec: 15.57 - lr: 0.000005
2023-05-25 16:11:00,945 epoch 2 - iter 1554/7770 - loss 0.15789138 - samples/sec: 15.84 - lr: 0.000005
2023-05-25 16:14:32,109 epoch 2 - iter 2331/7770 - loss 0.15707261 - samples/sec: 14.73 - lr: 0.000005
2023-05-25 16:18:18,255 epoch 2 - iter 3108/7770 - loss 0.15698363 - samples/sec: 13.75 - lr: 0.000005
2023-05-25 16:22:00,795 epoch 2 - iter 3885/7770 - loss 0.15924070 - samples/sec: 13.97 - lr: 0.000005
2023-05-25 16:25:57,966 epoch 2 - iter 4662/7770 - loss 0.15957146 - samples/sec: 13.11 - lr: 0.000005
2023-05-25 16:29:46,899 epoch 2 - iter 5439/7770 - loss 0.15938135 - samples/sec: 13.58 - lr: 0.000005
2023-05-25 16:33:32,057 epoch 2 - iter 6216/7770 - loss 0.15904705 - samples/sec: 13.81 - lr: 0.000005
2023-05-25 16:37:15,837 epoch 2 - iter 6993/7770 - loss 0.15816732 - samples/sec: 13.90 - lr: 0.000005
2023-05-25 16:41:00,001 epoch 2 - iter 7770/7770 - loss 0.15750485 - samples/sec: 13.87 - lr: 0.000004
2023-05-25 16:41:00,004 ----------------------------------------------------------------------------------------------------
2023-05-25 16:41:00,005 EPOCH 2 done: loss 0.1575 - lr 0.000004
2023-05-25 16:44:15,313 Evaluating as a multi-label problem: False
2023-05-25 16:44:15,418 DEV : loss 0.08231594413518906 - f1-score (micro avg)  0.9584
2023-05-25 16:44:15,669 BAD EPOCHS (no improvement): 4
2023-05-25 16:44:15,675 ----------------------------------------------------------------------------------------------------
2023-05-25 16:48:02,823 epoch 3 - iter 777/7770 - loss 0.14989362 - samples/sec: 13.69 - lr: 0.000004
2023-05-25 16:51:46,016 epoch 3 - iter 1554/7770 - loss 0.15264241 - samples/sec: 13.93 - lr: 0.000004
2023-05-25 16:55:26,594 epoch 3 - iter 2331/7770 - loss 0.15396424 - samples/sec: 14.10 - lr: 0.000004
2023-05-25 16:59:06,526 epoch 3 - iter 3108/7770 - loss 0.15365173 - samples/sec: 14.14 - lr: 0.000004
2023-05-25 17:02:44,866 epoch 3 - iter 3885/7770 - loss 0.15303457 - samples/sec: 14.24 - lr: 0.000004
2023-05-25 17:06:23,578 epoch 3 - iter 4662/7770 - loss 0.15392619 - samples/sec: 14.22 - lr: 0.000004
2023-05-25 17:10:01,415 epoch 3 - iter 5439/7770 - loss 0.15351411 - samples/sec: 14.27 - lr: 0.000004
2023-05-25 17:13:39,668 epoch 3 - iter 6216/7770 - loss 0.15387652 - samples/sec: 14.25 - lr: 0.000004
2023-05-25 17:17:26,837 epoch 3 - iter 6993/7770 - loss 0.15457629 - samples/sec: 13.69 - lr: 0.000004
2023-05-25 17:21:19,839 epoch 3 - iter 7770/7770 - loss 0.15410323 - samples/sec: 13.35 - lr: 0.000004
2023-05-25 17:21:19,844 ----------------------------------------------------------------------------------------------------
2023-05-25 17:21:19,844 EPOCH 3 done: loss 0.1541 - lr 0.000004
2023-05-25 17:24:35,723 Evaluating as a multi-label problem: False
2023-05-25 17:24:35,840 DEV : loss 0.07639921456575394 - f1-score (micro avg)  0.957
2023-05-25 17:24:36,093 BAD EPOCHS (no improvement): 4
2023-05-25 17:24:36,096 ----------------------------------------------------------------------------------------------------
2023-05-25 17:28:44,362 epoch 4 - iter 777/7770 - loss 0.14464323 - samples/sec: 12.52 - lr: 0.000004
2023-05-25 17:32:46,780 epoch 4 - iter 1554/7770 - loss 0.14529353 - samples/sec: 12.83 - lr: 0.000004
2023-05-25 17:36:48,711 epoch 4 - iter 2331/7770 - loss 0.14883635 - samples/sec: 12.85 - lr: 0.000004
2023-05-25 17:40:48,452 epoch 4 - iter 3108/7770 - loss 0.14789075 - samples/sec: 12.97 - lr: 0.000004
2023-05-25 17:44:47,160 epoch 4 - iter 3885/7770 - loss 0.14834864 - samples/sec: 13.03 - lr: 0.000004
2023-05-25 17:48:42,336 epoch 4 - iter 4662/7770 - loss 0.14930426 - samples/sec: 13.22 - lr: 0.000004
2023-05-25 17:52:37,498 epoch 4 - iter 5439/7770 - loss 0.14886455 - samples/sec: 13.22 - lr: 0.000004
2023-05-25 17:56:31,972 epoch 4 - iter 6216/7770 - loss 0.14889128 - samples/sec: 13.26 - lr: 0.000003
2023-05-25 18:00:27,955 epoch 4 - iter 6993/7770 - loss 0.14896477 - samples/sec: 13.18 - lr: 0.000003
2023-05-25 18:04:22,291 epoch 4 - iter 7770/7770 - loss 0.14939539 - samples/sec: 13.27 - lr: 0.000003
2023-05-25 18:04:22,295 ----------------------------------------------------------------------------------------------------
2023-05-25 18:04:22,295 EPOCH 4 done: loss 0.1494 - lr 0.000003
2023-05-25 18:07:54,407 Evaluating as a multi-label problem: False
2023-05-25 18:07:54,518 DEV : loss 0.08041971176862717 - f1-score (micro avg)  0.9594
2023-05-25 18:07:54,780 BAD EPOCHS (no improvement): 4
2023-05-25 18:07:54,784 ----------------------------------------------------------------------------------------------------
2023-05-25 18:11:56,305 epoch 5 - iter 777/7770 - loss 0.15342401 - samples/sec: 12.87 - lr: 0.000003
2023-05-25 18:15:57,583 epoch 5 - iter 1554/7770 - loss 0.15091546 - samples/sec: 12.89 - lr: 0.000003
2023-05-25 18:19:36,419 epoch 5 - iter 2331/7770 - loss 0.15153078 - samples/sec: 14.21 - lr: 0.000003
2023-05-25 18:22:55,719 epoch 5 - iter 3108/7770 - loss 0.15126151 - samples/sec: 15.60 - lr: 0.000003
2023-05-25 18:26:13,365 epoch 5 - iter 3885/7770 - loss 0.15083794 - samples/sec: 15.73 - lr: 0.000003
2023-05-25 18:29:42,776 epoch 5 - iter 4662/7770 - loss 0.14919909 - samples/sec: 14.85 - lr: 0.000003
2023-05-25 18:33:02,621 epoch 5 - iter 5439/7770 - loss 0.14854362 - samples/sec: 15.56 - lr: 0.000003
2023-05-25 18:36:22,377 epoch 5 - iter 6216/7770 - loss 0.14836739 - samples/sec: 15.57 - lr: 0.000003
2023-05-25 18:39:39,642 epoch 5 - iter 6993/7770 - loss 0.14796175 - samples/sec: 15.76 - lr: 0.000003
2023-05-25 18:43:12,278 epoch 5 - iter 7770/7770 - loss 0.14763840 - samples/sec: 14.62 - lr: 0.000003
2023-05-25 18:43:12,282 ----------------------------------------------------------------------------------------------------
2023-05-25 18:43:12,283 EPOCH 5 done: loss 0.1476 - lr 0.000003
2023-05-25 18:46:23,693 Evaluating as a multi-label problem: False
2023-05-25 18:46:23,800 DEV : loss 0.08128722012042999 - f1-score (micro avg)  0.9605
2023-05-25 18:46:24,070 BAD EPOCHS (no improvement): 4
2023-05-25 18:46:24,074 ----------------------------------------------------------------------------------------------------
2023-05-25 18:50:09,856 epoch 6 - iter 777/7770 - loss 0.14404206 - samples/sec: 13.77 - lr: 0.000003
2023-05-25 18:53:51,680 epoch 6 - iter 1554/7770 - loss 0.14321944 - samples/sec: 14.02 - lr: 0.000003
2023-05-25 18:57:32,683 epoch 6 - iter 2331/7770 - loss 0.14241473 - samples/sec: 14.07 - lr: 0.000003
2023-05-25 19:01:13,583 epoch 6 - iter 3108/7770 - loss 0.14075821 - samples/sec: 14.08 - lr: 0.000003
2023-05-25 19:04:54,898 epoch 6 - iter 3885/7770 - loss 0.14045056 - samples/sec: 14.05 - lr: 0.000003
2023-05-25 19:08:24,221 epoch 6 - iter 4662/7770 - loss 0.13988604 - samples/sec: 14.86 - lr: 0.000002
2023-05-25 19:11:38,091 epoch 6 - iter 5439/7770 - loss 0.14000343 - samples/sec: 16.04 - lr: 0.000002
2023-05-25 19:14:52,419 epoch 6 - iter 6216/7770 - loss 0.13983471 - samples/sec: 16.00 - lr: 0.000002
2023-05-25 19:18:05,532 epoch 6 - iter 6993/7770 - loss 0.14004743 - samples/sec: 16.10 - lr: 0.000002
2023-05-25 19:21:37,432 epoch 6 - iter 7770/7770 - loss 0.14013982 - samples/sec: 14.67 - lr: 0.000002
2023-05-25 19:21:37,436 ----------------------------------------------------------------------------------------------------
2023-05-25 19:21:37,436 EPOCH 6 done: loss 0.1401 - lr 0.000002
2023-05-25 19:24:38,106 Evaluating as a multi-label problem: False
2023-05-25 19:24:38,199 DEV : loss 0.0815788060426712 - f1-score (micro avg)  0.9604
2023-05-25 19:24:38,470 BAD EPOCHS (no improvement): 4
2023-05-25 19:24:38,473 ----------------------------------------------------------------------------------------------------
2023-05-25 19:27:56,287 epoch 7 - iter 777/7770 - loss 0.14292387 - samples/sec: 15.72 - lr: 0.000002
2023-05-25 19:31:22,464 epoch 7 - iter 1554/7770 - loss 0.14206712 - samples/sec: 15.08 - lr: 0.000002
2023-05-25 19:34:42,618 epoch 7 - iter 2331/7770 - loss 0.13986964 - samples/sec: 15.54 - lr: 0.000002
2023-05-25 19:38:00,827 epoch 7 - iter 3108/7770 - loss 0.13892990 - samples/sec: 15.69 - lr: 0.000002
2023-05-25 19:41:16,812 epoch 7 - iter 3885/7770 - loss 0.13838170 - samples/sec: 15.87 - lr: 0.000002
2023-05-25 19:44:32,275 epoch 7 - iter 4662/7770 - loss 0.13863918 - samples/sec: 15.91 - lr: 0.000002
2023-05-25 19:47:46,487 epoch 7 - iter 5439/7770 - loss 0.13899296 - samples/sec: 16.01 - lr: 0.000002
2023-05-25 19:51:00,966 epoch 7 - iter 6216/7770 - loss 0.13916783 - samples/sec: 15.99 - lr: 0.000002
2023-05-25 19:54:15,313 epoch 7 - iter 6993/7770 - loss 0.13899017 - samples/sec: 16.00 - lr: 0.000002
2023-05-25 19:57:29,519 epoch 7 - iter 7770/7770 - loss 0.13867446 - samples/sec: 16.01 - lr: 0.000002
2023-05-25 19:57:29,530 ----------------------------------------------------------------------------------------------------
2023-05-25 19:57:29,530 EPOCH 7 done: loss 0.1387 - lr 0.000002
2023-05-25 20:00:43,946 Evaluating as a multi-label problem: False
2023-05-25 20:00:44,052 DEV : loss 0.08476084470748901 - f1-score (micro avg)  0.9617
2023-05-25 20:00:44,333 BAD EPOCHS (no improvement): 4
2023-05-25 20:00:44,336 ----------------------------------------------------------------------------------------------------
2023-05-25 20:04:02,597 epoch 8 - iter 777/7770 - loss 0.13875799 - samples/sec: 15.69 - lr: 0.000002
2023-05-25 20:07:20,831 epoch 8 - iter 1554/7770 - loss 0.13914645 - samples/sec: 15.69 - lr: 0.000002
2023-05-25 20:10:38,970 epoch 8 - iter 2331/7770 - loss 0.13909049 - samples/sec: 15.69 - lr: 0.000002
2023-05-25 20:13:56,524 epoch 8 - iter 3108/7770 - loss 0.13822522 - samples/sec: 15.74 - lr: 0.000001
2023-05-25 20:17:12,391 epoch 8 - iter 3885/7770 - loss 0.13755605 - samples/sec: 15.88 - lr: 0.000001
2023-05-25 20:20:39,138 epoch 8 - iter 4662/7770 - loss 0.13714404 - samples/sec: 15.04 - lr: 0.000001
2023-05-25 20:23:55,873 epoch 8 - iter 5439/7770 - loss 0.13626224 - samples/sec: 15.81 - lr: 0.000001
2023-05-25 20:27:12,117 epoch 8 - iter 6216/7770 - loss 0.13615818 - samples/sec: 15.85 - lr: 0.000001
2023-05-25 20:30:29,619 epoch 8 - iter 6993/7770 - loss 0.13588878 - samples/sec: 15.74 - lr: 0.000001
2023-05-25 20:33:45,074 epoch 8 - iter 7770/7770 - loss 0.13587890 - samples/sec: 15.91 - lr: 0.000001
2023-05-25 20:33:45,078 ----------------------------------------------------------------------------------------------------
2023-05-25 20:33:45,078 EPOCH 8 done: loss 0.1359 - lr 0.000001
2023-05-25 20:36:43,898 Evaluating as a multi-label problem: False
2023-05-25 20:36:44,008 DEV : loss 0.08129655569791794 - f1-score (micro avg)  0.9616
2023-05-25 20:36:44,288 BAD EPOCHS (no improvement): 4
2023-05-25 20:36:44,291 ----------------------------------------------------------------------------------------------------
2023-05-25 20:40:01,641 epoch 9 - iter 777/7770 - loss 0.13799539 - samples/sec: 15.76 - lr: 0.000001
2023-05-25 20:43:19,204 epoch 9 - iter 1554/7770 - loss 0.13578091 - samples/sec: 15.74 - lr: 0.000001
2023-05-25 20:46:33,752 epoch 9 - iter 2331/7770 - loss 0.13580414 - samples/sec: 15.98 - lr: 0.000001
2023-05-25 20:49:50,396 epoch 9 - iter 3108/7770 - loss 0.13669343 - samples/sec: 15.81 - lr: 0.000001
2023-05-25 20:53:04,047 epoch 9 - iter 3885/7770 - loss 0.13530715 - samples/sec: 16.06 - lr: 0.000001
2023-05-25 20:56:18,478 epoch 9 - iter 4662/7770 - loss 0.13528238 - samples/sec: 15.99 - lr: 0.000001
2023-05-25 20:59:36,279 epoch 9 - iter 5439/7770 - loss 0.13555484 - samples/sec: 15.72 - lr: 0.000001
2023-05-25 21:02:51,629 epoch 9 - iter 6216/7770 - loss 0.13574583 - samples/sec: 15.92 - lr: 0.000001
2023-05-25 21:06:06,629 epoch 9 - iter 6993/7770 - loss 0.13553083 - samples/sec: 15.95 - lr: 0.000001
2023-05-25 21:09:36,353 epoch 9 - iter 7770/7770 - loss 0.13533302 - samples/sec: 14.83 - lr: 0.000001
2023-05-25 21:09:36,358 ----------------------------------------------------------------------------------------------------
2023-05-25 21:09:36,359 EPOCH 9 done: loss 0.1353 - lr 0.000001
2023-05-25 21:12:42,628 Evaluating as a multi-label problem: False
2023-05-25 21:12:42,732 DEV : loss 0.08072510361671448 - f1-score (micro avg)  0.9631
2023-05-25 21:12:42,978 BAD EPOCHS (no improvement): 4
2023-05-25 21:12:43,440 ----------------------------------------------------------------------------------------------------
2023-05-25 21:16:03,136 epoch 10 - iter 777/7770 - loss 0.12911515 - samples/sec: 15.57 - lr: 0.000001
2023-05-25 21:19:31,168 epoch 10 - iter 1554/7770 - loss 0.12935615 - samples/sec: 14.95 - lr: 0.000000
2023-05-25 21:23:11,498 epoch 10 - iter 2331/7770 - loss 0.13207913 - samples/sec: 14.11 - lr: 0.000000
2023-05-25 21:26:30,285 epoch 10 - iter 3108/7770 - loss 0.13299263 - samples/sec: 15.64 - lr: 0.000000
2023-05-25 21:29:48,784 epoch 10 - iter 3885/7770 - loss 0.13385425 - samples/sec: 15.67 - lr: 0.000000
2023-05-25 21:33:04,636 epoch 10 - iter 4662/7770 - loss 0.13329409 - samples/sec: 15.88 - lr: 0.000000
2023-05-25 21:36:19,385 epoch 10 - iter 5439/7770 - loss 0.13273748 - samples/sec: 15.97 - lr: 0.000000
2023-05-25 21:39:34,776 epoch 10 - iter 6216/7770 - loss 0.13262028 - samples/sec: 15.91 - lr: 0.000000
2023-05-25 21:42:49,168 epoch 10 - iter 6993/7770 - loss 0.13273686 - samples/sec: 16.00 - lr: 0.000000
2023-05-25 21:46:04,019 epoch 10 - iter 7770/7770 - loss 0.13297595 - samples/sec: 15.96 - lr: 0.000000
2023-05-25 21:46:04,023 ----------------------------------------------------------------------------------------------------
2023-05-25 21:46:04,023 EPOCH 10 done: loss 0.1330 - lr 0.000000
2023-05-25 21:49:16,227 Evaluating as a multi-label problem: False
2023-05-25 21:49:16,350 DEV : loss 0.08080463111400604 - f1-score (micro avg)  0.964
2023-05-25 21:49:16,627 BAD EPOCHS (no improvement): 4
2023-05-25 21:49:29,667 ----------------------------------------------------------------------------------------------------
2023-05-25 21:49:29,672 Testing using last state of model ...
2023-05-25 21:53:59,570 Evaluating as a multi-label problem: False
2023-05-25 21:53:59,691 0.9342	0.9443	0.9392	0.9137
2023-05-25 21:53:59,692 
Results:
- F-score (micro) 0.9392
- F-score (macro) 0.9364
- Accuracy 0.9137

By class:
              precision    recall  f1-score   support

         PER     0.9769    0.9812    0.9791      2715
         ORG     0.8920    0.9453    0.9179      2543
         LOC     0.9549    0.9369    0.9458      2442
        MISC     0.9061    0.8994    0.9028      1889

   micro avg     0.9342    0.9443    0.9392      9589
   macro avg     0.9325    0.9407    0.9364      9589
weighted avg     0.9349    0.9443    0.9394      9589

2023-05-25 21:53:59,692 ----------------------------------------------------------------------------------------------------
2023-05-25 21:53:59,693 ----------------------------------------------------------------------------------------------------
2023-05-25 21:56:45,407 Evaluating as a multi-label problem: False
2023-05-25 21:56:45,458 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-25 21:56:45,458 0.9421	0.9411	0.9416	0.9252
2023-05-25 21:56:45,459 ----------------------------------------------------------------------------------------------------
2023-05-25 21:58:28,194 Evaluating as a multi-label problem: False
2023-05-25 21:58:28,265 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-25 21:58:28,266 0.9288	0.9465	0.9376	0.9059
