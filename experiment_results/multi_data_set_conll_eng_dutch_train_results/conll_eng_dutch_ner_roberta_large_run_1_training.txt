2023-05-23 21:38:03,558 ----------------------------------------------------------------------------------------------------
2023-05-23 21:38:03,562 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-23 21:38:03,567 ----------------------------------------------------------------------------------------------------
2023-05-23 21:38:03,568 Corpus: "MultiCorpus: 3108 train + 644 dev + 900 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-23 21:38:03,570 ----------------------------------------------------------------------------------------------------
2023-05-23 21:38:03,570 Parameters:
2023-05-23 21:38:03,570  - learning_rate: "0.000005"
2023-05-23 21:38:03,570  - mini_batch_size: "4"
2023-05-23 21:38:03,570  - patience: "3"
2023-05-23 21:38:03,570  - anneal_factor: "0.5"
2023-05-23 21:38:03,570  - max_epochs: "10"
2023-05-23 21:38:03,570  - shuffle: "True"
2023-05-23 21:38:03,570  - train_with_dev: "False"
2023-05-23 21:38:03,570  - batch_growth_annealing: "False"
2023-05-23 21:38:03,570 ----------------------------------------------------------------------------------------------------
2023-05-23 21:38:03,571 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_1"
2023-05-23 21:38:03,571 ----------------------------------------------------------------------------------------------------
2023-05-23 21:38:03,571 Device: cuda:0
2023-05-23 21:38:03,571 ----------------------------------------------------------------------------------------------------
2023-05-23 21:38:03,571 Embeddings storage mode: none
2023-05-23 21:38:03,571 ----------------------------------------------------------------------------------------------------
2023-05-23 21:38:23,019 epoch 1 - iter 77/777 - loss 3.43187537 - samples/sec: 15.85 - lr: 0.000000
2023-05-23 21:38:41,671 epoch 1 - iter 154/777 - loss 3.30254074 - samples/sec: 16.52 - lr: 0.000001
2023-05-23 21:39:00,339 epoch 1 - iter 231/777 - loss 2.93192415 - samples/sec: 16.51 - lr: 0.000001
2023-05-23 21:39:19,909 epoch 1 - iter 308/777 - loss 2.36850896 - samples/sec: 15.75 - lr: 0.000002
2023-05-23 21:39:39,555 epoch 1 - iter 385/777 - loss 1.94667183 - samples/sec: 15.69 - lr: 0.000002
2023-05-23 21:39:58,289 epoch 1 - iter 462/777 - loss 1.77198129 - samples/sec: 16.45 - lr: 0.000003
2023-05-23 21:40:16,912 epoch 1 - iter 539/777 - loss 1.62143447 - samples/sec: 16.55 - lr: 0.000003
2023-05-23 21:40:36,373 epoch 1 - iter 616/777 - loss 1.50217413 - samples/sec: 15.84 - lr: 0.000004
2023-05-23 21:40:55,939 epoch 1 - iter 693/777 - loss 1.37925680 - samples/sec: 15.75 - lr: 0.000004
2023-05-23 21:41:14,466 epoch 1 - iter 770/777 - loss 1.27409647 - samples/sec: 16.63 - lr: 0.000005
2023-05-23 21:41:16,169 ----------------------------------------------------------------------------------------------------
2023-05-23 21:41:16,169 EPOCH 1 done: loss 1.2679 - lr 0.000005
2023-05-23 21:41:32,505 Evaluating as a multi-label problem: False
2023-05-23 21:41:32,524 DEV : loss 0.24333778023719788 - f1-score (micro avg)  0.5579
2023-05-23 21:41:32,546 BAD EPOCHS (no improvement): 4
2023-05-23 21:41:32,549 ----------------------------------------------------------------------------------------------------
2023-05-23 21:41:51,705 epoch 2 - iter 77/777 - loss 0.42355762 - samples/sec: 16.09 - lr: 0.000005
2023-05-23 21:42:10,811 epoch 2 - iter 154/777 - loss 0.41142824 - samples/sec: 16.13 - lr: 0.000005
2023-05-23 21:42:30,367 epoch 2 - iter 231/777 - loss 0.39106682 - samples/sec: 15.76 - lr: 0.000005
2023-05-23 21:42:49,865 epoch 2 - iter 308/777 - loss 0.38186497 - samples/sec: 15.81 - lr: 0.000005
2023-05-23 21:43:08,895 epoch 2 - iter 385/777 - loss 0.37959266 - samples/sec: 16.20 - lr: 0.000005
2023-05-23 21:43:28,176 epoch 2 - iter 462/777 - loss 0.36302103 - samples/sec: 15.98 - lr: 0.000005
2023-05-23 21:43:47,225 epoch 2 - iter 539/777 - loss 0.35326547 - samples/sec: 16.18 - lr: 0.000005
2023-05-23 21:44:06,514 epoch 2 - iter 616/777 - loss 0.34511384 - samples/sec: 15.98 - lr: 0.000005
2023-05-23 21:44:25,815 epoch 2 - iter 693/777 - loss 0.33861061 - samples/sec: 15.97 - lr: 0.000005
2023-05-23 21:44:44,765 epoch 2 - iter 770/777 - loss 0.33838489 - samples/sec: 16.26 - lr: 0.000004
2023-05-23 21:44:46,413 ----------------------------------------------------------------------------------------------------
2023-05-23 21:44:46,413 EPOCH 2 done: loss 0.3377 - lr 0.000004
2023-05-23 21:45:01,903 Evaluating as a multi-label problem: False
2023-05-23 21:45:01,921 DEV : loss 0.11125160753726959 - f1-score (micro avg)  0.8256
2023-05-23 21:45:01,941 BAD EPOCHS (no improvement): 4
2023-05-23 21:45:01,944 ----------------------------------------------------------------------------------------------------
2023-05-23 21:45:21,546 epoch 3 - iter 77/777 - loss 0.26079159 - samples/sec: 15.72 - lr: 0.000004
2023-05-23 21:45:40,711 epoch 3 - iter 154/777 - loss 0.24728633 - samples/sec: 16.08 - lr: 0.000004
2023-05-23 21:45:59,743 epoch 3 - iter 231/777 - loss 0.24333817 - samples/sec: 16.20 - lr: 0.000004
2023-05-23 21:46:18,465 epoch 3 - iter 308/777 - loss 0.23631306 - samples/sec: 16.46 - lr: 0.000004
2023-05-23 21:46:37,577 epoch 3 - iter 385/777 - loss 0.23703321 - samples/sec: 16.13 - lr: 0.000004
2023-05-23 21:46:57,122 epoch 3 - iter 462/777 - loss 0.24404077 - samples/sec: 15.77 - lr: 0.000004
2023-05-23 21:47:17,122 epoch 3 - iter 539/777 - loss 0.24696469 - samples/sec: 15.41 - lr: 0.000004
2023-05-23 21:47:36,978 epoch 3 - iter 616/777 - loss 0.24716679 - samples/sec: 15.52 - lr: 0.000004
2023-05-23 21:48:01,187 epoch 3 - iter 693/777 - loss 0.24805839 - samples/sec: 12.73 - lr: 0.000004
2023-05-23 21:48:20,815 epoch 3 - iter 770/777 - loss 0.24811523 - samples/sec: 15.71 - lr: 0.000004
2023-05-23 21:48:22,614 ----------------------------------------------------------------------------------------------------
2023-05-23 21:48:22,615 EPOCH 3 done: loss 0.2479 - lr 0.000004
2023-05-23 21:48:39,813 Evaluating as a multi-label problem: False
2023-05-23 21:48:39,829 DEV : loss 0.1044292226433754 - f1-score (micro avg)  0.8953
2023-05-23 21:48:39,859 BAD EPOCHS (no improvement): 4
2023-05-23 21:48:39,862 ----------------------------------------------------------------------------------------------------
2023-05-23 21:48:58,964 epoch 4 - iter 77/777 - loss 0.21748115 - samples/sec: 16.14 - lr: 0.000004
2023-05-23 21:49:18,554 epoch 4 - iter 154/777 - loss 0.22646472 - samples/sec: 15.73 - lr: 0.000004
2023-05-23 21:49:37,672 epoch 4 - iter 231/777 - loss 0.20735810 - samples/sec: 16.12 - lr: 0.000004
2023-05-23 21:49:57,264 epoch 4 - iter 308/777 - loss 0.21399194 - samples/sec: 15.73 - lr: 0.000004
2023-05-23 21:50:16,763 epoch 4 - iter 385/777 - loss 0.21086822 - samples/sec: 15.81 - lr: 0.000004
2023-05-23 21:50:36,690 epoch 4 - iter 462/777 - loss 0.21743428 - samples/sec: 15.47 - lr: 0.000004
2023-05-23 21:50:56,583 epoch 4 - iter 539/777 - loss 0.21530441 - samples/sec: 15.49 - lr: 0.000004
2023-05-23 21:51:15,902 epoch 4 - iter 616/777 - loss 0.21882488 - samples/sec: 15.95 - lr: 0.000003
2023-05-23 21:51:35,548 epoch 4 - iter 693/777 - loss 0.22209883 - samples/sec: 15.69 - lr: 0.000003
2023-05-23 21:51:55,771 epoch 4 - iter 770/777 - loss 0.22398481 - samples/sec: 15.24 - lr: 0.000003
2023-05-23 21:51:57,618 ----------------------------------------------------------------------------------------------------
2023-05-23 21:51:57,618 EPOCH 4 done: loss 0.2246 - lr 0.000003
2023-05-23 21:52:14,608 Evaluating as a multi-label problem: False
2023-05-23 21:52:14,624 DEV : loss 0.09473726898431778 - f1-score (micro avg)  0.8973
2023-05-23 21:52:14,656 BAD EPOCHS (no improvement): 4
2023-05-23 21:52:14,658 ----------------------------------------------------------------------------------------------------
2023-05-23 21:52:33,737 epoch 5 - iter 77/777 - loss 0.23490906 - samples/sec: 16.16 - lr: 0.000003
2023-05-23 21:52:53,848 epoch 5 - iter 154/777 - loss 0.21995296 - samples/sec: 15.33 - lr: 0.000003
2023-05-23 21:53:13,567 epoch 5 - iter 231/777 - loss 0.21909069 - samples/sec: 15.63 - lr: 0.000003
2023-05-23 21:53:33,840 epoch 5 - iter 308/777 - loss 0.21675941 - samples/sec: 15.21 - lr: 0.000003
2023-05-23 21:53:53,298 epoch 5 - iter 385/777 - loss 0.21566908 - samples/sec: 15.84 - lr: 0.000003
2023-05-23 21:54:13,454 epoch 5 - iter 462/777 - loss 0.21865995 - samples/sec: 15.29 - lr: 0.000003
2023-05-23 21:54:33,766 epoch 5 - iter 539/777 - loss 0.21668647 - samples/sec: 15.18 - lr: 0.000003
2023-05-23 21:54:54,013 epoch 5 - iter 616/777 - loss 0.21669836 - samples/sec: 15.22 - lr: 0.000003
2023-05-23 21:55:13,472 epoch 5 - iter 693/777 - loss 0.21559603 - samples/sec: 15.84 - lr: 0.000003
2023-05-23 21:55:32,160 epoch 5 - iter 770/777 - loss 0.21609598 - samples/sec: 16.49 - lr: 0.000003
2023-05-23 21:55:33,916 ----------------------------------------------------------------------------------------------------
2023-05-23 21:55:33,917 EPOCH 5 done: loss 0.2158 - lr 0.000003
2023-05-23 21:55:50,616 Evaluating as a multi-label problem: False
2023-05-23 21:55:50,631 DEV : loss 0.09257195144891739 - f1-score (micro avg)  0.9257
2023-05-23 21:55:50,655 BAD EPOCHS (no improvement): 4
2023-05-23 21:55:50,657 ----------------------------------------------------------------------------------------------------
2023-05-23 21:56:09,866 epoch 6 - iter 77/777 - loss 0.19653798 - samples/sec: 16.05 - lr: 0.000003
2023-05-23 21:56:28,820 epoch 6 - iter 154/777 - loss 0.19227821 - samples/sec: 16.26 - lr: 0.000003
2023-05-23 21:56:47,866 epoch 6 - iter 231/777 - loss 0.19942950 - samples/sec: 16.18 - lr: 0.000003
2023-05-23 21:57:06,771 epoch 6 - iter 308/777 - loss 0.20613939 - samples/sec: 16.30 - lr: 0.000003
2023-05-23 21:57:25,848 epoch 6 - iter 385/777 - loss 0.20872431 - samples/sec: 16.16 - lr: 0.000003
2023-05-23 21:57:45,359 epoch 6 - iter 462/777 - loss 0.20437245 - samples/sec: 15.80 - lr: 0.000002
2023-05-23 21:58:04,963 epoch 6 - iter 539/777 - loss 0.20537728 - samples/sec: 15.72 - lr: 0.000002
2023-05-23 21:58:25,404 epoch 6 - iter 616/777 - loss 0.20780018 - samples/sec: 15.08 - lr: 0.000002
2023-05-23 21:58:44,292 epoch 6 - iter 693/777 - loss 0.20779097 - samples/sec: 16.32 - lr: 0.000002
2023-05-23 21:59:04,078 epoch 6 - iter 770/777 - loss 0.20611251 - samples/sec: 15.58 - lr: 0.000002
2023-05-23 21:59:05,804 ----------------------------------------------------------------------------------------------------
2023-05-23 21:59:05,804 EPOCH 6 done: loss 0.2062 - lr 0.000002
2023-05-23 21:59:22,422 Evaluating as a multi-label problem: False
2023-05-23 21:59:22,450 DEV : loss 0.09719356149435043 - f1-score (micro avg)  0.9234
2023-05-23 21:59:22,481 BAD EPOCHS (no improvement): 4
2023-05-23 21:59:22,483 ----------------------------------------------------------------------------------------------------
2023-05-23 21:59:41,821 epoch 7 - iter 77/777 - loss 0.18651607 - samples/sec: 15.94 - lr: 0.000002
2023-05-23 22:00:00,595 epoch 7 - iter 154/777 - loss 0.20414720 - samples/sec: 16.42 - lr: 0.000002
2023-05-23 22:00:20,061 epoch 7 - iter 231/777 - loss 0.19507355 - samples/sec: 15.83 - lr: 0.000002
2023-05-23 22:00:39,585 epoch 7 - iter 308/777 - loss 0.19829772 - samples/sec: 15.79 - lr: 0.000002
2023-05-23 22:00:58,882 epoch 7 - iter 385/777 - loss 0.19523606 - samples/sec: 15.97 - lr: 0.000002
2023-05-23 22:01:17,848 epoch 7 - iter 462/777 - loss 0.19408550 - samples/sec: 16.25 - lr: 0.000002
2023-05-23 22:01:36,945 epoch 7 - iter 539/777 - loss 0.19211402 - samples/sec: 16.14 - lr: 0.000002
2023-05-23 22:01:56,008 epoch 7 - iter 616/777 - loss 0.19173333 - samples/sec: 16.17 - lr: 0.000002
2023-05-23 22:02:15,312 epoch 7 - iter 693/777 - loss 0.18946269 - samples/sec: 15.97 - lr: 0.000002
2023-05-23 22:02:34,731 epoch 7 - iter 770/777 - loss 0.19011038 - samples/sec: 15.87 - lr: 0.000002
2023-05-23 22:02:36,518 ----------------------------------------------------------------------------------------------------
2023-05-23 22:02:36,518 EPOCH 7 done: loss 0.1900 - lr 0.000002
2023-05-23 22:02:53,461 Evaluating as a multi-label problem: False
2023-05-23 22:02:53,483 DEV : loss 0.09728363156318665 - f1-score (micro avg)  0.9269
2023-05-23 22:02:53,510 BAD EPOCHS (no improvement): 4
2023-05-23 22:02:53,512 ----------------------------------------------------------------------------------------------------
2023-05-23 22:03:13,304 epoch 8 - iter 77/777 - loss 0.17679302 - samples/sec: 15.58 - lr: 0.000002
2023-05-23 22:03:32,079 epoch 8 - iter 154/777 - loss 0.18037189 - samples/sec: 16.42 - lr: 0.000002
2023-05-23 22:03:51,383 epoch 8 - iter 231/777 - loss 0.17999934 - samples/sec: 15.97 - lr: 0.000002
2023-05-23 22:04:10,686 epoch 8 - iter 308/777 - loss 0.18479853 - samples/sec: 15.97 - lr: 0.000001
2023-05-23 22:04:29,402 epoch 8 - iter 385/777 - loss 0.18395871 - samples/sec: 16.47 - lr: 0.000001
2023-05-23 22:04:49,338 epoch 8 - iter 462/777 - loss 0.18653056 - samples/sec: 15.46 - lr: 0.000001
2023-05-23 22:05:15,993 epoch 8 - iter 539/777 - loss 0.18434512 - samples/sec: 11.56 - lr: 0.000001
2023-05-23 22:05:35,109 epoch 8 - iter 616/777 - loss 0.18635522 - samples/sec: 16.12 - lr: 0.000001
2023-05-23 22:05:53,939 epoch 8 - iter 693/777 - loss 0.18852045 - samples/sec: 16.37 - lr: 0.000001
2023-05-23 22:06:12,462 epoch 8 - iter 770/777 - loss 0.18806304 - samples/sec: 16.64 - lr: 0.000001
2023-05-23 22:06:14,122 ----------------------------------------------------------------------------------------------------
2023-05-23 22:06:14,123 EPOCH 8 done: loss 0.1884 - lr 0.000001
2023-05-23 22:06:30,441 Evaluating as a multi-label problem: False
2023-05-23 22:06:30,460 DEV : loss 0.09388348460197449 - f1-score (micro avg)  0.9235
2023-05-23 22:06:30,483 BAD EPOCHS (no improvement): 4
2023-05-23 22:06:30,486 ----------------------------------------------------------------------------------------------------
2023-05-23 22:06:49,362 epoch 9 - iter 77/777 - loss 0.17226144 - samples/sec: 16.33 - lr: 0.000001
2023-05-23 22:07:08,433 epoch 9 - iter 154/777 - loss 0.18373209 - samples/sec: 16.16 - lr: 0.000001
2023-05-23 22:07:27,057 epoch 9 - iter 231/777 - loss 0.18654455 - samples/sec: 16.55 - lr: 0.000001
2023-05-23 22:07:45,688 epoch 9 - iter 308/777 - loss 0.18587316 - samples/sec: 16.54 - lr: 0.000001
2023-05-23 22:08:04,387 epoch 9 - iter 385/777 - loss 0.17962651 - samples/sec: 16.48 - lr: 0.000001
2023-05-23 22:08:23,214 epoch 9 - iter 462/777 - loss 0.18244370 - samples/sec: 16.37 - lr: 0.000001
2023-05-23 22:08:42,136 epoch 9 - iter 539/777 - loss 0.18316439 - samples/sec: 16.29 - lr: 0.000001
2023-05-23 22:09:00,699 epoch 9 - iter 616/777 - loss 0.18115255 - samples/sec: 16.60 - lr: 0.000001
2023-05-23 22:09:19,537 epoch 9 - iter 693/777 - loss 0.17862205 - samples/sec: 16.36 - lr: 0.000001
2023-05-23 22:09:37,942 epoch 9 - iter 770/777 - loss 0.17938417 - samples/sec: 16.75 - lr: 0.000001
2023-05-23 22:09:39,531 ----------------------------------------------------------------------------------------------------
2023-05-23 22:09:39,531 EPOCH 9 done: loss 0.1797 - lr 0.000001
2023-05-23 22:09:55,272 Evaluating as a multi-label problem: False
2023-05-23 22:09:55,288 DEV : loss 0.09780493378639221 - f1-score (micro avg)  0.9278
2023-05-23 22:09:55,307 BAD EPOCHS (no improvement): 4
2023-05-23 22:09:55,310 ----------------------------------------------------------------------------------------------------
2023-05-23 22:10:14,026 epoch 10 - iter 77/777 - loss 0.19941541 - samples/sec: 16.47 - lr: 0.000001
2023-05-23 22:10:32,769 epoch 10 - iter 154/777 - loss 0.18917973 - samples/sec: 16.44 - lr: 0.000000
2023-05-23 22:10:51,335 epoch 10 - iter 231/777 - loss 0.19477119 - samples/sec: 16.60 - lr: 0.000000
2023-05-23 22:11:10,622 epoch 10 - iter 308/777 - loss 0.18710509 - samples/sec: 15.98 - lr: 0.000000
2023-05-23 22:11:29,540 epoch 10 - iter 385/777 - loss 0.18756014 - samples/sec: 16.29 - lr: 0.000000
2023-05-23 22:11:47,910 epoch 10 - iter 462/777 - loss 0.18865397 - samples/sec: 16.78 - lr: 0.000000
2023-05-23 22:12:06,848 epoch 10 - iter 539/777 - loss 0.18461331 - samples/sec: 16.27 - lr: 0.000000
2023-05-23 22:12:25,313 epoch 10 - iter 616/777 - loss 0.18607024 - samples/sec: 16.69 - lr: 0.000000
2023-05-23 22:12:44,131 epoch 10 - iter 693/777 - loss 0.18671090 - samples/sec: 16.38 - lr: 0.000000
2023-05-23 22:13:02,869 epoch 10 - iter 770/777 - loss 0.18652985 - samples/sec: 16.45 - lr: 0.000000
2023-05-23 22:13:04,536 ----------------------------------------------------------------------------------------------------
2023-05-23 22:13:04,537 EPOCH 10 done: loss 0.1877 - lr 0.000000
2023-05-23 22:13:20,965 Evaluating as a multi-label problem: False
2023-05-23 22:13:20,980 DEV : loss 0.09646110981702805 - f1-score (micro avg)  0.9223
2023-05-23 22:13:21,001 BAD EPOCHS (no improvement): 4
2023-05-23 22:13:33,587 ----------------------------------------------------------------------------------------------------
2023-05-23 22:13:33,589 Testing using last state of model ...
2023-05-23 22:13:56,494 Evaluating as a multi-label problem: False
2023-05-23 22:13:56,517 0.913	0.9012	0.9071	0.8575
2023-05-23 22:13:56,517 
Results:
- F-score (micro) 0.9071
- F-score (macro) 0.896
- Accuracy 0.8575

By class:
              precision    recall  f1-score   support

         PER     0.9739    0.9739    0.9739       306
         LOC     0.9336    0.9192    0.9264       260
         ORG     0.8911    0.8701    0.8805       254
        MISC     0.8101    0.7967    0.8033       182

   micro avg     0.9130    0.9012    0.9071      1002
   macro avg     0.9022    0.8900    0.8960      1002
weighted avg     0.9127    0.9012    0.9069      1002

2023-05-23 22:13:56,517 ----------------------------------------------------------------------------------------------------
2023-05-23 22:13:56,517 ----------------------------------------------------------------------------------------------------
2023-05-23 22:16:22,551 Evaluating as a multi-label problem: False
2023-05-23 22:16:22,598 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-23 22:16:22,598 0.8943	0.8653	0.8795	0.8389
2023-05-23 22:16:22,599 ----------------------------------------------------------------------------------------------------
2023-05-23 22:17:54,902 Evaluating as a multi-label problem: False
2023-05-23 22:17:54,957 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-23 22:17:54,958 0.8966	0.9058	0.9012	0.8476
