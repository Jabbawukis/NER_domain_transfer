2023-05-25 09:25:51,063 ----------------------------------------------------------------------------------------------------
2023-05-25 09:25:51,068 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-25 09:25:51,076 ----------------------------------------------------------------------------------------------------
2023-05-25 09:25:51,077 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-25 09:25:51,077 ----------------------------------------------------------------------------------------------------
2023-05-25 09:25:51,077 Parameters:
2023-05-25 09:25:51,077  - learning_rate: "0.000005"
2023-05-25 09:25:51,077  - mini_batch_size: "4"
2023-05-25 09:25:51,077  - patience: "3"
2023-05-25 09:25:51,077  - anneal_factor: "0.5"
2023-05-25 09:25:51,077  - max_epochs: "10"
2023-05-25 09:25:51,077  - shuffle: "True"
2023-05-25 09:25:51,078  - train_with_dev: "False"
2023-05-25 09:25:51,078  - batch_growth_annealing: "False"
2023-05-25 09:25:51,078 ----------------------------------------------------------------------------------------------------
2023-05-25 09:25:51,078 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_1"
2023-05-25 09:25:51,078 ----------------------------------------------------------------------------------------------------
2023-05-25 09:25:51,078 Device: cuda:0
2023-05-25 09:25:51,078 ----------------------------------------------------------------------------------------------------
2023-05-25 09:25:51,078 Embeddings storage mode: none
2023-05-25 09:25:51,078 ----------------------------------------------------------------------------------------------------
2023-05-25 09:29:04,843 epoch 1 - iter 777/7770 - loss 2.75444350 - samples/sec: 16.05 - lr: 0.000001
2023-05-25 09:32:17,513 epoch 1 - iter 1554/7770 - loss 1.64572946 - samples/sec: 16.14 - lr: 0.000001
2023-05-25 09:35:31,476 epoch 1 - iter 2331/7770 - loss 1.29107295 - samples/sec: 16.03 - lr: 0.000002
2023-05-25 09:38:48,204 epoch 1 - iter 3108/7770 - loss 1.04599304 - samples/sec: 15.81 - lr: 0.000002
2023-05-25 09:42:12,111 epoch 1 - iter 3885/7770 - loss 0.88728838 - samples/sec: 15.25 - lr: 0.000003
2023-05-25 09:45:21,806 epoch 1 - iter 4662/7770 - loss 0.78991455 - samples/sec: 16.39 - lr: 0.000003
2023-05-25 09:48:27,513 epoch 1 - iter 5439/7770 - loss 0.72288775 - samples/sec: 16.74 - lr: 0.000003
2023-05-25 09:51:37,135 epoch 1 - iter 6216/7770 - loss 0.66293414 - samples/sec: 16.40 - lr: 0.000004
2023-05-25 09:54:46,728 epoch 1 - iter 6993/7770 - loss 0.61479125 - samples/sec: 16.40 - lr: 0.000005
2023-05-25 09:57:57,156 epoch 1 - iter 7770/7770 - loss 0.57096018 - samples/sec: 16.33 - lr: 0.000005
2023-05-25 09:57:57,159 ----------------------------------------------------------------------------------------------------
2023-05-25 09:57:57,159 EPOCH 1 done: loss 0.5710 - lr 0.000005
2023-05-25 10:00:46,951 Evaluating as a multi-label problem: False
2023-05-25 10:00:47,057 DEV : loss 0.11507590860128403 - f1-score (micro avg)  0.9166
2023-05-25 10:00:47,259 BAD EPOCHS (no improvement): 4
2023-05-25 10:00:47,262 ----------------------------------------------------------------------------------------------------
2023-05-25 10:04:02,051 epoch 2 - iter 777/7770 - loss 0.24034316 - samples/sec: 15.96 - lr: 0.000005
2023-05-25 10:07:13,504 epoch 2 - iter 1554/7770 - loss 0.23927970 - samples/sec: 16.24 - lr: 0.000005
2023-05-25 10:10:27,590 epoch 2 - iter 2331/7770 - loss 0.23848288 - samples/sec: 16.02 - lr: 0.000005
2023-05-25 10:13:48,425 epoch 2 - iter 3108/7770 - loss 0.23254262 - samples/sec: 15.48 - lr: 0.000005
2023-05-25 10:17:04,446 epoch 2 - iter 3885/7770 - loss 0.23178217 - samples/sec: 15.86 - lr: 0.000005
2023-05-25 10:20:18,522 epoch 2 - iter 4662/7770 - loss 0.22935282 - samples/sec: 16.02 - lr: 0.000005
2023-05-25 10:23:36,494 epoch 2 - iter 5439/7770 - loss 0.22661790 - samples/sec: 15.71 - lr: 0.000005
2023-05-25 10:26:50,804 epoch 2 - iter 6216/7770 - loss 0.22515492 - samples/sec: 16.00 - lr: 0.000005
2023-05-25 10:30:06,622 epoch 2 - iter 6993/7770 - loss 0.22411700 - samples/sec: 15.88 - lr: 0.000005
2023-05-25 10:33:20,340 epoch 2 - iter 7770/7770 - loss 0.22260888 - samples/sec: 16.05 - lr: 0.000004
2023-05-25 10:33:20,344 ----------------------------------------------------------------------------------------------------
2023-05-25 10:33:20,344 EPOCH 2 done: loss 0.2226 - lr 0.000004
2023-05-25 10:36:10,429 Evaluating as a multi-label problem: False
2023-05-25 10:36:10,528 DEV : loss 0.07388976216316223 - f1-score (micro avg)  0.9534
2023-05-25 10:36:10,716 BAD EPOCHS (no improvement): 4
2023-05-25 10:36:10,718 ----------------------------------------------------------------------------------------------------
2023-05-25 10:39:26,507 epoch 3 - iter 777/7770 - loss 0.18524633 - samples/sec: 15.88 - lr: 0.000004
2023-05-25 10:42:39,560 epoch 3 - iter 1554/7770 - loss 0.19017376 - samples/sec: 16.11 - lr: 0.000004
2023-05-25 10:45:55,873 epoch 3 - iter 2331/7770 - loss 0.19416501 - samples/sec: 15.84 - lr: 0.000004
2023-05-25 10:49:12,832 epoch 3 - iter 3108/7770 - loss 0.19424837 - samples/sec: 15.79 - lr: 0.000004
2023-05-25 10:52:28,951 epoch 3 - iter 3885/7770 - loss 0.19576141 - samples/sec: 15.86 - lr: 0.000004
2023-05-25 10:55:56,323 epoch 3 - iter 4662/7770 - loss 0.19497512 - samples/sec: 15.00 - lr: 0.000004
2023-05-25 10:59:12,699 epoch 3 - iter 5439/7770 - loss 0.19432628 - samples/sec: 15.84 - lr: 0.000004
2023-05-25 11:02:27,233 epoch 3 - iter 6216/7770 - loss 0.19460648 - samples/sec: 15.99 - lr: 0.000004
2023-05-25 11:05:43,189 epoch 3 - iter 6993/7770 - loss 0.19404656 - samples/sec: 15.87 - lr: 0.000004
2023-05-25 11:09:01,293 epoch 3 - iter 7770/7770 - loss 0.19286268 - samples/sec: 15.70 - lr: 0.000004
2023-05-25 11:09:01,297 ----------------------------------------------------------------------------------------------------
2023-05-25 11:09:01,297 EPOCH 3 done: loss 0.1929 - lr 0.000004
2023-05-25 11:11:54,743 Evaluating as a multi-label problem: False
2023-05-25 11:11:54,851 DEV : loss 0.06280117481946945 - f1-score (micro avg)  0.9586
2023-05-25 11:11:55,070 BAD EPOCHS (no improvement): 4
2023-05-25 11:11:55,074 ----------------------------------------------------------------------------------------------------
2023-05-25 11:15:09,522 epoch 4 - iter 777/7770 - loss 0.17596383 - samples/sec: 15.99 - lr: 0.000004
2023-05-25 11:18:23,514 epoch 4 - iter 1554/7770 - loss 0.17442839 - samples/sec: 16.03 - lr: 0.000004
2023-05-25 11:21:37,049 epoch 4 - iter 2331/7770 - loss 0.17284937 - samples/sec: 16.07 - lr: 0.000004
2023-05-25 11:24:51,711 epoch 4 - iter 3108/7770 - loss 0.17353243 - samples/sec: 15.97 - lr: 0.000004
2023-05-25 11:28:06,420 epoch 4 - iter 3885/7770 - loss 0.17384047 - samples/sec: 15.97 - lr: 0.000004
2023-05-25 11:31:23,266 epoch 4 - iter 4662/7770 - loss 0.17278036 - samples/sec: 15.80 - lr: 0.000004
2023-05-25 11:34:47,387 epoch 4 - iter 5439/7770 - loss 0.17373024 - samples/sec: 15.23 - lr: 0.000004
2023-05-25 11:38:00,159 epoch 4 - iter 6216/7770 - loss 0.17416824 - samples/sec: 16.13 - lr: 0.000003
2023-05-25 11:41:13,008 epoch 4 - iter 6993/7770 - loss 0.17489583 - samples/sec: 16.13 - lr: 0.000003
2023-05-25 11:44:25,060 epoch 4 - iter 7770/7770 - loss 0.17441215 - samples/sec: 16.19 - lr: 0.000003
2023-05-25 11:44:25,064 ----------------------------------------------------------------------------------------------------
2023-05-25 11:44:25,065 EPOCH 4 done: loss 0.1744 - lr 0.000003
2023-05-25 11:47:14,509 Evaluating as a multi-label problem: False
2023-05-25 11:47:14,613 DEV : loss 0.058398548513650894 - f1-score (micro avg)  0.9617
2023-05-25 11:47:14,834 BAD EPOCHS (no improvement): 4
2023-05-25 11:47:14,836 ----------------------------------------------------------------------------------------------------
2023-05-25 11:50:29,669 epoch 5 - iter 777/7770 - loss 0.16890108 - samples/sec: 15.96 - lr: 0.000003
2023-05-25 11:53:42,193 epoch 5 - iter 1554/7770 - loss 0.16639541 - samples/sec: 16.15 - lr: 0.000003
2023-05-25 11:56:53,702 epoch 5 - iter 2331/7770 - loss 0.16972082 - samples/sec: 16.24 - lr: 0.000003
2023-05-25 12:00:07,727 epoch 5 - iter 3108/7770 - loss 0.16993327 - samples/sec: 16.03 - lr: 0.000003
2023-05-25 12:03:22,363 epoch 5 - iter 3885/7770 - loss 0.16841292 - samples/sec: 15.98 - lr: 0.000003
2023-05-25 12:06:36,932 epoch 5 - iter 4662/7770 - loss 0.16905571 - samples/sec: 15.98 - lr: 0.000003
2023-05-25 12:09:53,030 epoch 5 - iter 5439/7770 - loss 0.16735562 - samples/sec: 15.86 - lr: 0.000003
2023-05-25 12:13:09,898 epoch 5 - iter 6216/7770 - loss 0.16706609 - samples/sec: 15.80 - lr: 0.000003
2023-05-25 12:16:34,173 epoch 5 - iter 6993/7770 - loss 0.16734723 - samples/sec: 15.22 - lr: 0.000003
2023-05-25 12:19:48,042 epoch 5 - iter 7770/7770 - loss 0.16623666 - samples/sec: 16.04 - lr: 0.000003
2023-05-25 12:19:48,045 ----------------------------------------------------------------------------------------------------
2023-05-25 12:19:48,046 EPOCH 5 done: loss 0.1662 - lr 0.000003
2023-05-25 12:22:36,169 Evaluating as a multi-label problem: False
2023-05-25 12:22:36,286 DEV : loss 0.06339645385742188 - f1-score (micro avg)  0.9633
2023-05-25 12:22:36,468 BAD EPOCHS (no improvement): 4
2023-05-25 12:22:36,471 ----------------------------------------------------------------------------------------------------
2023-05-25 12:25:50,111 epoch 6 - iter 777/7770 - loss 0.15953912 - samples/sec: 16.06 - lr: 0.000003
2023-05-25 12:29:05,100 epoch 6 - iter 1554/7770 - loss 0.15836118 - samples/sec: 15.95 - lr: 0.000003
2023-05-25 12:32:19,136 epoch 6 - iter 2331/7770 - loss 0.15784358 - samples/sec: 16.03 - lr: 0.000003
2023-05-25 12:35:33,237 epoch 6 - iter 3108/7770 - loss 0.15745048 - samples/sec: 16.02 - lr: 0.000003
2023-05-25 12:38:48,815 epoch 6 - iter 3885/7770 - loss 0.15695491 - samples/sec: 15.90 - lr: 0.000003
2023-05-25 12:42:03,790 epoch 6 - iter 4662/7770 - loss 0.15722899 - samples/sec: 15.95 - lr: 0.000002
2023-05-25 12:45:18,439 epoch 6 - iter 5439/7770 - loss 0.15740570 - samples/sec: 15.98 - lr: 0.000002
2023-05-25 12:48:35,491 epoch 6 - iter 6216/7770 - loss 0.15713102 - samples/sec: 15.78 - lr: 0.000002
2023-05-25 12:51:53,628 epoch 6 - iter 6993/7770 - loss 0.15691442 - samples/sec: 15.69 - lr: 0.000002
2023-05-25 12:55:18,933 epoch 6 - iter 7770/7770 - loss 0.15639488 - samples/sec: 15.15 - lr: 0.000002
2023-05-25 12:55:18,938 ----------------------------------------------------------------------------------------------------
2023-05-25 12:55:18,938 EPOCH 6 done: loss 0.1564 - lr 0.000002
2023-05-25 12:58:02,532 Evaluating as a multi-label problem: False
2023-05-25 12:58:02,628 DEV : loss 0.06291868537664413 - f1-score (micro avg)  0.9635
2023-05-25 12:58:02,853 BAD EPOCHS (no improvement): 4
2023-05-25 12:58:02,856 ----------------------------------------------------------------------------------------------------
2023-05-25 13:01:23,566 epoch 7 - iter 777/7770 - loss 0.14749881 - samples/sec: 15.49 - lr: 0.000002
2023-05-25 13:04:37,193 epoch 7 - iter 1554/7770 - loss 0.15108185 - samples/sec: 16.06 - lr: 0.000002
2023-05-25 13:07:50,239 epoch 7 - iter 2331/7770 - loss 0.15026696 - samples/sec: 16.11 - lr: 0.000002
2023-05-25 13:11:03,940 epoch 7 - iter 3108/7770 - loss 0.15110276 - samples/sec: 16.05 - lr: 0.000002
2023-05-25 13:14:18,548 epoch 7 - iter 3885/7770 - loss 0.15203028 - samples/sec: 15.98 - lr: 0.000002
2023-05-25 13:17:33,122 epoch 7 - iter 4662/7770 - loss 0.15213853 - samples/sec: 15.98 - lr: 0.000002
2023-05-25 13:20:46,259 epoch 7 - iter 5439/7770 - loss 0.15226800 - samples/sec: 16.10 - lr: 0.000002
2023-05-25 13:24:01,949 epoch 7 - iter 6216/7770 - loss 0.15200545 - samples/sec: 15.89 - lr: 0.000002
2023-05-25 13:27:16,130 epoch 7 - iter 6993/7770 - loss 0.15191811 - samples/sec: 16.01 - lr: 0.000002
2023-05-25 13:30:31,376 epoch 7 - iter 7770/7770 - loss 0.15180032 - samples/sec: 15.93 - lr: 0.000002
2023-05-25 13:30:31,380 ----------------------------------------------------------------------------------------------------
2023-05-25 13:30:31,380 EPOCH 7 done: loss 0.1518 - lr 0.000002
2023-05-25 13:33:27,690 Evaluating as a multi-label problem: False
2023-05-25 13:33:27,783 DEV : loss 0.06545940041542053 - f1-score (micro avg)  0.9629
2023-05-25 13:33:27,991 BAD EPOCHS (no improvement): 4
2023-05-25 13:33:27,999 ----------------------------------------------------------------------------------------------------
2023-05-25 13:36:43,094 epoch 8 - iter 777/7770 - loss 0.14781006 - samples/sec: 15.94 - lr: 0.000002
2023-05-25 13:40:03,993 epoch 8 - iter 1554/7770 - loss 0.14805750 - samples/sec: 15.48 - lr: 0.000002
2023-05-25 13:43:18,369 epoch 8 - iter 2331/7770 - loss 0.14714375 - samples/sec: 16.00 - lr: 0.000002
2023-05-25 13:46:32,506 epoch 8 - iter 3108/7770 - loss 0.14666256 - samples/sec: 16.02 - lr: 0.000001
2023-05-25 13:49:46,047 epoch 8 - iter 3885/7770 - loss 0.14618989 - samples/sec: 16.07 - lr: 0.000001
2023-05-25 13:52:57,967 epoch 8 - iter 4662/7770 - loss 0.14571365 - samples/sec: 16.20 - lr: 0.000001
2023-05-25 13:56:11,994 epoch 8 - iter 5439/7770 - loss 0.14512964 - samples/sec: 16.03 - lr: 0.000001
2023-05-25 13:59:26,243 epoch 8 - iter 6216/7770 - loss 0.14521830 - samples/sec: 16.01 - lr: 0.000001
2023-05-25 14:02:39,320 epoch 8 - iter 6993/7770 - loss 0.14501904 - samples/sec: 16.11 - lr: 0.000001
2023-05-25 14:05:53,889 epoch 8 - iter 7770/7770 - loss 0.14513655 - samples/sec: 15.98 - lr: 0.000001
2023-05-25 14:05:53,893 ----------------------------------------------------------------------------------------------------
2023-05-25 14:05:53,893 EPOCH 8 done: loss 0.1451 - lr 0.000001
2023-05-25 14:08:50,285 Evaluating as a multi-label problem: False
2023-05-25 14:08:50,384 DEV : loss 0.06366586685180664 - f1-score (micro avg)  0.9646
2023-05-25 14:08:50,621 BAD EPOCHS (no improvement): 4
2023-05-25 14:08:50,625 ----------------------------------------------------------------------------------------------------
2023-05-25 14:12:05,780 epoch 9 - iter 777/7770 - loss 0.14293335 - samples/sec: 15.93 - lr: 0.000001
2023-05-25 14:15:19,819 epoch 9 - iter 1554/7770 - loss 0.14317751 - samples/sec: 16.03 - lr: 0.000001
2023-05-25 14:18:39,493 epoch 9 - iter 2331/7770 - loss 0.14215656 - samples/sec: 15.57 - lr: 0.000001
2023-05-25 14:21:52,472 epoch 9 - iter 3108/7770 - loss 0.14164184 - samples/sec: 16.11 - lr: 0.000001
2023-05-25 14:25:04,224 epoch 9 - iter 3885/7770 - loss 0.14252336 - samples/sec: 16.22 - lr: 0.000001
2023-05-25 14:28:15,661 epoch 9 - iter 4662/7770 - loss 0.14236875 - samples/sec: 16.24 - lr: 0.000001
2023-05-25 14:31:29,933 epoch 9 - iter 5439/7770 - loss 0.14220197 - samples/sec: 16.01 - lr: 0.000001
2023-05-25 14:34:41,845 epoch 9 - iter 6216/7770 - loss 0.14269390 - samples/sec: 16.20 - lr: 0.000001
2023-05-25 14:37:55,777 epoch 9 - iter 6993/7770 - loss 0.14275168 - samples/sec: 16.03 - lr: 0.000001
2023-05-25 14:41:07,749 epoch 9 - iter 7770/7770 - loss 0.14337004 - samples/sec: 16.20 - lr: 0.000001
2023-05-25 14:41:07,754 ----------------------------------------------------------------------------------------------------
2023-05-25 14:41:07,754 EPOCH 9 done: loss 0.1434 - lr 0.000001
2023-05-25 14:44:02,746 Evaluating as a multi-label problem: False
2023-05-25 14:44:02,853 DEV : loss 0.06493140012025833 - f1-score (micro avg)  0.9657
2023-05-25 14:44:03,095 BAD EPOCHS (no improvement): 4
2023-05-25 14:44:03,098 ----------------------------------------------------------------------------------------------------
2023-05-25 14:47:18,406 epoch 10 - iter 777/7770 - loss 0.14203164 - samples/sec: 15.92 - lr: 0.000001
2023-05-25 14:50:31,878 epoch 10 - iter 1554/7770 - loss 0.14179441 - samples/sec: 16.07 - lr: 0.000000
2023-05-25 14:53:44,733 epoch 10 - iter 2331/7770 - loss 0.14168742 - samples/sec: 16.12 - lr: 0.000000
2023-05-25 14:56:58,262 epoch 10 - iter 3108/7770 - loss 0.14217122 - samples/sec: 16.07 - lr: 0.000000
2023-05-25 15:00:17,502 epoch 10 - iter 3885/7770 - loss 0.14129200 - samples/sec: 15.61 - lr: 0.000000
2023-05-25 15:03:29,758 epoch 10 - iter 4662/7770 - loss 0.14150733 - samples/sec: 16.17 - lr: 0.000000
2023-05-25 15:06:41,957 epoch 10 - iter 5439/7770 - loss 0.14047584 - samples/sec: 16.18 - lr: 0.000000
2023-05-25 15:09:55,220 epoch 10 - iter 6216/7770 - loss 0.14051767 - samples/sec: 16.09 - lr: 0.000000
2023-05-25 15:13:08,034 epoch 10 - iter 6993/7770 - loss 0.14069167 - samples/sec: 16.13 - lr: 0.000000
2023-05-25 15:16:21,548 epoch 10 - iter 7770/7770 - loss 0.14022250 - samples/sec: 16.07 - lr: 0.000000
2023-05-25 15:16:21,552 ----------------------------------------------------------------------------------------------------
2023-05-25 15:16:21,553 EPOCH 10 done: loss 0.1402 - lr 0.000000
2023-05-25 15:19:14,556 Evaluating as a multi-label problem: False
2023-05-25 15:19:14,660 DEV : loss 0.06631673127412796 - f1-score (micro avg)  0.9654
2023-05-25 15:19:14,912 BAD EPOCHS (no improvement): 4
2023-05-25 15:19:29,240 ----------------------------------------------------------------------------------------------------
2023-05-25 15:19:29,244 Testing using last state of model ...
2023-05-25 15:23:57,051 Evaluating as a multi-label problem: False
2023-05-25 15:23:57,161 0.9352	0.9418	0.9385	0.9132
2023-05-25 15:23:57,162 
Results:
- F-score (micro) 0.9385
- F-score (macro) 0.9353
- Accuracy 0.9132

By class:
              precision    recall  f1-score   support

         PER     0.9747    0.9786    0.9767      2715
         ORG     0.9044    0.9410    0.9223      2543
         LOC     0.9541    0.9373    0.9457      2442
        MISC     0.8971    0.8957    0.8964      1889

   micro avg     0.9352    0.9418    0.9385      9589
   macro avg     0.9326    0.9382    0.9353      9589
weighted avg     0.9355    0.9418    0.9386      9589

2023-05-25 15:23:57,162 ----------------------------------------------------------------------------------------------------
2023-05-25 15:23:57,162 ----------------------------------------------------------------------------------------------------
2023-05-25 15:26:44,173 Evaluating as a multi-label problem: False
2023-05-25 15:26:44,224 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-25 15:26:44,225 0.9402	0.9373	0.9388	0.9207
2023-05-25 15:26:44,225 ----------------------------------------------------------------------------------------------------
2023-05-25 15:28:26,601 Evaluating as a multi-label problem: False
2023-05-25 15:28:26,680 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-25 15:28:26,681 0.9317	0.9449	0.9383	0.9081
