2023-05-25 21:58:28,310 ----------------------------------------------------------------------------------------------------
2023-05-25 21:58:28,322 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=17, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2023-05-25 21:58:28,323 ----------------------------------------------------------------------------------------------------
2023-05-25 21:58:28,324 Corpus: "MultiCorpus: 31080 train + 6435 dev + 8998 test sentences
 - CONLL_03_DUTCH Corpus: 16093 train + 2969 dev + 5314 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
 - CONLL_03 Corpus: 14987 train + 3466 dev + 3684 test sentences - /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03"
2023-05-25 21:58:28,325 ----------------------------------------------------------------------------------------------------
2023-05-25 21:58:28,325 Parameters:
2023-05-25 21:58:28,325  - learning_rate: "0.000005"
2023-05-25 21:58:28,325  - mini_batch_size: "4"
2023-05-25 21:58:28,325  - patience: "3"
2023-05-25 21:58:28,325  - anneal_factor: "0.5"
2023-05-25 21:58:28,325  - max_epochs: "10"
2023-05-25 21:58:28,325  - shuffle: "True"
2023-05-25 21:58:28,325  - train_with_dev: "False"
2023-05-25 21:58:28,325  - batch_growth_annealing: "False"
2023-05-25 21:58:28,325 ----------------------------------------------------------------------------------------------------
2023-05-25 21:58:28,325 Model training base path: "resources/taggers/conll_eng_dutch_ner_roberta_large_run_3"
2023-05-25 21:58:28,326 ----------------------------------------------------------------------------------------------------
2023-05-25 21:58:28,326 Device: cuda:0
2023-05-25 21:58:28,326 ----------------------------------------------------------------------------------------------------
2023-05-25 21:58:28,326 Embeddings storage mode: none
2023-05-25 21:58:28,326 ----------------------------------------------------------------------------------------------------
2023-05-25 22:01:52,219 epoch 1 - iter 777/7770 - loss 1.93173035 - samples/sec: 15.25 - lr: 0.000001
2023-05-25 22:05:14,282 epoch 1 - iter 1554/7770 - loss 1.09769012 - samples/sec: 15.39 - lr: 0.000001
2023-05-25 22:08:44,001 epoch 1 - iter 2331/7770 - loss 0.81354384 - samples/sec: 14.83 - lr: 0.000002
2023-05-25 22:12:05,283 epoch 1 - iter 3108/7770 - loss 0.64409295 - samples/sec: 15.45 - lr: 0.000002
2023-05-25 22:15:27,878 epoch 1 - iter 3885/7770 - loss 0.53773331 - samples/sec: 15.35 - lr: 0.000003
2023-05-25 22:18:43,272 epoch 1 - iter 4662/7770 - loss 0.46965011 - samples/sec: 15.91 - lr: 0.000003
2023-05-25 22:21:50,993 epoch 1 - iter 5439/7770 - loss 0.42756102 - samples/sec: 16.56 - lr: 0.000003
2023-05-25 22:25:02,041 epoch 1 - iter 6216/7770 - loss 0.39028760 - samples/sec: 16.28 - lr: 0.000004
2023-05-25 22:28:14,054 epoch 1 - iter 6993/7770 - loss 0.36186725 - samples/sec: 16.19 - lr: 0.000005
2023-05-25 22:31:25,500 epoch 1 - iter 7770/7770 - loss 0.33674777 - samples/sec: 16.24 - lr: 0.000005
2023-05-25 22:31:25,503 ----------------------------------------------------------------------------------------------------
2023-05-25 22:31:25,504 EPOCH 1 done: loss 0.3367 - lr 0.000005
2023-05-25 22:34:35,575 Evaluating as a multi-label problem: False
2023-05-25 22:34:35,676 DEV : loss 0.08852541446685791 - f1-score (micro avg)  0.96
2023-05-25 22:34:35,946 BAD EPOCHS (no improvement): 4
2023-05-25 22:34:35,948 ----------------------------------------------------------------------------------------------------
2023-05-25 22:37:56,051 epoch 2 - iter 777/7770 - loss 0.14835408 - samples/sec: 15.54 - lr: 0.000005
2023-05-25 22:41:15,090 epoch 2 - iter 1554/7770 - loss 0.14700334 - samples/sec: 15.62 - lr: 0.000005
2023-05-25 22:44:33,913 epoch 2 - iter 2331/7770 - loss 0.14866737 - samples/sec: 15.64 - lr: 0.000005
2023-05-25 22:47:50,869 epoch 2 - iter 3108/7770 - loss 0.14915881 - samples/sec: 15.79 - lr: 0.000005
2023-05-25 22:51:06,423 epoch 2 - iter 3885/7770 - loss 0.14994000 - samples/sec: 15.90 - lr: 0.000005
2023-05-25 22:54:22,844 epoch 2 - iter 4662/7770 - loss 0.14940503 - samples/sec: 15.83 - lr: 0.000005
2023-05-25 22:57:48,395 epoch 2 - iter 5439/7770 - loss 0.14796837 - samples/sec: 15.13 - lr: 0.000005
2023-05-25 23:01:08,568 epoch 2 - iter 6216/7770 - loss 0.14721988 - samples/sec: 15.53 - lr: 0.000005
2023-05-25 23:04:26,278 epoch 2 - iter 6993/7770 - loss 0.14763735 - samples/sec: 15.73 - lr: 0.000005
2023-05-25 23:07:43,391 epoch 2 - iter 7770/7770 - loss 0.14785949 - samples/sec: 15.78 - lr: 0.000004
2023-05-25 23:07:43,395 ----------------------------------------------------------------------------------------------------
2023-05-25 23:07:43,395 EPOCH 2 done: loss 0.1479 - lr 0.000004
2023-05-25 23:10:47,845 Evaluating as a multi-label problem: False
2023-05-25 23:10:47,953 DEV : loss 0.08876064419746399 - f1-score (micro avg)  0.9582
2023-05-25 23:10:48,225 BAD EPOCHS (no improvement): 4
2023-05-25 23:10:48,234 ----------------------------------------------------------------------------------------------------
2023-05-25 23:14:08,558 epoch 3 - iter 777/7770 - loss 0.14598527 - samples/sec: 15.52 - lr: 0.000004
2023-05-25 23:17:26,085 epoch 3 - iter 1554/7770 - loss 0.14655586 - samples/sec: 15.74 - lr: 0.000004
2023-05-25 23:20:43,315 epoch 3 - iter 2331/7770 - loss 0.14861301 - samples/sec: 15.77 - lr: 0.000004
2023-05-25 23:23:59,752 epoch 3 - iter 3108/7770 - loss 0.14808534 - samples/sec: 15.83 - lr: 0.000004
2023-05-25 23:27:16,578 epoch 3 - iter 3885/7770 - loss 0.14787249 - samples/sec: 15.80 - lr: 0.000004
2023-05-25 23:30:31,283 epoch 3 - iter 4662/7770 - loss 0.14573756 - samples/sec: 15.97 - lr: 0.000004
2023-05-25 23:33:44,969 epoch 3 - iter 5439/7770 - loss 0.14662661 - samples/sec: 16.06 - lr: 0.000004
2023-05-25 23:36:59,422 epoch 3 - iter 6216/7770 - loss 0.14610324 - samples/sec: 15.99 - lr: 0.000004
2023-05-25 23:40:13,563 epoch 3 - iter 6993/7770 - loss 0.14569659 - samples/sec: 16.02 - lr: 0.000004
2023-05-25 23:43:26,669 epoch 3 - iter 7770/7770 - loss 0.14569300 - samples/sec: 16.10 - lr: 0.000004
2023-05-25 23:43:26,673 ----------------------------------------------------------------------------------------------------
2023-05-25 23:43:26,673 EPOCH 3 done: loss 0.1457 - lr 0.000004
2023-05-25 23:46:48,807 Evaluating as a multi-label problem: False
2023-05-25 23:46:48,910 DEV : loss 0.09370982646942139 - f1-score (micro avg)  0.9589
2023-05-25 23:46:49,187 BAD EPOCHS (no improvement): 4
2023-05-25 23:46:49,194 ----------------------------------------------------------------------------------------------------
2023-05-25 23:50:09,084 epoch 4 - iter 777/7770 - loss 0.13452645 - samples/sec: 15.56 - lr: 0.000004
2023-05-25 23:53:29,520 epoch 4 - iter 1554/7770 - loss 0.14036717 - samples/sec: 15.51 - lr: 0.000004
2023-05-25 23:56:47,506 epoch 4 - iter 2331/7770 - loss 0.14331971 - samples/sec: 15.71 - lr: 0.000004
2023-05-26 00:00:12,762 epoch 4 - iter 3108/7770 - loss 0.14383987 - samples/sec: 15.15 - lr: 0.000004
2023-05-26 00:03:30,264 epoch 4 - iter 3885/7770 - loss 0.14335955 - samples/sec: 15.75 - lr: 0.000004
2023-05-26 00:06:46,911 epoch 4 - iter 4662/7770 - loss 0.14344178 - samples/sec: 15.81 - lr: 0.000004
2023-05-26 00:10:04,648 epoch 4 - iter 5439/7770 - loss 0.14362538 - samples/sec: 15.73 - lr: 0.000004
2023-05-26 00:13:21,237 epoch 4 - iter 6216/7770 - loss 0.14386869 - samples/sec: 15.82 - lr: 0.000003
2023-05-26 00:16:37,349 epoch 4 - iter 6993/7770 - loss 0.14309856 - samples/sec: 15.86 - lr: 0.000003
2023-05-26 00:19:51,905 epoch 4 - iter 7770/7770 - loss 0.14294769 - samples/sec: 15.98 - lr: 0.000003
2023-05-26 00:19:51,909 ----------------------------------------------------------------------------------------------------
2023-05-26 00:19:51,909 EPOCH 4 done: loss 0.1429 - lr 0.000003
2023-05-26 00:22:59,207 Evaluating as a multi-label problem: False
2023-05-26 00:22:59,318 DEV : loss 0.09892524778842926 - f1-score (micro avg)  0.9583
2023-05-26 00:22:59,607 BAD EPOCHS (no improvement): 4
2023-05-26 00:22:59,611 ----------------------------------------------------------------------------------------------------
2023-05-26 00:26:21,946 epoch 5 - iter 777/7770 - loss 0.13620101 - samples/sec: 15.37 - lr: 0.000003
2023-05-26 00:29:39,699 epoch 5 - iter 1554/7770 - loss 0.13715794 - samples/sec: 15.72 - lr: 0.000003
2023-05-26 00:32:58,438 epoch 5 - iter 2331/7770 - loss 0.13964750 - samples/sec: 15.65 - lr: 0.000003
2023-05-26 00:36:17,070 epoch 5 - iter 3108/7770 - loss 0.13971217 - samples/sec: 15.66 - lr: 0.000003
2023-05-26 00:39:31,854 epoch 5 - iter 3885/7770 - loss 0.13984190 - samples/sec: 15.96 - lr: 0.000003
2023-05-26 00:42:46,580 epoch 5 - iter 4662/7770 - loss 0.13883702 - samples/sec: 15.97 - lr: 0.000003
2023-05-26 00:46:11,107 epoch 5 - iter 5439/7770 - loss 0.13880560 - samples/sec: 15.20 - lr: 0.000003
2023-05-26 00:49:27,933 epoch 5 - iter 6216/7770 - loss 0.13888220 - samples/sec: 15.80 - lr: 0.000003
2023-05-26 00:52:45,199 epoch 5 - iter 6993/7770 - loss 0.13890864 - samples/sec: 15.76 - lr: 0.000003
2023-05-26 00:56:01,666 epoch 5 - iter 7770/7770 - loss 0.13876889 - samples/sec: 15.83 - lr: 0.000003
2023-05-26 00:56:01,670 ----------------------------------------------------------------------------------------------------
2023-05-26 00:56:01,670 EPOCH 5 done: loss 0.1388 - lr 0.000003
2023-05-26 00:59:06,950 Evaluating as a multi-label problem: False
2023-05-26 00:59:07,069 DEV : loss 0.08801161497831345 - f1-score (micro avg)  0.9607
2023-05-26 00:59:07,354 BAD EPOCHS (no improvement): 4
2023-05-26 00:59:07,357 ----------------------------------------------------------------------------------------------------
2023-05-26 01:02:24,222 epoch 6 - iter 777/7770 - loss 0.13627625 - samples/sec: 15.80 - lr: 0.000003
2023-05-26 01:05:41,176 epoch 6 - iter 1554/7770 - loss 0.13422983 - samples/sec: 15.79 - lr: 0.000003
2023-05-26 01:08:59,440 epoch 6 - iter 2331/7770 - loss 0.13465432 - samples/sec: 15.68 - lr: 0.000003
2023-05-26 01:12:16,549 epoch 6 - iter 3108/7770 - loss 0.13597244 - samples/sec: 15.78 - lr: 0.000003
2023-05-26 01:15:31,115 epoch 6 - iter 3885/7770 - loss 0.13651729 - samples/sec: 15.98 - lr: 0.000003
2023-05-26 01:18:46,007 epoch 6 - iter 4662/7770 - loss 0.13588060 - samples/sec: 15.96 - lr: 0.000002
2023-05-26 01:21:53,728 epoch 6 - iter 5439/7770 - loss 0.13655394 - samples/sec: 16.57 - lr: 0.000002
2023-05-26 01:25:02,240 epoch 6 - iter 6216/7770 - loss 0.13717074 - samples/sec: 16.50 - lr: 0.000002
2023-05-26 01:28:06,346 epoch 6 - iter 6993/7770 - loss 0.13671517 - samples/sec: 16.89 - lr: 0.000002
2023-05-26 01:31:10,177 epoch 6 - iter 7770/7770 - loss 0.13689333 - samples/sec: 16.92 - lr: 0.000002
2023-05-26 01:31:10,183 ----------------------------------------------------------------------------------------------------
2023-05-26 01:31:10,183 EPOCH 6 done: loss 0.1369 - lr 0.000002
2023-05-26 01:34:32,277 Evaluating as a multi-label problem: False
2023-05-26 01:34:32,383 DEV : loss 0.08767431229352951 - f1-score (micro avg)  0.9632
2023-05-26 01:34:32,639 BAD EPOCHS (no improvement): 4
2023-05-26 01:34:32,643 ----------------------------------------------------------------------------------------------------
2023-05-26 01:37:39,687 epoch 7 - iter 777/7770 - loss 0.12924721 - samples/sec: 16.63 - lr: 0.000002
2023-05-26 01:40:43,919 epoch 7 - iter 1554/7770 - loss 0.13061852 - samples/sec: 16.88 - lr: 0.000002
2023-05-26 01:43:48,237 epoch 7 - iter 2331/7770 - loss 0.13166192 - samples/sec: 16.87 - lr: 0.000002
2023-05-26 01:47:04,220 epoch 7 - iter 3108/7770 - loss 0.13204519 - samples/sec: 15.87 - lr: 0.000002
2023-05-26 01:50:11,385 epoch 7 - iter 3885/7770 - loss 0.13270418 - samples/sec: 16.61 - lr: 0.000002
2023-05-26 01:53:15,452 epoch 7 - iter 4662/7770 - loss 0.13352966 - samples/sec: 16.89 - lr: 0.000002
2023-05-26 01:56:18,940 epoch 7 - iter 5439/7770 - loss 0.13425890 - samples/sec: 16.95 - lr: 0.000002
2023-05-26 01:59:22,997 epoch 7 - iter 6216/7770 - loss 0.13435456 - samples/sec: 16.89 - lr: 0.000002
2023-05-26 02:02:31,854 epoch 7 - iter 6993/7770 - loss 0.13395488 - samples/sec: 16.47 - lr: 0.000002
2023-05-26 02:05:34,724 epoch 7 - iter 7770/7770 - loss 0.13356445 - samples/sec: 17.00 - lr: 0.000002
2023-05-26 02:05:34,728 ----------------------------------------------------------------------------------------------------
2023-05-26 02:05:34,728 EPOCH 7 done: loss 0.1336 - lr 0.000002
2023-05-26 02:08:26,538 Evaluating as a multi-label problem: False
2023-05-26 02:08:26,632 DEV : loss 0.09484239667654037 - f1-score (micro avg)  0.9617
2023-05-26 02:08:26,901 BAD EPOCHS (no improvement): 4
2023-05-26 02:08:26,904 ----------------------------------------------------------------------------------------------------
2023-05-26 02:11:33,526 epoch 8 - iter 777/7770 - loss 0.12994456 - samples/sec: 16.66 - lr: 0.000002
2023-05-26 02:14:39,178 epoch 8 - iter 1554/7770 - loss 0.13198028 - samples/sec: 16.75 - lr: 0.000002
2023-05-26 02:17:42,856 epoch 8 - iter 2331/7770 - loss 0.13199913 - samples/sec: 16.93 - lr: 0.000002
2023-05-26 02:20:47,915 epoch 8 - iter 3108/7770 - loss 0.13269897 - samples/sec: 16.80 - lr: 0.000001
2023-05-26 02:23:51,937 epoch 8 - iter 3885/7770 - loss 0.13230637 - samples/sec: 16.90 - lr: 0.000001
2023-05-26 02:26:53,449 epoch 8 - iter 4662/7770 - loss 0.13317744 - samples/sec: 17.13 - lr: 0.000001
2023-05-26 02:29:56,669 epoch 8 - iter 5439/7770 - loss 0.13276918 - samples/sec: 16.97 - lr: 0.000001
2023-05-26 02:33:13,840 epoch 8 - iter 6216/7770 - loss 0.13274938 - samples/sec: 15.77 - lr: 0.000001
2023-05-26 02:36:18,870 epoch 8 - iter 6993/7770 - loss 0.13311346 - samples/sec: 16.81 - lr: 0.000001
2023-05-26 02:39:25,618 epoch 8 - iter 7770/7770 - loss 0.13287843 - samples/sec: 16.65 - lr: 0.000001
2023-05-26 02:39:25,622 ----------------------------------------------------------------------------------------------------
2023-05-26 02:39:25,622 EPOCH 8 done: loss 0.1329 - lr 0.000001
2023-05-26 02:42:20,304 Evaluating as a multi-label problem: False
2023-05-26 02:42:20,406 DEV : loss 0.09246755391359329 - f1-score (micro avg)  0.9617
2023-05-26 02:42:20,689 BAD EPOCHS (no improvement): 4
2023-05-26 02:42:20,692 ----------------------------------------------------------------------------------------------------
2023-05-26 02:45:30,785 epoch 9 - iter 777/7770 - loss 0.13078415 - samples/sec: 16.36 - lr: 0.000001
2023-05-26 02:48:35,750 epoch 9 - iter 1554/7770 - loss 0.13130177 - samples/sec: 16.81 - lr: 0.000001
2023-05-26 02:51:41,638 epoch 9 - iter 2331/7770 - loss 0.13161021 - samples/sec: 16.73 - lr: 0.000001
2023-05-26 02:54:47,286 epoch 9 - iter 3108/7770 - loss 0.13229563 - samples/sec: 16.75 - lr: 0.000001
2023-05-26 02:57:50,166 epoch 9 - iter 3885/7770 - loss 0.13303147 - samples/sec: 17.00 - lr: 0.000001
2023-05-26 03:00:53,786 epoch 9 - iter 4662/7770 - loss 0.13321034 - samples/sec: 16.93 - lr: 0.000001
2023-05-26 03:03:57,500 epoch 9 - iter 5439/7770 - loss 0.13268183 - samples/sec: 16.93 - lr: 0.000001
2023-05-26 03:06:59,653 epoch 9 - iter 6216/7770 - loss 0.13257140 - samples/sec: 17.07 - lr: 0.000001
2023-05-26 03:10:02,899 epoch 9 - iter 6993/7770 - loss 0.13209739 - samples/sec: 16.97 - lr: 0.000001
2023-05-26 03:13:05,138 epoch 9 - iter 7770/7770 - loss 0.13185710 - samples/sec: 17.06 - lr: 0.000001
2023-05-26 03:13:05,143 ----------------------------------------------------------------------------------------------------
2023-05-26 03:13:05,143 EPOCH 9 done: loss 0.1319 - lr 0.000001
2023-05-26 03:16:06,678 Evaluating as a multi-label problem: False
2023-05-26 03:16:06,772 DEV : loss 0.09316574782133102 - f1-score (micro avg)  0.9621
2023-05-26 03:16:07,030 BAD EPOCHS (no improvement): 4
2023-05-26 03:16:07,033 ----------------------------------------------------------------------------------------------------
2023-05-26 03:19:15,301 epoch 10 - iter 777/7770 - loss 0.12823494 - samples/sec: 16.52 - lr: 0.000001
2023-05-26 03:22:23,116 epoch 10 - iter 1554/7770 - loss 0.12957271 - samples/sec: 16.56 - lr: 0.000000
2023-05-26 03:25:35,557 epoch 10 - iter 2331/7770 - loss 0.12931173 - samples/sec: 16.16 - lr: 0.000000
2023-05-26 03:28:41,694 epoch 10 - iter 3108/7770 - loss 0.12832364 - samples/sec: 16.71 - lr: 0.000000
2023-05-26 03:31:46,531 epoch 10 - iter 3885/7770 - loss 0.12876797 - samples/sec: 16.82 - lr: 0.000000
2023-05-26 03:34:52,878 epoch 10 - iter 4662/7770 - loss 0.12871078 - samples/sec: 16.69 - lr: 0.000000
2023-05-26 03:37:56,414 epoch 10 - iter 5439/7770 - loss 0.12920554 - samples/sec: 16.94 - lr: 0.000000
2023-05-26 03:41:00,106 epoch 10 - iter 6216/7770 - loss 0.12915586 - samples/sec: 16.93 - lr: 0.000000
2023-05-26 03:44:03,086 epoch 10 - iter 6993/7770 - loss 0.12943241 - samples/sec: 16.99 - lr: 0.000000
2023-05-26 03:47:07,024 epoch 10 - iter 7770/7770 - loss 0.12940556 - samples/sec: 16.91 - lr: 0.000000
2023-05-26 03:47:07,028 ----------------------------------------------------------------------------------------------------
2023-05-26 03:47:07,029 EPOCH 10 done: loss 0.1294 - lr 0.000000
2023-05-26 03:50:02,071 Evaluating as a multi-label problem: False
2023-05-26 03:50:02,176 DEV : loss 0.0935710221529007 - f1-score (micro avg)  0.9624
2023-05-26 03:50:02,457 BAD EPOCHS (no improvement): 4
2023-05-26 03:50:16,206 ----------------------------------------------------------------------------------------------------
2023-05-26 03:50:16,209 Testing using last state of model ...
2023-05-26 03:54:19,929 Evaluating as a multi-label problem: False
2023-05-26 03:54:20,035 0.9313	0.9395	0.9354	0.9086
2023-05-26 03:54:20,036 
Results:
- F-score (micro) 0.9354
- F-score (macro) 0.9324
- Accuracy 0.9086

By class:
              precision    recall  f1-score   support

         PER     0.9757    0.9779    0.9768      2715
         ORG     0.8908    0.9371    0.9134      2543
         LOC     0.9493    0.9353    0.9422      2442
        MISC     0.9012    0.8931    0.8971      1889

   micro avg     0.9313    0.9395    0.9354      9589
   macro avg     0.9293    0.9358    0.9324      9589
weighted avg     0.9318    0.9395    0.9355      9589

2023-05-26 03:54:20,036 ----------------------------------------------------------------------------------------------------
2023-05-26 03:54:20,037 ----------------------------------------------------------------------------------------------------
2023-05-26 03:56:48,775 Evaluating as a multi-label problem: False
2023-05-26 03:56:48,820 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03_dutch
2023-05-26 03:56:48,821 0.9403	0.9394	0.9398	0.9234
2023-05-26 03:56:48,821 ----------------------------------------------------------------------------------------------------
2023-05-26 03:58:22,031 Evaluating as a multi-label problem: False
2023-05-26 03:58:22,105 /vol/fob-vol4/mi17/christod/.flair/datasets/conll_03
2023-05-26 03:58:22,105 0.9252	0.9398	0.9325	0.8987
